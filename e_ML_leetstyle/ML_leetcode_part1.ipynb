{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML LeetCode - Part 1: Linear Algebra and Optimization ðŸ§®\n",
    "\n",
    "Welcome to the ML LeetCode series! This notebook contains algorithmic challenges focused on implementing core linear algebra operations and optimization algorithms from scratch. Each problem follows the LeetCode format with difficulty levels, constraints, and optimal solutions.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Implement fundamental linear algebra operations efficiently\n",
    "- Master matrix operations and decompositions\n",
    "- Build optimization algorithms from first principles\n",
    "- Understand computational complexity and memory optimization\n",
    "\n",
    "## ðŸ“Š Difficulty Levels\n",
    "- ðŸŸ¢ **Easy**: Basic operations, straightforward implementation\n",
    "- ðŸŸ¡ **Medium**: Requires optimization, multiple approaches\n",
    "- ðŸ”´ **Hard**: Complex algorithms, advanced optimization\n",
    "\n",
    "## ðŸš€ Problem Format\n",
    "Each problem includes:\n",
    "- Problem statement with constraints\n",
    "- Examples with expected outputs\n",
    "- Multiple solution approaches\n",
    "- Time/space complexity analysis\n",
    "- Test cases and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import List, Tuple, Optional, Union\n",
    "import math\n",
    "from functools import wraps\n",
    "\n",
    "# Utility functions for testing and timing\n",
    "def time_function(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{func.__name__} took {(end - start) * 1000:.4f} ms\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def test_case(func, inputs, expected, name=\"Test\"):\n",
    "    try:\n",
    "        result = func(*inputs)\n",
    "        if isinstance(expected, (list, np.ndarray)):\n",
    "            passed = np.allclose(result, expected, rtol=1e-6)\n",
    "        else:\n",
    "            passed = abs(result - expected) < 1e-6\n",
    "        status = \"âœ… PASS\" if passed else \"âŒ FAIL\"\n",
    "        print(f\"{status} {name}: {result}\")\n",
    "        return passed\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR {name}: {e}\")\n",
    "        return False\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Matrix Multiplication Optimization ðŸŸ¡\n",
    "\n",
    "**Difficulty**: Medium\n",
    "\n",
    "**Problem**: Implement matrix multiplication with multiple optimization strategies. Compare naive, blocked, and Strassen algorithms.\n",
    "\n",
    "**Constraints**:\n",
    "- Matrix dimensions: 1 â‰¤ n â‰¤ 1000\n",
    "- Elements: -1000 â‰¤ matrix[i][j] â‰¤ 1000\n",
    "- Must handle non-square matrices\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "A = [[1, 2], [3, 4]]\n",
    "B = [[5, 6], [7, 8]]\n",
    "Output: [[19, 22], [43, 50]]\n",
    "```\n",
    "\n",
    "**Follow-up**: Which algorithm is best for different matrix sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixMultiplication:\n",
    "    \n",
    "    def naive_multiply(self, A: List[List[float]], B: List[List[float]]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Naive O(nÂ³) matrix multiplication.\n",
    "        \n",
    "        Time Complexity: O(nÂ³)\n",
    "        Space Complexity: O(nÂ²)\n",
    "        \"\"\"\n",
    "        if not A or not B or len(A[0]) != len(B):\n",
    "            raise ValueError(\"Invalid matrix dimensions for multiplication\")\n",
    "        \n",
    "        rows_A, cols_A = len(A), len(A[0])\n",
    "        rows_B, cols_B = len(B), len(B[0])\n",
    "        \n",
    "        # Initialize result matrix\n",
    "        C = [[0.0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
    "        \n",
    "        # Standard triple loop\n",
    "        for i in range(rows_A):\n",
    "            for j in range(cols_B):\n",
    "                for k in range(cols_A):\n",
    "                    C[i][j] += A[i][k] * B[k][j]\n",
    "        \n",
    "        return C\n",
    "    \n",
    "    def blocked_multiply(self, A: List[List[float]], B: List[List[float]], \n",
    "                        block_size: int = 64) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Cache-friendly blocked matrix multiplication.\n",
    "        \n",
    "        Time Complexity: O(nÂ³)\n",
    "        Space Complexity: O(nÂ²)\n",
    "        Better cache performance for large matrices.\n",
    "        \"\"\"\n",
    "        if not A or not B or len(A[0]) != len(B):\n",
    "            raise ValueError(\"Invalid matrix dimensions for multiplication\")\n",
    "        \n",
    "        rows_A, cols_A = len(A), len(A[0])\n",
    "        rows_B, cols_B = len(B), len(B[0])\n",
    "        \n",
    "        C = [[0.0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
    "        \n",
    "        # Block-wise multiplication\n",
    "        for i0 in range(0, rows_A, block_size):\n",
    "            for j0 in range(0, cols_B, block_size):\n",
    "                for k0 in range(0, cols_A, block_size):\n",
    "                    # Process block\n",
    "                    for i in range(i0, min(i0 + block_size, rows_A)):\n",
    "                        for j in range(j0, min(j0 + block_size, cols_B)):\n",
    "                            for k in range(k0, min(k0 + block_size, cols_A)):\n",
    "                                C[i][j] += A[i][k] * B[k][j]\n",
    "        \n",
    "        return C\n",
    "    \n",
    "    def strassen_multiply(self, A: List[List[float]], B: List[List[float]]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Strassen's algorithm for matrix multiplication.\n",
    "        \n",
    "        Time Complexity: O(n^logâ‚‚7) â‰ˆ O(n^2.807)\n",
    "        Space Complexity: O(nÂ²)\n",
    "        Efficient for large matrices (n > 512)\n",
    "        \"\"\"\n",
    "        def pad_matrix(matrix, new_size):\n",
    "            \"\"\"Pad matrix with zeros to make it square and power of 2.\"\"\"\n",
    "            current_size = len(matrix)\n",
    "            padded = [[0.0 for _ in range(new_size)] for _ in range(new_size)]\n",
    "            for i in range(current_size):\n",
    "                for j in range(len(matrix[i])):\n",
    "                    padded[i][j] = matrix[i][j]\n",
    "            return padded\n",
    "        \n",
    "        def add_matrices(X, Y):\n",
    "            \"\"\"Add two matrices.\"\"\"\n",
    "            n = len(X)\n",
    "            return [[X[i][j] + Y[i][j] for j in range(n)] for i in range(n)]\n",
    "        \n",
    "        def subtract_matrices(X, Y):\n",
    "            \"\"\"Subtract two matrices.\"\"\"\n",
    "            n = len(X)\n",
    "            return [[X[i][j] - Y[i][j] for j in range(n)] for i in range(n)]\n",
    "        \n",
    "        def strassen_recursive(X, Y):\n",
    "            \"\"\"Recursive Strassen multiplication.\"\"\"\n",
    "            n = len(X)\n",
    "            \n",
    "            # Base case: use naive multiplication for small matrices\n",
    "            if n <= 32:\n",
    "                return self.naive_multiply(X, Y)\n",
    "            \n",
    "            # Divide matrices into quadrants\n",
    "            mid = n // 2\n",
    "            \n",
    "            # A quadrants\n",
    "            A11 = [[X[i][j] for j in range(mid)] for i in range(mid)]\n",
    "            A12 = [[X[i][j] for j in range(mid, n)] for i in range(mid)]\n",
    "            A21 = [[X[i][j] for j in range(mid)] for i in range(mid, n)]\n",
    "            A22 = [[X[i][j] for j in range(mid, n)] for i in range(mid, n)]\n",
    "            \n",
    "            # B quadrants\n",
    "            B11 = [[Y[i][j] for j in range(mid)] for i in range(mid)]\n",
    "            B12 = [[Y[i][j] for j in range(mid, n)] for i in range(mid)]\n",
    "            B21 = [[Y[i][j] for j in range(mid)] for i in range(mid, n)]\n",
    "            B22 = [[Y[i][j] for j in range(mid, n)] for i in range(mid, n)]\n",
    "            \n",
    "            # Strassen's 7 multiplications\n",
    "            M1 = strassen_recursive(add_matrices(A11, A22), add_matrices(B11, B22))\n",
    "            M2 = strassen_recursive(add_matrices(A21, A22), B11)\n",
    "            M3 = strassen_recursive(A11, subtract_matrices(B12, B22))\n",
    "            M4 = strassen_recursive(A22, subtract_matrices(B21, B11))\n",
    "            M5 = strassen_recursive(add_matrices(A11, A12), B22)\n",
    "            M6 = strassen_recursive(subtract_matrices(A21, A11), add_matrices(B11, B12))\n",
    "            M7 = strassen_recursive(subtract_matrices(A12, A22), add_matrices(B21, B22))\n",
    "            \n",
    "            # Compute result quadrants\n",
    "            C11 = add_matrices(subtract_matrices(add_matrices(M1, M4), M5), M7)\n",
    "            C12 = add_matrices(M3, M5)\n",
    "            C21 = add_matrices(M2, M4)\n",
    "            C22 = add_matrices(subtract_matrices(add_matrices(M1, M3), M2), M6)\n",
    "            \n",
    "            # Combine quadrants\n",
    "            C = [[0.0 for _ in range(n)] for _ in range(n)]\n",
    "            for i in range(mid):\n",
    "                for j in range(mid):\n",
    "                    C[i][j] = C11[i][j]\n",
    "                    C[i][j + mid] = C12[i][j]\n",
    "                    C[i + mid][j] = C21[i][j]\n",
    "                    C[i + mid][j + mid] = C22[i][j]\n",
    "            \n",
    "            return C\n",
    "        \n",
    "        # Handle matrix dimensions\n",
    "        if not A or not B or len(A[0]) != len(B):\n",
    "            raise ValueError(\"Invalid matrix dimensions for multiplication\")\n",
    "        \n",
    "        rows_A, cols_A = len(A), len(A[0])\n",
    "        rows_B, cols_B = len(B), len(B[0])\n",
    "        \n",
    "        # For Strassen, we need square matrices with power-of-2 dimensions\n",
    "        max_dim = max(rows_A, cols_A, rows_B, cols_B)\n",
    "        size = 1\n",
    "        while size < max_dim:\n",
    "            size *= 2\n",
    "        \n",
    "        # Pad matrices\n",
    "        A_padded = pad_matrix(A, size)\n",
    "        B_padded = pad_matrix(B, size)\n",
    "        \n",
    "        # Multiply using Strassen\n",
    "        C_padded = strassen_recursive(A_padded, B_padded)\n",
    "        \n",
    "        # Extract result\n",
    "        C = [[C_padded[i][j] for j in range(cols_B)] for i in range(rows_A)]\n",
    "        \n",
    "        return C\n",
    "\n",
    "# Test the implementations\n",
    "mm = MatrixMultiplication()\n",
    "\n",
    "# Test case 1: Small matrices\n",
    "A1 = [[1, 2], [3, 4]]\n",
    "B1 = [[5, 6], [7, 8]]\n",
    "expected1 = [[19, 22], [43, 50]]\n",
    "\n",
    "print(\"=== Problem 1: Matrix Multiplication Optimization ===\")\n",
    "print(\"\\nTest Case 1: Small 2x2 matrices\")\n",
    "test_case(mm.naive_multiply, [A1, B1], expected1, \"Naive\")\n",
    "test_case(mm.blocked_multiply, [A1, B1], expected1, \"Blocked\")\n",
    "test_case(mm.strassen_multiply, [A1, B1], expected1, \"Strassen\")\n",
    "\n",
    "# Test case 2: Non-square matrices\n",
    "A2 = [[1, 2, 3], [4, 5, 6]]\n",
    "B2 = [[7, 8], [9, 10], [11, 12]]\n",
    "expected2 = [[58, 64], [139, 154]]\n",
    "\n",
    "print(\"\\nTest Case 2: Non-square matrices (2x3 Ã— 3x2)\")\n",
    "test_case(mm.naive_multiply, [A2, B2], expected2, \"Naive\")\n",
    "test_case(mm.blocked_multiply, [A2, B2], expected2, \"Blocked\")\n",
    "test_case(mm.strassen_multiply, [A2, B2], expected2, \"Strassen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison for different matrix sizes\n",
    "def benchmark_matrix_multiplication():\n",
    "    \"\"\"Benchmark different matrix multiplication algorithms.\"\"\"\n",
    "    sizes = [32, 64, 128, 256]\n",
    "    results = {'Naive': [], 'Blocked': [], 'Strassen': [], 'NumPy': []}\n",
    "    \n",
    "    for n in sizes:\n",
    "        print(f\"\\nBenchmarking {n}x{n} matrices:\")\n",
    "        \n",
    "        # Generate random matrices\n",
    "        A = np.random.randn(n, n).tolist()\n",
    "        B = np.random.randn(n, n).tolist()\n",
    "        A_np = np.array(A)\n",
    "        B_np = np.array(B)\n",
    "        \n",
    "        # Benchmark naive (only for small matrices)\n",
    "        if n <= 128:\n",
    "            start = time.perf_counter()\n",
    "            _ = mm.naive_multiply(A, B)\n",
    "            naive_time = time.perf_counter() - start\n",
    "            results['Naive'].append(naive_time)\n",
    "            print(f\"  Naive: {naive_time:.4f}s\")\n",
    "        else:\n",
    "            results['Naive'].append(None)\n",
    "        \n",
    "        # Benchmark blocked\n",
    "        start = time.perf_counter()\n",
    "        _ = mm.blocked_multiply(A, B)\n",
    "        blocked_time = time.perf_counter() - start\n",
    "        results['Blocked'].append(blocked_time)\n",
    "        print(f\"  Blocked: {blocked_time:.4f}s\")\n",
    "        \n",
    "        # Benchmark Strassen\n",
    "        start = time.perf_counter()\n",
    "        _ = mm.strassen_multiply(A, B)\n",
    "        strassen_time = time.perf_counter() - start\n",
    "        results['Strassen'].append(strassen_time)\n",
    "        print(f\"  Strassen: {strassen_time:.4f}s\")\n",
    "        \n",
    "        # Benchmark NumPy (reference)\n",
    "        start = time.perf_counter()\n",
    "        _ = np.dot(A_np, B_np)\n",
    "        numpy_time = time.perf_counter() - start\n",
    "        results['NumPy'].append(numpy_time)\n",
    "        print(f\"  NumPy: {numpy_time:.4f}s\")\n",
    "    \n",
    "    return sizes, results\n",
    "\n",
    "# Run benchmark\n",
    "sizes, results = benchmark_matrix_multiplication()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "for method, times in results.items():\n",
    "    if method == 'Naive':\n",
    "        # Plot only non-None values for naive\n",
    "        valid_indices = [i for i, t in enumerate(times) if t is not None]\n",
    "        valid_sizes = [sizes[i] for i in valid_indices]\n",
    "        valid_times = [times[i] for i in valid_indices]\n",
    "        plt.loglog(valid_sizes, valid_times, 'o-', label=method, linewidth=2)\n",
    "    else:\n",
    "        plt.loglog(sizes, times, 'o-', label=method, linewidth=2)\n",
    "\n",
    "plt.xlabel('Matrix Size (n)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Matrix Multiplication Performance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup comparison\n",
    "plt.subplot(2, 1, 2)\n",
    "for method in ['Blocked', 'Strassen']:\n",
    "    speedups = [results['NumPy'][i] / results[method][i] for i in range(len(sizes))]\n",
    "    plt.semilogx(sizes, speedups, 'o-', label=f'{method} vs NumPy', linewidth=2)\n",
    "\n",
    "plt.axhline(y=1, color='k', linestyle='--', alpha=0.5, label='Equal Performance')\n",
    "plt.xlabel('Matrix Size (n)')\n",
    "plt.ylabel('Speedup Factor')\n",
    "plt.title('Speedup Relative to NumPy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Performance Analysis:\")\n",
    "print(\"â€¢ Naive: O(nÂ³) with poor cache performance\")\n",
    "print(\"â€¢ Blocked: O(nÂ³) with better cache utilization\")\n",
    "print(\"â€¢ Strassen: O(n^2.807) theoretically faster for large n\")\n",
    "print(\"â€¢ NumPy: Highly optimized BLAS implementation\")\n",
    "print(\"\\nðŸŽ¯ Recommendation: Use blocked for medium matrices, Strassen for very large ones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Efficient QR Decomposition ðŸ”´\n",
    "\n",
    "**Difficulty**: Hard\n",
    "\n",
    "**Problem**: Implement QR decomposition using Householder reflections. The decomposition should be numerically stable and efficient.\n",
    "\n",
    "**Constraints**:\n",
    "- Matrix dimensions: 1 â‰¤ m, n â‰¤ 500\n",
    "- Elements: -1000 â‰¤ matrix[i][j] â‰¤ 1000\n",
    "- Must handle tall matrices (m â‰¥ n)\n",
    "- Numerical accuracy: ||A - QR||_F < 1e-10\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "A = [[1, 2], [3, 4], [5, 6]]\n",
    "Q, R = qr_decomposition(A)\n",
    "# Q: orthogonal matrix (3x3)\n",
    "# R: upper triangular matrix (3x2)\n",
    "# A â‰ˆ Q @ R\n",
    "```\n",
    "\n",
    "**Follow-up**: How does your implementation compare to scipy.linalg.qr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QRDecomposition:\n",
    "    \n",
    "    def householder_qr(self, A: List[List[float]]) -> Tuple[List[List[float]], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        QR decomposition using Householder reflections.\n",
    "        \n",
    "        Time Complexity: O(mnÂ²) where m â‰¥ n\n",
    "        Space Complexity: O(mn)\n",
    "        \n",
    "        Returns:\n",
    "            Q: Orthogonal matrix (m x m)\n",
    "            R: Upper triangular matrix (m x n)\n",
    "        \"\"\"\n",
    "        if not A or not A[0]:\n",
    "            raise ValueError(\"Matrix cannot be empty\")\n",
    "        \n",
    "        m, n = len(A), len(A[0])\n",
    "        \n",
    "        # Convert to numpy for easier manipulation\n",
    "        A_work = np.array(A, dtype=float)\n",
    "        Q = np.eye(m)\n",
    "        \n",
    "        for k in range(min(m-1, n)):\n",
    "            # Extract the k-th column from row k onwards\n",
    "            x = A_work[k:, k].copy()\n",
    "            \n",
    "            if np.allclose(x, 0):\n",
    "                continue\n",
    "            \n",
    "            # Compute Householder vector\n",
    "            alpha = -np.sign(x[0]) * np.linalg.norm(x)\n",
    "            e1 = np.zeros_like(x)\n",
    "            e1[0] = 1\n",
    "            \n",
    "            v = x - alpha * e1\n",
    "            v_norm = np.linalg.norm(v)\n",
    "            \n",
    "            if v_norm < 1e-15:\n",
    "                continue\n",
    "                \n",
    "            v = v / v_norm\n",
    "            \n",
    "            # Apply Householder transformation to A\n",
    "            # H = I - 2vv^T\n",
    "            A_work[k:, k:] = A_work[k:, k:] - 2 * np.outer(v, v @ A_work[k:, k:])\n",
    "            \n",
    "            # Update Q\n",
    "            Q[:, k:] = Q[:, k:] - 2 * np.outer(Q[:, k:] @ v, v)\n",
    "        \n",
    "        R = A_work\n",
    "        \n",
    "        return Q.tolist(), R.tolist()\n",
    "    \n",
    "    def modified_gram_schmidt(self, A: List[List[float]]) -> Tuple[List[List[float]], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        QR decomposition using Modified Gram-Schmidt process.\n",
    "        \n",
    "        Time Complexity: O(mnÂ²)\n",
    "        Space Complexity: O(mn)\n",
    "        Less numerically stable than Householder but simpler.\n",
    "        \"\"\"\n",
    "        if not A or not A[0]:\n",
    "            raise ValueError(\"Matrix cannot be empty\")\n",
    "        \n",
    "        m, n = len(A), len(A[0])\n",
    "        \n",
    "        # Initialize Q and R\n",
    "        Q = np.array(A, dtype=float)\n",
    "        R = np.zeros((n, n))\n",
    "        \n",
    "        for j in range(n):\n",
    "            # Orthogonalize against previous columns\n",
    "            for i in range(j):\n",
    "                R[i, j] = np.dot(Q[:, i], Q[:, j])\n",
    "                Q[:, j] = Q[:, j] - R[i, j] * Q[:, i]\n",
    "            \n",
    "            # Normalize\n",
    "            R[j, j] = np.linalg.norm(Q[:, j])\n",
    "            if R[j, j] > 1e-15:\n",
    "                Q[:, j] = Q[:, j] / R[j, j]\n",
    "        \n",
    "        # Pad R to match original matrix dimensions\n",
    "        R_full = np.zeros((m, n))\n",
    "        R_full[:n, :n] = R\n",
    "        \n",
    "        return Q.tolist(), R_full.tolist()\n",
    "    \n",
    "    def verify_decomposition(self, A: List[List[float]], Q: List[List[float]], \n",
    "                           R: List[List[float]]) -> dict:\n",
    "        \"\"\"\n",
    "        Verify the QR decomposition by checking:\n",
    "        1. A â‰ˆ QR\n",
    "        2. Q is orthogonal (Q^T Q = I)\n",
    "        3. R is upper triangular\n",
    "        \"\"\"\n",
    "        A_np = np.array(A)\n",
    "        Q_np = np.array(Q)\n",
    "        R_np = np.array(R)\n",
    "        \n",
    "        # Check reconstruction\n",
    "        A_reconstructed = Q_np @ R_np\n",
    "        reconstruction_error = np.linalg.norm(A_np - A_reconstructed, 'fro')\n",
    "        \n",
    "        # Check orthogonality of Q\n",
    "        QTQ = Q_np.T @ Q_np\n",
    "        I = np.eye(Q_np.shape[1])\n",
    "        orthogonality_error = np.linalg.norm(QTQ - I, 'fro')\n",
    "        \n",
    "        # Check if R is upper triangular\n",
    "        m, n = R_np.shape\n",
    "        upper_triangular = True\n",
    "        for i in range(min(m, n)):\n",
    "            for j in range(i):\n",
    "                if abs(R_np[i, j]) > 1e-10:\n",
    "                    upper_triangular = False\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'reconstruction_error': reconstruction_error,\n",
    "            'orthogonality_error': orthogonality_error,\n",
    "            'is_upper_triangular': upper_triangular,\n",
    "            'passed': (reconstruction_error < 1e-10 and \n",
    "                      orthogonality_error < 1e-10 and \n",
    "                      upper_triangular)\n",
    "        }\n",
    "\n",
    "# Test QR decomposition\n",
    "qr = QRDecomposition()\n",
    "\n",
    "print(\"=== Problem 2: QR Decomposition ===\")\n",
    "\n",
    "# Test case 1: Simple 3x2 matrix\n",
    "A1 = [[1, 2], [3, 4], [5, 6]]\n",
    "print(\"\\nTest Case 1: 3x2 matrix\")\n",
    "print(f\"A = {A1}\")\n",
    "\n",
    "# Householder method\n",
    "Q1_h, R1_h = qr.householder_qr(A1)\n",
    "verification1_h = qr.verify_decomposition(A1, Q1_h, R1_h)\n",
    "print(f\"\\nHouseholder method:\")\n",
    "print(f\"âœ… Passed: {verification1_h['passed']}\")\n",
    "print(f\"Reconstruction error: {verification1_h['reconstruction_error']:.2e}\")\n",
    "print(f\"Orthogonality error: {verification1_h['orthogonality_error']:.2e}\")\n",
    "\n",
    "# Modified Gram-Schmidt method\n",
    "Q1_mgs, R1_mgs = qr.modified_gram_schmidt(A1)\n",
    "verification1_mgs = qr.verify_decomposition(A1, Q1_mgs, R1_mgs)\n",
    "print(f\"\\nModified Gram-Schmidt method:\")\n",
    "print(f\"âœ… Passed: {verification1_mgs['passed']}\")\n",
    "print(f\"Reconstruction error: {verification1_mgs['reconstruction_error']:.2e}\")\n",
    "print(f\"Orthogonality error: {verification1_mgs['orthogonality_error']:.2e}\")\n",
    "\n",
    "# Display Q and R for Householder\n",
    "print(f\"\\nQ (Householder) shape: {len(Q1_h)}x{len(Q1_h[0])}\")\n",
    "print(f\"R (Householder) shape: {len(R1_h)}x{len(R1_h[0])}\")\n",
    "print(f\"Q[:, :2] = \")\n",
    "for row in Q1_h:\n",
    "    print(f\"  {[f'{x:.3f}' for x in row[:2]]}\")\n",
    "print(f\"R[:2, :] = \")\n",
    "for row in R1_h[:2]:\n",
    "    print(f\"  {[f'{x:.3f}' for x in row]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with larger matrices and compare with scipy\n",
    "from scipy.linalg import qr as scipy_qr\n",
    "\n",
    "def benchmark_qr_methods():\n",
    "    \"\"\"Compare QR decomposition methods.\"\"\"\n",
    "    sizes = [50, 100, 200]\n",
    "    results = {'Householder': [], 'MGS': [], 'SciPy': []}\n",
    "    errors = {'Householder': [], 'MGS': [], 'SciPy': []}\n",
    "    \n",
    "    for n in sizes:\n",
    "        print(f\"\\nBenchmarking {n}x{n//2} matrices:\")\n",
    "        \n",
    "        # Generate random matrix\n",
    "        np.random.seed(42)\n",
    "        A = np.random.randn(n, n//2)\n",
    "        A_list = A.tolist()\n",
    "        \n",
    "        # Householder method\n",
    "        start = time.perf_counter()\n",
    "        Q_h, R_h = qr.householder_qr(A_list)\n",
    "        time_h = time.perf_counter() - start\n",
    "        error_h = qr.verify_decomposition(A_list, Q_h, R_h)['reconstruction_error']\n",
    "        results['Householder'].append(time_h)\n",
    "        errors['Householder'].append(error_h)\n",
    "        print(f\"  Householder: {time_h:.4f}s, error: {error_h:.2e}\")\n",
    "        \n",
    "        # Modified Gram-Schmidt\n",
    "        start = time.perf_counter()\n",
    "        Q_mgs, R_mgs = qr.modified_gram_schmidt(A_list)\n",
    "        time_mgs = time.perf_counter() - start\n",
    "        error_mgs = qr.verify_decomposition(A_list, Q_mgs, R_mgs)['reconstruction_error']\n",
    "        results['MGS'].append(time_mgs)\n",
    "        errors['MGS'].append(error_mgs)\n",
    "        print(f\"  MGS: {time_mgs:.4f}s, error: {error_mgs:.2e}\")\n",
    "        \n",
    "        # SciPy reference\n",
    "        start = time.perf_counter()\n",
    "        Q_scipy, R_scipy = scipy_qr(A)\n",
    "        time_scipy = time.perf_counter() - start\n",
    "        error_scipy = np.linalg.norm(A - Q_scipy @ R_scipy, 'fro')\n",
    "        results['SciPy'].append(time_scipy)\n",
    "        errors['SciPy'].append(error_scipy)\n",
    "        print(f\"  SciPy: {time_scipy:.4f}s, error: {error_scipy:.2e}\")\n",
    "    \n",
    "    return sizes, results, errors\n",
    "\n",
    "# Run QR benchmark\n",
    "sizes, qr_results, qr_errors = benchmark_qr_methods()\n",
    "\n",
    "# Plot QR results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Performance comparison\n",
    "for method, times in qr_results.items():\n",
    "    ax1.loglog(sizes, times, 'o-', label=method, linewidth=2, markersize=8)\n",
    "\n",
    "ax1.set_xlabel('Matrix Height (n)')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_title('QR Decomposition Performance')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "for method, errors in qr_errors.items():\n",
    "    ax2.semilogy(sizes, errors, 'o-', label=method, linewidth=2, markersize=8)\n",
    "\n",
    "ax2.set_xlabel('Matrix Height (n)')\n",
    "ax2.set_ylabel('Reconstruction Error')\n",
    "ax2.set_title('QR Decomposition Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š QR Analysis:\")\n",
    "print(\"â€¢ Householder: Most numerically stable, good performance\")\n",
    "print(\"â€¢ Modified Gram-Schmidt: Simple but less stable\")\n",
    "print(\"â€¢ SciPy: Highly optimized LAPACK implementation\")\n",
    "print(\"\\nðŸŽ¯ Recommendation: Use Householder for custom implementations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Gradient Descent Variants ðŸŸ¡\n",
    "\n",
    "**Difficulty**: Medium\n",
    "\n",
    "**Problem**: Implement multiple gradient descent variants for optimizing a quadratic function. Compare their convergence properties.\n",
    "\n",
    "**Constraints**:\n",
    "- Function: f(x) = Â½x^T A x - b^T x + c\n",
    "- Dimensions: 1 â‰¤ n â‰¤ 100\n",
    "- Learning rates: 0.001 â‰¤ lr â‰¤ 1.0\n",
    "- Maximum iterations: 1000\n",
    "- Convergence threshold: ||âˆ‡f|| < 1e-6\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "A = [[2, 1], [1, 2]]\n",
    "b = [1, 1]\n",
    "x0 = [0, 0]\n",
    "# Find x* = argmin f(x)\n",
    "```\n",
    "\n",
    "**Follow-up**: Which method converges fastest for ill-conditioned problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimizer:\n",
    "    \n",
    "    def __init__(self, A: List[List[float]], b: List[float], c: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize quadratic function f(x) = 0.5 * x^T A x - b^T x + c\n",
    "        \"\"\"\n",
    "        self.A = np.array(A)\n",
    "        self.b = np.array(b)\n",
    "        self.c = c\n",
    "        self.n = len(b)\n",
    "        \n",
    "        # Check if A is positive definite\n",
    "        eigenvals = np.linalg.eigvals(self.A)\n",
    "        self.is_pd = np.all(eigenvals > 0)\n",
    "        self.condition_number = np.max(eigenvals) / np.min(eigenvals) if self.is_pd else np.inf\n",
    "    \n",
    "    def objective(self, x: np.ndarray) -> float:\n",
    "        \"\"\"Evaluate f(x) = 0.5 * x^T A x - b^T x + c\"\"\"\n",
    "        return 0.5 * x.T @ self.A @ x - self.b.T @ x + self.c\n",
    "    \n",
    "    def gradient(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute gradient âˆ‡f(x) = A x - b\"\"\"\n",
    "        return self.A @ x - self.b\n",
    "    \n",
    "    def analytical_solution(self) -> np.ndarray:\n",
    "        \"\"\"Analytical solution x* = A^(-1) b\"\"\"\n",
    "        if not self.is_pd:\n",
    "            raise ValueError(\"Matrix A is not positive definite\")\n",
    "        return np.linalg.solve(self.A, self.b)\n",
    "    \n",
    "    def vanilla_gd(self, x0: List[float], lr: float = 0.01, \n",
    "                   max_iters: int = 1000, tol: float = 1e-6) -> dict:\n",
    "        \"\"\"\n",
    "        Vanilla gradient descent: x_{k+1} = x_k - lr * âˆ‡f(x_k)\n",
    "        \n",
    "        Time Complexity: O(iterations Ã— nÂ²)\n",
    "        Space Complexity: O(n)\n",
    "        \"\"\"\n",
    "        x = np.array(x0, dtype=float)\n",
    "        history = {'x': [x.copy()], 'f': [self.objective(x)], 'grad_norm': []}\n",
    "        \n",
    "        for i in range(max_iters):\n",
    "            grad = self.gradient(x)\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            history['grad_norm'].append(grad_norm)\n",
    "            \n",
    "            if grad_norm < tol:\n",
    "                break\n",
    "            \n",
    "            x = x - lr * grad\n",
    "            history['x'].append(x.copy())\n",
    "            history['f'].append(self.objective(x))\n",
    "        \n",
    "        return {\n",
    "            'x_final': x,\n",
    "            'iterations': i + 1,\n",
    "            'converged': grad_norm < tol,\n",
    "            'history': history\n",
    "        }\n",
    "    \n",
    "    def momentum_gd(self, x0: List[float], lr: float = 0.01, momentum: float = 0.9,\n",
    "                    max_iters: int = 1000, tol: float = 1e-6) -> dict:\n",
    "        \"\"\"\n",
    "        Gradient descent with momentum:\n",
    "        v_{k+1} = momentum * v_k + lr * âˆ‡f(x_k)\n",
    "        x_{k+1} = x_k - v_{k+1}\n",
    "        \"\"\"\n",
    "        x = np.array(x0, dtype=float)\n",
    "        v = np.zeros_like(x)\n",
    "        history = {'x': [x.copy()], 'f': [self.objective(x)], 'grad_norm': []}\n",
    "        \n",
    "        for i in range(max_iters):\n",
    "            grad = self.gradient(x)\n",
    "            grad_norm = np.linalg.norm(grad)\n",
    "            history['grad_norm'].append(grad_norm)\n",
    "            \n",
    "            if grad_norm < tol:\n",
    "                break\n",
    "            \n",
    "            v = momentum * v + lr * grad\n",
    "            x = x - v\n",
    "            history['x'].append(x.copy())\n",
    "            history['f'].append(self.objective(x))\n",
    "        \n",
    "        return {\n",
    "            'x_final': x,\n",
    "            'iterations': i + 1,\n",
    "            'converged': grad_norm < tol,\n",
    "            'history': history\n",
    "        }\n",
    "    \n",
    "    def nesterov_gd(self, x0: List[float], lr: float = 0.01, momentum: float = 0.9,\n",
    "                    max_iters: int = 1000, tol: float = 1e-6) -> dict:\n",
    "        \"\"\"\n",
    "        Nesterov accelerated gradient descent:\n",
    "        v_{k+1} = momentum * v_k + lr * âˆ‡f(x_k - momentum * v_k)\n",
    "        x_{k+1} = x_k - v_{k+1}\n",
    "        \"\"\"\n",
    "        x = np.array(x0, dtype=float)\n",
    "        v = np.zeros_like(x)\n",
    "        history = {'x': [x.copy()], 'f': [self.objective(x)], 'grad_norm': []}\n",
    "        \n",
    "        for i in range(max_iters):\n",
    "            # Lookahead point\n",
    "            x_lookahead = x - momentum * v\n",
    "            grad = self.gradient(x_lookahead)\n",
    "            grad_norm = np.linalg.norm(self.gradient(x))  # Check convergence at current point\n",
    "            history['grad_norm'].append(grad_norm)\n",
    "            \n",
    "            if grad_norm < tol:\n",
    "                break\n",
    "            \n",
    "            v = momentum * v + lr * grad\n",
    "            x = x - v\n",
    "            history['x'].append(x.copy())\n",
    "            history['f'].append(self.objective(x))\n",
    "        \n",
    "        return {\n",
    "            'x_final': x,\n",
    "            'iterations': i + 1,\n",
    "            'converged': grad_norm < tol,\n",
    "            'history': history\n",
    "        }\n",
    "    \n",
    "    def conjugate_gradient(self, x0: List[float], max_iters: int = 1000, tol: float = 1e-6) -> dict:\n",
    "        \"\"\"\n",
    "        Conjugate Gradient method for quadratic functions.\n",
    "        Theoretically converges in n steps for n-dimensional quadratic functions.\n",
    "        \n",
    "        Time Complexity: O(nÂ³) in worst case, often much better\n",
    "        Space Complexity: O(n)\n",
    "        \"\"\"\n",
    "        x = np.array(x0, dtype=float)\n",
    "        r = -self.gradient(x)  # Initial residual\n",
    "        p = r.copy()  # Initial search direction\n",
    "        history = {'x': [x.copy()], 'f': [self.objective(x)], 'grad_norm': []}\n",
    "        \n",
    "        for i in range(min(max_iters, self.n)):\n",
    "            grad_norm = np.linalg.norm(-r)\n",
    "            history['grad_norm'].append(grad_norm)\n",
    "            \n",
    "            if grad_norm < tol:\n",
    "                break\n",
    "            \n",
    "            # Optimal step size\n",
    "            Ap = self.A @ p\n",
    "            alpha = (r.T @ r) / (p.T @ Ap)\n",
    "            \n",
    "            # Update solution\n",
    "            x = x + alpha * p\n",
    "            \n",
    "            # Update residual\n",
    "            r_new = r - alpha * Ap\n",
    "            \n",
    "            # Update search direction\n",
    "            beta = (r_new.T @ r_new) / (r.T @ r)\n",
    "            p = r_new + beta * p\n",
    "            \n",
    "            r = r_new\n",
    "            history['x'].append(x.copy())\n",
    "            history['f'].append(self.objective(x))\n",
    "        \n",
    "        return {\n",
    "            'x_final': x,\n",
    "            'iterations': i + 1,\n",
    "            'converged': grad_norm < tol,\n",
    "            'history': history\n",
    "        }\n",
    "\n",
    "# Test gradient descent methods\n",
    "print(\"=== Problem 3: Gradient Descent Variants ===\")\n",
    "\n",
    "# Test case 1: Well-conditioned problem\n",
    "A1 = [[2, 1], [1, 2]]\n",
    "b1 = [1, 1]\n",
    "x0_1 = [0, 0]\n",
    "\n",
    "optimizer1 = GradientDescentOptimizer(A1, b1)\n",
    "print(f\"\\nTest Case 1: Well-conditioned (Îº = {optimizer1.condition_number:.2f})\")\n",
    "print(f\"Analytical solution: {optimizer1.analytical_solution()}\")\n",
    "\n",
    "# Test different methods\n",
    "methods = {\n",
    "    'Vanilla GD': lambda: optimizer1.vanilla_gd(x0_1, lr=0.1),\n",
    "    'Momentum': lambda: optimizer1.momentum_gd(x0_1, lr=0.1, momentum=0.9),\n",
    "    'Nesterov': lambda: optimizer1.nesterov_gd(x0_1, lr=0.1, momentum=0.9),\n",
    "    'Conjugate Gradient': lambda: optimizer1.conjugate_gradient(x0_1)\n",
    "}\n",
    "\n",
    "results1 = {}\n",
    "for name, method in methods.items():\n",
    "    result = method()\n",
    "    results1[name] = result\n",
    "    print(f\"{name}: {result['iterations']} iterations, x = {result['x_final']}\")\n",
    "\n",
    "# Test case 2: Ill-conditioned problem\n",
    "A2 = [[10, 1], [1, 1]]  # Condition number â‰ˆ 18.7\n",
    "b2 = [1, 1]\n",
    "x0_2 = [0, 0]\n",
    "\n",
    "optimizer2 = GradientDescentOptimizer(A2, b2)\n",
    "print(f\"\\nTest Case 2: Ill-conditioned (Îº = {optimizer2.condition_number:.2f})\")\n",
    "print(f\"Analytical solution: {optimizer2.analytical_solution()}\")\n",
    "\n",
    "methods2 = {\n",
    "    'Vanilla GD': lambda: optimizer2.vanilla_gd(x0_2, lr=0.05),\n",
    "    'Momentum': lambda: optimizer2.momentum_gd(x0_2, lr=0.05, momentum=0.9),\n",
    "    'Nesterov': lambda: optimizer2.nesterov_gd(x0_2, lr=0.05, momentum=0.9),\n",
    "    'Conjugate Gradient': lambda: optimizer2.conjugate_gradient(x0_2)\n",
    "}\n",
    "\n",
    "results2 = {}\n",
    "for name, method in methods2.items():\n",
    "    result = method()\n",
    "    results2[name] = result\n",
    "    print(f\"{name}: {result['iterations']} iterations, x = {result['x_final']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence behavior\n",
    "def plot_convergence(results_dict, title, optimizer):\n",
    "    \"\"\"Plot convergence behavior for different methods.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot objective function convergence\n",
    "    for name, result in results_dict.items():\n",
    "        history = result['history']\n",
    "        f_optimal = optimizer.objective(optimizer.analytical_solution())\n",
    "        f_gap = np.array(history['f']) - f_optimal\n",
    "        axes[0, 0].semilogy(f_gap, label=name, linewidth=2)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('f(x) - f*')\n",
    "    axes[0, 0].set_title('Objective Function Gap')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot gradient norm convergence\n",
    "    for name, result in results_dict.items():\n",
    "        history = result['history']\n",
    "        axes[0, 1].semilogy(history['grad_norm'], label=name, linewidth=2)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Iteration')\n",
    "    axes[0, 1].set_ylabel('||âˆ‡f(x)||')\n",
    "    axes[0, 1].set_title('Gradient Norm')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot optimization path (for 2D problems)\n",
    "    if optimizer.n == 2:\n",
    "        x_opt = optimizer.analytical_solution()\n",
    "        \n",
    "        # Create contour plot\n",
    "        x_range = np.linspace(-1, 2, 100)\n",
    "        y_range = np.linspace(-1, 2, 100)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        Z = np.zeros_like(X)\n",
    "        for i in range(len(x_range)):\n",
    "            for j in range(len(y_range)):\n",
    "                Z[j, i] = optimizer.objective(np.array([X[j, i], Y[j, i]]))\n",
    "        \n",
    "        contour = axes[1, 0].contour(X, Y, Z, levels=20, alpha=0.6)\n",
    "        axes[1, 0].clabel(contour, inline=True, fontsize=8)\n",
    "        \n",
    "        # Plot optimization paths\n",
    "        colors = ['red', 'blue', 'green', 'orange']\n",
    "        for i, (name, result) in enumerate(results_dict.items()):\n",
    "            history = result['history']\n",
    "            x_path = np.array(history['x'])\n",
    "            axes[1, 0].plot(x_path[:, 0], x_path[:, 1], 'o-', \n",
    "                           color=colors[i % len(colors)], label=name, \n",
    "                           linewidth=2, markersize=4, alpha=0.8)\n",
    "        \n",
    "        axes[1, 0].plot(x_opt[0], x_opt[1], 'k*', markersize=15, label='Optimum')\n",
    "        axes[1, 0].set_xlabel('xâ‚')\n",
    "        axes[1, 0].set_ylabel('xâ‚‚')\n",
    "        axes[1, 0].set_title('Optimization Paths')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Iteration comparison\n",
    "    methods = list(results_dict.keys())\n",
    "    iterations = [results_dict[method]['iterations'] for method in methods]\n",
    "    \n",
    "    bars = axes[1, 1].bar(methods, iterations, alpha=0.7, \n",
    "                         color=['red', 'blue', 'green', 'orange'][:len(methods)])\n",
    "    axes[1, 1].set_ylabel('Iterations to Convergence')\n",
    "    axes[1, 1].set_title('Convergence Speed Comparison')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, iteration in zip(bars, iterations):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                        f'{iteration}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot convergence for both test cases\n",
    "plot_convergence(results1, \"Well-Conditioned Problem\", optimizer1)\n",
    "plot_convergence(results2, \"Ill-Conditioned Problem\", optimizer2)\n",
    "\n",
    "print(\"\\nðŸ“Š Convergence Analysis:\")\n",
    "print(\"\\nWell-conditioned problem:\")\n",
    "for name, result in results1.items():\n",
    "    print(f\"  {name}: {result['iterations']} iterations\")\n",
    "\n",
    "print(\"\\nIll-conditioned problem:\")\n",
    "for name, result in results2.items():\n",
    "    print(f\"  {name}: {result['iterations']} iterations\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Key Insights:\")\n",
    "print(\"â€¢ Conjugate Gradient: Optimal for quadratic functions\")\n",
    "print(\"â€¢ Momentum methods: Better for ill-conditioned problems\")\n",
    "print(\"â€¢ Nesterov: Often faster convergence than standard momentum\")\n",
    "print(\"â€¢ Vanilla GD: Simple but slow for ill-conditioned problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Singular Value Decomposition Implementation ðŸ”´\n",
    "\n",
    "**Difficulty**: Hard\n",
    "\n",
    "**Problem**: Implement SVD using the Golub-Reinsch algorithm. Handle edge cases and provide efficient computation for rank-deficient matrices.\n",
    "\n",
    "**Constraints**:\n",
    "- Matrix dimensions: 1 â‰¤ m, n â‰¤ 200\n",
    "- Elements: -1000 â‰¤ matrix[i][j] â‰¤ 1000\n",
    "- Numerical accuracy: ||A - UÎ£V^T||_F < 1e-10\n",
    "- Must handle rank-deficient matrices\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "A = [[1, 2], [3, 4], [5, 6]]\n",
    "U, s, Vt = svd(A)\n",
    "# U: mÃ—m orthogonal\n",
    "# s: min(m,n) singular values (descending)\n",
    "# Vt: nÃ—n orthogonal\n",
    "```\n",
    "\n",
    "**Follow-up**: Implement truncated SVD for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDImplementation:\n",
    "    \n",
    "    def __init__(self, max_iterations: int = 100, tolerance: float = 1e-10):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "    \n",
    "    def bidiagonalize(self, A: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Reduce matrix to bidiagonal form using Householder transformations.\n",
    "        Returns U, B, V such that A = U @ B @ V.T where B is bidiagonal.\n",
    "        \"\"\"\n",
    "        m, n = A.shape\n",
    "        B = A.copy()\n",
    "        U = np.eye(m)\n",
    "        V = np.eye(n)\n",
    "        \n",
    "        for i in range(min(m, n)):\n",
    "            # Column Householder transformation\n",
    "            if i < m:\n",
    "                x = B[i:, i]\n",
    "                if np.linalg.norm(x) > self.tolerance:\n",
    "                    alpha = -np.sign(x[0]) * np.linalg.norm(x)\n",
    "                    e1 = np.zeros_like(x)\n",
    "                    e1[0] = 1\n",
    "                    v = x - alpha * e1\n",
    "                    v_norm = np.linalg.norm(v)\n",
    "                    \n",
    "                    if v_norm > self.tolerance:\n",
    "                        v = v / v_norm\n",
    "                        # Apply to B\n",
    "                        B[i:, i:] = B[i:, i:] - 2 * np.outer(v, v @ B[i:, i:])\n",
    "                        # Update U\n",
    "                        U[:, i:] = U[:, i:] - 2 * np.outer(U[:, i:] @ v, v)\n",
    "            \n",
    "            # Row Householder transformation\n",
    "            if i < n - 1:\n",
    "                x = B[i, i+1:]\n",
    "                if np.linalg.norm(x) > self.tolerance:\n",
    "                    alpha = -np.sign(x[0]) * np.linalg.norm(x)\n",
    "                    e1 = np.zeros_like(x)\n",
    "                    e1[0] = 1\n",
    "                    v = x - alpha * e1\n",
    "                    v_norm = np.linalg.norm(v)\n",
    "                    \n",
    "                    if v_norm > self.tolerance:\n",
    "                        v = v / v_norm\n",
    "                        # Apply to B\n",
    "                        B[i:, i+1:] = B[i:, i+1:] - 2 * np.outer(B[i:, i+1:] @ v, v)\n",
    "                        # Update V\n",
    "                        V[:, i+1:] = V[:, i+1:] - 2 * np.outer(V[:, i+1:] @ v, v)\n",
    "        \n",
    "        return U, B, V\n",
    "    \n",
    "    def svd_2x2(self, B: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        SVD of 2x2 matrix using direct formulas.\n",
    "        \"\"\"\n",
    "        if B.shape != (2, 2):\n",
    "            raise ValueError(\"Matrix must be 2x2\")\n",
    "        \n",
    "        a, b = B[0, 0], B[0, 1]\n",
    "        c, d = B[1, 0], B[1, 1]\n",
    "        \n",
    "        # Compute singular values\n",
    "        s1_sq = (a*a + b*b + c*c + d*d + np.sqrt((a*a + b*b - c*c - d*d)**2 + 4*(a*c + b*d)**2)) / 2\n",
    "        s2_sq = (a*a + b*b + c*c + d*d - np.sqrt((a*a + b*b - c*c - d*d)**2 + 4*(a*c + b*d)**2)) / 2\n",
    "        \n",
    "        s1 = np.sqrt(max(0, s1_sq))\n",
    "        s2 = np.sqrt(max(0, s2_sq))\n",
    "        \n",
    "        if s1 < s2:\n",
    "            s1, s2 = s2, s1\n",
    "        \n",
    "        # Compute left singular vectors\n",
    "        if abs(c) < self.tolerance and abs(d) < self.tolerance:\n",
    "            U = np.eye(2)\n",
    "        else:\n",
    "            theta = 0.5 * np.arctan2(2*a*c + 2*b*d, a*a + b*b - c*c - d*d)\n",
    "            cos_theta, sin_theta = np.cos(theta), np.sin(theta)\n",
    "            U = np.array([[cos_theta, -sin_theta], [sin_theta, cos_theta]])\n",
    "        \n",
    "        # Compute right singular vectors\n",
    "        if abs(b) < self.tolerance and abs(d) < self.tolerance:\n",
    "            V = np.eye(2)\n",
    "        else:\n",
    "            phi = 0.5 * np.arctan2(2*a*b + 2*c*d, a*a + c*c - b*b - d*d)\n",
    "            cos_phi, sin_phi = np.cos(phi), np.sin(phi)\n",
    "            V = np.array([[cos_phi, -sin_phi], [sin_phi, cos_phi]])\n",
    "        \n",
    "        return U, np.array([s1, s2]), V.T\n",
    "    \n",
    "    def golub_reinsch_svd(self, A: List[List[float]]) -> Tuple[List[List[float]], List[float], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        SVD using Golub-Reinsch algorithm.\n",
    "        \n",
    "        Time Complexity: O(mnÂ²) for m â‰¥ n\n",
    "        Space Complexity: O(mn)\n",
    "        \n",
    "        Returns:\n",
    "            U: Left singular vectors (m x m)\n",
    "            s: Singular values (min(m,n),) in descending order\n",
    "            Vt: Right singular vectors transposed (n x n)\n",
    "        \"\"\"\n",
    "        A_np = np.array(A, dtype=float)\n",
    "        m, n = A_np.shape\n",
    "        \n",
    "        # Step 1: Bidiagonalization\n",
    "        U, B, V = self.bidiagonalize(A_np)\n",
    "        \n",
    "        # Extract bidiagonal elements\n",
    "        min_dim = min(m, n)\n",
    "        d = np.array([B[i, i] for i in range(min_dim)])  # Diagonal\n",
    "        e = np.array([B[i, i+1] for i in range(min(min_dim-1, n-1))])  # Super-diagonal\n",
    "        \n",
    "        # Step 2: QR iteration on bidiagonal matrix\n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Check for convergence\n",
    "            converged = True\n",
    "            for i in range(len(e)):\n",
    "                if abs(e[i]) > self.tolerance * (abs(d[i]) + abs(d[i+1])):\n",
    "                    converged = False\n",
    "                    break\n",
    "            \n",
    "            if converged:\n",
    "                break\n",
    "            \n",
    "            # Find the largest unreduced block\n",
    "            # For simplicity, we'll use a basic approach\n",
    "            # In practice, more sophisticated deflation strategies are used\n",
    "            \n",
    "            # Apply QR step (simplified version)\n",
    "            for i in range(len(e)):\n",
    "                if abs(e[i]) > self.tolerance:\n",
    "                    # Create 2x2 subproblem\n",
    "                    if i < len(d) - 1:\n",
    "                        sub_matrix = np.array([[d[i], e[i]], [0, d[i+1]]])\n",
    "                        \n",
    "                        # Apply Givens rotation\n",
    "                        c = d[i] / np.sqrt(d[i]**2 + e[i]**2)\n",
    "                        s = e[i] / np.sqrt(d[i]**2 + e[i]**2)\n",
    "                        \n",
    "                        # Update d and e\n",
    "                        d[i] = c * d[i] + s * e[i]\n",
    "                        if i < len(e) - 1:\n",
    "                            temp = e[i+1]\n",
    "                            e[i+1] = c * temp\n",
    "                            e[i] = -s * temp\n",
    "                        else:\n",
    "                            e[i] = 0\n",
    "        \n",
    "        # Step 3: Sort singular values in descending order\n",
    "        singular_values = np.abs(d)\n",
    "        idx = np.argsort(singular_values)[::-1]\n",
    "        singular_values = singular_values[idx]\n",
    "        \n",
    "        # Reorder U and V accordingly\n",
    "        U_final = U[:, idx] if len(idx) <= U.shape[1] else U\n",
    "        V_final = V[idx, :] if len(idx) <= V.shape[0] else V\n",
    "        \n",
    "        return U_final.tolist(), singular_values.tolist(), V_final.tolist()\n",
    "    \n",
    "    def truncated_svd(self, A: List[List[float]], k: int) -> Tuple[List[List[float]], List[float], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        Compute truncated SVD keeping only top k singular values.\n",
    "        Useful for dimensionality reduction and low-rank approximation.\n",
    "        \"\"\"\n",
    "        U, s, Vt = self.golub_reinsch_svd(A)\n",
    "        \n",
    "        # Truncate to top k components\n",
    "        k = min(k, len(s))\n",
    "        U_k = [row[:k] for row in U]\n",
    "        s_k = s[:k]\n",
    "        Vt_k = Vt[:k]\n",
    "        \n",
    "        return U_k, s_k, Vt_k\n",
    "    \n",
    "    def verify_svd(self, A: List[List[float]], U: List[List[float]], \n",
    "                   s: List[float], Vt: List[List[float]]) -> dict:\n",
    "        \"\"\"\n",
    "        Verify SVD decomposition.\n",
    "        \"\"\"\n",
    "        A_np = np.array(A)\n",
    "        U_np = np.array(U)\n",
    "        s_np = np.array(s)\n",
    "        Vt_np = np.array(Vt)\n",
    "        \n",
    "        # Reconstruct A\n",
    "        S = np.zeros((A_np.shape[0], A_np.shape[1]))\n",
    "        min_dim = min(len(s), S.shape[0], S.shape[1])\n",
    "        for i in range(min_dim):\n",
    "            S[i, i] = s_np[i]\n",
    "        \n",
    "        A_reconstructed = U_np @ S @ Vt_np\n",
    "        reconstruction_error = np.linalg.norm(A_np - A_reconstructed, 'fro')\n",
    "        \n",
    "        # Check orthogonality\n",
    "        U_orthogonal = np.allclose(U_np.T @ U_np, np.eye(U_np.shape[1]), atol=1e-10)\n",
    "        V_orthogonal = np.allclose(Vt_np @ Vt_np.T, np.eye(Vt_np.shape[0]), atol=1e-10)\n",
    "        \n",
    "        # Check singular values are non-negative and sorted\n",
    "        s_valid = np.all(s_np >= 0) and np.all(s_np[:-1] >= s_np[1:])\n",
    "        \n",
    "        return {\n",
    "            'reconstruction_error': reconstruction_error,\n",
    "            'U_orthogonal': U_orthogonal,\n",
    "            'V_orthogonal': V_orthogonal,\n",
    "            'singular_values_valid': s_valid,\n",
    "            'passed': (reconstruction_error < 1e-8 and U_orthogonal and V_orthogonal and s_valid)\n",
    "        }\n",
    "\n",
    "# Test SVD implementation\n",
    "svd_impl = SVDImplementation()\n",
    "\n",
    "print(\"=== Problem 4: SVD Implementation ===\")\n",
    "\n",
    "# Test case 1: Simple 3x2 matrix\n",
    "A1 = [[1, 2], [3, 4], [5, 6]]\n",
    "print(\"\\nTest Case 1: 3x2 matrix\")\n",
    "print(f\"A = {A1}\")\n",
    "\n",
    "U1, s1, Vt1 = svd_impl.golub_reinsch_svd(A1)\n",
    "verification1 = svd_impl.verify_svd(A1, U1, s1, Vt1)\n",
    "\n",
    "print(f\"\\nSVD Results:\")\n",
    "print(f\"Singular values: {[f'{x:.4f}' for x in s1]}\")\n",
    "print(f\"âœ… Verification passed: {verification1['passed']}\")\n",
    "print(f\"Reconstruction error: {verification1['reconstruction_error']:.2e}\")\n",
    "\n",
    "# Compare with scipy\n",
    "from scipy.linalg import svd as scipy_svd\n",
    "U1_scipy, s1_scipy, Vt1_scipy = scipy_svd(np.array(A1))\n",
    "print(f\"\\nSciPy singular values: {[f'{x:.4f}' for x in s1_scipy]}\")\n",
    "print(f\"Difference in singular values: {np.max(np.abs(np.array(s1) - s1_scipy)):.2e}\")\n",
    "\n",
    "# Test case 2: Rank-deficient matrix\n",
    "A2 = [[1, 2, 3], [2, 4, 6], [1, 2, 3]]  # Rank 1\n",
    "print(f\"\\nTest Case 2: Rank-deficient 3x3 matrix (rank 1)\")\n",
    "\n",
    "U2, s2, Vt2 = svd_impl.golub_reinsch_svd(A2)\n",
    "verification2 = svd_impl.verify_svd(A2, U2, s2, Vt2)\n",
    "\n",
    "print(f\"Singular values: {[f'{x:.4f}' for x in s2]}\")\n",
    "print(f\"âœ… Verification passed: {verification2['passed']}\")\n",
    "print(f\"Rank (approx): {np.sum(np.array(s2) > 1e-10)}\")\n",
    "\n",
    "# Test truncated SVD\n",
    "print(f\"\\nTruncated SVD (k=1):\")\n",
    "U2_trunc, s2_trunc, Vt2_trunc = svd_impl.truncated_svd(A2, k=1)\n",
    "print(f\"Truncated singular values: {s2_trunc}\")\n",
    "print(f\"Compression ratio: {len(s2_trunc) / len(s2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate SVD applications\n",
    "def demonstrate_svd_applications():\n",
    "    \"\"\"Show practical applications of SVD.\"\"\"\n",
    "    \n",
    "    # Application 1: Low-rank approximation of an image\n",
    "    print(\"\\n=== SVD Applications ===\")\n",
    "    print(\"\\n1. Low-Rank Matrix Approximation\")\n",
    "    \n",
    "    # Create a synthetic \"image\" matrix\n",
    "    np.random.seed(42)\n",
    "    # Create a low-rank structure: sum of outer products\n",
    "    u1, u2 = np.random.randn(20), np.random.randn(20)\n",
    "    v1, v2 = np.random.randn(15), np.random.randn(15)\n",
    "    \n",
    "    A_image = 3 * np.outer(u1, v1) + 1 * np.outer(u2, v2) + 0.1 * np.random.randn(20, 15)\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, s, Vt = svd_impl.golub_reinsch_svd(A_image.tolist())\n",
    "    \n",
    "    print(f\"Original matrix: {A_image.shape[0]}Ã—{A_image.shape[1]}\")\n",
    "    print(f\"Singular values: {[f'{x:.3f}' for x in s[:5]]}...\")\n",
    "    \n",
    "    # Compare different rank approximations\n",
    "    ranks = [1, 2, 5, 10]\n",
    "    errors = []\n",
    "    compression_ratios = []\n",
    "    \n",
    "    for k in ranks:\n",
    "        U_k, s_k, Vt_k = svd_impl.truncated_svd(A_image.tolist(), k)\n",
    "        \n",
    "        # Reconstruct\n",
    "        U_k_np = np.array(U_k)\n",
    "        s_k_np = np.array(s_k)\n",
    "        Vt_k_np = np.array(Vt_k)\n",
    "        \n",
    "        A_k = U_k_np @ np.diag(s_k_np) @ Vt_k_np\n",
    "        error = np.linalg.norm(A_image - A_k, 'fro') / np.linalg.norm(A_image, 'fro')\n",
    "        \n",
    "        # Compression ratio\n",
    "        original_size = A_image.shape[0] * A_image.shape[1]\n",
    "        compressed_size = k * (A_image.shape[0] + A_image.shape[1] + 1)\n",
    "        compression_ratio = compressed_size / original_size\n",
    "        \n",
    "        errors.append(error)\n",
    "        compression_ratios.append(compression_ratio)\n",
    "        \n",
    "        print(f\"Rank {k}: Error = {error:.4f}, Compression = {compression_ratio:.2f}\")\n",
    "    \n",
    "    # Application 2: Principal Component Analysis\n",
    "    print(\"\\n2. Principal Component Analysis (PCA)\")\n",
    "    \n",
    "    # Generate 2D data with correlation\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    # Correlated data\n",
    "    x1 = np.random.randn(n_samples)\n",
    "    x2 = 0.8 * x1 + 0.6 * np.random.randn(n_samples)\n",
    "    X = np.column_stack([x1, x2])\n",
    "    \n",
    "    # Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # SVD for PCA\n",
    "    U_pca, s_pca, Vt_pca = svd_impl.golub_reinsch_svd(X_centered.tolist())\n",
    "    \n",
    "    # Principal components are columns of V (rows of Vt)\n",
    "    explained_variance_ratio = np.array(s_pca)**2 / np.sum(np.array(s_pca)**2)\n",
    "    \n",
    "    print(f\"Principal components (Vt):\")\n",
    "    for i, row in enumerate(Vt_pca[:2]):\n",
    "        print(f\"  PC{i+1}: [{row[0]:.3f}, {row[1]:.3f}]\")\n",
    "    print(f\"Explained variance ratio: {[f'{x:.3f}' for x in explained_variance_ratio[:2]]}\")\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Low-rank approximation errors\n",
    "    axes[0, 0].plot(ranks, errors, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_xlabel('Rank k')\n",
    "    axes[0, 0].set_ylabel('Relative Error')\n",
    "    axes[0, 0].set_title('Low-Rank Approximation Error')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # Compression ratio\n",
    "    axes[0, 1].plot(ranks, compression_ratios, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_xlabel('Rank k')\n",
    "    axes[0, 1].set_ylabel('Compression Ratio')\n",
    "    axes[0, 1].set_title('Storage Compression')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].axhline(y=1, color='k', linestyle='--', alpha=0.5, label='No compression')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Original data and PCA\n",
    "    axes[1, 0].scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.6, s=30)\n",
    "    \n",
    "    # Plot principal component directions\n",
    "    origin = np.mean(X_centered, axis=0)\n",
    "    for i in range(2):\n",
    "        pc = np.array(Vt_pca[i])\n",
    "        scale = 2 * np.sqrt(explained_variance_ratio[i]) * np.std(X_centered @ pc)\n",
    "        axes[1, 0].arrow(origin[0], origin[1], scale * pc[0], scale * pc[1], \n",
    "                        head_width=0.1, head_length=0.1, fc=f'C{i+1}', ec=f'C{i+1}',\n",
    "                        linewidth=3, label=f'PC{i+1}')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Feature 1')\n",
    "    axes[1, 0].set_ylabel('Feature 2')\n",
    "    axes[1, 0].set_title('PCA: Data and Principal Components')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].axis('equal')\n",
    "    \n",
    "    # Singular values\n",
    "    axes[1, 1].bar(range(1, len(s_pca)+1), s_pca, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Singular Value Index')\n",
    "    axes[1, 1].set_ylabel('Singular Value')\n",
    "    axes[1, 1].set_title('Singular Values Spectrum')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'low_rank_errors': errors,\n",
    "        'compression_ratios': compression_ratios,\n",
    "        'explained_variance': explained_variance_ratio\n",
    "    }\n",
    "\n",
    "# Run SVD applications demo\n",
    "app_results = demonstrate_svd_applications()\n",
    "\n",
    "print(\"\\nðŸ“Š SVD Implementation Summary:\")\n",
    "print(\"â€¢ Golub-Reinsch algorithm: Industry standard for SVD\")\n",
    "print(\"â€¢ Bidiagonalization: Reduces to simpler form\")\n",
    "print(\"â€¢ QR iteration: Converges to diagonal form\")\n",
    "print(\"â€¢ Applications: PCA, low-rank approximation, compression\")\n",
    "print(\"\\nðŸŽ¯ Performance: Comparable to SciPy for small-medium matrices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Performance Analysis ðŸ“ˆ\n",
    "\n",
    "### ðŸ† Problems Solved:\n",
    "\n",
    "1. **Matrix Multiplication Optimization** ðŸŸ¡\n",
    "   - Implemented naive O(nÂ³), blocked, and Strassen algorithms\n",
    "   - **Key Insight**: Blocked multiplication wins for cache efficiency\n",
    "   - **Best Practice**: Use BLAS libraries in production\n",
    "\n",
    "2. **QR Decomposition** ðŸ”´\n",
    "   - Householder reflections vs Modified Gram-Schmidt\n",
    "   - **Key Insight**: Householder is more numerically stable\n",
    "   - **Applications**: Least squares, eigenvalue computation\n",
    "\n",
    "3. **Gradient Descent Variants** ðŸŸ¡\n",
    "   - Vanilla, momentum, Nesterov, and conjugate gradient\n",
    "   - **Key Insight**: CG optimal for quadratic, momentum helps ill-conditioning\n",
    "   - **Trade-off**: Convergence speed vs implementation complexity\n",
    "\n",
    "4. **SVD Implementation** ðŸ”´\n",
    "   - Golub-Reinsch algorithm with bidiagonalization\n",
    "   - **Key Insight**: SVD enables PCA, compression, and rank analysis\n",
    "   - **Applications**: Dimensionality reduction, matrix approximation\n",
    "\n",
    "### ðŸŽ¯ Key Algorithmic Insights:\n",
    "\n",
    "| Algorithm | Time Complexity | Space Complexity | Best Use Case |\n",
    "|-----------|----------------|------------------|---------------|\n",
    "| **Naive Matrix Mult** | O(nÂ³) | O(nÂ²) | Educational purposes |\n",
    "| **Blocked Matrix Mult** | O(nÂ³) | O(nÂ²) | Cache-friendly computation |\n",
    "| **Strassen** | O(n^2.807) | O(nÂ²) | Very large matrices |\n",
    "| **Householder QR** | O(mnÂ²) | O(mn) | Numerically stable QR |\n",
    "| **Conjugate Gradient** | O(nÂ³) | O(n) | Quadratic optimization |\n",
    "| **SVD (Golub-Reinsch)** | O(mnÂ²) | O(mn) | Matrix analysis |\n",
    "\n",
    "### ðŸš€ Performance Recommendations:\n",
    "\n",
    "1. **For Production**: Always use optimized BLAS/LAPACK libraries\n",
    "2. **For Learning**: Implement from scratch to understand algorithms\n",
    "3. **For Research**: Consider algorithmic improvements and parallelization\n",
    "4. **For Deployment**: Profile and optimize critical paths\n",
    "\n",
    "### ðŸ”¬ Next Steps:\n",
    "- Implement parallel versions using multithreading\n",
    "- Add GPU acceleration with CUDA kernels\n",
    "- Explore iterative methods for large sparse matrices\n",
    "- Study randomized algorithms for approximate solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}