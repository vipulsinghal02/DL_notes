{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML LeetCode - Part 3: Data Structures and Efficiency 🚀\n",
    "\n",
    "This notebook focuses on efficient data structures and algorithms crucial for machine learning at scale. Each problem emphasizes computational efficiency, memory optimization, and scalable solutions.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "- Master efficient nearest neighbor search algorithms\n",
    "- Implement streaming algorithms for large datasets\n",
    "- Optimize memory usage with smart data structures\n",
    "- Handle high-dimensional data efficiently\n",
    "\n",
    "## 📊 Difficulty Levels\n",
    "- 🟢 **Easy**: Basic data structure implementations\n",
    "- 🟡 **Medium**: Optimized algorithms with trade-offs\n",
    "- 🔴 **Hard**: Advanced algorithms for production systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional, Dict, Any, Set\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict, deque\n",
    "import heapq\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "def benchmark_algorithm(func, *args, name=\"Algorithm\", n_runs=5):\n",
    "    \"\"\"Benchmark an algorithm's performance.\"\"\"\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args)\n",
    "        end = time.perf_counter()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    print(f\"⚡ {name}: {avg_time*1000:.3f}ms (±{np.std(times)*1000:.3f}ms)\")\n",
    "    return result, avg_time\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: KD-Tree for Efficient Nearest Neighbor Search 🔴\n",
    "\n",
    "**Difficulty**: Hard\n",
    "\n",
    "**Problem**: Implement a KD-tree data structure for efficient nearest neighbor search in multi-dimensional space.\n",
    "\n",
    "**Constraints**:\n",
    "- 1 ≤ n_points ≤ 100,000\n",
    "- 1 ≤ dimensions ≤ 20\n",
    "- Support k-nearest neighbors and range queries\n",
    "- Handle dynamic insertions and deletions\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "points = [[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]\n",
    "kdtree = KDTree(points)\n",
    "nearest = kdtree.query([9, 2], k=2)\n",
    "# Expected: closest points to [9, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDNode:\n",
    "    def __init__(self, point, axis, left=None, right=None):\n",
    "        self.point = point\n",
    "        self.axis = axis\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class KDTree:\n",
    "    \n",
    "    def __init__(self, points: List[List[float]] = None):\n",
    "        self.root = None\n",
    "        self.dimensions = 0\n",
    "        if points:\n",
    "            self.build(points)\n",
    "    \n",
    "    def build(self, points: List[List[float]]):\n",
    "        \"\"\"\n",
    "        Build KD-tree from list of points.\n",
    "        \n",
    "        Time Complexity: O(n log n)\n",
    "        Space Complexity: O(n)\n",
    "        \"\"\"\n",
    "        if not points:\n",
    "            return\n",
    "        \n",
    "        points = [np.array(p) for p in points]\n",
    "        self.dimensions = len(points[0])\n",
    "        self.root = self._build_recursive(points, depth=0)\n",
    "    \n",
    "    def _build_recursive(self, points: List[np.ndarray], depth: int) -> Optional[KDNode]:\n",
    "        \"\"\"Recursively build KD-tree.\"\"\"\n",
    "        if not points:\n",
    "            return None\n",
    "        \n",
    "        # Select axis based on depth\n",
    "        axis = depth % self.dimensions\n",
    "        \n",
    "        # Sort points by current axis\n",
    "        points.sort(key=lambda x: x[axis])\n",
    "        \n",
    "        # Choose median as pivot\n",
    "        median = len(points) // 2\n",
    "        \n",
    "        # Create node and construct subtrees\n",
    "        node = KDNode(\n",
    "            point=points[median],\n",
    "            axis=axis,\n",
    "            left=self._build_recursive(points[:median], depth + 1),\n",
    "            right=self._build_recursive(points[median + 1:], depth + 1)\n",
    "        )\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def query(self, query_point: List[float], k: int = 1) -> List[Tuple[float, List[float]]]:\n",
    "        \"\"\"\n",
    "        Find k nearest neighbors.\n",
    "        \n",
    "        Time Complexity: O(log n) average, O(n) worst case\n",
    "        Space Complexity: O(log n)\n",
    "        \n",
    "        Returns: List of (distance, point) tuples\n",
    "        \"\"\"\n",
    "        query_point = np.array(query_point)\n",
    "        best_neighbors = []  # Max heap of (distance, point)\n",
    "        \n",
    "        def add_neighbor(point, distance):\n",
    "            if len(best_neighbors) < k:\n",
    "                heapq.heappush(best_neighbors, (-distance, point.tolist()))\n",
    "            elif distance < -best_neighbors[0][0]:\n",
    "                heapq.heappop(best_neighbors)\n",
    "                heapq.heappush(best_neighbors, (-distance, point.tolist()))\n",
    "        \n",
    "        def search_recursive(node):\n",
    "            if node is None:\n",
    "                return\n",
    "            \n",
    "            # Calculate distance to current node\n",
    "            distance = np.linalg.norm(query_point - node.point)\n",
    "            add_neighbor(node.point, distance)\n",
    "            \n",
    "            # Determine which side to search first\n",
    "            axis = node.axis\n",
    "            diff = query_point[axis] - node.point[axis]\n",
    "            \n",
    "            # Search near side first\n",
    "            near_side = node.left if diff <= 0 else node.right\n",
    "            far_side = node.right if diff <= 0 else node.left\n",
    "            \n",
    "            search_recursive(near_side)\n",
    "            \n",
    "            # Check if we need to search far side\n",
    "            if (len(best_neighbors) < k or \n",
    "                abs(diff) < -best_neighbors[0][0]):\n",
    "                search_recursive(far_side)\n",
    "        \n",
    "        search_recursive(self.root)\n",
    "        \n",
    "        # Convert max heap to sorted list (closest first)\n",
    "        result = [(-dist, point) for dist, point in best_neighbors]\n",
    "        result.sort()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def range_query(self, center: List[float], radius: float) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Find all points within radius of center.\n",
    "        \n",
    "        Time Complexity: O(n) worst case\n",
    "        Space Complexity: O(n)\n",
    "        \"\"\"\n",
    "        center = np.array(center)\n",
    "        result = []\n",
    "        \n",
    "        def search_recursive(node):\n",
    "            if node is None:\n",
    "                return\n",
    "            \n",
    "            # Check if current point is in range\n",
    "            distance = np.linalg.norm(center - node.point)\n",
    "            if distance <= radius:\n",
    "                result.append(node.point.tolist())\n",
    "            \n",
    "            # Check if we need to search child nodes\n",
    "            axis = node.axis\n",
    "            diff = center[axis] - node.point[axis]\n",
    "            \n",
    "            if diff <= radius:\n",
    "                search_recursive(node.left)\n",
    "            if diff >= -radius:\n",
    "                search_recursive(node.right)\n",
    "        \n",
    "        search_recursive(self.root)\n",
    "        return result\n",
    "    \n",
    "    def insert(self, point: List[float]):\n",
    "        \"\"\"\n",
    "        Insert a new point into the tree.\n",
    "        Note: This is a simplified insertion that may lead to unbalanced trees.\n",
    "        \"\"\"\n",
    "        point = np.array(point)\n",
    "        if self.root is None:\n",
    "            self.dimensions = len(point)\n",
    "            self.root = KDNode(point, 0)\n",
    "        else:\n",
    "            self._insert_recursive(self.root, point, 0)\n",
    "    \n",
    "    def _insert_recursive(self, node: KDNode, point: np.ndarray, depth: int):\n",
    "        \"\"\"Recursively insert point.\"\"\"\n",
    "        axis = depth % self.dimensions\n",
    "        \n",
    "        if point[axis] <= node.point[axis]:\n",
    "            if node.left is None:\n",
    "                node.left = KDNode(point, (depth + 1) % self.dimensions)\n",
    "            else:\n",
    "                self._insert_recursive(node.left, point, depth + 1)\n",
    "        else:\n",
    "            if node.right is None:\n",
    "                node.right = KDNode(point, (depth + 1) % self.dimensions)\n",
    "            else:\n",
    "                self._insert_recursive(node.right, point, depth + 1)\n",
    "\n",
    "# Test KD-Tree implementation\n",
    "print(\"=== Problem 1: KD-Tree Implementation ===\")\n",
    "\n",
    "# Test case 1: Small 2D dataset\n",
    "points_2d = [[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]\n",
    "kdtree = KDTree(points_2d)\n",
    "\n",
    "query_point = [9, 2]\n",
    "nearest = kdtree.query(query_point, k=3)\n",
    "\n",
    "print(f\"\\nTest Case 1: 2D Nearest Neighbors\")\n",
    "print(f\"Query point: {query_point}\")\n",
    "print(f\"3 nearest neighbors:\")\n",
    "for i, (dist, point) in enumerate(nearest, 1):\n",
    "    print(f\"  {i}. {point} (distance: {dist:.3f})\")\n",
    "\n",
    "# Test range query\n",
    "center = [5, 4]\n",
    "radius = 3.0\n",
    "points_in_range = kdtree.range_query(center, radius)\n",
    "\n",
    "print(f\"\\nRange Query:\")\n",
    "print(f\"Center: {center}, Radius: {radius}\")\n",
    "print(f\"Points in range: {points_in_range}\")\n",
    "\n",
    "# Benchmark against brute force\n",
    "def brute_force_knn(points, query, k):\n",
    "    \"\"\"Brute force k-nearest neighbors.\"\"\"\n",
    "    distances = [(np.linalg.norm(np.array(p) - np.array(query)), p) for p in points]\n",
    "    distances.sort()\n",
    "    return distances[:k]\n",
    "\n",
    "# Generate larger dataset for benchmarking\n",
    "np.random.seed(42)\n",
    "large_points = np.random.randn(1000, 3).tolist()\n",
    "large_kdtree = KDTree(large_points)\n",
    "test_query = [0.5, 0.5, 0.5]\n",
    "\n",
    "print(f\"\\nPerformance Comparison (1000 points, 3D):\")\n",
    "kd_result, kd_time = benchmark_algorithm(large_kdtree.query, test_query, 5, name=\"KD-Tree\")\n",
    "bf_result, bf_time = benchmark_algorithm(brute_force_knn, large_points, test_query, 5, name=\"Brute Force\")\n",
    "\n",
    "print(f\"Speedup: {bf_time/kd_time:.1f}x\")\n",
    "\n",
    "# Verify results match\n",
    "kd_distances = [dist for dist, _ in kd_result]\n",
    "bf_distances = [dist for dist, _ in bf_result]\n",
    "matches = np.allclose(kd_distances, bf_distances, rtol=1e-10)\n",
    "print(f\"Results match: {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Locality Sensitive Hashing (LSH) 🟡\n",
    "\n",
    "**Difficulty**: Medium\n",
    "\n",
    "**Problem**: Implement LSH for approximate nearest neighbor search in high-dimensional spaces.\n",
    "\n",
    "**Constraints**:\n",
    "- 1 ≤ n_points ≤ 50,000\n",
    "- 10 ≤ dimensions ≤ 1000\n",
    "- Support different hash families (random projections, min-hash)\n",
    "- Configurable recall vs speed trade-off\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "points = [[1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 0, 0]]\n",
    "lsh = LSH(n_hashes=10, n_bands=5)\n",
    "lsh.fit(points)\n",
    "candidates = lsh.query([1, 0, 0, 1], max_candidates=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH:\n",
    "    \n",
    "    def __init__(self, n_hashes: int = 10, n_bands: int = 5, hash_family: str = 'random_projection'):\n",
    "        self.n_hashes = n_hashes\n",
    "        self.n_bands = n_bands\n",
    "        self.rows_per_band = n_hashes // n_bands\n",
    "        self.hash_family = hash_family\n",
    "        self.hash_functions = []\n",
    "        self.hash_tables = [defaultdict(list) for _ in range(n_bands)]\n",
    "        self.points = []\n",
    "        self.dimensions = 0\n",
    "    \n",
    "    def _generate_hash_functions(self, dimensions: int):\n",
    "        \"\"\"Generate hash functions based on chosen family.\"\"\"\n",
    "        self.hash_functions = []\n",
    "        \n",
    "        if self.hash_family == 'random_projection':\n",
    "            # Random hyperplanes for cosine similarity\n",
    "            for _ in range(self.n_hashes):\n",
    "                random_vector = np.random.randn(dimensions)\n",
    "                self.hash_functions.append(random_vector)\n",
    "        \n",
    "        elif self.hash_family == 'p_stable':\n",
    "            # p-stable hash for Euclidean distance (p=2)\n",
    "            self.hash_width = 1.0  # Hash bucket width\n",
    "            for _ in range(self.n_hashes):\n",
    "                a = np.random.randn(dimensions)\n",
    "                b = np.random.uniform(0, self.hash_width)\n",
    "                self.hash_functions.append((a, b))\n",
    "    \n",
    "    def _hash_point(self, point: np.ndarray) -> List[int]:\n",
    "        \"\"\"Compute hash values for a point.\"\"\"\n",
    "        hashes = []\n",
    "        \n",
    "        if self.hash_family == 'random_projection':\n",
    "            for random_vector in self.hash_functions:\n",
    "                # Hash is 1 if dot product > 0, else 0\n",
    "                hash_val = 1 if np.dot(point, random_vector) > 0 else 0\n",
    "                hashes.append(hash_val)\n",
    "        \n",
    "        elif self.hash_family == 'p_stable':\n",
    "            for a, b in self.hash_functions:\n",
    "                # Hash = floor((a·x + b) / w)\n",
    "                hash_val = int((np.dot(a, point) + b) / self.hash_width)\n",
    "                hashes.append(hash_val)\n",
    "        \n",
    "        return hashes\n",
    "    \n",
    "    def _create_band_hashes(self, hashes: List[int]) -> List[str]:\n",
    "        \"\"\"Create band hashes from individual hash values.\"\"\"\n",
    "        band_hashes = []\n",
    "        \n",
    "        for band in range(self.n_bands):\n",
    "            start_idx = band * self.rows_per_band\n",
    "            end_idx = min(start_idx + self.rows_per_band, len(hashes))\n",
    "            \n",
    "            # Create composite hash for this band\n",
    "            band_signature = tuple(hashes[start_idx:end_idx])\n",
    "            band_hash = str(hash(band_signature))\n",
    "            band_hashes.append(band_hash)\n",
    "        \n",
    "        return band_hashes\n",
    "    \n",
    "    def fit(self, points: List[List[float]]):\n",
    "        \"\"\"\n",
    "        Build LSH index.\n",
    "        \n",
    "        Time Complexity: O(n * d * h) where h is number of hashes\n",
    "        Space Complexity: O(n * h)\n",
    "        \"\"\"\n",
    "        self.points = [np.array(p) for p in points]\n",
    "        self.dimensions = len(self.points[0])\n",
    "        \n",
    "        # Generate hash functions\n",
    "        self._generate_hash_functions(self.dimensions)\n",
    "        \n",
    "        # Hash all points and add to tables\n",
    "        for point_idx, point in enumerate(self.points):\n",
    "            hashes = self._hash_point(point)\n",
    "            band_hashes = self._create_band_hashes(hashes)\n",
    "            \n",
    "            # Add to each band's hash table\n",
    "            for band_idx, band_hash in enumerate(band_hashes):\n",
    "                self.hash_tables[band_idx][band_hash].append(point_idx)\n",
    "    \n",
    "    def query(self, query_point: List[float], max_candidates: int = 100) -> List[int]:\n",
    "        \"\"\"\n",
    "        Find candidate nearest neighbors.\n",
    "        \n",
    "        Time Complexity: O(d * h + c) where c is number of candidates\n",
    "        Space Complexity: O(c)\n",
    "        \n",
    "        Returns: List of point indices that are candidates\n",
    "        \"\"\"\n",
    "        query_point = np.array(query_point)\n",
    "        candidates = set()\n",
    "        \n",
    "        # Hash query point\n",
    "        hashes = self._hash_point(query_point)\n",
    "        band_hashes = self._create_band_hashes(hashes)\n",
    "        \n",
    "        # Find candidates from each band\n",
    "        for band_idx, band_hash in enumerate(band_hashes):\n",
    "            if band_hash in self.hash_tables[band_idx]:\n",
    "                candidates.update(self.hash_tables[band_idx][band_hash])\n",
    "            \n",
    "            if len(candidates) >= max_candidates:\n",
    "                break\n",
    "        \n",
    "        return list(candidates)[:max_candidates]\n",
    "    \n",
    "    def query_with_distances(self, query_point: List[float], k: int = 5, \n",
    "                           max_candidates: int = 100) -> List[Tuple[float, int]]:\n",
    "        \"\"\"\n",
    "        Find k nearest neighbors from LSH candidates.\n",
    "        \"\"\"\n",
    "        query_point = np.array(query_point)\n",
    "        candidates = self.query(query_point, max_candidates)\n",
    "        \n",
    "        # Calculate actual distances to candidates\n",
    "        distances = []\n",
    "        for candidate_idx in candidates:\n",
    "            candidate_point = self.points[candidate_idx]\n",
    "            distance = np.linalg.norm(query_point - candidate_point)\n",
    "            distances.append((distance, candidate_idx))\n",
    "        \n",
    "        # Return k closest\n",
    "        distances.sort()\n",
    "        return distances[:k]\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the LSH index.\"\"\"\n",
    "        total_buckets = sum(len(table) for table in self.hash_tables)\n",
    "        max_bucket_size = max(max(len(bucket) for bucket in table.values()) \n",
    "                             if table else 0 for table in self.hash_tables)\n",
    "        avg_bucket_size = np.mean([len(bucket) for table in self.hash_tables \n",
    "                                  for bucket in table.values()]) if total_buckets > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_buckets': total_buckets,\n",
    "            'max_bucket_size': max_bucket_size,\n",
    "            'avg_bucket_size': avg_bucket_size,\n",
    "            'n_points': len(self.points),\n",
    "            'dimensions': self.dimensions\n",
    "        }\n",
    "\n",
    "# Test LSH implementation\n",
    "print(\"\\n=== Problem 2: Locality Sensitive Hashing ===\")\n",
    "\n",
    "# Generate high-dimensional test data\n",
    "np.random.seed(42)\n",
    "n_points, n_dims = 1000, 50\n",
    "points_hd = np.random.randn(n_points, n_dims)\n",
    "\n",
    "# Add some clustered structure\n",
    "cluster_centers = np.random.randn(5, n_dims) * 3\n",
    "for i in range(n_points):\n",
    "    cluster_id = i % 5\n",
    "    points_hd[i] += cluster_centers[cluster_id]\n",
    "\n",
    "points_hd_list = points_hd.tolist()\n",
    "\n",
    "print(f\"\\nDataset: {n_points} points in {n_dims} dimensions\")\n",
    "\n",
    "# Test different LSH configurations\n",
    "configs = [\n",
    "    {'n_hashes': 20, 'n_bands': 4, 'hash_family': 'random_projection'},\n",
    "    {'n_hashes': 20, 'n_bands': 10, 'hash_family': 'random_projection'},\n",
    "    {'n_hashes': 20, 'n_bands': 4, 'hash_family': 'p_stable'}\n",
    "]\n",
    "\n",
    "results = {}\n",
    "query_point = points_hd[0]  # Use first point as query\n",
    "true_knn = 5\n",
    "\n",
    "# Brute force for ground truth\n",
    "def brute_force_knn_hd(points, query, k):\n",
    "    distances = [(np.linalg.norm(p - query), i) for i, p in enumerate(points)]\n",
    "    distances.sort()\n",
    "    return distances[:k]\n",
    "\n",
    "true_neighbors = brute_force_knn_hd(points_hd, query_point, true_knn)\n",
    "true_indices = {idx for _, idx in true_neighbors}\n",
    "\n",
    "print(f\"\\nTrue {true_knn}-NN indices: {sorted(true_indices)}\")\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"\\nConfiguration {i+1}: {config}\")\n",
    "    \n",
    "    # Build LSH index\n",
    "    lsh = LSH(**config)\n",
    "    build_time = time.perf_counter()\n",
    "    lsh.fit(points_hd_list)\n",
    "    build_time = time.perf_counter() - build_time\n",
    "    \n",
    "    # Query\n",
    "    query_time = time.perf_counter()\n",
    "    lsh_neighbors = lsh.query_with_distances(query_point.tolist(), k=true_knn, max_candidates=200)\n",
    "    query_time = time.perf_counter() - query_time\n",
    "    \n",
    "    lsh_indices = {idx for _, idx in lsh_neighbors}\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = len(true_indices.intersection(lsh_indices)) / len(true_indices)\n",
    "    \n",
    "    # Get LSH stats\n",
    "    stats = lsh.get_stats()\n",
    "    \n",
    "    results[i] = {\n",
    "        'config': config,\n",
    "        'recall': recall,\n",
    "        'build_time': build_time,\n",
    "        'query_time': query_time,\n",
    "        'stats': stats\n",
    "    }\n",
    "    \n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  Build time: {build_time*1000:.2f}ms\")\n",
    "    print(f\"  Query time: {query_time*1000:.4f}ms\")\n",
    "    print(f\"  Avg bucket size: {stats['avg_bucket_size']:.1f}\")\n",
    "    print(f\"  LSH neighbors: {sorted(lsh_indices)}\")\n",
    "\n",
    "# Visualize recall vs speed trade-off\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Recall vs Query Time\n",
    "recalls = [results[i]['recall'] for i in range(len(configs))]\n",
    "query_times = [results[i]['query_time'] * 1000 for i in range(len(configs))]\n",
    "config_labels = [f\"Config {i+1}\" for i in range(len(configs))]\n",
    "\n",
    "ax1.scatter(query_times, recalls, s=100, alpha=0.7)\n",
    "for i, label in enumerate(config_labels):\n",
    "    ax1.annotate(label, (query_times[i], recalls[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax1.set_xlabel('Query Time (ms)')\n",
    "ax1.set_ylabel('Recall')\n",
    "ax1.set_title('LSH Recall vs Speed Trade-off')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bucket size comparison\n",
    "bucket_sizes = [results[i]['stats']['avg_bucket_size'] for i in range(len(configs))]\n",
    "ax2.bar(config_labels, bucket_sizes, alpha=0.7)\n",
    "ax2.set_ylabel('Average Bucket Size')\n",
    "ax2.set_title('Hash Table Bucket Sizes')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 LSH Analysis:\")\n",
    "print(\"• More bands → Higher recall but slower queries\")\n",
    "print(\"• Random projection good for cosine similarity\")\n",
    "print(\"• p-stable hashing good for Euclidean distance\")\n",
    "print(\"• Trade-off between recall and speed is configurable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Count-Min Sketch for Streaming Data 🟡\n",
    "\n",
    "**Difficulty**: Medium\n",
    "\n",
    "**Problem**: Implement Count-Min Sketch for approximate frequency counting in data streams.\n",
    "\n",
    "**Constraints**:\n",
    "- Memory usage: O(ε⁻¹ log δ⁻¹)\n",
    "- Error guarantee: |count(x) - count*(x)| ≤ ε·N with probability 1-δ\n",
    "- Support for point queries and heavy hitter detection\n",
    "- Handle arbitrary data types as items\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "cms = CountMinSketch(epsilon=0.01, delta=0.01)\n",
    "for item in stream:\n",
    "    cms.add(item)\n",
    "estimated_count = cms.query(\"frequent_item\")\n",
    "heavy_hitters = cms.get_heavy_hitters(threshold=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3  # MurmurHash3 for better hash distribution\n",
    "\n",
    "class CountMinSketch:\n",
    "    \n",
    "    def __init__(self, epsilon: float = 0.01, delta: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initialize Count-Min Sketch.\n",
    "        \n",
    "        Args:\n",
    "            epsilon: Relative error (smaller = more accurate)\n",
    "            delta: Failure probability (smaller = more reliable)\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        \n",
    "        # Calculate dimensions based on error parameters\n",
    "        self.width = int(math.ceil(math.e / epsilon))  # Number of buckets per hash\n",
    "        self.height = int(math.ceil(math.log(1 / delta)))  # Number of hash functions\n",
    "        \n",
    "        # Initialize count matrix\n",
    "        self.counts = np.zeros((self.height, self.width), dtype=np.int64)\n",
    "        \n",
    "        # Generate hash function seeds\n",
    "        random.seed(42)  # For reproducibility\n",
    "        self.hash_seeds = [random.randint(0, 2**31 - 1) for _ in range(self.height)]\n",
    "        \n",
    "        # Track total number of items\n",
    "        self.total_count = 0\n",
    "        \n",
    "        print(f\"Count-Min Sketch: {self.height}x{self.width} matrix\")\n",
    "        print(f\"Memory usage: {self.height * self.width * 8} bytes\")\n",
    "    \n",
    "    def _hash(self, item: Any, seed: int) -> int:\n",
    "        \"\"\"Hash function for arbitrary items.\"\"\"\n",
    "        # Convert item to string for hashing\n",
    "        item_str = str(item).encode('utf-8')\n",
    "        \n",
    "        # Use MurmurHash3 for good distribution\n",
    "        try:\n",
    "            hash_val = mmh3.hash(item_str, seed, signed=False)\n",
    "        except:\n",
    "            # Fallback to built-in hash if mmh3 not available\n",
    "            hash_val = hash((item_str, seed)) % (2**32)\n",
    "        \n",
    "        return hash_val % self.width\n",
    "    \n",
    "    def add(self, item: Any, count: int = 1):\n",
    "        \"\"\"\n",
    "        Add item to the sketch.\n",
    "        \n",
    "        Time Complexity: O(height)\n",
    "        Space Complexity: O(1)\n",
    "        \"\"\"\n",
    "        for i in range(self.height):\n",
    "            j = self._hash(item, self.hash_seeds[i])\n",
    "            self.counts[i, j] += count\n",
    "        \n",
    "        self.total_count += count\n",
    "    \n",
    "    def query(self, item: Any) -> int:\n",
    "        \"\"\"\n",
    "        Estimate count of item.\n",
    "        \n",
    "        Time Complexity: O(height)\n",
    "        Space Complexity: O(1)\n",
    "        \n",
    "        Returns: Estimated count (upper bound)\n",
    "        \"\"\"\n",
    "        min_count = float('inf')\n",
    "        \n",
    "        for i in range(self.height):\n",
    "            j = self._hash(item, self.hash_seeds[i])\n",
    "            min_count = min(min_count, self.counts[i, j])\n",
    "        \n",
    "        return int(min_count)\n",
    "    \n",
    "    def get_heavy_hitters(self, threshold: int) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Find potential heavy hitters above threshold.\n",
    "        Note: This is a simplified version that checks bucket values.\n",
    "        \"\"\"\n",
    "        candidates = set()\n",
    "        \n",
    "        # Find buckets with counts above threshold\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                if self.counts[i, j] >= threshold:\n",
    "                    # This bucket might contain heavy hitters\n",
    "                    candidates.add((i, j, self.counts[i, j]))\n",
    "        \n",
    "        # Return unique candidate buckets\n",
    "        heavy_hitters = [(f\"bucket_{i}_{j}\", count) for i, j, count in candidates]\n",
    "        heavy_hitters.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return heavy_hitters\n",
    "    \n",
    "    def merge(self, other: 'CountMinSketch') -> 'CountMinSketch':\n",
    "        \"\"\"\n",
    "        Merge two Count-Min Sketches.\n",
    "        Both sketches must have same dimensions.\n",
    "        \"\"\"\n",
    "        if (self.height != other.height or \n",
    "            self.width != other.width or \n",
    "            self.hash_seeds != other.hash_seeds):\n",
    "            raise ValueError(\"Sketches must have same parameters for merging\")\n",
    "        \n",
    "        merged = CountMinSketch(self.epsilon, self.delta)\n",
    "        merged.height = self.height\n",
    "        merged.width = self.width\n",
    "        merged.hash_seeds = self.hash_seeds\n",
    "        merged.counts = self.counts + other.counts\n",
    "        merged.total_count = self.total_count + other.total_count\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the sketch.\"\"\"\n",
    "        return {\n",
    "            'dimensions': (self.height, self.width),\n",
    "            'total_count': self.total_count,\n",
    "            'memory_bytes': self.height * self.width * 8,\n",
    "            'epsilon': self.epsilon,\n",
    "            'delta': self.delta,\n",
    "            'max_bucket_count': int(np.max(self.counts)),\n",
    "            'avg_bucket_count': float(np.mean(self.counts))\n",
    "        }\n",
    "\n",
    "# Test Count-Min Sketch\n",
    "print(\"\\n=== Problem 3: Count-Min Sketch ===\")\n",
    "\n",
    "# Simulate a data stream with Zipfian distribution\n",
    "def generate_zipfian_stream(n_items: int, n_unique: int, alpha: float = 1.0) -> List[str]:\n",
    "    \"\"\"Generate stream with Zipfian frequency distribution.\"\"\"\n",
    "    # Generate frequencies according to Zipf's law\n",
    "    frequencies = np.array([1/i**alpha for i in range(1, n_unique + 1)])\n",
    "    frequencies = frequencies / frequencies.sum()\n",
    "    \n",
    "    # Generate items\n",
    "    items = [f\"item_{i}\" for i in range(n_unique)]\n",
    "    stream = np.random.choice(items, size=n_items, p=frequencies)\n",
    "    \n",
    "    return stream.tolist()\n",
    "\n",
    "# Generate test stream\n",
    "np.random.seed(42)\n",
    "stream_size = 10000\n",
    "unique_items = 1000\n",
    "stream = generate_zipfian_stream(stream_size, unique_items, alpha=1.2)\n",
    "\n",
    "print(f\"\\nStream: {stream_size} items, {unique_items} unique\")\n",
    "\n",
    "# True counts for comparison\n",
    "true_counts = Counter(stream)\n",
    "most_frequent = true_counts.most_common(10)\n",
    "\n",
    "print(\"\\nTrue top 10 items:\")\n",
    "for item, count in most_frequent:\n",
    "    print(f\"  {item}: {count}\")\n",
    "\n",
    "# Test different sketch configurations\n",
    "configs = [\n",
    "    {'epsilon': 0.01, 'delta': 0.01},  # High accuracy\n",
    "    {'epsilon': 0.05, 'delta': 0.05},  # Medium accuracy\n",
    "    {'epsilon': 0.1, 'delta': 0.1}    # Lower accuracy, less memory\n",
    "]\n",
    "\n",
    "sketch_results = []\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"\\nConfiguration {i+1}: ε={config['epsilon']}, δ={config['delta']}\")\n",
    "    \n",
    "    # Build sketch\n",
    "    cms = CountMinSketch(**config)\n",
    "    \n",
    "    # Process stream\n",
    "    start_time = time.perf_counter()\n",
    "    for item in stream:\n",
    "        cms.add(item)\n",
    "    processing_time = time.perf_counter() - start_time\n",
    "    \n",
    "    # Test queries on most frequent items\n",
    "    errors = []\n",
    "    for item, true_count in most_frequent:\n",
    "        estimated_count = cms.query(item)\n",
    "        error = abs(estimated_count - true_count) / true_count\n",
    "        errors.append(error)\n",
    "    \n",
    "    avg_error = np.mean(errors)\n",
    "    max_error = np.max(errors)\n",
    "    \n",
    "    stats = cms.get_stats()\n",
    "    \n",
    "    sketch_results.append({\n",
    "        'config': config,\n",
    "        'avg_error': avg_error,\n",
    "        'max_error': max_error,\n",
    "        'processing_time': processing_time,\n",
    "        'memory_bytes': stats['memory_bytes']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Processing time: {processing_time*1000:.2f}ms\")\n",
    "    print(f\"  Memory usage: {stats['memory_bytes']} bytes\")\n",
    "    print(f\"  Average error: {avg_error:.3f}\")\n",
    "    print(f\"  Maximum error: {max_error:.3f}\")\n",
    "    \n",
    "    # Show estimates for top items\n",
    "    print(f\"  Estimates for top 5 items:\")\n",
    "    for item, true_count in most_frequent[:5]:\n",
    "        estimated = cms.query(item)\n",
    "        error_pct = (estimated - true_count) / true_count * 100\n",
    "        print(f\"    {item}: {estimated} (true: {true_count}, error: {error_pct:+.1f}%)\")\n",
    "\n",
    "# Visualize accuracy vs memory trade-off\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Error vs Memory\n",
    "memory_usage = [r['memory_bytes'] for r in sketch_results]\n",
    "avg_errors = [r['avg_error'] for r in sketch_results]\n",
    "config_labels = [f\"ε={r['config']['epsilon']}\" for r in sketch_results]\n",
    "\n",
    "ax1.scatter(memory_usage, avg_errors, s=100, alpha=0.7)\n",
    "for i, label in enumerate(config_labels):\n",
    "    ax1.annotate(label, (memory_usage[i], avg_errors[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax1.set_xlabel('Memory Usage (bytes)')\n",
    "ax1.set_ylabel('Average Relative Error')\n",
    "ax1.set_title('Count-Min Sketch: Accuracy vs Memory')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Processing speed\n",
    "processing_times = [r['processing_time'] * 1000 for r in sketch_results]\n",
    "ax2.bar(config_labels, processing_times, alpha=0.7)\n",
    "ax2.set_ylabel('Processing Time (ms)')\n",
    "ax2.set_title('Stream Processing Speed')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Count-Min Sketch Analysis:\")\n",
    "print(\"• Probabilistic data structure for frequency estimation\")\n",
    "print(\"• Always overestimates (provides upper bound)\")\n",
    "print(\"• Error decreases with smaller ε (more memory)\")\n",
    "print(\"• Excellent for finding heavy hitters in streams\")\n",
    "print(\"• Mergeable for distributed computing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Bloom Filter for Set Membership 🟢\n",
    "\n",
    "**Difficulty**: Easy\n",
    "\n",
    "**Problem**: Implement a Bloom filter for space-efficient approximate set membership testing.\n",
    "\n",
    "**Constraints**:\n",
    "- Support configurable false positive rate\n",
    "- Memory usage: O(m) where m is optimal based on n and desired FP rate\n",
    "- No false negatives allowed\n",
    "- Support union operation\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "bf = BloomFilter(capacity=1000, error_rate=0.01)\n",
    "bf.add(\"item1\")\n",
    "bf.add(\"item2\")\n",
    "if \"item1\" in bf:  # Definitely in set\n",
    "    print(\"Found\")\n",
    "if \"item3\" in bf:  # Might be false positive\n",
    "    print(\"Maybe found\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomFilter:\n",
    "    \n",
    "    def __init__(self, capacity: int, error_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initialize Bloom Filter.\n",
    "        \n",
    "        Args:\n",
    "            capacity: Expected number of items\n",
    "            error_rate: Desired false positive rate\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.error_rate = error_rate\n",
    "        \n",
    "        # Calculate optimal parameters\n",
    "        self.size = self._optimal_size(capacity, error_rate)\n",
    "        self.hash_count = self._optimal_hash_count(self.size, capacity)\n",
    "        \n",
    "        # Initialize bit array\n",
    "        self.bit_array = np.zeros(self.size, dtype=bool)\n",
    "        \n",
    "        # Generate hash function seeds\n",
    "        random.seed(42)\n",
    "        self.hash_seeds = [random.randint(0, 2**31 - 1) for _ in range(self.hash_count)]\n",
    "        \n",
    "        # Track number of items added\n",
    "        self.items_added = 0\n",
    "        \n",
    "        print(f\"Bloom Filter: {self.size} bits, {self.hash_count} hash functions\")\n",
    "        print(f\"Expected FP rate: {self.error_rate:.4f}\")\n",
    "    \n",
    "    def _optimal_size(self, n: int, p: float) -> int:\n",
    "        \"\"\"Calculate optimal bit array size.\"\"\"\n",
    "        m = -(n * math.log(p)) / (math.log(2) ** 2)\n",
    "        return int(math.ceil(m))\n",
    "    \n",
    "    def _optimal_hash_count(self, m: int, n: int) -> int:\n",
    "        \"\"\"Calculate optimal number of hash functions.\"\"\"\n",
    "        k = (m / n) * math.log(2)\n",
    "        return int(math.ceil(k))\n",
    "    \n",
    "    def _hash(self, item: Any, seed: int) -> int:\n",
    "        \"\"\"Hash function for arbitrary items.\"\"\"\n",
    "        item_str = str(item).encode('utf-8')\n",
    "        \n",
    "        try:\n",
    "            hash_val = mmh3.hash(item_str, seed, signed=False)\n",
    "        except:\n",
    "            hash_val = hash((item_str, seed)) % (2**32)\n",
    "        \n",
    "        return hash_val % self.size\n",
    "    \n",
    "    def add(self, item: Any):\n",
    "        \"\"\"\n",
    "        Add item to the filter.\n",
    "        \n",
    "        Time Complexity: O(k) where k is number of hash functions\n",
    "        Space Complexity: O(1)\n",
    "        \"\"\"\n",
    "        for seed in self.hash_seeds:\n",
    "            index = self._hash(item, seed)\n",
    "            self.bit_array[index] = True\n",
    "        \n",
    "        self.items_added += 1\n",
    "    \n",
    "    def __contains__(self, item: Any) -> bool:\n",
    "        \"\"\"\n",
    "        Test membership (might have false positives).\n",
    "        \n",
    "        Time Complexity: O(k)\n",
    "        Space Complexity: O(1)\n",
    "        \"\"\"\n",
    "        for seed in self.hash_seeds:\n",
    "            index = self._hash(item, seed)\n",
    "            if not self.bit_array[index]:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def union(self, other: 'BloomFilter') -> 'BloomFilter':\n",
    "        \"\"\"\n",
    "        Union of two Bloom filters.\n",
    "        Both filters must have same parameters.\n",
    "        \"\"\"\n",
    "        if (self.size != other.size or \n",
    "            self.hash_count != other.hash_count or \n",
    "            self.hash_seeds != other.hash_seeds):\n",
    "            raise ValueError(\"Filters must have same parameters for union\")\n",
    "        \n",
    "        result = BloomFilter(self.capacity, self.error_rate)\n",
    "        result.size = self.size\n",
    "        result.hash_count = self.hash_count\n",
    "        result.hash_seeds = self.hash_seeds\n",
    "        result.bit_array = self.bit_array | other.bit_array\n",
    "        result.items_added = self.items_added + other.items_added\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def false_positive_rate(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate current false positive rate.\n",
    "        \"\"\"\n",
    "        if self.items_added == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Fraction of bits that are set\n",
    "        fraction_set = np.sum(self.bit_array) / self.size\n",
    "        \n",
    "        # Estimated FP rate\n",
    "        fp_rate = fraction_set ** self.hash_count\n",
    "        return fp_rate\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the filter.\"\"\"\n",
    "        bits_set = int(np.sum(self.bit_array))\n",
    "        load_factor = bits_set / self.size\n",
    "        \n",
    "        return {\n",
    "            'size': self.size,\n",
    "            'hash_count': self.hash_count,\n",
    "            'items_added': self.items_added,\n",
    "            'bits_set': bits_set,\n",
    "            'load_factor': load_factor,\n",
    "            'expected_fp_rate': self.error_rate,\n",
    "            'actual_fp_rate': self.false_positive_rate(),\n",
    "            'memory_bytes': self.size // 8\n",
    "        }\n",
    "\n",
    "# Test Bloom Filter\n",
    "print(\"\\n=== Problem 4: Bloom Filter ===\")\n",
    "\n",
    "# Test different configurations\n",
    "test_items = [f\"item_{i}\" for i in range(1000)]\n",
    "non_existent_items = [f\"missing_{i}\" for i in range(500)]\n",
    "\n",
    "configs = [\n",
    "    {'capacity': 1000, 'error_rate': 0.001},  # Very low FP rate\n",
    "    {'capacity': 1000, 'error_rate': 0.01},   # Standard FP rate\n",
    "    {'capacity': 1000, 'error_rate': 0.1}     # High FP rate, less memory\n",
    "]\n",
    "\n",
    "bloom_results = []\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"\\nConfiguration {i+1}: capacity={config['capacity']}, error_rate={config['error_rate']}\")\n",
    "    \n",
    "    # Create and populate filter\n",
    "    bf = BloomFilter(**config)\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    for item in test_items:\n",
    "        bf.add(item)\n",
    "    add_time = time.perf_counter() - start_time\n",
    "    \n",
    "    # Test membership for existing items (should all be True)\n",
    "    start_time = time.perf_counter()\n",
    "    existing_results = [item in bf for item in test_items[:100]]\n",
    "    existing_time = time.perf_counter() - start_time\n",
    "    \n",
    "    # Test membership for non-existent items (count false positives)\n",
    "    start_time = time.perf_counter()\n",
    "    fp_results = [item in bf for item in non_existent_items]\n",
    "    query_time = time.perf_counter() - start_time\n",
    "    \n",
    "    false_positives = sum(fp_results)\n",
    "    fp_rate = false_positives / len(non_existent_items)\n",
    "    \n",
    "    stats = bf.get_stats()\n",
    "    \n",
    "    bloom_results.append({\n",
    "        'config': config,\n",
    "        'measured_fp_rate': fp_rate,\n",
    "        'expected_fp_rate': config['error_rate'],\n",
    "        'add_time': add_time,\n",
    "        'query_time': query_time,\n",
    "        'memory_bytes': stats['memory_bytes'],\n",
    "        'load_factor': stats['load_factor']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Memory usage: {stats['memory_bytes']} bytes\")\n",
    "    print(f\"  Add time: {add_time*1000:.2f}ms for {len(test_items)} items\")\n",
    "    print(f\"  Query time: {query_time*1000:.3f}ms for {len(non_existent_items)} items\")\n",
    "    print(f\"  False positives: {false_positives}/{len(non_existent_items)} ({fp_rate:.4f})\")\n",
    "    print(f\"  Expected FP rate: {config['error_rate']:.4f}\")\n",
    "    print(f\"  Load factor: {stats['load_factor']:.3f}\")\n",
    "    \n",
    "    # Verify no false negatives\n",
    "    false_negatives = sum(1 for item in test_items[:100] if item not in bf)\n",
    "    print(f\"  False negatives: {false_negatives} (should be 0)\")\n",
    "\n",
    "# Test union operation\n",
    "print(f\"\\nTesting Union Operation:\")\n",
    "bf1 = BloomFilter(capacity=500, error_rate=0.01)\n",
    "bf2 = BloomFilter(capacity=500, error_rate=0.01)\n",
    "\n",
    "# Add different items to each filter\n",
    "items1 = [f\"set1_item_{i}\" for i in range(250)]\n",
    "items2 = [f\"set2_item_{i}\" for i in range(250)]\n",
    "\n",
    "for item in items1:\n",
    "    bf1.add(item)\n",
    "for item in items2:\n",
    "    bf2.add(item)\n",
    "\n",
    "# Union filters\n",
    "bf_union = bf1.union(bf2)\n",
    "\n",
    "# Test that union contains items from both sets\n",
    "items1_in_union = sum(1 for item in items1[:50] if item in bf_union)\n",
    "items2_in_union = sum(1 for item in items2[:50] if item in bf_union)\n",
    "\n",
    "print(f\"Items from set1 found in union: {items1_in_union}/50\")\n",
    "print(f\"Items from set2 found in union: {items2_in_union}/50\")\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# FP Rate Comparison\n",
    "expected_rates = [r['expected_fp_rate'] for r in bloom_results]\n",
    "measured_rates = [r['measured_fp_rate'] for r in bloom_results]\n",
    "config_labels = [f\"ε={r['config']['error_rate']}\" for r in bloom_results]\n",
    "\n",
    "x = np.arange(len(config_labels))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, expected_rates, width, label='Expected', alpha=0.7)\n",
    "ax1.bar(x + width/2, measured_rates, width, label='Measured', alpha=0.7)\n",
    "\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('False Positive Rate')\n",
    "ax1.set_title('Bloom Filter: Expected vs Measured FP Rate')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(config_labels)\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Memory vs Accuracy\n",
    "memory_usage = [r['memory_bytes'] for r in bloom_results]\n",
    "ax2.scatter(memory_usage, measured_rates, s=100, alpha=0.7)\n",
    "for i, label in enumerate(config_labels):\n",
    "    ax2.annotate(label, (memory_usage[i], measured_rates[i]), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax2.set_xlabel('Memory Usage (bytes)')\n",
    "ax2.set_ylabel('False Positive Rate')\n",
    "ax2.set_title('Memory vs Accuracy Trade-off')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Bloom Filter Analysis:\")\n",
    "print(\"• No false negatives, only false positives\")\n",
    "print(\"• Memory usage is independent of item size\")\n",
    "print(\"• Lower error rate requires more memory\")\n",
    "print(\"• Excellent for cache filtering and duplicate detection\")\n",
    "print(\"• Supports set operations (union, but not intersection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Performance Analysis 📈\n",
    "\n",
    "### 🏆 Data Structures Implemented:\n",
    "\n",
    "1. **KD-Tree** 🔴\n",
    "   - Efficient nearest neighbor search in low-medium dimensions\n",
    "   - **Key Insight**: Performance degrades in high dimensions (curse of dimensionality)\n",
    "   - **Best Use**: 2D-10D spatial data, exact nearest neighbors\n",
    "\n",
    "2. **Locality Sensitive Hashing (LSH)** 🟡\n",
    "   - Approximate nearest neighbors in high dimensions\n",
    "   - **Key Insight**: Trade recall for speed, configurable parameters\n",
    "   - **Best Use**: High-dimensional data, approximate similarity search\n",
    "\n",
    "3. **Count-Min Sketch** 🟡\n",
    "   - Frequency estimation in data streams\n",
    "   - **Key Insight**: Always overestimates, tunable accuracy vs memory\n",
    "   - **Best Use**: Heavy hitter detection, streaming analytics\n",
    "\n",
    "4. **Bloom Filter** 🟢\n",
    "   - Space-efficient set membership testing\n",
    "   - **Key Insight**: No false negatives, controllable false positive rate\n",
    "   - **Best Use**: Cache filtering, duplicate detection\n",
    "\n",
    "### ⚡ Complexity Analysis:\n",
    "\n",
    "| Data Structure | Construction | Query | Space | Best Dimension |\n",
    "|----------------|-------------|-------|-------|----------------|\n",
    "| **KD-Tree** | O(n log n) | O(log n) avg | O(n) | Low (≤10) |\n",
    "| **LSH** | O(nhd) | O(h + c) | O(nh) | High (≥50) |\n",
    "| **Count-Min** | O(1) | O(d) | O(1/ε log 1/δ) | Streaming |\n",
    "| **Bloom Filter** | O(1) | O(k) | O(m) | Set membership |\n",
    "\n",
    "### 🎯 Key Implementation Insights:\n",
    "\n",
    "1. **Dimension Sensitivity**: KD-trees excel in low dimensions, LSH in high dimensions\n",
    "2. **Accuracy Trade-offs**: Probabilistic structures offer configurable accuracy\n",
    "3. **Memory Efficiency**: Sketches and filters use sub-linear space\n",
    "4. **Streaming Capability**: Count-Min Sketch handles infinite streams\n",
    "\n",
    "### 🚀 Production Considerations:\n",
    "\n",
    "1. **Choose the Right Tool**:\n",
    "   - KD-Tree: Exact NN in ≤10D\n",
    "   - LSH: Approximate NN in high-D\n",
    "   - Count-Min: Frequency queries\n",
    "   - Bloom Filter: Membership tests\n",
    "\n",
    "2. **Parameter Tuning**:\n",
    "   - Balance accuracy vs memory vs speed\n",
    "   - Consider data distribution and query patterns\n",
    "   - Profile with realistic workloads\n",
    "\n",
    "3. **Scalability**:\n",
    "   - Distributed versions available for most structures\n",
    "   - Consider mergeable properties for parallel processing\n",
    "   - Plan for incremental updates vs rebuilds\n",
    "\n",
    "### 📚 Next Steps:\n",
    "- Implement advanced variants (Ball trees, Random projection trees)\n",
    "- Study distributed versions (Cassandra's Bloom filters, Redis HyperLogLog)\n",
    "- Explore learned indices and ML-optimized data structures\n",
    "- Practice with real-world datasets and benchmarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}