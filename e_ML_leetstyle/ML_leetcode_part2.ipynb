{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML LeetCode - Part 2: Core ML Algorithms from Scratch ü§ñ\n",
    "\n",
    "This notebook contains algorithmic challenges focused on implementing fundamental machine learning algorithms from scratch. Each problem follows the LeetCode format with multiple solution approaches and complexity analysis.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Implement classic ML algorithms without libraries\n",
    "- Understand algorithmic complexity and optimization\n",
    "- Master different approaches to the same problem\n",
    "- Practice efficient coding for ML problems\n",
    "\n",
    "## üìä Difficulty Levels\n",
    "- üü¢ **Easy**: Basic implementations\n",
    "- üü° **Medium**: Optimized versions with edge cases\n",
    "- üî¥ **Hard**: Advanced algorithms with multiple optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import heapq\n",
    "\n",
    "def test_algorithm(func, test_cases, name=\"Algorithm\"):\n",
    "    \"\"\"Test an algorithm with multiple test cases.\"\"\"\n",
    "    print(f\"\\nüß™ Testing {name}:\")\n",
    "    for i, (inputs, expected) in enumerate(test_cases, 1):\n",
    "        try:\n",
    "            result = func(*inputs)\n",
    "            if isinstance(expected, (list, np.ndarray)):\n",
    "                passed = np.allclose(result, expected, rtol=1e-6)\n",
    "            else:\n",
    "                passed = abs(result - expected) < 1e-6 if isinstance(result, (int, float)) else result == expected\n",
    "            status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "            print(f\"  {status} Test {i}: Expected {expected}, Got {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Test {i}: Error - {e}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: K-Means Clustering Implementation üü°\n",
    "\n",
    "**Difficulty**: Medium\n",
    "\n",
    "**Problem**: Implement K-means clustering algorithm with multiple initialization strategies and convergence criteria.\n",
    "\n",
    "**Constraints**:\n",
    "- 1 ‚â§ n_samples ‚â§ 1000\n",
    "- 1 ‚â§ n_features ‚â§ 50\n",
    "- 1 ‚â§ k ‚â§ min(20, n_samples)\n",
    "- max_iters ‚â§ 1000\n",
    "- Must handle edge cases (empty clusters, convergence)\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "X = [[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]\n",
    "k = 2\n",
    "labels, centers = kmeans(X, k)\n",
    "# Expected: Two clusters around [1, 2] and [10, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansImplementation:\n",
    "    \n",
    "    def __init__(self, k: int, max_iters: int = 100, tol: float = 1e-4, init: str = 'random'):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.init = init\n",
    "    \n",
    "    def fit(self, X: List[List[float]]) -> Tuple[List[int], List[List[float]]]:\n",
    "        \"\"\"\n",
    "        K-means clustering algorithm.\n",
    "        \n",
    "        Time Complexity: O(n*k*d*iterations)\n",
    "        Space Complexity: O(n + k*d)\n",
    "        \n",
    "        Returns:\n",
    "            labels: Cluster assignment for each point\n",
    "            centers: Final cluster centers\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if self.k > n_samples:\n",
    "            raise ValueError(f\"k ({self.k}) cannot be larger than n_samples ({n_samples})\")\n",
    "        \n",
    "        # Initialize centers\n",
    "        centers = self._init_centers(X)\n",
    "        \n",
    "        for iteration in range(self.max_iters):\n",
    "            # Assign points to nearest centers\n",
    "            labels = self._assign_clusters(X, centers)\n",
    "            \n",
    "            # Update centers\n",
    "            new_centers = self._update_centers(X, labels)\n",
    "            \n",
    "            # Check convergence\n",
    "            center_shift = np.max([np.linalg.norm(new_centers[i] - centers[i]) \n",
    "                                  for i in range(self.k)])\n",
    "            \n",
    "            centers = new_centers\n",
    "            \n",
    "            if center_shift < self.tol:\n",
    "                break\n",
    "        \n",
    "        return labels.tolist(), centers.tolist()\n",
    "    \n",
    "    def _init_centers(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Initialize cluster centers.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if self.init == 'random':\n",
    "            # Random initialization\n",
    "            indices = np.random.choice(n_samples, self.k, replace=False)\n",
    "            return X[indices].copy()\n",
    "        \n",
    "        elif self.init == 'kmeans++':\n",
    "            # K-means++ initialization\n",
    "            centers = np.zeros((self.k, n_features))\n",
    "            \n",
    "            # Choose first center randomly\n",
    "            centers[0] = X[np.random.choice(n_samples)]\n",
    "            \n",
    "            for i in range(1, self.k):\n",
    "                # Calculate distances to nearest center\n",
    "                distances = np.array([min([np.linalg.norm(x - c)**2 \n",
    "                                          for c in centers[:i]]) for x in X])\n",
    "                \n",
    "                # Choose next center with probability proportional to squared distance\n",
    "                probabilities = distances / distances.sum()\n",
    "                centers[i] = X[np.random.choice(n_samples, p=probabilities)]\n",
    "            \n",
    "            return centers\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown initialization method: {self.init}\")\n",
    "    \n",
    "    def _assign_clusters(self, X: np.ndarray, centers: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Assign each point to the nearest cluster center.\"\"\"\n",
    "        distances = np.sqrt(((X - centers[:, np.newaxis])**2).sum(axis=2))\n",
    "        return np.argmin(distances, axis=0)\n",
    "    \n",
    "    def _update_centers(self, X: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Update cluster centers based on current assignments.\"\"\"\n",
    "        centers = np.zeros((self.k, X.shape[1]))\n",
    "        \n",
    "        for i in range(self.k):\n",
    "            mask = labels == i\n",
    "            if np.any(mask):\n",
    "                centers[i] = X[mask].mean(axis=0)\n",
    "            else:\n",
    "                # Handle empty cluster by reinitializing\n",
    "                centers[i] = X[np.random.choice(len(X))]\n",
    "        \n",
    "        return centers\n",
    "    \n",
    "    def inertia(self, X: List[List[float]], labels: List[int], centers: List[List[float]]) -> float:\n",
    "        \"\"\"Calculate within-cluster sum of squares.\"\"\"\n",
    "        X = np.array(X)\n",
    "        centers = np.array(centers)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        inertia = 0\n",
    "        for i in range(self.k):\n",
    "            mask = labels == i\n",
    "            if np.any(mask):\n",
    "                inertia += np.sum((X[mask] - centers[i])**2)\n",
    "        \n",
    "        return inertia\n",
    "\n",
    "# Test K-means implementation\n",
    "print(\"=== Problem 1: K-Means Clustering ===\")\n",
    "\n",
    "# Test case 1: Simple 2D data\n",
    "X1 = [[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]]\n",
    "kmeans1 = KMeansImplementation(k=2, init='random')\n",
    "labels1, centers1 = kmeans1.fit(X1)\n",
    "\n",
    "print(f\"\\nTest Case 1: 2D data with k=2\")\n",
    "print(f\"Labels: {labels1}\")\n",
    "print(f\"Centers: {[[round(x, 2) for x in center] for center in centers1]}\")\n",
    "print(f\"Inertia: {kmeans1.inertia(X1, labels1, centers1):.2f}\")\n",
    "\n",
    "# Test case 2: K-means++ initialization\n",
    "kmeans2 = KMeansImplementation(k=2, init='kmeans++')\n",
    "labels2, centers2 = kmeans2.fit(X1)\n",
    "\n",
    "print(f\"\\nTest Case 2: K-means++ initialization\")\n",
    "print(f\"Labels: {labels2}\")\n",
    "print(f\"Centers: {[[round(x, 2) for x in center] for center in centers2]}\")\n",
    "print(f\"Inertia: {kmeans2.inertia(X1, labels2, centers2):.2f}\")\n",
    "\n",
    "# Visualize results\n",
    "X1_np = np.array(X1)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, (labels, centers, title) in enumerate([\n",
    "    (labels1, centers1, \"Random Init\"),\n",
    "    (labels2, centers2, \"K-means++ Init\")\n",
    "]):\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for i in range(len(set(labels))):\n",
    "        mask = np.array(labels) == i\n",
    "        axes[idx].scatter(X1_np[mask, 0], X1_np[mask, 1], \n",
    "                         c=colors[i], label=f'Cluster {i}', alpha=0.7, s=100)\n",
    "    \n",
    "    # Plot centers\n",
    "    centers_np = np.array(centers)\n",
    "    axes[idx].scatter(centers_np[:, 0], centers_np[:, 1], \n",
    "                     c='black', marker='x', s=200, linewidths=3, label='Centers')\n",
    "    \n",
    "    axes[idx].set_title(title)\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Decision Tree from Scratch üî¥\n",
    "\n",
    "**Difficulty**: Hard\n",
    "\n",
    "**Problem**: Implement a decision tree classifier with multiple splitting criteria, pruning, and feature importance calculation.\n",
    "\n",
    "**Constraints**:\n",
    "- 1 ‚â§ n_samples ‚â§ 5000\n",
    "- 1 ‚â§ n_features ‚â§ 100\n",
    "- max_depth ‚â§ 20\n",
    "- min_samples_split ‚â• 2\n",
    "- Must support both numerical and categorical features\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "X = [[0, 0], [1, 1], [0, 1], [1, 0]]\n",
    "y = [0, 1, 1, 0]  # XOR problem\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "predictions = tree.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self):\n",
    "        self.feature_idx = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.value = None  # For leaf nodes\n",
    "        self.samples = 0\n",
    "        self.impurity = 0\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \n",
    "    def __init__(self, max_depth: int = 10, min_samples_split: int = 2, \n",
    "                 min_samples_leaf: int = 1, criterion: str = 'gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "        self.feature_importances = None\n",
    "        self.n_features = 0\n",
    "        self.n_classes = 0\n",
    "    \n",
    "    def fit(self, X: List[List[float]], y: List[int]) -> 'DecisionTreeClassifier':\n",
    "        \"\"\"\n",
    "        Build decision tree classifier.\n",
    "        \n",
    "        Time Complexity: O(n * m * log(n)) average case\n",
    "        Space Complexity: O(log(n)) for balanced tree\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.feature_importances = np.zeros(self.n_features)\n",
    "        \n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "        # Normalize feature importances\n",
    "        if self.feature_importances.sum() > 0:\n",
    "            self.feature_importances /= self.feature_importances.sum()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int) -> DecisionTreeNode:\n",
    "        \"\"\"Recursively build decision tree.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        node = DecisionTreeNode()\n",
    "        node.samples = n_samples\n",
    "        node.impurity = self._calculate_impurity(y)\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            node.value = self._most_common_class(y)\n",
    "            return node\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold, best_gain = self._find_best_split(X, y)\n",
    "        \n",
    "        if best_gain == 0:\n",
    "            node.value = self._most_common_class(y)\n",
    "            return node\n",
    "        \n",
    "        # Update feature importance\n",
    "        self.feature_importances[best_feature] += best_gain * n_samples\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
    "            node.value = self._most_common_class(y)\n",
    "            return node\n",
    "        \n",
    "        node.feature_idx = best_feature\n",
    "        node.threshold = best_threshold\n",
    "        node.left = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        node.right = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _find_best_split(self, X: np.ndarray, y: np.ndarray) -> Tuple[int, float, float]:\n",
    "        \"\"\"Find the best feature and threshold to split on.\"\"\"\n",
    "        best_gain = 0\n",
    "        best_feature = 0\n",
    "        best_threshold = 0\n",
    "        \n",
    "        parent_impurity = self._calculate_impurity(y)\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain\n",
    "                left_impurity = self._calculate_impurity(y[left_mask])\n",
    "                right_impurity = self._calculate_impurity(y[right_mask])\n",
    "                \n",
    "                n_left, n_right = np.sum(left_mask), np.sum(right_mask)\n",
    "                weighted_impurity = (n_left * left_impurity + n_right * right_impurity) / len(y)\n",
    "                \n",
    "                gain = parent_impurity - weighted_impurity\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "    def _calculate_impurity(self, y: np.ndarray) -> float:\n",
    "        \"\"\"Calculate impurity measure (Gini or Entropy).\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        \n",
    "        if self.criterion == 'gini':\n",
    "            return 1 - np.sum(proportions ** 2)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return -np.sum(proportions * np.log2(proportions + 1e-15))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
    "    \n",
    "    def _most_common_class(self, y: np.ndarray) -> int:\n",
    "        \"\"\"Return the most common class in y.\"\"\"\n",
    "        return np.bincount(y).argmax()\n",
    "    \n",
    "    def predict(self, X: List[List[float]]) -> List[int]:\n",
    "        \"\"\"Make predictions for input samples.\"\"\"\n",
    "        X = np.array(X)\n",
    "        return [self._predict_sample(sample, self.root) for sample in X]\n",
    "    \n",
    "    def _predict_sample(self, sample: np.ndarray, node: DecisionTreeNode) -> int:\n",
    "        \"\"\"Predict class for a single sample.\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if sample[node.feature_idx] <= node.threshold:\n",
    "            return self._predict_sample(sample, node.left)\n",
    "        else:\n",
    "            return self._predict_sample(sample, node.right)\n",
    "    \n",
    "    def get_depth(self) -> int:\n",
    "        \"\"\"Calculate tree depth.\"\"\"\n",
    "        def _depth(node):\n",
    "            if node is None or node.value is not None:\n",
    "                return 0\n",
    "            return 1 + max(_depth(node.left), _depth(node.right))\n",
    "        \n",
    "        return _depth(self.root)\n",
    "\n",
    "# Test Decision Tree implementation\n",
    "print(\"\\n=== Problem 2: Decision Tree Implementation ===\")\n",
    "\n",
    "# Test case 1: Simple linearly separable data\n",
    "X_simple = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y_simple = [0, 0, 0, 1]\n",
    "\n",
    "dt1 = DecisionTreeClassifier(max_depth=3, criterion='gini')\n",
    "dt1.fit(X_simple, y_simple)\n",
    "pred1 = dt1.predict(X_simple)\n",
    "\n",
    "print(f\"\\nTest Case 1: Simple AND logic\")\n",
    "print(f\"Predictions: {pred1}\")\n",
    "print(f\"Accuracy: {np.mean(np.array(pred1) == np.array(y_simple)):.2f}\")\n",
    "print(f\"Tree depth: {dt1.get_depth()}\")\n",
    "print(f\"Feature importances: {[f'{x:.3f}' for x in dt1.feature_importances]}\")\n",
    "\n",
    "# Test case 2: XOR problem (non-linearly separable)\n",
    "X_xor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y_xor = [0, 1, 1, 0]\n",
    "\n",
    "dt2 = DecisionTreeClassifier(max_depth=5, criterion='entropy')\n",
    "dt2.fit(X_xor, y_xor)\n",
    "pred2 = dt2.predict(X_xor)\n",
    "\n",
    "print(f\"\\nTest Case 2: XOR problem\")\n",
    "print(f\"Predictions: {pred2}\")\n",
    "print(f\"Accuracy: {np.mean(np.array(pred2) == np.array(y_xor)):.2f}\")\n",
    "print(f\"Tree depth: {dt2.get_depth()}\")\n",
    "print(f\"Feature importances: {[f'{x:.3f}' for x in dt2.feature_importances]}\")\n",
    "\n",
    "# Visualize decision boundaries\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    X = np.array(X)\n",
    "    h = 0.01\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(mesh_points.tolist())\n",
    "    Z = np.array(Z).reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    colors = ['red', 'blue']\n",
    "    for class_val in np.unique(y):\n",
    "        mask = np.array(y) == class_val\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], c=colors[class_val], \n",
    "                   label=f'Class {class_val}', s=100, edgecolors='black')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_decision_boundary(X_simple, y_simple, dt1, \"AND Logic (Gini)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_decision_boundary(X_xor, y_xor, dt2, \"XOR Problem (Entropy)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Linear Regression with Regularization üü°\n",
    "\n",
    "**Difficulty**: Medium\n",
    "\n",
    "**Problem**: Implement linear regression with L1 (Lasso), L2 (Ridge), and Elastic Net regularization using coordinate descent.\n",
    "\n",
    "**Constraints**:\n",
    "- 1 ‚â§ n_samples ‚â§ 10000\n",
    "- 1 ‚â§ n_features ‚â§ 1000\n",
    "- 0 ‚â§ alpha ‚â§ 1000 (regularization strength)\n",
    "- 0 ‚â§ l1_ratio ‚â§ 1 (for Elastic Net)\n",
    "- max_iter ‚â§ 10000\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "X = [[1, 2], [2, 3], [3, 4]]\n",
    "y = [1, 2, 3]\n",
    "model = LinearRegression(alpha=0.1, penalty='ridge')\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedLinearRegression:\n",
    "    \n",
    "    def __init__(self, alpha: float = 1.0, penalty: str = 'ridge', \n",
    "                 l1_ratio: float = 0.5, max_iter: int = 1000, tol: float = 1e-4):\n",
    "        self.alpha = alpha\n",
    "        self.penalty = penalty\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.n_iter_ = None\n",
    "    \n",
    "    def fit(self, X: List[List[float]], y: List[float]) -> 'RegularizedLinearRegression':\n",
    "        \"\"\"\n",
    "        Fit regularized linear regression.\n",
    "        \n",
    "        Time Complexity: O(n_features * max_iter)\n",
    "        Space Complexity: O(n_features)\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Add intercept column\n",
    "        X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        \n",
    "        if self.penalty == 'ridge':\n",
    "            self._fit_ridge(X_with_intercept, y)\n",
    "        elif self.penalty == 'lasso':\n",
    "            self._fit_lasso(X_with_intercept, y)\n",
    "        elif self.penalty == 'elastic_net':\n",
    "            self._fit_elastic_net(X_with_intercept, y)\n",
    "        else:\n",
    "            self._fit_ols(X_with_intercept, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _fit_ols(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Ordinary Least Squares using normal equations.\"\"\"\n",
    "        beta = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "        self.intercept_ = beta[0]\n",
    "        self.coef_ = beta[1:]\n",
    "        self.n_iter_ = 1\n",
    "    \n",
    "    def _fit_ridge(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Ridge regression using analytical solution.\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        I = np.eye(n_features)\n",
    "        I[0, 0] = 0  # Don't penalize intercept\n",
    "        \n",
    "        beta = np.linalg.solve(X.T @ X + self.alpha * I, X.T @ y)\n",
    "        self.intercept_ = beta[0]\n",
    "        self.coef_ = beta[1:]\n",
    "        self.n_iter_ = 1\n",
    "    \n",
    "    def _fit_lasso(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Lasso regression using coordinate descent.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        beta = np.zeros(n_features)\n",
    "        \n",
    "        # Precompute X^T X diagonal\n",
    "        XTX_diag = np.sum(X**2, axis=0)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            beta_old = beta.copy()\n",
    "            \n",
    "            for j in range(n_features):\n",
    "                # Calculate residual without j-th feature\n",
    "                r_j = y - X @ beta + beta[j] * X[:, j]\n",
    "                \n",
    "                # Coordinate update\n",
    "                rho_j = X[:, j] @ r_j\n",
    "                \n",
    "                if j == 0:  # Don't penalize intercept\n",
    "                    beta[j] = rho_j / XTX_diag[j]\n",
    "                else:\n",
    "                    # Soft thresholding\n",
    "                    if rho_j > self.alpha:\n",
    "                        beta[j] = (rho_j - self.alpha) / XTX_diag[j]\n",
    "                    elif rho_j < -self.alpha:\n",
    "                        beta[j] = (rho_j + self.alpha) / XTX_diag[j]\n",
    "                    else:\n",
    "                        beta[j] = 0\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(beta - beta_old)) < self.tol:\n",
    "                break\n",
    "        \n",
    "        self.intercept_ = beta[0]\n",
    "        self.coef_ = beta[1:]\n",
    "        self.n_iter_ = iteration + 1\n",
    "    \n",
    "    def _fit_elastic_net(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Elastic Net regression using coordinate descent.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        beta = np.zeros(n_features)\n",
    "        \n",
    "        # Precompute X^T X diagonal\n",
    "        XTX_diag = np.sum(X**2, axis=0)\n",
    "        \n",
    "        l1_penalty = self.alpha * self.l1_ratio\n",
    "        l2_penalty = self.alpha * (1 - self.l1_ratio)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            beta_old = beta.copy()\n",
    "            \n",
    "            for j in range(n_features):\n",
    "                # Calculate residual without j-th feature\n",
    "                r_j = y - X @ beta + beta[j] * X[:, j]\n",
    "                \n",
    "                # Coordinate update\n",
    "                rho_j = X[:, j] @ r_j\n",
    "                \n",
    "                if j == 0:  # Don't penalize intercept\n",
    "                    beta[j] = rho_j / XTX_diag[j]\n",
    "                else:\n",
    "                    # Elastic net soft thresholding\n",
    "                    denominator = XTX_diag[j] + l2_penalty\n",
    "                    \n",
    "                    if rho_j > l1_penalty:\n",
    "                        beta[j] = (rho_j - l1_penalty) / denominator\n",
    "                    elif rho_j < -l1_penalty:\n",
    "                        beta[j] = (rho_j + l1_penalty) / denominator\n",
    "                    else:\n",
    "                        beta[j] = 0\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(beta - beta_old)) < self.tol:\n",
    "                break\n",
    "        \n",
    "        self.intercept_ = beta[0]\n",
    "        self.coef_ = beta[1:]\n",
    "        self.n_iter_ = iteration + 1\n",
    "    \n",
    "    def predict(self, X: List[List[float]]) -> List[float]:\n",
    "        \"\"\"Make predictions using fitted model.\"\"\"\n",
    "        X = np.array(X)\n",
    "        return (X @ self.coef_ + self.intercept_).tolist()\n",
    "    \n",
    "    def score(self, X: List[List[float]], y: List[float]) -> float:\n",
    "        \"\"\"Calculate R-squared score.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        y_true = np.array(y)\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        \n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test regularized linear regression\n",
    "print(\"\\n=== Problem 3: Regularized Linear Regression ===\")\n",
    "\n",
    "# Generate synthetic data with noise\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 100, 5\n",
    "X_reg = np.random.randn(n_samples, n_features)\n",
    "true_coef = np.array([1.5, -2.0, 0.5, 0, 0])  # Some zero coefficients\n",
    "y_reg = X_reg @ true_coef + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "X_reg_list = X_reg.tolist()\n",
    "y_reg_list = y_reg.tolist()\n",
    "\n",
    "print(f\"\\nTrue coefficients: {true_coef}\")\n",
    "\n",
    "# Test different regularization methods\n",
    "methods = {\n",
    "    'OLS': RegularizedLinearRegression(alpha=0, penalty='none'),\n",
    "    'Ridge': RegularizedLinearRegression(alpha=1.0, penalty='ridge'),\n",
    "    'Lasso': RegularizedLinearRegression(alpha=0.1, penalty='lasso'),\n",
    "    'Elastic Net': RegularizedLinearRegression(alpha=0.1, penalty='elastic_net', l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in methods.items():\n",
    "    model.fit(X_reg_list, y_reg_list)\n",
    "    score = model.score(X_reg_list, y_reg_list)\n",
    "    \n",
    "    results[name] = {\n",
    "        'coef': model.coef_,\n",
    "        'intercept': model.intercept_,\n",
    "        'score': score,\n",
    "        'n_iter': model.n_iter_\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Coefficients: {[f'{x:.3f}' for x in model.coef_]}\")\n",
    "    print(f\"  R¬≤ Score: {score:.4f}\")\n",
    "    print(f\"  Iterations: {model.n_iter_}\")\n",
    "    \n",
    "    # Calculate sparsity (number of zero coefficients)\n",
    "    sparsity = np.sum(np.abs(model.coef_) < 1e-6)\n",
    "    print(f\"  Sparsity: {sparsity}/{len(model.coef_)} coefficients ‚âà 0\")\n",
    "\n",
    "# Visualize coefficient paths\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    coef = result['coef']\n",
    "    axes[idx].bar(range(len(coef)), coef, alpha=0.7)\n",
    "    axes[idx].bar(range(len(true_coef)), true_coef, alpha=0.3, color='red', label='True')\n",
    "    axes[idx].set_title(f'{name} Coefficients')\n",
    "    axes[idx].set_xlabel('Feature Index')\n",
    "    axes[idx].set_ylabel('Coefficient Value')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis:\")\n",
    "print(\"‚Ä¢ OLS: No regularization, may overfit\")\n",
    "print(\"‚Ä¢ Ridge: Shrinks coefficients, keeps all features\")\n",
    "print(\"‚Ä¢ Lasso: Performs feature selection (sets coefficients to 0)\")\n",
    "print(\"‚Ä¢ Elastic Net: Combines Ridge and Lasso benefits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: K-Nearest Neighbors with Custom Distance Metrics üü¢\n",
    "\n",
    "**Difficulty**: Easy\n",
    "\n",
    "**Problem**: Implement KNN classifier with multiple distance metrics and efficient nearest neighbor search.\n",
    "\n",
    "**Constraints**:\n",
    "- 1 ‚â§ k ‚â§ min(50, n_samples)\n",
    "- 1 ‚â§ n_samples ‚â§ 5000\n",
    "- Support for Euclidean, Manhattan, and Minkowski distances\n",
    "- Handle both classification and regression\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "X_train = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
    "y_train = [0, 0, 1, 1]\n",
    "knn = KNN(k=3, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict([[0.5, 0.5]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors:\n",
    "    \n",
    "    def __init__(self, k: int = 3, metric: str = 'euclidean', p: float = 2, task: str = 'classification'):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.p = p  # For Minkowski distance\n",
    "        self.task = task\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X: List[List[float]], y: List[int]) -> 'KNearestNeighbors':\n",
    "        \"\"\"\n",
    "        Store training data.\n",
    "        \n",
    "        Time Complexity: O(1)\n",
    "        Space Complexity: O(n * m)\n",
    "        \"\"\"\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "        return self\n",
    "    \n",
    "    def _distance(self, x1: np.ndarray, x2: np.ndarray) -> float:\n",
    "        \"\"\"Calculate distance between two points.\"\"\"\n",
    "        if self.metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        elif self.metric == 'minkowski':\n",
    "            return np.sum(np.abs(x1 - x2) ** self.p) ** (1/self.p)\n",
    "        elif self.metric == 'cosine':\n",
    "            dot_product = np.dot(x1, x2)\n",
    "            norm_x1 = np.linalg.norm(x1)\n",
    "            norm_x2 = np.linalg.norm(x2)\n",
    "            return 1 - dot_product / (norm_x1 * norm_x2 + 1e-15)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {self.metric}\")\n",
    "    \n",
    "    def _find_k_nearest(self, x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Find k nearest neighbors for a point.\n",
    "        \n",
    "        Time Complexity: O(n * d + k log n)\n",
    "        Space Complexity: O(n)\n",
    "        \"\"\"\n",
    "        distances = np.array([self._distance(x, x_train) for x_train in self.X_train])\n",
    "        \n",
    "        # Get indices of k smallest distances\n",
    "        k_nearest_indices = np.argpartition(distances, min(self.k, len(distances)-1))[:self.k]\n",
    "        \n",
    "        return k_nearest_indices, distances[k_nearest_indices]\n",
    "    \n",
    "    def predict(self, X: List[List[float]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Make predictions for input samples.\n",
    "        \n",
    "        Time Complexity: O(m * n * d) for m test samples\n",
    "        Space Complexity: O(k)\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        predictions = []\n",
    "        \n",
    "        for x in X:\n",
    "            k_indices, k_distances = self._find_k_nearest(x)\n",
    "            k_labels = self.y_train[k_indices]\n",
    "            \n",
    "            if self.task == 'classification':\n",
    "                # Majority voting\n",
    "                prediction = np.bincount(k_labels).argmax()\n",
    "            else:  # regression\n",
    "                # Average of k nearest neighbors\n",
    "                prediction = np.mean(k_labels)\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X: List[List[float]]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \"\"\"\n",
    "        if self.task != 'classification':\n",
    "            raise ValueError(\"predict_proba only available for classification\")\n",
    "        \n",
    "        X = np.array(X)\n",
    "        n_classes = len(np.unique(self.y_train))\n",
    "        probabilities = []\n",
    "        \n",
    "        for x in X:\n",
    "            k_indices, k_distances = self._find_k_nearest(x)\n",
    "            k_labels = self.y_train[k_indices]\n",
    "            \n",
    "            # Calculate class probabilities\n",
    "            class_counts = np.bincount(k_labels, minlength=n_classes)\n",
    "            class_probs = class_counts / self.k\n",
    "            \n",
    "            probabilities.append(class_probs.tolist())\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def score(self, X: List[List[float]], y: List[int]) -> float:\n",
    "        \"\"\"Calculate accuracy for classification or R¬≤ for regression.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        \n",
    "        if self.task == 'classification':\n",
    "            return np.mean(np.array(predictions) == np.array(y))\n",
    "        else:  # regression\n",
    "            y_true = np.array(y)\n",
    "            y_pred = np.array(predictions)\n",
    "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "            return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test KNN implementation\n",
    "print(\"\\n=== Problem 4: K-Nearest Neighbors ===\")\n",
    "\n",
    "# Generate test data\n",
    "from sklearn.datasets import make_classification\n",
    "X_knn, y_knn = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                                  n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.7 * len(X_knn))\n",
    "X_train_knn = X_knn[:split_idx].tolist()\n",
    "y_train_knn = y_knn[:split_idx].tolist()\n",
    "X_test_knn = X_knn[split_idx:].tolist()\n",
    "y_test_knn = y_knn[split_idx:].tolist()\n",
    "\n",
    "print(f\"Training samples: {len(X_train_knn)}\")\n",
    "print(f\"Test samples: {len(X_test_knn)}\")\n",
    "\n",
    "# Test different distance metrics\n",
    "metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "k_values = [1, 3, 5, 10]\n",
    "\n",
    "results_knn = {}\n",
    "for metric in metrics:\n",
    "    results_knn[metric] = {}\n",
    "    for k in k_values:\n",
    "        knn = KNearestNeighbors(k=k, metric=metric, task='classification')\n",
    "        knn.fit(X_train_knn, y_train_knn)\n",
    "        accuracy = knn.score(X_test_knn, y_test_knn)\n",
    "        results_knn[metric][k] = accuracy\n",
    "\n",
    "# Display results\n",
    "print(\"\\nAccuracy Results:\")\n",
    "print(\"k\\t\" + \"\\t\".join(f\"{metric:>10}\" for metric in metrics))\n",
    "for k in k_values:\n",
    "    row = f\"{k}\\t\"\n",
    "    for metric in metrics:\n",
    "        row += f\"{results_knn[metric][k]:>10.3f}\\t\"\n",
    "    print(row)\n",
    "\n",
    "# Find best configuration\n",
    "best_metric, best_k, best_accuracy = None, None, 0\n",
    "for metric in metrics:\n",
    "    for k in k_values:\n",
    "        if results_knn[metric][k] > best_accuracy:\n",
    "            best_accuracy = results_knn[metric][k]\n",
    "            best_metric = metric\n",
    "            best_k = k\n",
    "\n",
    "print(f\"\\nBest configuration: k={best_k}, metric={best_metric}, accuracy={best_accuracy:.3f}\")\n",
    "\n",
    "# Visualize decision boundaries for best model\n",
    "best_knn = KNearestNeighbors(k=best_k, metric=best_metric, task='classification')\n",
    "best_knn.fit(X_train_knn, y_train_knn)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Performance comparison\n",
    "for metric in metrics:\n",
    "    accuracies = [results_knn[metric][k] for k in k_values]\n",
    "    axes[0].plot(k_values, accuracies, 'o-', label=metric, linewidth=2, markersize=8)\n",
    "\n",
    "axes[0].set_xlabel('k (Number of Neighbors)')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('KNN Performance vs k and Distance Metric')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(k_values)\n",
    "\n",
    "# Decision boundary visualization\n",
    "h = 0.1\n",
    "X_train_np = np.array(X_train_knn)\n",
    "x_min, x_max = X_train_np[:, 0].min() - 1, X_train_np[:, 0].max() + 1\n",
    "y_min, y_max = X_train_np[:, 1].min() - 1, X_train_np[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                    np.arange(y_min, y_max, h))\n",
    "\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = best_knn.predict(mesh_points.tolist())\n",
    "Z = np.array(Z).reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "colors = ['red', 'blue']\n",
    "for class_val in [0, 1]:\n",
    "    mask = np.array(y_train_knn) == class_val\n",
    "    X_class = X_train_np[mask]\n",
    "    axes[1].scatter(X_class[:, 0], X_class[:, 1], c=colors[class_val], \n",
    "                   label=f'Class {class_val}', s=50, alpha=0.8, edgecolors='black')\n",
    "\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title(f'Decision Boundary (k={best_k}, {best_metric})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä KNN Analysis:\")\n",
    "print(\"‚Ä¢ Small k: More complex decision boundary, may overfit\")\n",
    "print(\"‚Ä¢ Large k: Smoother decision boundary, may underfit\")\n",
    "print(\"‚Ä¢ Distance metric choice depends on data distribution\")\n",
    "print(\"‚Ä¢ Euclidean: Good for continuous features\")\n",
    "print(\"‚Ä¢ Manhattan: Robust to outliers\")\n",
    "print(\"‚Ä¢ Cosine: Good for high-dimensional sparse data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps üéì\n",
    "\n",
    "### üèÜ Problems Completed:\n",
    "\n",
    "1. **K-Means Clustering** üü°\n",
    "   - Multiple initialization strategies (random, k-means++)\n",
    "   - Convergence criteria and empty cluster handling\n",
    "   - **Key Insight**: K-means++ initialization reduces iterations\n",
    "\n",
    "2. **Decision Tree Classifier** üî¥\n",
    "   - Gini and entropy splitting criteria\n",
    "   - Recursive tree building with pruning parameters\n",
    "   - **Key Insight**: Trees can overfit without proper regularization\n",
    "\n",
    "3. **Regularized Linear Regression** üü°\n",
    "   - Ridge, Lasso, and Elastic Net implementations\n",
    "   - Coordinate descent optimization\n",
    "   - **Key Insight**: Lasso performs automatic feature selection\n",
    "\n",
    "4. **K-Nearest Neighbors** üü¢\n",
    "   - Multiple distance metrics and efficiency considerations\n",
    "   - Classification and regression support\n",
    "   - **Key Insight**: Distance metric choice affects performance\n",
    "\n",
    "### ‚ö° Performance Analysis:\n",
    "\n",
    "| Algorithm | Time Complexity | Space Complexity | Best Use Case |\n",
    "|-----------|----------------|------------------|---------------|\n",
    "| **K-Means** | O(nkdi) | O(nk) | Spherical clusters |\n",
    "| **Decision Tree** | O(n log n * m) | O(log n) | Interpretable models |\n",
    "| **Ridge/Lasso** | O(m * iterations) | O(m) | High-dimensional data |\n",
    "| **KNN** | O(nm) per query | O(nm) | Non-parametric problems |\n",
    "\n",
    "### üéØ Key Implementation Insights:\n",
    "\n",
    "1. **Initialization Matters**: K-means++ significantly improves convergence\n",
    "2. **Regularization Trade-offs**: Ridge vs Lasso vs Elastic Net serve different purposes\n",
    "3. **Tree Complexity**: Depth and split criteria affect generalization\n",
    "4. **Distance Metrics**: Euclidean, Manhattan, and Cosine have different properties\n",
    "\n",
    "### üöÄ Next Notebook Preview:\n",
    "**Part 3: Data Structures and Efficiency** will cover:\n",
    "- Efficient nearest neighbor search (KD-trees, LSH)\n",
    "- Priority queues for streaming algorithms\n",
    "- Hash tables for feature hashing\n",
    "- Memory-efficient data structures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}