{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics Part 1 - Tensors and Fundamentals\n",
    "\n",
    "Introduction to PyTorch tensors, operations, and core concepts for scientific computing\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "**Tensors** are mathematical objects that generalize scalars, vectors, and matrices to higher dimensions. In PyTorch, tensors are the fundamental data structure for all computations.\n",
    "\n",
    "### Tensor Hierarchy:\n",
    "- **Scalar** (0-tensor): $s \\in \\mathbb{R}$\n",
    "- **Vector** (1-tensor): $\\mathbf{v} \\in \\mathbb{R}^n$\n",
    "- **Matrix** (2-tensor): $\\mathbf{M} \\in \\mathbb{R}^{m \\times n}$\n",
    "- **Higher-order tensor**: $\\mathcal{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_k}$\n",
    "\n",
    "### Key Properties:\n",
    "- **Shape**: The dimensions of a tensor $(d_1, d_2, \\ldots, d_k)$\n",
    "- **Rank**: Number of dimensions $k$\n",
    "- **Size**: Total number of elements $\\prod_{i=1}^k d_i$\n",
    "- **Dtype**: Data type (float32, int64, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0\n",
      "CUDA available: False\n",
      "MPS (Metal) available: True\n",
      "MPS built: True\n",
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nUsing the Metal GPU (MPS) on your M2 Air is actually very simple.\\n   Here\\'s what you need to know:\\n\\n  Basic MPS Usage\\n\\n  Just change your device:\\n\\n  # Instead of this (CPU):\\n  device = torch.device(\"cpu\")\\n\\n  # Use this (Metal GPU):\\n  device = torch.device(\"mps\")\\n\\n  # Or automatically choose best available:\\n  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\\n\\n  Complete Example\\n\\n  import torch\\n\\n  # Set device\\n  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\\n  print(f\"Using device: {device}\")\\n\\n  # Create tensors on GPU\\n  x = torch.randn(1000, 1000, device=device)\\n  y = torch.randn(1000, 1000, device=device)\\n\\n  # Operations run on GPU automatically\\n  z = torch.matmul(x, y)  # This runs on Metal GPU!\\n\\n  # Or move existing tensors to GPU\\n  cpu_tensor = torch.randn(100, 100)\\n  gpu_tensor = cpu_tensor.to(device)\\n\\n  For Neural Networks\\n\\n  import torch.nn as nn\\n\\n  # Create model\\n  model = nn.Linear(784, 10)\\n\\n  # Move model to GPU\\n  model = model.to(device)\\n\\n  # Now all operations use Metal GPU\\n  output = model(input_tensor.to(device))\\n\\n  Important Notes\\n\\n  What works:\\n  - Most PyTorch operations\\n  - Neural network training\\n  - Matrix operations\\n  - Common ML algorithms\\n\\n  What might not work:\\n  - Some newer/experimental features\\n  - A few specialized operations (rare)\\n\\n  Fallback behavior:\\n  - If operation isn\\'t supported on MPS, PyTorch automatically falls back to CPU\\n  - You\\'ll see a warning but code won\\'t break\\n\\n  Performance Tip\\n\\n  For learning, you might want to stick with CPU initially because:\\n  - Easier debugging (no device transfers)\\n  - Small datasets don\\'t benefit much from GPU\\n  - Some operations are actually faster on M2 CPU for small data\\n\\n  Use GPU when:\\n  - Training larger models\\n  - Working with bigger datasets\\n  - Doing intensive matrix operations\\n\\n  Simple Template\\n\\n  # At start of notebook\\n  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\\n\\n  # Move everything to device\\n  model = model.to(device)\\n  data = data.to(device)\\n  target = target.to(device)\\n\\n  # Everything else stays the same!\\n\\n  Bottom line: It\\'s just changing device=\"cpu\" to device=\"mps\" - PyTorch handles\\n  all the complexity for you!\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for Apple Silicon GPU (Metal Performance Shaders)\n",
    "print(f\"MPS (Metal) available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Metal GPU (MPS) on your M2 Air is actually very simple.\n",
    "Here's what you need to know:\n",
    "\n",
    "### Basic MPS Usage\n",
    "#### Just change your device:\n",
    "```\n",
    "  # Instead of this (CPU):\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "  # Use this (Metal GPU):\n",
    "  device = torch.device(\"mps\")\n",
    "\n",
    "  # Or automatically choose best available:\n",
    "  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    "### Complete Example\n",
    "```\n",
    "  import torch\n",
    "\n",
    "  # Set device\n",
    "  device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "  print(f\"Using device: {device}\")\n",
    "\n",
    "  # Create tensors on GPU\n",
    "  x = torch.randn(1000, 1000, device=device)\n",
    "  y = torch.randn(1000, 1000, device=device)\n",
    "\n",
    "  # Operations run on GPU automatically\n",
    "  z = torch.matmul(x, y)  # This runs on Metal GPU!\n",
    "\n",
    "  # Or move existing tensors to GPU\n",
    "  cpu_tensor = torch.randn(100, 100)\n",
    "  gpu_tensor = cpu_tensor.to(device)\n",
    "\n",
    "  For Neural Networks\n",
    "\n",
    "  import torch.nn as nn\n",
    "\n",
    "  # Create model\n",
    "  model = nn.Linear(784, 10)\n",
    "\n",
    "  # Move model to GPU\n",
    "  model = model.to(device)\n",
    "\n",
    "  # Now all operations use Metal GPU\n",
    "  output = model(input_tensor.to(device))\n",
    "```\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "  What works:\n",
    "  - Most PyTorch operations\n",
    "  - Neural network training\n",
    "  - Matrix operations\n",
    "  - Common ML algorithms\n",
    "\n",
    "  What might not work:\n",
    "  - Some newer/experimental features\n",
    "  - A few specialized operations (rare)\n",
    "\n",
    "  Fallback behavior:\n",
    "  - If operation isn't supported on MPS, PyTorch automatically falls back to CPU\n",
    "  - You'll see a warning but code won't break\n",
    "\n",
    "  Performance Tip\n",
    "\n",
    "  For learning, you might want to stick with CPU initially because:\n",
    "  - Easier debugging (no device transfers)\n",
    "  - Small datasets don't benefit much from GPU\n",
    "  - Some operations are actually faster on M2 CPU for small data\n",
    "\n",
    "  Use GPU when:\n",
    "  - Training larger models\n",
    "  - Working with bigger datasets\n",
    "  - Doing intensive matrix operations\n",
    "\n",
    "### Simple Template\n",
    "\n",
    "####  At start of notebook\n",
    "  ```device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")```\n",
    "\n",
    "#### Move everything to device\n",
    "``` \n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "target = target.to(device)\n",
    "```\n",
    "\n",
    "Everything else stays the same!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unified memory: 16.0 GB\n",
      "✅ MPS allocation successful\n",
      "Tensor size: 3.8 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    # Check total system memory (unified on M2)\n",
    "    import psutil\n",
    "    total_memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    print(f\"Total unified memory: {total_memory_gb:.1f} GB\")\n",
    "\n",
    "    # Try to allocate tensors to see GPU memory\n",
    "    try:\n",
    "        # Allocate a large tensor to test\n",
    "        device = torch.device(\"mps\")\n",
    "        x = torch.randn(1000, 1000, device=device)\n",
    "        print(\"✅ MPS allocation successful\")\n",
    "\n",
    "        # Get current memory usage\n",
    "        print(f\"Tensor size: {x.element_size() * x.nelement() / 1024**2:.1f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"MPS allocation failed: {e}\")\n",
    "else:\n",
    "    print(\"MPS not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors\n",
    "\n",
    "### Mathematical Context\n",
    "Tensor creation involves mapping discrete data structures to continuous mathematical objects. The fundamental operations include:\n",
    "\n",
    "**Tensor Construction from Data:**\n",
    "$$\\mathcal{T}: \\text{data} \\mapsto \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_k}$$\n",
    "\n",
    "**Random Tensor Generation:**\n",
    "- Uniform distribution: $X \\sim \\mathcal{U}(0, 1)$\n",
    "- Normal distribution: $X \\sim \\mathcal{N}(0, 1)$\n",
    "- Discrete uniform: $X \\sim \\text{DiscreteUniform}(a, b)$\n",
    "\n",
    "**Special Tensors:**\n",
    "- Zero tensor: $\\mathbf{0} \\in \\mathbb{R}^{m \\times n}$ where $[\\mathbf{0}]_{ij} = 0$\n",
    "- Ones tensor: $\\mathbf{1} \\in \\mathbb{R}^{m \\times n}$ where $[\\mathbf{1}]_{ij} = 1$\n",
    "- Identity matrix: $\\mathbf{I} \\in \\mathbb{R}^{n \\times n}$ where $[\\mathbf{I}]_{ij} = \\delta_{ij}$ (Kronecker delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Tensors ===\n",
      "From list: tensor([1, 2, 3, 4, 5])\n",
      "Shape: torch.Size([5]), dtype: torch.int64\n",
      "\n",
      "2D tensor: \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Shape: torch.Size([2, 3]), dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors from scratch\n",
    "print(\"=== Creating Tensors ===\")\n",
    "\n",
    "# From Python lists\n",
    "data_list = [1, 2, 3, 4, 5]\n",
    "tensor_from_list = torch.tensor(data_list)\n",
    "print(f\"From list: {tensor_from_list}\")\n",
    "print(f\"Shape: {tensor_from_list.shape}, dtype: {tensor_from_list.dtype}\")\n",
    "\n",
    "# 2D tensor from nested lists\n",
    "data_2d = [[1, 2, 3], [4, 5, 6]]\n",
    "tensor_2d = torch.tensor(data_2d)\n",
    "print(f\"\\n2D tensor: \\n{tensor_2d}\")\n",
    "print(f\"Shape: {tensor_2d.shape}, dtype: {tensor_2d.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "From NumPy: tensor([1.5000, 2.5000, 3.5000], dtype=torch.float64)\n",
      "dtype: torch.float64\n",
      "\n",
      "Float tensor: tensor([1., 2., 3.]), dtype: torch.float32\n",
      "Int tensor: tensor([1, 2, 3]), dtype: torch.int64\n",
      "\n",
      "=== Special Tensor Creation ===\n",
      "Zeros (3x4): \n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "Ones shape: torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# From NumPy arrays\n",
    "numpy_array = np.array([1.5, 2.5, 3.5])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"\\nFrom NumPy: {tensor_from_numpy}\")\n",
    "print(f\"dtype: {tensor_from_numpy.dtype}\")\n",
    "\n",
    "# Creating with specific data types\n",
    "float_tensor = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "int_tensor = torch.tensor([1.5, 2.5, 3.5], dtype=torch.int64)\n",
    "print(f\"\\nFloat tensor: {float_tensor}, dtype: {float_tensor.dtype}\")\n",
    "print(f\"Int tensor: {int_tensor}, dtype: {int_tensor.dtype}\")\n",
    "\n",
    "# Special tensor creation functions\n",
    "print(\"\\n=== Special Tensor Creation ===\")\n",
    "\n",
    "# Zeros and ones\n",
    "zeros_tensor = torch.zeros(3, 4)\n",
    "ones_tensor = torch.ones(2, 3, 5)\n",
    "print(f\"Zeros (3x4): \\n{zeros_tensor}\")\n",
    "print(f\"Ones shape: {ones_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identity matrix: \n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "\n",
      "Random uniform: \n",
      "tensor([[0.5472, 0.0062, 0.9516],\n",
      "        [0.0753, 0.8860, 0.5832]])\n",
      "Random normal: \n",
      "tensor([[-0.4220, -1.3323, -0.3639],\n",
      "        [ 0.1513, -0.3514, -0.7906]])\n",
      "Random int: \n",
      "tensor([[3, 4, 3],\n",
      "        [7, 0, 9]])\n",
      "\n",
      "Range tensor: tensor([0, 2, 4, 6, 8])\n",
      "Linspace tensor: tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "\n",
      "Template: \n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Zeros like: \n",
      "tensor([[0, 0],\n",
      "        [0, 0]])\n",
      "Random like: \n",
      "tensor([[0.1165, 0.9103],\n",
      "        [0.6440, 0.7071]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Identity matrix\n",
    "identity = torch.eye(4)\n",
    "print(f\"\\nIdentity matrix: \\n{identity}\")\n",
    "\n",
    "# Random tensors\n",
    "random_uniform = torch.rand(2, 3)  # Uniform [0, 1)\n",
    "random_normal = torch.randn(2, 3)  # Standard normal\n",
    "random_int = torch.randint(0, 10, (2, 3))  # Random integers\n",
    "\n",
    "print(f\"\\nRandom uniform: \\n{random_uniform}\")\n",
    "print(f\"Random normal: \\n{random_normal}\")\n",
    "print(f\"Random int: \\n{random_int}\")\n",
    "\n",
    "# Range tensors\n",
    "range_tensor = torch.arange(0, 10, 2)  # start, end, step\n",
    "linspace_tensor = torch.linspace(0, 1, 5)  # start, end, steps\n",
    "print(f\"\\nRange tensor: {range_tensor}\")\n",
    "print(f\"Linspace tensor: {linspace_tensor}\")\n",
    "\n",
    "# Like operations (same shape as existing tensor)\n",
    "template = torch.tensor([[1, 2], [3, 4]])\n",
    "zeros_like = torch.zeros_like(template)\n",
    "ones_like = torch.ones_like(template)\n",
    "rand_like = torch.rand_like(template.float())  # Need float for rand_like\n",
    "\n",
    "print(f\"\\nTemplate: \\n{template}\")\n",
    "print(f\"Zeros like: \\n{zeros_like}\")\n",
    "print(f\"Random like: \\n{rand_like}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Properties and Information\n",
    "\n",
    "### Mathematical Properties\n",
    "Understanding tensor properties is crucial for mathematical operations:\n",
    "\n",
    "**Shape and Dimensionality:**\n",
    "- Shape: $\\text{shape}(\\mathcal{T}) = (d_1, d_2, \\ldots, d_k)$\n",
    "- Rank/Order: $\\text{rank}(\\mathcal{T}) = k$ (number of indices needed)\n",
    "- Total elements: $|\\mathcal{T}| = \\prod_{i=1}^k d_i$\n",
    "\n",
    "**Memory Layout and Strides:**\n",
    "For a tensor $\\mathcal{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_k}$, the memory address mapping is:\n",
    "$$\\text{addr}(i_1, i_2, \\ldots, i_k) = \\text{base} + \\sum_{j=1}^k i_j \\cdot s_j$$\n",
    "\n",
    "Where $s_j$ are the strides: $s_k = 1$, $s_{k-1} = d_k$, $s_{k-2} = d_k \\cdot d_{k-1}$, etc.\n",
    "\n",
    "**Data Type Mapping:**\n",
    "- $\\mathbb{Z}$ → `torch.int64`\n",
    "- $\\mathbb{R}$ → `torch.float32` \n",
    "- $\\{0, 1\\}$ → `torch.bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tensor Properties ===\n",
      "Sample tensor shape: torch.Size([2, 3, 4])\n",
      "Shape: torch.Size([2, 3, 4])\n",
      "Size: torch.Size([2, 3, 4])\n",
      "Number of dimensions: 3\n",
      "Number of elements: 24\n",
      "Data type: torch.float32\n",
      "Device: cpu\n",
      "Requires gradient: False\n",
      "\n",
      "Is contiguous: True\n",
      "Memory format: (12, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Tensor properties\n",
    "print(\"=== Tensor Properties ===\")\n",
    "\n",
    "# Create a sample tensor\n",
    "sample_tensor = torch.randn(2, 3, 4)\n",
    "print(f\"Sample tensor shape: {sample_tensor.shape}\")\n",
    "\n",
    "# Basic properties\n",
    "print(f\"Shape: {sample_tensor.shape}\")\n",
    "print(f\"Size: {sample_tensor.size()}\")\n",
    "print(f\"Number of dimensions: {sample_tensor.ndim}\")\n",
    "print(f\"Number of elements: {sample_tensor.numel()}\")\n",
    "print(f\"Data type: {sample_tensor.dtype}\")\n",
    "print(f\"Device: {sample_tensor.device}\")\n",
    "print(f\"Requires gradient: {sample_tensor.requires_grad}\")\n",
    "\n",
    "# Memory layout\n",
    "print(f\"\\nIs contiguous: {sample_tensor.is_contiguous()}\")\n",
    "print(f\"Memory format: {sample_tensor.stride()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Type Conversions ===\n",
      "Original: tensor([1, 2, 3]), dtype: torch.int64\n",
      "Float: tensor([1., 2., 3.]), dtype: torch.float32\n",
      "Double: tensor([1., 2., 3.], dtype=torch.float64), dtype: torch.float64\n",
      "Bool: tensor([True, True, True]), dtype: torch.bool\n",
      "Using .to(): tensor([1., 2., 3.]), dtype: torch.float32\n",
      "\n",
      "=== Device Management ===\n",
      "CPU tensor device: cpu\n",
      "CUDA not available\n",
      "Tensor created on device: mps:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data type conversions\n",
    "print(\"\\n=== Data Type Conversions ===\")\n",
    "\n",
    "int_tensor = torch.tensor([1, 2, 3])\n",
    "print(f\"Original: {int_tensor}, dtype: {int_tensor.dtype}\")\n",
    "\n",
    "# Convert to different types\n",
    "float_version = int_tensor.float()\n",
    "double_version = int_tensor.double()\n",
    "bool_version = int_tensor.bool()\n",
    "\n",
    "print(f\"Float: {float_version}, dtype: {float_version.dtype}\")\n",
    "print(f\"Double: {double_version}, dtype: {double_version.dtype}\")\n",
    "print(f\"Bool: {bool_version}, dtype: {bool_version.dtype}\")\n",
    "\n",
    "# Using .to() method\n",
    "converted = int_tensor.to(torch.float32)\n",
    "print(f\"Using .to(): {converted}, dtype: {converted.dtype}\")\n",
    "\n",
    "# Device placement\n",
    "print(\"\\n=== Device Management ===\")\n",
    "\n",
    "cpu_tensor = torch.randn(3, 3)\n",
    "print(f\"CPU tensor device: {cpu_tensor.device}\")\n",
    "\n",
    "# Move to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    gpu_tensor = cpu_tensor.cuda()  # or .to('cuda')\n",
    "    print(f\"GPU tensor device: {gpu_tensor.device}\")\n",
    "    \n",
    "    # Move back to CPU\n",
    "    back_to_cpu = gpu_tensor.cpu()\n",
    "    print(f\"Back to CPU device: {back_to_cpu.device}\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "\n",
    "# Alternative device specification\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tensor_on_device = torch.randn(2, 2, device=device)\n",
    "print(f\"Tensor created on device: {tensor_on_device.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Indexing and Slicing\n",
    "\n",
    "### Mathematical Foundation of Indexing\n",
    "Tensor indexing is a fundamental operation for extracting subtensors:\n",
    "\n",
    "**Basic Indexing:**\n",
    "For tensor $\\mathcal{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_k}$:\n",
    "- Element access: $\\mathcal{T}[i_1, i_2, \\ldots, i_k] \\in \\mathbb{R}$\n",
    "- Slice notation: $\\mathcal{T}[a:b] = \\{\\mathcal{T}[i] : a \\leq i < b\\}$\n",
    "\n",
    "**Boolean Indexing:**\n",
    "Given predicate $P: \\mathbb{R} \\rightarrow \\{0, 1\\}$:\n",
    "$$\\mathcal{T}[P(\\mathcal{T})] = \\{t \\in \\mathcal{T} : P(t) = 1\\}$$\n",
    "\n",
    "**Advanced Indexing (Fancy Indexing):**\n",
    "For index tensors $\\mathbf{I}_1, \\mathbf{I}_2, \\ldots$:\n",
    "$$\\mathcal{T}[\\mathbf{I}_1, \\mathbf{I}_2, \\ldots] = \\{\\mathcal{T}[i_1, i_2, \\ldots] : i_1 \\in \\mathbf{I}_1, i_2 \\in \\mathbf{I}_2, \\ldots\\}$$\n",
    "\n",
    "This enables powerful data manipulation and selection operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor indexing and slicing\n",
    "print(\"=== Tensor Indexing and Slicing ===\")\n",
    "\n",
    "# Create a sample tensor\n",
    "data = torch.arange(24).reshape(4, 6)\n",
    "print(f\"Original tensor (4x6): \\n{data}\")\n",
    "\n",
    "# Basic indexing\n",
    "print(f\"\\nElement at [2, 3]: {data[2, 3]}\")\n",
    "print(f\"First row: {data[0]}\")\n",
    "print(f\"First column: {data[:, 0]}\")\n",
    "print(f\"Last row: {data[-1]}\")\n",
    "\n",
    "# Slicing\n",
    "print(f\"\\nFirst 2 rows: \\n{data[:2]}\")\n",
    "print(f\"Last 3 columns: \\n{data[:, -3:]}\")\n",
    "print(f\"Middle section [1:3, 2:5]: \\n{data[1:3, 2:5]}\")\n",
    "\n",
    "# Step slicing\n",
    "print(f\"\\nEvery other row: \\n{data[::2]}\")\n",
    "print(f\"Every other column: \\n{data[:, ::2]}\")\n",
    "print(f\"Reverse rows: \\n{data[::-1]}\")\n",
    "\n",
    "# Advanced indexing\n",
    "print(\"\\n=== Advanced Indexing ===\")\n",
    "\n",
    "# Boolean indexing\n",
    "bool_mask = data > 10\n",
    "print(f\"Boolean mask (>10): \\n{bool_mask}\")\n",
    "print(f\"Elements > 10: {data[bool_mask]}\")\n",
    "\n",
    "# Fancy indexing with lists/tensors\n",
    "row_indices = torch.tensor([0, 2, 3])\n",
    "col_indices = torch.tensor([1, 3, 5])\n",
    "print(f\"\\nSelected rows {row_indices}: \\n{data[row_indices]}\")\n",
    "print(f\"Selected elements at specific indices: {data[row_indices, col_indices]}\")\n",
    "\n",
    "# Where function (like numpy.where)\n",
    "condition = data > 15\n",
    "result = torch.where(condition, data, torch.zeros_like(data))\n",
    "print(f\"\\nWhere condition (>15, else 0): \\n{result}\")\n",
    "\n",
    "# Masked select\n",
    "mask = data % 3 == 0  # Divisible by 3\n",
    "selected = torch.masked_select(data, mask)\n",
    "print(f\"\\nElements divisible by 3: {selected}\")\n",
    "\n",
    "# Non-zero indices\n",
    "nonzero_indices = torch.nonzero(data > 20)\n",
    "print(f\"\\nIndices where data > 20: \\n{nonzero_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations\n",
    "\n",
    "### Mathematical Operations on Tensors\n",
    "\n",
    "**Element-wise Operations (Hadamard Operations):**\n",
    "For tensors $\\mathcal{A}, \\mathcal{B} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_k}$:\n",
    "- Addition: $(\\mathcal{A} + \\mathcal{B})_{i_1,i_2,\\ldots,i_k} = \\mathcal{A}_{i_1,i_2,\\ldots,i_k} + \\mathcal{B}_{i_1,i_2,\\ldots,i_k}$\n",
    "- Hadamard product: $(\\mathcal{A} \\odot \\mathcal{B})_{i_1,i_2,\\ldots,i_k} = \\mathcal{A}_{i_1,i_2,\\ldots,i_k} \\cdot \\mathcal{B}_{i_1,i_2,\\ldots,i_k}$\n",
    "\n",
    "**Broadcasting:**\n",
    "Automatic shape alignment following rules:\n",
    "1. Align shapes from rightmost dimension\n",
    "2. Dimensions of size 1 are \"stretched\" to match\n",
    "3. Missing dimensions are assumed to be 1\n",
    "\n",
    "**Mathematical Functions:**\n",
    "- Exponential: $\\exp(\\mathcal{T}) = e^{\\mathcal{T}}$ (element-wise)\n",
    "- Logarithm: $\\log(\\mathcal{T})$ with domain $\\mathcal{T} > 0$\n",
    "- Trigonometric: $\\sin(\\mathcal{T}), \\cos(\\mathcal{T}), \\tan(\\mathcal{T})$\n",
    "- Power: $\\mathcal{T}^p$ for scalar $p$\n",
    "\n",
    "**Clamping/Clipping:**\n",
    "$$\\text{clamp}(x, a, b) = \\begin{cases}\n",
    "a & \\text{if } x < a \\\\\n",
    "x & \\text{if } a \\leq x \\leq b \\\\\n",
    "b & \\text{if } x > b\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tensor operations\n",
    "print(\"=== Basic Tensor Operations ===\")\n",
    "\n",
    "# Create sample tensors\n",
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "print(f\"Tensor a: \\n{a}\")\n",
    "print(f\"Tensor b: \\n{b}\")\n",
    "\n",
    "# Element-wise operations\n",
    "print(\"\\n=== Element-wise Operations ===\")\n",
    "print(f\"Addition (a + b): \\n{a + b}\")\n",
    "print(f\"Subtraction (a - b): \\n{a - b}\")\n",
    "print(f\"Multiplication (a * b): \\n{a * b}\")\n",
    "print(f\"Division (a / b): \\n{a / b}\")\n",
    "print(f\"Power (a ** 2): \\n{a ** 2}\")\n",
    "\n",
    "# In-place operations (memory efficient)\n",
    "print(\"\\n=== In-place Operations ===\")\n",
    "c = a.clone()  # Make a copy\n",
    "print(f\"Original c: \\n{c}\")\n",
    "c.add_(b)  # In-place addition\n",
    "print(f\"After c.add_(b): \\n{c}\")\n",
    "\n",
    "# Other in-place operations\n",
    "d = torch.ones(2, 2)\n",
    "d.mul_(5)  # d *= 5\n",
    "d.sub_(2)  # d -= 2\n",
    "print(f\"After in-place operations: \\n{d}\")\n",
    "\n",
    "# Broadcasting\n",
    "print(\"\\n=== Broadcasting ===\")\n",
    "matrix = torch.randn(3, 4)\n",
    "vector = torch.randn(4)\n",
    "scalar = 5\n",
    "\n",
    "print(f\"Matrix shape: {matrix.shape}\")\n",
    "print(f\"Vector shape: {vector.shape}\")\n",
    "\n",
    "# Broadcasting examples\n",
    "result1 = matrix + vector  # Vector broadcasted to each row\n",
    "result2 = matrix + scalar  # Scalar broadcasted to all elements\n",
    "result3 = matrix + vector.unsqueeze(0)  # Explicit dimension addition\n",
    "\n",
    "print(f\"Matrix + vector result shape: {result1.shape}\")\n",
    "print(f\"Matrix + scalar result shape: {result2.shape}\")\n",
    "\n",
    "# Mathematical functions\n",
    "print(\"\\n=== Mathematical Functions ===\")\n",
    "x = torch.tensor([0., 1., 2., 3.])\n",
    "print(f\"Original: {x}\")\n",
    "print(f\"Exponential: {torch.exp(x)}\")\n",
    "print(f\"Logarithm: {torch.log(x + 1)}\")\n",
    "print(f\"Square root: {torch.sqrt(x)}\")\n",
    "print(f\"Sine: {torch.sin(x)}\")\n",
    "print(f\"Cosine: {torch.cos(x)}\")\n",
    "\n",
    "# Trigonometric functions\n",
    "angles = torch.tensor([0., np.pi/4, np.pi/2, np.pi])\n",
    "print(f\"\\nAngles: {angles}\")\n",
    "print(f\"Sin(angles): {torch.sin(angles)}\")\n",
    "print(f\"Cos(angles): {torch.cos(angles)}\")\n",
    "\n",
    "# Rounding and clipping\n",
    "values = torch.tensor([-2.5, -1.3, 0.7, 1.9, 3.2])\n",
    "print(f\"\\nOriginal values: {values}\")\n",
    "print(f\"Floor: {torch.floor(values)}\")\n",
    "print(f\"Ceil: {torch.ceil(values)}\")\n",
    "print(f\"Round: {torch.round(values)}\")\n",
    "print(f\"Clamp(-1, 2): {torch.clamp(values, -1, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Reshaping and Manipulation\n",
    "\n",
    "### Mathematical Basis of Tensor Transformations\n",
    "\n",
    "**Reshaping (View Transformation):**\n",
    "Given tensor $\\mathcal{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_k}$ with $|\\mathcal{T}| = \\prod_{i=1}^k d_i$ elements, reshaping creates a new view:\n",
    "$$\\text{reshape}(\\mathcal{T}, (m_1, m_2, \\ldots, m_\\ell)) \\in \\mathbb{R}^{m_1 \\times m_2 \\times \\cdots \\times m_\\ell}$$\n",
    "where $\\prod_{i=1}^k d_i = \\prod_{j=1}^\\ell m_j$ (conservation of elements).\n",
    "\n",
    "**Transpose Operations:**\n",
    "For matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$:\n",
    "$$(\\mathbf{A}^T)_{ij} = \\mathbf{A}_{ji}$$\n",
    "\n",
    "For higher-order tensors, transpose swaps specified dimensions:\n",
    "$$\\text{transpose}(\\mathcal{T}, \\text{dim}_1, \\text{dim}_2)$$\n",
    "\n",
    "**Dimension Manipulation:**\n",
    "- **Squeeze**: Remove dimensions of size 1: $\\mathbb{R}^{d_1 \\times 1 \\times d_3} \\rightarrow \\mathbb{R}^{d_1 \\times d_3}$\n",
    "- **Unsqueeze**: Add dimensions of size 1: $\\mathbb{R}^{d_1 \\times d_2} \\rightarrow \\mathbb{R}^{d_1 \\times 1 \\times d_2}$\n",
    "\n",
    "**Tensor Concatenation and Stacking:**\n",
    "- **Concatenation**: Join along existing dimension\n",
    "  $$\\text{cat}([\\mathcal{A}, \\mathcal{B}], \\text{dim}) \\text{ where } \\mathcal{A}, \\mathcal{B} \\text{ have compatible shapes}$$\n",
    "- **Stacking**: Join along new dimension\n",
    "  $$\\text{stack}([\\mathcal{A}, \\mathcal{B}], \\text{dim}) \\text{ creates new dimension}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor reshaping and manipulation\n",
    "print(\"=== Tensor Reshaping and Manipulation ===\")\n",
    "\n",
    "# Create a sample tensor\n",
    "original = torch.arange(24)\n",
    "print(f\"Original tensor: {original}\")\n",
    "print(f\"Shape: {original.shape}\")\n",
    "\n",
    "# Reshaping\n",
    "print(\"\\n=== Reshaping ===\")\n",
    "reshaped_2d = original.reshape(4, 6)\n",
    "reshaped_3d = original.reshape(2, 3, 4)\n",
    "reshaped_infer = original.reshape(-1, 8)  # Infer one dimension\n",
    "\n",
    "print(f\"Reshaped to 4x6: \\n{reshaped_2d}\")\n",
    "print(f\"Reshaped to 2x3x4 shape: {reshaped_3d.shape}\")\n",
    "print(f\"Reshaped with -1 (inferred): {reshaped_infer.shape}\")\n",
    "\n",
    "# View vs reshape\n",
    "print(\"\\n=== View vs Reshape ===\")\n",
    "viewed = original.view(4, 6)  # Must be contiguous\n",
    "print(f\"View (shares memory): {viewed.shape}\")\n",
    "print(f\"Shares memory with original: {viewed.storage().data_ptr() == original.storage().data_ptr()}\")\n",
    "\n",
    "# Transpose and permute\n",
    "print(\"\\n=== Transpose and Permute ===\")\n",
    "matrix = torch.randn(3, 4)\n",
    "print(f\"Original matrix shape: {matrix.shape}\")\n",
    "\n",
    "# Transpose (2D)\n",
    "transposed = matrix.t()  # or matrix.T\n",
    "print(f\"Transposed shape: {transposed.shape}\")\n",
    "\n",
    "# Transpose specific dimensions\n",
    "tensor_3d = torch.randn(2, 3, 4)\n",
    "transposed_3d = tensor_3d.transpose(0, 2)  # Swap dim 0 and 2\n",
    "print(f\"3D tensor original: {tensor_3d.shape}\")\n",
    "print(f\"After transpose(0,2): {transposed_3d.shape}\")\n",
    "\n",
    "# Permute (rearrange all dimensions)\n",
    "permuted = tensor_3d.permute(2, 0, 1)\n",
    "print(f\"After permute(2,0,1): {permuted.shape}\")\n",
    "\n",
    "# Squeeze and unsqueeze\n",
    "print(\"\\n=== Squeeze and Unsqueeze ===\")\n",
    "tensor_with_ones = torch.randn(1, 3, 1, 4)\n",
    "print(f\"Original with size-1 dims: {tensor_with_ones.shape}\")\n",
    "\n",
    "# Remove size-1 dimensions\n",
    "squeezed = tensor_with_ones.squeeze()\n",
    "print(f\"After squeeze(): {squeezed.shape}\")\n",
    "\n",
    "# Remove specific dimension\n",
    "squeezed_dim = tensor_with_ones.squeeze(0)\n",
    "print(f\"After squeeze(0): {squeezed_dim.shape}\")\n",
    "\n",
    "# Add dimensions\n",
    "unsqueezed = squeezed.unsqueeze(1)\n",
    "print(f\"After unsqueeze(1): {unsqueezed.shape}\")\n",
    "\n",
    "# Flatten\n",
    "print(\"\\n=== Flatten ===\")\n",
    "to_flatten = torch.randn(2, 3, 4)\n",
    "print(f\"Original shape: {to_flatten.shape}\")\n",
    "\n",
    "flattened = to_flatten.flatten()\n",
    "print(f\"Flattened: {flattened.shape}\")\n",
    "\n",
    "# Flatten starting from specific dimension\n",
    "partial_flatten = to_flatten.flatten(start_dim=1)\n",
    "print(f\"Flattened from dim 1: {partial_flatten.shape}\")\n",
    "\n",
    "# Stack and concatenate\n",
    "print(\"\\n=== Stack and Concatenate ===\")\n",
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(2, 3)\n",
    "c = torch.randn(2, 3)\n",
    "\n",
    "# Stack (creates new dimension)\n",
    "stacked = torch.stack([a, b, c])\n",
    "print(f\"Stacked shape: {stacked.shape}\")\n",
    "\n",
    "stacked_dim1 = torch.stack([a, b, c], dim=1)\n",
    "print(f\"Stacked along dim 1: {stacked_dim1.shape}\")\n",
    "\n",
    "# Concatenate (along existing dimension)\n",
    "concat_dim0 = torch.cat([a, b, c], dim=0)\n",
    "concat_dim1 = torch.cat([a, b, c], dim=1)\n",
    "print(f\"Concatenated along dim 0: {concat_dim0.shape}\")\n",
    "print(f\"Concatenated along dim 1: {concat_dim1.shape}\")\n",
    "\n",
    "# Split and chunk\n",
    "print(\"\\n=== Split and Chunk ===\")\n",
    "large_tensor = torch.randn(6, 4)\n",
    "print(f\"Large tensor shape: {large_tensor.shape}\")\n",
    "\n",
    "# Split into equal parts\n",
    "chunks = torch.chunk(large_tensor, 3, dim=0)  # 3 chunks along dim 0\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"First chunk shape: {chunks[0].shape}\")\n",
    "\n",
    "# Split with specific sizes\n",
    "splits = torch.split(large_tensor, [2, 3, 1], dim=0)\n",
    "print(f\"Split sizes: {[s.shape for s in splits]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction Operations and Statistics\n",
    "\n",
    "### Mathematical Foundation of Reductions\n",
    "\n",
    "**Reduction Operations** collapse tensor dimensions by applying functions across specified axes.\n",
    "\n",
    "**Basic Reductions:**\n",
    "For tensor $\\mathcal{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_k}$:\n",
    "\n",
    "- **Sum**: $\\sum_{\\mathcal{T}} = \\sum_{i_1=1}^{d_1} \\sum_{i_2=1}^{d_2} \\cdots \\sum_{i_k=1}^{d_k} \\mathcal{T}_{i_1,i_2,\\ldots,i_k}$\n",
    "\n",
    "- **Mean**: $\\bar{\\mathcal{T}} = \\frac{1}{|\\mathcal{T}|} \\sum_{\\mathcal{T}}$ where $|\\mathcal{T}| = \\prod_{i=1}^k d_i$\n",
    "\n",
    "- **Variance**: $\\text{Var}(\\mathcal{T}) = \\frac{1}{|\\mathcal{T}|} \\sum (\\mathcal{T} - \\bar{\\mathcal{T}})^2$\n",
    "\n",
    "- **Standard Deviation**: $\\sigma(\\mathcal{T}) = \\sqrt{\\text{Var}(\\mathcal{T})}$\n",
    "\n",
    "**Dimensional Reductions:**\n",
    "Reducing along dimension $j$:\n",
    "$$\\text{reduce}_j(\\mathcal{T}) \\in \\mathbb{R}^{d_1 \\times \\cdots \\times d_{j-1} \\times d_{j+1} \\times \\cdots \\times d_k}$$\n",
    "\n",
    "**Order Statistics:**\n",
    "- **Minimum**: $\\min(\\mathcal{T}) = \\min_{i_1,\\ldots,i_k} \\mathcal{T}_{i_1,\\ldots,i_k}$\n",
    "- **Maximum**: $\\max(\\mathcal{T}) = \\max_{i_1,\\ldots,i_k} \\mathcal{T}_{i_1,\\ldots,i_k}$\n",
    "- **Quantile**: $Q_p(\\mathcal{T})$ such that $P(X \\leq Q_p) = p$\n",
    "\n",
    "**Norms:**\n",
    "- **L1 (Manhattan)**: $\\|\\mathcal{T}\\|_1 = \\sum |t_i|$\n",
    "- **L2 (Euclidean)**: $\\|\\mathcal{T}\\|_2 = \\sqrt{\\sum t_i^2}$\n",
    "- **L∞ (Maximum)**: $\\|\\mathcal{T}\\|_\\infty = \\max |t_i|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction operations and statistics\n",
    "print(\"=== Reduction Operations and Statistics ===\")\n",
    "\n",
    "# Create sample data\n",
    "data = torch.randn(4, 5)\n",
    "print(f\"Sample data (4x5): \\n{data}\")\n",
    "\n",
    "# Basic reductions\n",
    "print(\"\\n=== Basic Reductions ===\")\n",
    "print(f\"Sum (all elements): {data.sum()}\")\n",
    "print(f\"Mean (all elements): {data.mean()}\")\n",
    "print(f\"Standard deviation: {data.std()}\")\n",
    "print(f\"Variance: {data.var()}\")\n",
    "print(f\"Min: {data.min()}\")\n",
    "print(f\"Max: {data.max()}\")\n",
    "print(f\"Product: {data.prod()}\")\n",
    "\n",
    "# Reductions along specific dimensions\n",
    "print(\"\\n=== Dimension-wise Reductions ===\")\n",
    "print(f\"Sum along rows (dim=0): {data.sum(dim=0)}\")\n",
    "print(f\"Sum along columns (dim=1): {data.sum(dim=1)}\")\n",
    "print(f\"Mean along rows: {data.mean(dim=0)}\")\n",
    "print(f\"Mean along columns: {data.mean(dim=1)}\")\n",
    "\n",
    "# Keep dimensions\n",
    "sum_keepdim = data.sum(dim=1, keepdim=True)\n",
    "print(f\"\\nSum with keepdim=True: {sum_keepdim.shape}\")\n",
    "print(f\"Values: \\n{sum_keepdim}\")\n",
    "\n",
    "# Min/Max with indices\n",
    "print(\"\\n=== Min/Max with Indices ===\")\n",
    "min_values, min_indices = data.min(dim=1)\n",
    "max_values, max_indices = data.max(dim=1)\n",
    "\n",
    "print(f\"Min values per row: {min_values}\")\n",
    "print(f\"Min indices per row: {min_indices}\")\n",
    "print(f\"Max values per row: {max_values}\")\n",
    "print(f\"Max indices per row: {max_indices}\")\n",
    "\n",
    "# Quantiles and percentiles\n",
    "print(\"\\n=== Quantiles and Percentiles ===\")\n",
    "flattened = data.flatten()\n",
    "print(f\"Median: {torch.median(flattened)}\")\n",
    "print(f\"25th percentile: {torch.quantile(flattened, 0.25)}\")\n",
    "print(f\"75th percentile: {torch.quantile(flattened, 0.75)}\")\n",
    "\n",
    "# Multiple quantiles\n",
    "quantiles = torch.quantile(flattened, torch.tensor([0.1, 0.5, 0.9]))\n",
    "print(f\"10th, 50th, 90th percentiles: {quantiles}\")\n",
    "\n",
    "# Sorting and ranking\n",
    "print(\"\\n=== Sorting and Ranking ===\")\n",
    "sample_row = data[0]\n",
    "print(f\"Sample row: {sample_row}\")\n",
    "\n",
    "# Sort\n",
    "sorted_values, sorted_indices = torch.sort(sample_row)\n",
    "print(f\"Sorted values: {sorted_values}\")\n",
    "print(f\"Sort indices: {sorted_indices}\")\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_desc, indices_desc = torch.sort(sample_row, descending=True)\n",
    "print(f\"Sorted descending: {sorted_desc}\")\n",
    "\n",
    "# Top-k values\n",
    "top_k_values, top_k_indices = torch.topk(sample_row, k=3)\n",
    "print(f\"Top 3 values: {top_k_values}\")\n",
    "print(f\"Top 3 indices: {top_k_indices}\")\n",
    "\n",
    "# Unique values\n",
    "print(\"\\n=== Unique Values ===\")\n",
    "repeated_data = torch.tensor([1, 2, 2, 3, 3, 3, 4, 4, 4, 4])\n",
    "unique_values = torch.unique(repeated_data)\n",
    "unique_with_counts = torch.unique(repeated_data, return_counts=True)\n",
    "\n",
    "print(f\"Original: {repeated_data}\")\n",
    "print(f\"Unique values: {unique_values}\")\n",
    "print(f\"Unique values with counts: {unique_with_counts}\")\n",
    "\n",
    "# Statistical measures\n",
    "print(\"\\n=== Advanced Statistics ===\")\n",
    "sample_data = torch.randn(1000)\n",
    "\n",
    "# Moments\n",
    "mean = sample_data.mean()\n",
    "std = sample_data.std()\n",
    "var = sample_data.var()\n",
    "\n",
    "print(f\"Sample statistics (n=1000):\")\n",
    "print(f\"Mean: {mean:.4f}\")\n",
    "print(f\"Std: {std:.4f}\")\n",
    "print(f\"Variance: {var:.4f}\")\n",
    "\n",
    "# Manual calculation of higher moments\n",
    "centered = sample_data - mean\n",
    "skewness = (centered**3).mean() / (std**3)\n",
    "kurtosis = (centered**4).mean() / (std**4) - 3  # Excess kurtosis\n",
    "\n",
    "print(f\"Skewness: {skewness:.4f}\")\n",
    "print(f\"Excess kurtosis: {kurtosis:.4f}\")\n",
    "\n",
    "# Histogram\n",
    "print(\"\\n=== Histogram ===\")\n",
    "hist = torch.histc(sample_data, bins=10, min=-3, max=3)\n",
    "print(f\"Histogram counts: {hist}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Operations\n",
    "\n",
    "### Mathematical Foundation of Linear Algebra\n",
    "\n",
    "**Matrix Multiplication:**\n",
    "For matrices $\\mathbf{A} \\in \\mathbb{R}^{m \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times n}$:\n",
    "$$(\\mathbf{A}\\mathbf{B})_{ij} = \\sum_{\\ell=1}^k \\mathbf{A}_{i\\ell} \\mathbf{B}_{\\ell j}$$\n",
    "\n",
    "**Batch Matrix Multiplication:**\n",
    "For batch tensors $\\mathcal{A} \\in \\mathbb{R}^{b \\times m \\times k}$ and $\\mathcal{B} \\in \\mathbb{R}^{b \\times k \\times n}$:\n",
    "$$(\\mathcal{A} \\otimes \\mathcal{B})_{[i]} = \\mathcal{A}_{[i]} \\mathbf{B}_{[i]} \\text{ for each batch } i$$\n",
    "\n",
    "**Vector Operations:**\n",
    "- **Dot Product**: $\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^n u_i v_i = \\mathbf{u}^T \\mathbf{v}$\n",
    "- **Outer Product**: $\\mathbf{u} \\otimes \\mathbf{v} = \\mathbf{u}\\mathbf{v}^T \\in \\mathbb{R}^{m \\times n}$\n",
    "- **Cross Product**: $\\mathbf{u} \\times \\mathbf{v} = (u_2v_3 - u_3v_2, u_3v_1 - u_1v_3, u_1v_2 - u_2v_1)$\n",
    "\n",
    "**Matrix Decompositions:**\n",
    "\n",
    "**Eigendecomposition** (for symmetric $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$):\n",
    "$$\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T$$\n",
    "where $\\mathbf{Q}$ contains eigenvectors, $\\mathbf{\\Lambda}$ is diagonal with eigenvalues.\n",
    "\n",
    "**Singular Value Decomposition (SVD):**\n",
    "$$\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$$\n",
    "where $\\mathbf{U}, \\mathbf{V}$ are orthogonal, $\\mathbf{\\Sigma}$ contains singular values.\n",
    "\n",
    "**QR Decomposition:**\n",
    "$$\\mathbf{A} = \\mathbf{Q}\\mathbf{R}$$\n",
    "where $\\mathbf{Q}$ is orthogonal, $\\mathbf{R}$ is upper triangular.\n",
    "\n",
    "**Linear System Solution:**\n",
    "For $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$:\n",
    "- **Exact solution**: $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$ (when $\\mathbf{A}$ is invertible)\n",
    "- **Least squares**: $\\mathbf{x} = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\\mathbf{b}$ (when overdetermined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear algebra operations\n",
    "print(\"=== Linear Algebra Operations ===\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(\"=== Matrix Multiplication ===\")\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "print(f\"Matrix A shape: {A.shape}\")\n",
    "print(f\"Matrix B shape: {B.shape}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "C = torch.mm(A, B)  # or A @ B\n",
    "print(f\"A @ B shape: {C.shape}\")\n",
    "\n",
    "# Batch matrix multiplication\n",
    "batch_A = torch.randn(10, 3, 4)\n",
    "batch_B = torch.randn(10, 4, 5)\n",
    "batch_C = torch.bmm(batch_A, batch_B)\n",
    "print(f\"Batch multiplication result shape: {batch_C.shape}\")\n",
    "\n",
    "# More general matrix multiplication (handles broadcasting)\n",
    "general_A = torch.randn(2, 3, 4)\n",
    "general_B = torch.randn(4, 5)\n",
    "general_C = torch.matmul(general_A, general_B)\n",
    "print(f\"General matmul result shape: {general_C.shape}\")\n",
    "\n",
    "# Vector operations\n",
    "print(\"\\n=== Vector Operations ===\")\n",
    "v1 = torch.randn(5)\n",
    "v2 = torch.randn(5)\n",
    "\n",
    "# Dot product\n",
    "dot_product = torch.dot(v1, v2)\n",
    "print(f\"Dot product: {dot_product}\")\n",
    "\n",
    "# Outer product\n",
    "outer_product = torch.outer(v1, v2)\n",
    "print(f\"Outer product shape: {outer_product.shape}\")\n",
    "\n",
    "# Cross product (3D vectors)\n",
    "v3d_1 = torch.randn(3)\n",
    "v3d_2 = torch.randn(3)\n",
    "cross_product = torch.cross(v3d_1, v3d_2)\n",
    "print(f\"Cross product: {cross_product}\")\n",
    "\n",
    "# Norms\n",
    "print(\"\\n=== Vector and Matrix Norms ===\")\n",
    "vector = torch.tensor([3., 4., 5.])\n",
    "matrix = torch.randn(3, 3)\n",
    "\n",
    "# Vector norms\n",
    "l1_norm = torch.norm(vector, p=1)\n",
    "l2_norm = torch.norm(vector, p=2)  # Euclidean norm\n",
    "inf_norm = torch.norm(vector, p=float('inf'))\n",
    "\n",
    "print(f\"Vector: {vector}\")\n",
    "print(f\"L1 norm: {l1_norm}\")\n",
    "print(f\"L2 norm: {l2_norm}\")\n",
    "print(f\"Infinity norm: {inf_norm}\")\n",
    "\n",
    "# Matrix norms\n",
    "frobenius_norm = torch.norm(matrix, p='fro')\n",
    "nuclear_norm = torch.norm(matrix, p='nuc')\n",
    "print(f\"\\nMatrix Frobenius norm: {frobenius_norm}\")\n",
    "print(f\"Matrix nuclear norm: {nuclear_norm}\")\n",
    "\n",
    "# Matrix decompositions\n",
    "print(\"\\n=== Matrix Decompositions ===\")\n",
    "\n",
    "# Create a symmetric positive definite matrix\n",
    "random_matrix = torch.randn(4, 4)\n",
    "symmetric_matrix = random_matrix @ random_matrix.T + torch.eye(4)\n",
    "\n",
    "# Eigendecomposition\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(symmetric_matrix)\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Eigenvectors shape: {eigenvectors.shape}\")\n",
    "\n",
    "# SVD\n",
    "U, S, Vt = torch.linalg.svd(random_matrix)\n",
    "print(f\"\\nSVD shapes - U: {U.shape}, S: {S.shape}, Vt: {Vt.shape}\")\n",
    "print(f\"Singular values: {S}\")\n",
    "\n",
    "# QR decomposition\n",
    "Q, R = torch.linalg.qr(random_matrix)\n",
    "print(f\"\\nQR shapes - Q: {Q.shape}, R: {R.shape}\")\n",
    "\n",
    "# Cholesky decomposition\n",
    "try:\n",
    "    L = torch.linalg.cholesky(symmetric_matrix)\n",
    "    print(f\"Cholesky decomposition shape: {L.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cholesky failed: {e}\")\n",
    "\n",
    "# Matrix properties\n",
    "print(\"\\n=== Matrix Properties ===\")\n",
    "\n",
    "# Determinant\n",
    "det = torch.linalg.det(symmetric_matrix)\n",
    "print(f\"Determinant: {det}\")\n",
    "\n",
    "# Matrix rank\n",
    "rank = torch.linalg.matrix_rank(random_matrix)\n",
    "print(f\"Matrix rank: {rank}\")\n",
    "\n",
    "# Condition number\n",
    "cond = torch.linalg.cond(symmetric_matrix)\n",
    "print(f\"Condition number: {cond}\")\n",
    "\n",
    "# Trace\n",
    "trace = torch.trace(symmetric_matrix)\n",
    "print(f\"Trace: {trace}\")\n",
    "\n",
    "# Matrix inverse\n",
    "try:\n",
    "    inverse = torch.linalg.inv(symmetric_matrix)\n",
    "    print(f\"Inverse computed successfully\")\n",
    "    \n",
    "    # Verify inverse\n",
    "    identity_check = symmetric_matrix @ inverse\n",
    "    error = torch.norm(identity_check - torch.eye(4))\n",
    "    print(f\"Inverse verification error: {error}\")\n",
    "except Exception as e:\n",
    "    print(f\"Matrix inversion failed: {e}\")\n",
    "\n",
    "# Solving linear systems\n",
    "print(\"\\n=== Solving Linear Systems ===\")\n",
    "A_system = torch.randn(4, 4)\n",
    "b_system = torch.randn(4)\n",
    "\n",
    "# Solve Ax = b\n",
    "try:\n",
    "    x_solution = torch.linalg.solve(A_system, b_system)\n",
    "    print(f\"Solution x shape: {x_solution.shape}\")\n",
    "    \n",
    "    # Verify solution\n",
    "    residual = A_system @ x_solution - b_system\n",
    "    residual_norm = torch.norm(residual)\n",
    "    print(f\"Residual norm: {residual_norm}\")\n",
    "except Exception as e:\n",
    "    print(f\"Linear system solving failed: {e}\")\n",
    "\n",
    "# Least squares solution\n",
    "A_overdetermined = torch.randn(6, 4)  # More equations than unknowns\n",
    "b_overdetermined = torch.randn(6)\n",
    "\n",
    "x_lstsq = torch.linalg.lstsq(A_overdetermined, b_overdetermined).solution\n",
    "print(f\"\\nLeast squares solution shape: {x_lstsq.shape}\")\n",
    "\n",
    "# Compute residual\n",
    "residual_lstsq = A_overdetermined @ x_lstsq - b_overdetermined\n",
    "residual_norm_lstsq = torch.norm(residual_lstsq)\n",
    "print(f\"Least squares residual norm: {residual_norm_lstsq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with NumPy and Data Conversion\n",
    "\n",
    "### Mathematical Interoperability\n",
    "\n",
    "**Memory Sharing and Zero-Copy Operations:**\n",
    "PyTorch tensors and NumPy arrays can share the same memory buffer, enabling efficient data transfer without copying:\n",
    "\n",
    "$$\\text{Memory Layout: } \\text{base\\_ptr} + \\sum_{i=1}^k \\text{index}_i \\times \\text{stride}_i$$\n",
    "\n",
    "**Data Type Mapping:**\n",
    "The conversion preserves mathematical properties:\n",
    "- $\\mathbb{Z}_{64} \\leftrightarrow$ `np.int64` $\\leftrightarrow$ `torch.int64`\n",
    "- $\\mathbb{R}_{32} \\leftrightarrow$ `np.float32` $\\leftrightarrow$ `torch.float32`\n",
    "- $\\{0,1\\} \\leftrightarrow$ `np.bool` $\\leftrightarrow$ `torch.bool`\n",
    "\n",
    "**Important Considerations:**\n",
    "- **Shared Memory**: `torch.from_numpy()` creates views, not copies\n",
    "- **Device Constraints**: GPU tensors must be moved to CPU before NumPy conversion\n",
    "- **Gradient Tracking**: Conversion may affect autograd functionality\n",
    "\n",
    "**Data Flow Pipeline:**\n",
    "$$\\text{Raw Data} \\xrightarrow{\\text{NumPy}} \\text{Preprocessed} \\xrightarrow{\\text{PyTorch}} \\text{ML Pipeline} \\xrightarrow{\\text{Results}} \\text{Analysis}$$\n",
    "\n",
    "This enables seamless integration between scientific computing ecosystems while maintaining mathematical consistency and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with NumPy and data conversion\n",
    "print(\"=== Working with NumPy and Data Conversion ===\")\n",
    "\n",
    "# NumPy to PyTorch conversion\n",
    "print(\"=== NumPy to PyTorch ===\")\n",
    "numpy_array = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "print(f\"NumPy array: \\n{numpy_array}\")\n",
    "print(f\"NumPy dtype: {numpy_array.dtype}\")\n",
    "\n",
    "# Convert to PyTorch\n",
    "torch_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"\\nPyTorch tensor: \\n{torch_from_numpy}\")\n",
    "print(f\"PyTorch dtype: {torch_from_numpy.dtype}\")\n",
    "\n",
    "# Note: from_numpy() shares memory!\n",
    "print(f\"\\nShares memory: {torch_from_numpy.data_ptr() == numpy_array.__array_interface__['data'][0]}\")\n",
    "\n",
    "# Modify numpy array and see effect on tensor\n",
    "numpy_array[0, 0] = 999\n",
    "print(f\"After modifying NumPy array: \\n{torch_from_numpy}\")\n",
    "\n",
    "# PyTorch to NumPy conversion\n",
    "print(\"\\n=== PyTorch to NumPy ===\")\n",
    "torch_tensor = torch.randn(3, 4)\n",
    "print(f\"Original PyTorch tensor: \\n{torch_tensor}\")\n",
    "\n",
    "# Convert to NumPy\n",
    "numpy_from_torch = torch_tensor.numpy()\n",
    "print(f\"\\nConverted to NumPy: \\n{numpy_from_torch}\")\n",
    "print(f\"NumPy dtype: {numpy_from_torch.dtype}\")\n",
    "\n",
    "# Also shares memory\n",
    "torch_tensor[0, 0] = 777\n",
    "print(f\"After modifying PyTorch tensor: \\n{numpy_from_torch}\")\n",
    "\n",
    "# Safe conversion (copy)\n",
    "print(\"\\n=== Safe Conversion (Copy) ===\")\n",
    "torch_original = torch.randn(2, 3)\n",
    "numpy_copy = torch_original.detach().numpy().copy()\n",
    "torch_copy = torch.tensor(numpy_copy)  # Creates copy\n",
    "\n",
    "print(f\"Original: \\n{torch_original}\")\n",
    "torch_original[0, 0] = 555\n",
    "print(f\"After modification - copy unchanged: \\n{torch_copy}\")\n",
    "\n",
    "# GPU tensors require CPU conversion first\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n=== GPU Tensor Conversion ===\")\n",
    "    gpu_tensor = torch.randn(2, 2, device='cuda')\n",
    "    print(f\"GPU tensor device: {gpu_tensor.device}\")\n",
    "    \n",
    "    # Must move to CPU first\n",
    "    cpu_tensor = gpu_tensor.cpu()\n",
    "    numpy_from_gpu = cpu_tensor.numpy()\n",
    "    print(f\"Converted GPU tensor to NumPy: \\n{numpy_from_gpu}\")\n",
    "\n",
    "# Working with different data types\n",
    "print(\"\\n=== Data Type Conversions ===\")\n",
    "\n",
    "# Create tensors with different types\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "double_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
    "\n",
    "print(f\"Int tensor dtype: {int_tensor.dtype}\")\n",
    "print(f\"Float tensor dtype: {float_tensor.dtype}\")\n",
    "print(f\"Double tensor dtype: {double_tensor.dtype}\")\n",
    "\n",
    "# Convert to NumPy and see dtypes\n",
    "print(f\"\\nNumPy dtypes:\")\n",
    "print(f\"From int tensor: {int_tensor.numpy().dtype}\")\n",
    "print(f\"From float tensor: {float_tensor.numpy().dtype}\")\n",
    "print(f\"From double tensor: {double_tensor.numpy().dtype}\")\n",
    "\n",
    "# Working with pandas\n",
    "print(\"\\n=== Working with Pandas ===\")\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'feature1': np.random.randn(100),\n",
    "    'feature2': np.random.randn(100),\n",
    "    'target': np.random.randint(0, 2, 100)\n",
    "})\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"DataFrame head: \\n{df.head()}\")\n",
    "\n",
    "# Convert DataFrame to tensor\n",
    "features = torch.tensor(df[['feature1', 'feature2']].values, dtype=torch.float32)\n",
    "targets = torch.tensor(df['target'].values, dtype=torch.long)\n",
    "\n",
    "print(f\"\\nFeatures tensor shape: {features.shape}\")\n",
    "print(f\"Targets tensor shape: {targets.shape}\")\n",
    "print(f\"Features dtype: {features.dtype}\")\n",
    "print(f\"Targets dtype: {targets.dtype}\")\n",
    "\n",
    "# Convert back to pandas\n",
    "features_numpy = features.numpy()\n",
    "targets_numpy = targets.numpy()\n",
    "\n",
    "df_reconstructed = pd.DataFrame({\n",
    "    'feature1': features_numpy[:, 0],\n",
    "    'feature2': features_numpy[:, 1],\n",
    "    'target': targets_numpy\n",
    "})\n",
    "\n",
    "print(f\"\\nReconstructed DataFrame head: \\n{df_reconstructed.head()}\")\n",
    "\n",
    "# Memory considerations\n",
    "print(\"\\n=== Memory Considerations ===\")\n",
    "large_numpy = np.random.randn(1000, 1000)\n",
    "print(f\"NumPy array memory: {large_numpy.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Shared memory conversion\n",
    "large_torch_shared = torch.from_numpy(large_numpy)\n",
    "print(f\"Shared conversion - additional memory: ~0 MB\")\n",
    "\n",
    "# Copy conversion\n",
    "large_torch_copy = torch.tensor(large_numpy)\n",
    "print(f\"Copy conversion - doubles memory usage\")\n",
    "\n",
    "print(f\"\\nBoth tensors equal: {torch.equal(large_torch_shared, large_torch_copy)}\")\n",
    "print(f\"Share memory: {large_torch_shared.data_ptr() == large_torch_copy.data_ptr()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarray_dataex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
