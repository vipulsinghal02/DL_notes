{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics Part 2: Autograd and Neural Networks\n",
    "\n",
    "Automatic differentiation, gradients, and basic neural network building blocks\n",
    "\n",
    "## Mathematical Foundation of Automatic Differentiation\n",
    "\n",
    "**Automatic Differentiation (Autograd)** is the computational implementation of the chain rule from calculus. For composite functions, it enables efficient computation of derivatives without symbolic differentiation.\n",
    "\n",
    "### The Chain Rule\n",
    "For composite function $f(g(x))$:\n",
    "$$\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "### Computational Graph\n",
    "Each operation creates nodes in a **directed acyclic graph (DAG)**:\n",
    "- **Forward pass**: Compute function values $f(x)$\n",
    "- **Backward pass**: Compute gradients $\\frac{\\partial f}{\\partial x}$ using reverse-mode differentiation\n",
    "\n",
    "### Gradient Computation\n",
    "For scalar output $y = f(x_1, x_2, \\ldots, x_n)$:\n",
    "$$\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right)$$\n",
    "\n",
    "PyTorch implements **reverse-mode AD**, which is efficient for functions $\\mathbb{R}^n \\rightarrow \\mathbb{R}$ (many inputs, scalar output) - ideal for loss functions in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd: Automatic Differentiation\n",
    "\n",
    "### Mathematical Example\n",
    "Consider the function: $z = x^2 + y^3$, $\\text{loss} = \\sum z$\n",
    "\n",
    "**Manual Computation:**\n",
    "- $\\frac{\\partial z}{\\partial x} = 2x$\n",
    "- $\\frac{\\partial z}{\\partial y} = 3y^2$\n",
    "- $\\frac{\\partial \\text{loss}}{\\partial x} = \\frac{\\partial \\text{loss}}{\\partial z} \\frac{\\partial z}{\\partial x} = 1 \\cdot 2x = 2x$\n",
    "\n",
    "**Reverse-Mode Algorithm:**\n",
    "1. **Forward pass**: Compute function values and store intermediate results\n",
    "2. **Backward pass**: Apply chain rule from output to inputs\n",
    "\n",
    "This demonstrates how PyTorch autograd implements mathematical differentiation computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors with gradient tracking\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([1.0, 4.0], requires_grad=True)\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a function\n",
    "z = x**2 + y**3\n",
    "loss = z.sum()\n",
    "\n",
    "print(f\"z: {z}\")\n",
    "print(f\"loss: {loss}\")\n",
    "print(f\"loss.requires_grad: {loss.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(f\"x.grad: {x.grad}\")\n",
    "print(f\"y.grad: {y.grad}\")\n",
    "\n",
    "# Manual verification:\n",
    "# dz/dx = 2x, so at x=[2,3]: [4, 6]\n",
    "# dz/dy = 3y^2, so at y=[1,4]: [3, 48]\n",
    "print(f\"Expected x.grad: {2 * x}\")\n",
    "print(f\"Expected y.grad: {3 * y**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation and Zeroing\n",
    "\n",
    "### Mathematical Principle\n",
    "**Gradient Accumulation** follows the linearity of differentiation:\n",
    "\n",
    "For functions $f_1, f_2, \\ldots, f_n$ and scalar $c$:\n",
    "$$\\frac{\\partial}{\\partial x}[f_1(x) + f_2(x)] = \\frac{\\partial f_1}{\\partial x} + \\frac{\\partial f_2}{\\partial x}$$\n",
    "$$\\frac{\\partial}{\\partial x}[c \\cdot f(x)] = c \\cdot \\frac{\\partial f}{\\partial x}$$\n",
    "\n",
    "**Why Gradients Accumulate:**\n",
    "- Each `.backward()` call adds to existing gradients: $\\text{grad} \\leftarrow \\text{grad} + \\nabla_{\\text{new}} f$\n",
    "- This enables **gradient accumulation** across multiple loss terms or batches\n",
    "- **Must manually zero** gradients between independent computations\n",
    "\n",
    "**Mathematical Interpretation:**\n",
    "If we compute losses $L_1, L_2$ separately:\n",
    "$$\\nabla (L_1 + L_2) = \\nabla L_1 + \\nabla L_2$$\n",
    "\n",
    "PyTorch implements this by accumulating gradients, allowing flexible gradient computation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients accumulate by default\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x**2\n",
    "y1.sum().backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Second computation (gradients accumulate)\n",
    "y2 = x**3\n",
    "y2.sum().backward()\n",
    "print(f\"After second backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Zero gradients\n",
    "x.grad.zero_()\n",
    "print(f\"After zeroing: x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Modules\n",
    "\n",
    "### Mathematical Foundation of Linear Layers\n",
    "\n",
    "**Affine Transformation:**\n",
    "A linear layer implements the affine transformation:\n",
    "$$\\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{n}$ is the input vector\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ is the weight matrix\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^{m}$ is the bias vector\n",
    "- $\\mathbf{y} \\in \\mathbb{R}^{m}$ is the output vector\n",
    "\n",
    "**Batch Processing:**\n",
    "For batch input $\\mathbf{X} \\in \\mathbb{R}^{B \\times n}$ (B samples):\n",
    "$$\\mathbf{Y} = \\mathbf{X}\\mathbf{W}^T + \\mathbf{b}$$\n",
    "\n",
    "**Parameter Initialization:**\n",
    "- **Xavier/Glorot**: $\\mathcal{W} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\\right)$\n",
    "- **He initialization**: $\\mathcal{W} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_{\\text{in}}}\\right)$ (for ReLU)\n",
    "\n",
    "The linear layer forms the fundamental building block for deep neural networks, implementing learnable linear transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer (fully connected)\n",
    "linear = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "print(f\"Weight:\\n{linear.weight}\")\n",
    "print(f\"Bias: {linear.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through linear layer\n",
    "x = torch.randn(5, 3)  # batch_size=5, input_features=3\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "### Mathematical Foundation of Non-linearity\n",
    "\n",
    "**Activation functions** introduce non-linearity into neural networks, enabling them to approximate complex functions. Without activation functions, multiple linear layers would collapse to a single linear transformation.\n",
    "\n",
    "**Common Activation Functions:**\n",
    "\n",
    "**ReLU (Rectified Linear Unit):**\n",
    "$$\\text{ReLU}(x) = \\max(0, x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$$\n",
    "- **Derivative**: $\\frac{d}{dx}\\text{ReLU}(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{if } x \\leq 0 \\end{cases}$\n",
    "\n",
    "**Sigmoid:**\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "- **Properties**: $\\sigma(x) \\in (0, 1)$, $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$\n",
    "- **Issues**: Vanishing gradients for large $|x|$\n",
    "\n",
    "**Tanh (Hyperbolic Tangent):**\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\frac{e^{2x} - 1}{e^{2x} + 1}$$\n",
    "- **Properties**: $\\tanh(x) \\in (-1, 1)$, $\\tanh'(x) = 1 - \\tanh^2(x)$\n",
    "- **Advantage**: Zero-centered output\n",
    "\n",
    "**Universal Approximation Theorem**: Neural networks with at least one hidden layer and non-linear activation can approximate any continuous function on compact sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common activation functions\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# ReLU\n",
    "relu_output = F.relu(x)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid_output = torch.sigmoid(x)\n",
    "\n",
    "# Tanh\n",
    "tanh_output = torch.tanh(x)\n",
    "\n",
    "# Plot activations\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x.numpy(), relu_output.numpy())\n",
    "plt.title('ReLU')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x.numpy(), sigmoid_output.numpy())\n",
    "plt.title('Sigmoid')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x.numpy(), tanh_output.numpy())\n",
    "plt.title('Tanh')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Neural Network\n",
    "\n",
    "### Mathematical Architecture\n",
    "\n",
    "**Multi-Layer Perceptron (MLP):**\n",
    "For a 2-layer network with hidden layer:\n",
    "$$\\mathbf{h} = \\sigma_1(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1)$$\n",
    "$$\\mathbf{y} = \\sigma_2(\\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{d}$ is input\n",
    "- $\\mathbf{W}_1 \\in \\mathbb{R}^{h \\times d}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{h}$ (input to hidden)\n",
    "- $\\mathbf{W}_2 \\in \\mathbb{R}^{k \\times h}$, $\\mathbf{b}_2 \\in \\mathbb{R}^{k}$ (hidden to output)\n",
    "- $\\sigma_1, \\sigma_2$ are activation functions\n",
    "\n",
    "**Parameter Count:**\n",
    "Total parameters = $(d \\times h + h) + (h \\times k + k) = h(d + k) + (h + k)$\n",
    "\n",
    "**Forward Propagation:**\n",
    "The network computes the composition:\n",
    "$$f(\\mathbf{x}; \\boldsymbol{\\theta}) = \\sigma_2(\\mathbf{W}_2 \\sigma_1(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2)$$\n",
    "\n",
    "Where $\\boldsymbol{\\theta} = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{W}_2, \\mathbf{b}_2\\}$ are learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Create network\n",
    "net = SimpleNet(input_size=4, hidden_size=10, output_size=3)\n",
    "print(net)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through network\n",
    "x = torch.randn(8, 4)  # batch_size=8, input_features=4\n",
    "output = net(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "### Mathematical Foundation of Loss Functions\n",
    "\n",
    "Loss functions quantify the discrepancy between predictions and targets, providing the objective to minimize during training.\n",
    "\n",
    "**Mean Squared Error (MSE) - Regression:**\n",
    "$$L_{\\text{MSE}}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Properties:**\n",
    "- Differentiable everywhere\n",
    "- Penalizes large errors quadratically\n",
    "- Assumes Gaussian noise in targets\n",
    "\n",
    "**Cross-Entropy Loss - Classification:**\n",
    "For multiclass classification with true class $c$ and predicted probabilities $\\mathbf{p}$:\n",
    "$$L_{\\text{CE}} = -\\log p_c = -\\log\\left(\\frac{e^{z_c}}{\\sum_{j=1}^{K} e^{z_j}}\\right)$$\n",
    "\n",
    "Where $z_j$ are logits (pre-softmax outputs).\n",
    "\n",
    "**Softmax Function:**\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "**Gradient of Cross-Entropy + Softmax:**\n",
    "$$\\frac{\\partial L_{\\text{CE}}}{\\partial z_i} = p_i - \\delta_{ic}$$\n",
    "where $\\delta_{ic}$ is 1 if $i = c$ (true class), 0 otherwise.\n",
    "\n",
    "This combination has a clean gradient that drives predictions toward the correct distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error (MSE) for regression\n",
    "predictions = torch.randn(10, 1)\n",
    "targets = torch.randn(10, 1)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(predictions, targets)\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_mse = ((predictions - targets)**2).mean()\n",
    "print(f\"Manual MSE: {manual_mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss for classification\n",
    "# Raw logits (before softmax)\n",
    "logits = torch.randn(5, 3)  # 5 samples, 3 classes\n",
    "targets = torch.tensor([0, 1, 2, 1, 0])  # class indices\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(logits, targets)\n",
    "print(f\"Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Convert to probabilities\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "print(f\"Probabilities:\\n{probabilities}\")\n",
    "print(f\"Predicted classes: {probabilities.argmax(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "### Mathematical Foundation of Optimization\n",
    "\n",
    "**Gradient Descent** is the fundamental optimization algorithm for minimizing loss functions.\n",
    "\n",
    "**Vanilla Gradient Descent:**\n",
    "$$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}_t)$$\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{\\theta}$ are parameters\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla_{\\boldsymbol{\\theta}} L$ is the gradient of loss w.r.t. parameters\n",
    "\n",
    "**Stochastic Gradient Descent (SGD):**\n",
    "Instead of using the full dataset, SGD uses mini-batches:\n",
    "$$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} L_{\\text{batch}}(\\boldsymbol{\\theta}_t)$$\n",
    "\n",
    "**Advanced Optimizers:**\n",
    "\n",
    "**Adam (Adaptive Moment Estimation):**\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
    "\n",
    "Adam combines momentum (first moment) with adaptive learning rates (second moment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple optimization example\n",
    "net = SimpleNet(2, 5, 1)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Generate some dummy data\n",
    "x = torch.randn(20, 2)\n",
    "y = torch.randn(20, 1)\n",
    "\n",
    "print(\"Initial loss:\")\n",
    "initial_output = net(x)\n",
    "initial_loss = F.mse_loss(initial_output, y)\n",
    "print(f\"Loss: {initial_loss.item():.4f}\")\n",
    "\n",
    "# Training step\n",
    "optimizer.zero_grad()  # Clear gradients\n",
    "output = net(x)        # Forward pass\n",
    "loss = F.mse_loss(output, y)  # Compute loss\n",
    "loss.backward()        # Backward pass\n",
    "optimizer.step()       # Update parameters\n",
    "\n",
    "print(\"\\nAfter one optimization step:\")\n",
    "new_output = net(x)\n",
    "new_loss = F.mse_loss(new_output, y)\n",
    "print(f\"Loss: {new_loss.item():.4f}\")\n",
    "print(f\"Loss change: {new_loss.item() - initial_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Training Loop\n",
    "\n",
    "### Mathematical Training Process\n",
    "\n",
    "**Training Loop** implements the empirical risk minimization:\n",
    "$$\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}} \\frac{1}{n} \\sum_{i=1}^{n} L(f(\\mathbf{x}_i; \\boldsymbol{\\theta}), y_i)$$\n",
    "\n",
    "**Algorithm:**\n",
    "1. **Forward Pass**: Compute predictions $\\hat{\\mathbf{y}} = f(\\mathbf{X}; \\boldsymbol{\\theta})$\n",
    "2. **Loss Computation**: $L = \\frac{1}{B} \\sum_{i=1}^{B} \\ell(\\hat{y}_i, y_i)$\n",
    "3. **Backward Pass**: Compute $\\nabla_{\\boldsymbol{\\theta}} L$ via backpropagation\n",
    "4. **Parameter Update**: $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}} L$\n",
    "5. **Repeat** until convergence\n",
    "\n",
    "**Example: Linear Regression**\n",
    "True model: $y = \\mathbf{w}^T\\mathbf{x} + b + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "**Maximum Likelihood Estimation** under Gaussian noise leads to MSE loss:\n",
    "$$L(\\mathbf{w}, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\mathbf{w}^T\\mathbf{x}_i - b)^2$$\n",
    "\n",
    "The training process finds parameters that minimize this empirical risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple regression problem: y = 2x1 + 3x2 + noise\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate data\n",
    "n_samples = 100\n",
    "x = torch.randn(n_samples, 2)\n",
    "true_weights = torch.tensor([[2.0], [3.0]])\n",
    "y = x @ true_weights + 0.1 * torch.randn(n_samples, 1)\n",
    "\n",
    "# Create model\n",
    "model = nn.Linear(2, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    predictions = model(x)\n",
    "    loss = criterion(predictions, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nTrue weights: {true_weights.flatten()}\")\n",
    "print(f\"Learned weights: {model.weight.data.flatten()}\")\n",
    "print(f\"True bias: 0.0\")\n",
    "print(f\"Learned bias: {model.bias.data.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimizers\n",
    "def train_with_optimizer(optimizer_class, **kwargs):\n",
    "    model = nn.Linear(2, 1)\n",
    "    optimizer = optimizer_class(model.parameters(), **kwargs)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(50):\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses, model.weight.data.flatten()\n",
    "\n",
    "# Test different optimizers\n",
    "sgd_losses, sgd_weights = train_with_optimizer(torch.optim.SGD, lr=0.1)\n",
    "adam_losses, adam_weights = train_with_optimizer(torch.optim.Adam, lr=0.1)\n",
    "rmsprop_losses, rmsprop_weights = train_with_optimizer(torch.optim.RMSprop, lr=0.1)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(sgd_losses, label='SGD')\n",
    "plt.plot(adam_losses, label='Adam')\n",
    "plt.plot(rmsprop_losses, label='RMSprop')\n",
    "plt.title('Optimizer Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"True weights: {true_weights.flatten()}\")\n",
    "print(f\"SGD weights: {sgd_weights}\")\n",
    "print(f\"Adam weights: {adam_weights}\")\n",
    "print(f\"RMSprop weights: {rmsprop_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model state\n",
    "model = SimpleNet(3, 5, 2)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# Save entire model\n",
    "torch.save(model, 'complete_model.pth')\n",
    "\n",
    "print(\"Model saved successfully\")\n",
    "\n",
    "# Load model state (recommended approach)\n",
    "new_model = SimpleNet(3, 5, 2)\n",
    "new_model.load_state_dict(torch.load('model_weights.pth'))\n",
    "new_model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Clean up files\n",
    "import os\n",
    "os.remove('model_weights.pth')\n",
    "os.remove('complete_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
