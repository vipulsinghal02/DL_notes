{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Basics Part 9: Advanced Architectures and Specialized Domains\n\nExploring Graph Neural Networks, Vision Transformers, advanced computer vision, time series modeling, and multi-modal learning with mathematical foundations\n\n## Mathematical Framework for Advanced Architectures\n\nAdvanced neural architectures extend beyond standard feedforward networks to handle structured data, complex relationships, and multi-modal inputs:\n\n### Core Mathematical Concepts\n\n**1. Graph Neural Networks:**\n- **Message Passing**: $\\mathbf{h}_v^{(l+1)} = \\text{UPDATE}^{(l)}\\left(\\mathbf{h}_v^{(l)}, \\text{AGGREGATE}^{(l)}\\left(\\{\\mathbf{h}_u^{(l)} : u \\in \\mathcal{N}(v)\\}\\right)\\right)$\n- **Graph Convolution**: $\\mathbf{H}^{(l+1)} = \\sigma\\left(\\tilde{\\mathbf{A}}\\mathbf{H}^{(l)}\\mathbf{W}^{(l)}\\right)$\n  where $\\tilde{\\mathbf{A}} = \\mathbf{D}^{-1/2}(\\mathbf{A} + \\mathbf{I})\\mathbf{D}^{-1/2}$ is normalized adjacency\n\n**2. Vision Transformers:**\n- **Patch Embedding**: $\\mathbf{z}_0 = [\\mathbf{x}_{class}; \\mathbf{x}_{patch}^1\\mathbf{E}; \\ldots; \\mathbf{x}_{patch}^N\\mathbf{E}] + \\mathbf{E}_{pos}$\n- **Multi-Head Self-Attention**: $\\text{MSA}(\\mathbf{z}) = [\\text{head}_1; \\ldots; \\text{head}_h]\\mathbf{W}^O$\n  where $\\text{head}_i = \\text{Attention}(\\mathbf{z}\\mathbf{W}_i^Q, \\mathbf{z}\\mathbf{W}_i^K, \\mathbf{z}\\mathbf{W}_i^V)$\n\n**3. Object Detection:**\n- **Bounding Box Regression**: $t_x = (x - x_a)/w_a$, $t_y = (y - y_a)/h_a$\n  $t_w = \\log(w/w_a)$, $t_h = \\log(h/h_a)$\n- **IoU Loss**: $\\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}$\n\n**4. Time Series Modeling:**\n- **LSTM Gates**: \n  - $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$ (forget gate)\n  - $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$ (input gate)\n  - $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$ (output gate)\n- **Attention in Time**: $\\alpha_t = \\frac{\\exp(e_t)}{\\sum_{k=1}^T \\exp(e_k)}$ where $e_t = \\text{score}(h_t, s)$\n\n**5. Multi-Modal Fusion:**\n- **Early Fusion**: $\\mathbf{h} = f([\\mathbf{x}_1; \\mathbf{x}_2; \\ldots; \\mathbf{x}_m])$\n- **Late Fusion**: $\\mathbf{h} = g(f_1(\\mathbf{x}_1), f_2(\\mathbf{x}_2), \\ldots, f_m(\\mathbf{x}_m))$\n- **Cross-Modal Attention**: $\\mathbf{a}_{ij} = \\frac{\\exp(\\mathbf{h}_i^T\\mathbf{W}\\mathbf{h}_j)}{\\sum_k \\exp(\\mathbf{h}_i^T\\mathbf{W}\\mathbf{h}_k)}$\n\n**6. Inductive Biases:**\n- **Translation Equivariance** (CNNs): $f(T_g(x)) = T_g(f(x))$\n- **Permutation Invariance** (GNNs): $f(\\pi(\\mathbf{X})) = \\pi(f(\\mathbf{X}))$\n- **Sequence Modeling** (RNNs): Temporal dependencies through recurrent connections"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Graph Neural Networks (GNNs)\n\n**Mathematical Foundation:**\n\nGNNs operate on graph-structured data $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ where $\\mathcal{V}$ is the set of nodes and $\\mathcal{E}$ is the set of edges. The core idea is **message passing**:\n\n$$\\mathbf{m}_{u \\rightarrow v}^{(l)} = \\text{MESSAGE}^{(l)}(\\mathbf{h}_u^{(l)}, \\mathbf{h}_v^{(l)}, \\mathbf{e}_{u,v})$$\n\n$$\\mathbf{h}_v^{(l+1)} = \\text{UPDATE}^{(l)}\\left(\\mathbf{h}_v^{(l)}, \\text{AGGREGATE}^{(l)}\\left(\\{\\mathbf{m}_{u \\rightarrow v}^{(l)} : u \\in \\mathcal{N}(v)\\}\\right)\\right)$$\n\n**Graph Convolutional Networks (GCN):**\n$$\\mathbf{H}^{(l+1)} = \\sigma\\left(\\tilde{\\mathbf{D}}^{-1/2}\\tilde{\\mathbf{A}}\\tilde{\\mathbf{D}}^{-1/2}\\mathbf{H}^{(l)}\\mathbf{W}^{(l)}\\right)$$\n\nwhere:\n- $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ (add self-loops)\n- $\\tilde{\\mathbf{D}}_{ii} = \\sum_j \\tilde{\\mathbf{A}}_{ij}$ (degree matrix)\n- $\\mathbf{H}^{(l)} \\in \\mathbb{R}^{N \\times d_l}$ (node features)\n- $\\mathbf{W}^{(l)} \\in \\mathbb{R}^{d_l \\times d_{l+1}}$ (learnable weights)\n\nGNNs are used for social networks, molecular analysis, knowledge graphs, and recommendation systems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Graph Neural Network implementation\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Apply graph convolutions\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Final layer\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        \n",
    "        # Global pooling for graph-level predictions\n",
    "        if batch is not None:\n",
    "            x = global_mean_pool(x, batch)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create synthetic graph data\n",
    "def create_synthetic_graphs(num_graphs=100, num_nodes_range=(10, 30), num_features=5):\n",
    "    \"\"\"Create synthetic graph dataset\"\"\"\n",
    "    graphs = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(num_graphs):\n",
    "        # Random number of nodes\n",
    "        num_nodes = np.random.randint(num_nodes_range[0], num_nodes_range[1])\n",
    "        \n",
    "        # Create random graph\n",
    "        G = nx.erdos_renyi_graph(num_nodes, 0.3)\n",
    "        \n",
    "        # Node features\n",
    "        node_features = torch.randn(num_nodes, num_features)\n",
    "        \n",
    "        # Edge indices\n",
    "        edge_index = torch.LongTensor(list(G.edges)).t().contiguous()\n",
    "        if edge_index.numel() == 0:  # Handle graphs with no edges\n",
    "            edge_index = torch.LongTensor(2, 0)\n",
    "        \n",
    "        # Create graph label based on some property (e.g., number of nodes)\n",
    "        label = 1 if num_nodes > 20 else 0\n",
    "        \n",
    "        # Create PyTorch Geometric data object\n",
    "        graph_data = Data(x=node_features, edge_index=edge_index, y=torch.LongTensor([label]))\n",
    "        \n",
    "        graphs.append(graph_data)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return graphs, labels\n",
    "\n",
    "# Create synthetic graph dataset\n",
    "try:\n",
    "    graphs, graph_labels = create_synthetic_graphs(50, (5, 25), 3)\n",
    "    \n",
    "    print(f\"Created {len(graphs)} graphs\")\n",
    "    print(f\"Sample graph:\")\n",
    "    print(f\"  Nodes: {graphs[0].x.shape[0]}\")\n",
    "    print(f\"  Features per node: {graphs[0].x.shape[1]}\")\n",
    "    print(f\"  Edges: {graphs[0].edge_index.shape[1]}\")\n",
    "    print(f\"  Label: {graphs[0].y.item()}\")\n",
    "    \n",
    "    # Create GNN model\n",
    "    gnn_model = SimpleGNN(input_dim=3, hidden_dim=16, output_dim=2)\n",
    "    print(f\"\\nGNN parameters: {sum(p.numel() for p in gnn_model.parameters()):,}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    batch = Batch.from_data_list(graphs[:4])\n",
    "    output = gnn_model(batch.x, batch.edge_index, batch.batch)\n",
    "    print(f\"Batch output shape: {output.shape}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"PyTorch Geometric not available. Install with: pip install torch-geometric\")\n",
    "    print(\"Showing conceptual GNN implementation instead:\")\n",
    "    \n",
    "    # Conceptual GNN without PyTorch Geometric\n",
    "    class ConceptualGNN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "            super(ConceptualGNN, self).__init__()\n",
    "            self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "            self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "            \n",
    "        def forward(self, node_features, adjacency_matrix):\n",
    "            # Simple message passing: aggregate neighbor features\n",
    "            # H^(l+1) = Ïƒ(A * H^(l) * W^(l))\n",
    "            x = torch.mm(adjacency_matrix, node_features)\n",
    "            x = F.relu(self.linear1(x))\n",
    "            x = self.linear2(x)\n",
    "            return x\n",
    "    \n",
    "    # Example usage\n",
    "    conceptual_gnn = ConceptualGNN(5, 16, 2)\n",
    "    node_features = torch.randn(10, 5)  # 10 nodes, 5 features each\n",
    "    adjacency = torch.randn(10, 10)   # Adjacency matrix\n",
    "    output = conceptual_gnn(node_features, adjacency)\n",
    "    \n",
    "    print(f\"Conceptual GNN output shape: {output.shape}\")\n",
    "    print(f\"Parameters: {sum(p.numel() for p in conceptual_gnn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Vision Transformers (ViTs)\n\n**Mathematical Foundation:**\n\nVision Transformers adapt the transformer architecture to images by treating image patches as sequence tokens:\n\n**1. Patch Embedding:**\n$$\\mathbf{z}_0 = [\\mathbf{x}_{class}; \\mathbf{x}_{patch}^1\\mathbf{E}; \\mathbf{x}_{patch}^2\\mathbf{E}; \\ldots; \\mathbf{x}_{patch}^N\\mathbf{E}] + \\mathbf{E}_{pos}$$\n\nwhere:\n- $\\mathbf{x}_{patch}^i \\in \\mathbb{R}^{P^2 \\cdot C}$ (flattened patch)\n- $\\mathbf{E} \\in \\mathbb{R}^{P^2 \\cdot C \\times D}$ (embedding matrix)\n- $\\mathbf{E}_{pos} \\in \\mathbb{R}^{(N+1) \\times D}$ (position embeddings)\n\n**2. Multi-Head Self-Attention:**\n$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n\n$$\\text{MSA}(\\mathbf{z}) = [\\text{head}_1; \\text{head}_2; \\ldots; \\text{head}_h]\\mathbf{W}^O$$\n\n**3. Transformer Block:**\n$$\\mathbf{z}'_l = \\text{MSA}(\\text{LN}(\\mathbf{z}_{l-1})) + \\mathbf{z}_{l-1}$$\n$$\\mathbf{z}_l = \\text{MLP}(\\text{LN}(\\mathbf{z}'_l)) + \\mathbf{z}'_l$$\n\nVision Transformers excel with large datasets and global context understanding, competing with CNNs while being more scalable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformer implementation\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Convolutional layer to create patch embeddings\n",
    "        self.projection = nn.Conv2d(in_channels, embed_dim, \n",
    "                                   kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, height, width)\n",
    "        x = self.projection(x)  # (batch_size, embed_dim, num_patches_h, num_patches_w)\n",
    "        x = x.flatten(2)        # (batch_size, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)   # (batch_size, num_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_patches, embed_dim = x.shape\n",
    "        \n",
    "        # Generate Q, K, V\n",
    "        qkv = self.qkv(x).reshape(batch_size, num_patches, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, num_patches, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention computation\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = torch.matmul(attn, v)\n",
    "        x = x.transpose(1, 2).reshape(batch_size, num_patches, embed_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # MLP with residual connection\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        # Class token and position embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]  # Use class token for classification\n",
    "        \n",
    "        return self.head(cls_token_final)\n",
    "\n",
    "# Create a smaller ViT for demonstration\n",
    "vit_model = VisionTransformer(\n",
    "    img_size=32,      # Smaller image size\n",
    "    patch_size=4,     # Smaller patches\n",
    "    in_channels=3,\n",
    "    num_classes=10,\n",
    "    embed_dim=192,    # Smaller embedding dimension\n",
    "    depth=6,          # Fewer layers\n",
    "    num_heads=3,      # Fewer attention heads\n",
    "    mlp_ratio=4\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "test_input = torch.randn(2, 3, 32, 32)  # Batch of 2 images\n",
    "output = vit_model(test_input)\n",
    "\n",
    "print(f\"Vision Transformer:\")\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in vit_model.parameters()):,}\")\n",
    "print(f\"  Number of patches: {vit_model.patch_embed.num_patches}\")\n",
    "\n",
    "# Visualize attention (conceptual)\n",
    "print(f\"\\nViT Architecture Details:\")\n",
    "print(f\"  Patch size: {vit_model.patch_embed.patch_size}x{vit_model.patch_embed.patch_size}\")\n",
    "print(f\"  Embedding dim: {192}\")\n",
    "print(f\"  Transformer layers: {6}\")\n",
    "print(f\"  Attention heads per layer: {3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection with YOLO-Style Architecture\n",
    "\n",
    "Object detection involves both classification and localization of objects in images. YOLO (You Only Look Once) approaches this as a single regression problem, predicting bounding boxes and class probabilities directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified YOLO-style object detection\n",
    "class SimpleYOLO(nn.Module):\n",
    "    def __init__(self, num_classes=20, grid_size=7, num_boxes=2):\n",
    "        super(SimpleYOLO, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.grid_size = grid_size\n",
    "        self.num_boxes = num_boxes\n",
    "        \n",
    "        # Backbone network (simplified)\n",
    "        self.backbone = nn.Sequential(\n",
    "            # Convolutional layers\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((grid_size, grid_size))\n",
    "        )\n",
    "        \n",
    "        # Detection head\n",
    "        # Each grid cell predicts: (x, y, w, h, confidence) * num_boxes + class_probs\n",
    "        output_size = num_boxes * 5 + num_classes\n",
    "        self.detection_head = nn.Sequential(\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(1024, output_size, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Predict detections\n",
    "        detections = self.detection_head(features)\n",
    "        \n",
    "        # Reshape to (batch_size, grid_size, grid_size, output_size)\n",
    "        batch_size = x.size(0)\n",
    "        detections = detections.permute(0, 2, 3, 1)\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def decode_predictions(self, predictions, conf_threshold=0.5):\n",
    "        \"\"\"Decode predictions to bounding boxes\"\"\"\n",
    "        batch_size, grid_size, grid_size, _ = predictions.shape\n",
    "        detections = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            batch_detections = []\n",
    "            \n",
    "            for i in range(grid_size):\n",
    "                for j in range(grid_size):\n",
    "                    cell_pred = predictions[b, i, j]\n",
    "                    \n",
    "                    # Extract box predictions and class probabilities\n",
    "                    for box in range(self.num_boxes):\n",
    "                        start_idx = box * 5\n",
    "                        \n",
    "                        # Box coordinates (x, y, w, h)\n",
    "                        x = (j + torch.sigmoid(cell_pred[start_idx])) / grid_size\n",
    "                        y = (i + torch.sigmoid(cell_pred[start_idx + 1])) / grid_size\n",
    "                        w = torch.sigmoid(cell_pred[start_idx + 2])\n",
    "                        h = torch.sigmoid(cell_pred[start_idx + 3])\n",
    "                        \n",
    "                        # Confidence\n",
    "                        confidence = torch.sigmoid(cell_pred[start_idx + 4])\n",
    "                        \n",
    "                        # Class probabilities\n",
    "                        class_probs = F.softmax(cell_pred[self.num_boxes * 5:], dim=0)\n",
    "                        class_confidence = confidence * class_probs.max()\n",
    "                        \n",
    "                        if class_confidence > conf_threshold:\n",
    "                            batch_detections.append({\n",
    "                                'x': x.item(), 'y': y.item(),\n",
    "                                'w': w.item(), 'h': h.item(),\n",
    "                                'confidence': confidence.item(),\n",
    "                                'class': class_probs.argmax().item(),\n",
    "                                'class_prob': class_probs.max().item()\n",
    "                            })\n",
    "            \n",
    "            detections.append(batch_detections)\n",
    "        \n",
    "        return detections\n",
    "\n",
    "# Create YOLO model\n",
    "yolo_model = SimpleYOLO(num_classes=10, grid_size=7, num_boxes=2)\n",
    "\n",
    "# Test the model\n",
    "test_images = torch.randn(2, 3, 224, 224)\n",
    "predictions = yolo_model(test_images)\n",
    "\n",
    "print(f\"YOLO Object Detection:\")\n",
    "print(f\"  Input shape: {test_images.shape}\")\n",
    "print(f\"  Predictions shape: {predictions.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in yolo_model.parameters()):,}\")\n",
    "\n",
    "# Decode predictions\n",
    "with torch.no_grad():\n",
    "    detections = yolo_model.decode_predictions(predictions, conf_threshold=0.1)\n",
    "    print(f\"  Detections in first image: {len(detections[0])}\")\n",
    "    if detections[0]:\n",
    "        det = detections[0][0]\n",
    "        print(f\"  Sample detection: class={det['class']}, conf={det['confidence']:.3f}, bbox=({det['x']:.3f},{det['y']:.3f},{det['w']:.3f},{det['h']:.3f})\")\n",
    "\n",
    "# YOLO Loss function (simplified)\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, grid_size=7, num_boxes=2, num_classes=20):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Loss weights\n",
    "        self.lambda_coord = 5\n",
    "        self.lambda_noobj = 0.5\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"Simplified YOLO loss computation\"\"\"\n",
    "        # This is a conceptual implementation\n",
    "        # Real YOLO loss is more complex with IoU calculations\n",
    "        \n",
    "        batch_size = predictions.size(0)\n",
    "        \n",
    "        # Separate box coordinates, confidence, and class predictions\n",
    "        coord_loss = F.mse_loss(predictions[:, :, :, :4], targets[:, :, :, :4])\n",
    "        conf_loss = F.mse_loss(predictions[:, :, :, 4], targets[:, :, :, 4])\n",
    "        class_loss = F.cross_entropy(predictions[:, :, :, 5:].view(-1, self.num_classes),\n",
    "                                   targets[:, :, :, 5].view(-1).long())\n",
    "        \n",
    "        total_loss = self.lambda_coord * coord_loss + conf_loss + class_loss\n",
    "        return total_loss\n",
    "\n",
    "print(f\"\\nYOLO Architecture Summary:\")\n",
    "print(f\"  Grid size: {7}x{7} = {49} cells\")\n",
    "print(f\"  Boxes per cell: {2}\")\n",
    "print(f\"  Total possible detections: {7*7*2} = {98}\")\n",
    "print(f\"  Output per cell: {2*5 + 10} = {20} values (2 boxes * 5 params + 10 classes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting with Deep Learning\n",
    "\n",
    "Time series forecasting predicts future values based on historical data. Deep learning approaches can capture complex temporal patterns and non-linear relationships in sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Forecasting Models\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Use last time step for prediction\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply dropout and linear layer\n",
    "        output = self.dropout(last_output)\n",
    "        predictions = self.linear(output)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "class TransformerForecaster(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, output_size, dropout=0.1):\n",
    "        super(TransformerForecaster, self).__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, \n",
    "                                                  dim_feedforward=d_model*4, \n",
    "                                                  dropout=dropout,\n",
    "                                                  batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.output_projection = nn.Linear(d_model, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Project input and add positional encoding\n",
    "        x = self.input_projection(x)\n",
    "        x = x + self.pos_encoding[:seq_len].unsqueeze(0)\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Use last time step for prediction\n",
    "        predictions = self.output_projection(x[:, -1, :])\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Generate synthetic time series data\n",
    "def generate_time_series(n_samples=1000, seq_length=50, n_features=1):\n",
    "    \"\"\"Generate synthetic time series with trend and seasonality\"\"\"\n",
    "    time = np.arange(n_samples + seq_length)\n",
    "    \n",
    "    # Multiple components\n",
    "    trend = 0.02 * time\n",
    "    seasonal = 2 * np.sin(2 * np.pi * time / 50) + np.sin(2 * np.pi * time / 10)\n",
    "    noise = 0.5 * np.random.randn(len(time))\n",
    "    \n",
    "    series = trend + seasonal + noise\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(n_samples):\n",
    "        X.append(series[i:i+seq_length])\n",
    "        y.append(series[i+seq_length])\n",
    "    \n",
    "    X = np.array(X).reshape(n_samples, seq_length, n_features)\n",
    "    y = np.array(y).reshape(n_samples, n_features)\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.FloatTensor(y), series\n",
    "\n",
    "# Generate time series data\n",
    "X_ts, y_ts, full_series = generate_time_series(1000, 50, 1)\n",
    "\n",
    "print(f\"Time Series Data:\")\n",
    "print(f\"  Input sequences shape: {X_ts.shape}\")\n",
    "print(f\"  Targets shape: {y_ts.shape}\")\n",
    "\n",
    "# Visualize the time series\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot full series\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(full_series[:200])\n",
    "plt.title('Synthetic Time Series (First 200 points)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show sample sequence\n",
    "plt.subplot(3, 2, 2)\n",
    "sample_idx = 100\n",
    "plt.plot(range(50), X_ts[sample_idx].squeeze(), 'b-', label='Input Sequence')\n",
    "plt.plot(50, y_ts[sample_idx].item(), 'ro', label='Target', markersize=8)\n",
    "plt.title('Sample Input Sequence and Target')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Create and test models\n",
    "lstm_forecaster = LSTMForecaster(input_size=1, hidden_size=64, num_layers=2, output_size=1)\n",
    "transformer_forecaster = TransformerForecaster(input_size=1, d_model=64, nhead=4, \n",
    "                                             num_layers=3, output_size=1)\n",
    "\n",
    "# Test forward passes\n",
    "test_input = X_ts[:5]\n",
    "lstm_pred = lstm_forecaster(test_input)\n",
    "transformer_pred = transformer_forecaster(test_input)\n",
    "\n",
    "print(f\"\\nModel Comparison:\")\n",
    "print(f\"LSTM Forecaster:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in lstm_forecaster.parameters()):,}\")\n",
    "print(f\"  Prediction shape: {lstm_pred.shape}\")\n",
    "\n",
    "print(f\"Transformer Forecaster:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in transformer_forecaster.parameters()):,}\")\n",
    "print(f\"  Prediction shape: {transformer_pred.shape}\")\n",
    "\n",
    "# Simple training loop for LSTM\n",
    "def train_forecaster(model, X_train, y_train, epochs=50, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        losses.append(epoch_loss / len(dataloader))\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {losses[-1]:.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"\\nTraining LSTM forecaster...\")\n",
    "lstm_losses = train_forecaster(lstm_forecaster, X_ts[:800], y_ts[:800], epochs=30)\n",
    "\n",
    "# Evaluate on test set\n",
    "lstm_forecaster.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = lstm_forecaster(X_ts[800:900])\n",
    "    test_targets = y_ts[800:900]\n",
    "    test_mse = F.mse_loss(test_predictions, test_targets)\n",
    "    print(f\"Test MSE: {test_mse.item():.6f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(lstm_losses)\n",
    "plt.title('LSTM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(test_targets.numpy(), 'b-', label='Actual', alpha=0.7)\n",
    "plt.plot(test_predictions.numpy(), 'r-', label='Predicted', alpha=0.7)\n",
    "plt.title('LSTM Predictions vs Actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Multi-step forecasting\n",
    "def multi_step_forecast(model, initial_sequence, steps=20):\n",
    "    \"\"\"Forecast multiple steps ahead\"\"\"\n",
    "    model.eval()\n",
    "    forecasts = []\n",
    "    current_seq = initial_sequence.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            # Predict next value\n",
    "            next_pred = model(current_seq.unsqueeze(0))\n",
    "            forecasts.append(next_pred.item())\n",
    "            \n",
    "            # Update sequence (slide window)\n",
    "            current_seq = torch.cat([current_seq[1:], next_pred.unsqueeze(0)])\n",
    "    \n",
    "    return forecasts\n",
    "\n",
    "# Multi-step forecasting example\n",
    "initial_seq = X_ts[900]\n",
    "multi_forecasts = multi_step_forecast(lstm_forecaster, initial_seq, steps=20)\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(range(50), initial_seq.squeeze(), 'b-', label='Initial Sequence')\n",
    "plt.plot(range(50, 70), multi_forecasts, 'r-', label='Multi-step Forecast', marker='o')\n",
    "plt.plot(range(50, 70), full_series[950:970], 'g--', label='True Future', alpha=0.7)\n",
    "plt.title('Multi-step Forecasting')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "# Compare different sequence lengths\n",
    "sequence_lengths = [10, 20, 30, 50, 100]\n",
    "test_errors = []\n",
    "\n",
    "for seq_len in sequence_lengths:\n",
    "    # Create data with different sequence lengths\n",
    "    X_test, y_test, _ = generate_time_series(200, seq_len, 1)\n",
    "    \n",
    "    # Create and quickly train model\n",
    "    temp_model = LSTMForecaster(1, 32, 1, 1)\n",
    "    temp_losses = train_forecaster(temp_model, X_test[:150], y_test[:150], epochs=20, lr=0.01)\n",
    "    \n",
    "    # Test\n",
    "    temp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = temp_model(X_test[150:])\n",
    "        test_mse = F.mse_loss(test_pred, y_test[150:]).item()\n",
    "        test_errors.append(test_mse)\n",
    "\n",
    "plt.plot(sequence_lengths, test_errors, 'bo-')\n",
    "plt.title('Sequence Length vs Test Error')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTime Series Forecasting Summary:\")\n",
    "print(f\"  LSTM works well for sequential patterns\")\n",
    "print(f\"  Transformers can capture long-range dependencies\")\n",
    "print(f\"  Multi-step forecasting becomes less accurate over time\")\n",
    "print(f\"  Sequence length affects model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Modal Learning\n",
    "\n",
    "Multi-modal learning combines information from different types of data (text, images, audio, etc.) to make better predictions. This is increasingly important for applications like image captioning, visual question answering, and cross-modal retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-modal fusion model (Vision + Text)\n",
    "class MultiModalFusion(nn.Module):\n",
    "    def __init__(self, image_features=512, text_features=256, \n",
    "                 hidden_dim=128, num_classes=10, fusion_method='concat'):\n",
    "        super(MultiModalFusion, self).__init__()\n",
    "        self.fusion_method = fusion_method\n",
    "        \n",
    "        # Image encoder (simplified CNN)\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, image_features)\n",
    "        )\n",
    "        \n",
    "        # Text encoder (simplified)\n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Embedding(1000, 128),  # Vocabulary size 1000\n",
    "            nn.LSTM(128, text_features//2, batch_first=True, bidirectional=True)\n",
    "        )\n",
    "        \n",
    "        # Fusion layers\n",
    "        if fusion_method == 'concat':\n",
    "            fusion_input_dim = image_features + text_features\n",
    "        elif fusion_method == 'hadamard':  # Element-wise product\n",
    "            # Project to same dimension first\n",
    "            self.image_proj = nn.Linear(image_features, hidden_dim)\n",
    "            self.text_proj = nn.Linear(text_features, hidden_dim)\n",
    "            fusion_input_dim = hidden_dim\n",
    "        elif fusion_method == 'attention':\n",
    "            self.attention = nn.MultiheadAttention(image_features, num_heads=4, batch_first=True)\n",
    "            fusion_input_dim = image_features + text_features\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, text):\n",
    "        # Encode images\n",
    "        image_features = self.image_encoder(images)\n",
    "        \n",
    "        # Encode text\n",
    "        text_embedded = self.text_encoder[0](text)\n",
    "        text_lstm_out, (text_hidden, _) = self.text_encoder[1](text_embedded)\n",
    "        # Use last hidden state\n",
    "        text_features = text_hidden[-1]  # Take last layer, forward direction\n",
    "        \n",
    "        # Fusion\n",
    "        if self.fusion_method == 'concat':\n",
    "            fused_features = torch.cat([image_features, text_features], dim=1)\n",
    "        elif self.fusion_method == 'hadamard':\n",
    "            img_proj = self.image_proj(image_features)\n",
    "            txt_proj = self.text_proj(text_features)\n",
    "            fused_features = img_proj * txt_proj  # Element-wise product\n",
    "        elif self.fusion_method == 'attention':\n",
    "            # Use text as query, image as key/value\n",
    "            text_expanded = text_features.unsqueeze(1)  # Add sequence dimension\n",
    "            img_expanded = image_features.unsqueeze(1)\n",
    "            \n",
    "            attended_features, _ = self.attention(text_expanded, img_expanded, img_expanded)\n",
    "            fused_features = torch.cat([attended_features.squeeze(1), text_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(fused_features)\n",
    "        return output\n",
    "\n",
    "# Create synthetic multi-modal data\n",
    "def create_multimodal_data(n_samples=1000, img_size=32, max_text_length=20):\n",
    "    \"\"\"Create synthetic image-text pairs\"\"\"\n",
    "    # Random images\n",
    "    images = torch.randn(n_samples, 3, img_size, img_size)\n",
    "    \n",
    "    # Random text (word indices)\n",
    "    texts = torch.randint(1, 1000, (n_samples, max_text_length))\n",
    "    \n",
    "    # Labels based on some combination of image and text properties\n",
    "    # (This is synthetic - in practice, labels would be meaningful)\n",
    "    image_sum = images.mean(dim=(2, 3)).sum(dim=1)\n",
    "    text_sum = texts.float().mean(dim=1)\n",
    "    labels = ((image_sum + text_sum) > 0).long()\n",
    "    \n",
    "    return images, texts, labels\n",
    "\n",
    "# Generate multi-modal data\n",
    "mm_images, mm_texts, mm_labels = create_multimodal_data(500, 32, 15)\n",
    "\n",
    "print(f\"Multi-Modal Data:\")\n",
    "print(f\"  Images shape: {mm_images.shape}\")\n",
    "print(f\"  Texts shape: {mm_texts.shape}\")\n",
    "print(f\"  Labels shape: {mm_labels.shape}\")\n",
    "print(f\"  Label distribution: {torch.bincount(mm_labels)}\")\n",
    "\n",
    "# Test different fusion methods\n",
    "fusion_methods = ['concat', 'hadamard', 'attention']\n",
    "models = {}\n",
    "\n",
    "for method in fusion_methods:\n",
    "    model = MultiModalFusion(image_features=256, text_features=128, \n",
    "                            hidden_dim=64, num_classes=2, fusion_method=method)\n",
    "    models[method] = model\n",
    "    \n",
    "    # Test forward pass\n",
    "    test_images = mm_images[:3]\n",
    "    test_texts = mm_texts[:3]\n",
    "    \n",
    "    output = model(test_images, test_texts)\n",
    "    \n",
    "    print(f\"\\n{method.capitalize()} Fusion:\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n",
    "\n",
    "# Cross-modal attention mechanism\n",
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, dim1, dim2, hidden_dim):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        self.proj1 = nn.Linear(dim1, hidden_dim)\n",
    "        self.proj2 = nn.Linear(dim2, hidden_dim)\n",
    "        self.attention_weights = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "    def forward(self, features1, features2):\n",
    "        # Project both modalities to same dimension\n",
    "        proj1 = self.proj1(features1)\n",
    "        proj2 = self.proj2(features2)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        combined = torch.cat([proj1, proj2], dim=1)\n",
    "        attention = torch.sigmoid(self.attention_weights(combined))\n",
    "        \n",
    "        # Weighted combination\n",
    "        fused = attention * proj1 + (1 - attention) * proj2\n",
    "        \n",
    "        return fused, attention\n",
    "\n",
    "# Test cross-modal attention\n",
    "cross_attention = CrossModalAttention(256, 128, 64)\n",
    "feat1 = torch.randn(5, 256)\n",
    "feat2 = torch.randn(5, 128)\n",
    "\n",
    "fused_features, attention_weights = cross_attention(feat1, feat2)\n",
    "\n",
    "print(f\"\\nCross-Modal Attention:\")\n",
    "print(f\"  Input 1 shape: {feat1.shape}\")\n",
    "print(f\"  Input 2 shape: {feat2.shape}\")\n",
    "print(f\"  Fused features shape: {fused_features.shape}\")\n",
    "print(f\"  Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"  Sample attention weights: {attention_weights[:3, 0]}\")\n",
    "\n",
    "# Multi-modal training example\n",
    "def train_multimodal(model, images, texts, labels, epochs=30):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    dataset = TensorDataset(images, texts, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_images, batch_texts, batch_labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(batch_images, batch_texts)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "# Train concat fusion model\n",
    "print(\"\\nTraining concatenation fusion model...\")\n",
    "concat_model = models['concat']\n",
    "concat_losses, concat_accuracies = train_multimodal(\n",
    "    concat_model, mm_images[:400], mm_texts[:400], mm_labels[:400], epochs=20\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "concat_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = concat_model(mm_images[400:], mm_texts[400:])\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    test_accuracy = (predicted == mm_labels[400:]).float().mean() * 100\n",
    "    print(f\"Test accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Visualize training\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(concat_losses)\n",
    "plt.title('Multi-Modal Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(concat_accuracies)\n",
    "plt.title('Multi-Modal Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMulti-Modal Learning Summary:\")\n",
    "print(f\"  Fusion methods: concatenation, element-wise product, attention\")\n",
    "print(f\"  Cross-modal attention allows dynamic weighting\")\n",
    "print(f\"  Each modality contributes complementary information\")\n",
    "print(f\"  Applications: image captioning, VQA, multimodal retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Architecture Best Practices\n",
    "\n",
    "Working with advanced architectures requires understanding their specific characteristics, training requirements, and application domains. Here's a comprehensive guide to best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Advanced Architectures: Best Practices and Guidelines\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "architecture_guide = {\n",
    "    \"Graph Neural Networks (GNNs)\": {\n",
    "        \"When to Use\": [\n",
    "            \"Node/graph classification tasks\",\n",
    "            \"Social network analysis\",\n",
    "            \"Molecular property prediction\",\n",
    "            \"Knowledge graph reasoning\",\n",
    "            \"Recommendation systems\"\n",
    "        ],\n",
    "        \"Key Considerations\": [\n",
    "            \"Choose appropriate graph convolution (GCN, GraphSAGE, GAT)\",\n",
    "            \"Handle variable graph sizes with batching\",\n",
    "            \"Consider over-smoothing in deep GNNs\",\n",
    "            \"Use appropriate pooling for graph-level tasks\",\n",
    "            \"Be aware of scalability with large graphs\"\n",
    "        ],\n",
    "        \"Common Pitfalls\": [\n",
    "            \"Ignoring graph structure in data preprocessing\",\n",
    "            \"Using too many layers (over-smoothing)\",\n",
    "            \"Not handling isolated nodes properly\",\n",
    "            \"Inadequate graph augmentation strategies\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Vision Transformers (ViTs)\": {\n",
    "        \"When to Use\": [\n",
    "            \"Large-scale image classification\",\n",
    "            \"When you have abundant training data\",\n",
    "            \"Transfer learning from large pre-trained models\",\n",
    "            \"Tasks requiring global context\"\n",
    "        ],\n",
    "        \"Key Considerations\": [\n",
    "            \"Need large datasets or pre-trained models\",\n",
    "            \"Patch size affects model granularity\",\n",
    "            \"Position embeddings are crucial\",\n",
    "            \"Class token design for classification\",\n",
    "            \"Computational cost scales quadratically\"\n",
    "        ],\n",
    "        \"Common Pitfalls\": [\n",
    "            \"Training from scratch on small datasets\",\n",
    "            \"Using inappropriate patch sizes\",\n",
    "            \"Neglecting proper data augmentation\",\n",
    "            \"Not leveraging pre-trained models\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Object Detection (YOLO-style)\": {\n",
    "        \"When to Use\": [\n",
    "            \"Real-time object detection\",\n",
    "            \"Multiple object localization\",\n",
    "            \"Autonomous driving applications\",\n",
    "            \"Surveillance systems\"\n",
    "        ],\n",
    "        \"Key Considerations\": [\n",
    "            \"Anchor box design affects performance\",\n",
    "            \"Non-maximum suppression for post-processing\",\n",
    "            \"Multi-scale detection for different object sizes\",\n",
    "            \"Loss function balances classification and localization\",\n",
    "            \"Data augmentation specific to bounding boxes\"\n",
    "        ],\n",
    "        \"Common Pitfalls\": [\n",
    "            \"Imbalanced positive/negative samples\",\n",
    "            \"Inappropriate anchor box sizes\",\n",
    "            \"Ignoring IoU thresholds in evaluation\",\n",
    "            \"Not handling small objects properly\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Time Series Models\": {\n",
    "        \"When to Use\": [\n",
    "            \"Sequential data prediction\",\n",
    "            \"Financial forecasting\",\n",
    "            \"Weather prediction\",\n",
    "            \"IoT sensor data analysis\"\n",
    "        ],\n",
    "        \"Key Considerations\": [\n",
    "            \"Sequence length affects model memory\",\n",
    "            \"Stationarity assumptions in traditional methods\",\n",
    "            \"Multi-variate vs univariate forecasting\",\n",
    "            \"Seasonality and trend handling\",\n",
    "            \"Multi-step vs single-step predictions\"\n",
    "        ],\n",
    "        \"Common Pitfalls\": [\n",
    "            \"Data leakage in train/test splits\",\n",
    "            \"Not accounting for temporal dependencies\",\n",
    "            \"Ignoring data preprocessing (scaling, differencing)\",\n",
    "            \"Overfitting to recent patterns\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Multi-Modal Learning\": {\n",
    "        \"When to Use\": [\n",
    "            \"Image captioning\",\n",
    "            \"Visual question answering\",\n",
    "            \"Cross-modal retrieval\",\n",
    "            \"Multimodal sentiment analysis\"\n",
    "        ],\n",
    "        \"Key Considerations\": [\n",
    "            \"Choose appropriate fusion strategy\",\n",
    "            \"Handle modality imbalance\",\n",
    "            \"Alignment between modalities\",\n",
    "            \"Missing modality robustness\",\n",
    "            \"Cross-modal attention mechanisms\"\n",
    "        ],\n",
    "        \"Common Pitfalls\": [\n",
    "            \"One modality dominating the learning\",\n",
    "            \"Poor temporal/spatial alignment\",\n",
    "            \"Inadequate fusion strategy\",\n",
    "            \"Not handling missing modalities\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for architecture, info in architecture_guide.items():\n",
    "    print(f\"\\n{architecture}:\")\n",
    "    print(f\"  When to Use:\")\n",
    "    for use_case in info[\"When to Use\"]:\n",
    "        print(f\"    â€¢ {use_case}\")\n",
    "    \n",
    "    print(f\"  Key Considerations:\")\n",
    "    for consideration in info[\"Key Considerations\"]:\n",
    "        print(f\"    â€¢ {consideration}\")\n",
    "    \n",
    "    print(f\"  Common Pitfalls:\")\n",
    "    for pitfall in info[\"Common Pitfalls\"]:\n",
    "        print(f\"    âš   {pitfall}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERAL BEST PRACTICES FOR ADVANCED ARCHITECTURES:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "general_practices = {\n",
    "    \"Data Preparation\": [\n",
    "        \"Understand data structure and relationships\",\n",
    "        \"Apply domain-specific preprocessing\",\n",
    "        \"Use appropriate data augmentation\",\n",
    "        \"Handle missing or corrupted data gracefully\",\n",
    "        \"Ensure proper train/validation/test splits\"\n",
    "    ],\n",
    "    \n",
    "    \"Model Design\": [\n",
    "        \"Start with established architectures\",\n",
    "        \"Gradually increase model complexity\",\n",
    "        \"Use appropriate inductive biases\",\n",
    "        \"Consider computational constraints\",\n",
    "        \"Design for interpretability when needed\"\n",
    "    ],\n",
    "    \n",
    "    \"Training Strategies\": [\n",
    "        \"Use transfer learning when possible\",\n",
    "        \"Implement proper regularization\",\n",
    "        \"Monitor multiple metrics\",\n",
    "        \"Use learning rate scheduling\",\n",
    "        \"Apply gradient clipping for stability\"\n",
    "    ],\n",
    "    \n",
    "    \"Evaluation & Debugging\": [\n",
    "        \"Use domain-appropriate metrics\",\n",
    "        \"Validate on realistic test cases\",\n",
    "        \"Visualize intermediate representations\",\n",
    "        \"Perform ablation studies\",\n",
    "        \"Test edge cases and failure modes\"\n",
    "    ],\n",
    "    \n",
    "    \"Production Considerations\": [\n",
    "        \"Profile computational requirements\",\n",
    "        \"Plan for model versioning\",\n",
    "        \"Implement monitoring and alerting\",\n",
    "        \"Consider model compression techniques\",\n",
    "        \"Design for graceful degradation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in general_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  â€¢ {practice}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ARCHITECTURE SELECTION GUIDE:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "selection_guide = {\n",
    "    \"Data Type\": {\n",
    "        \"Images\": \"CNNs, Vision Transformers, Object Detection models\",\n",
    "        \"Text\": \"RNNs, LSTMs, Transformers, BERT-style models\",\n",
    "        \"Graphs\": \"Graph Neural Networks (GCN, GraphSAGE, GAT)\",\n",
    "        \"Time Series\": \"RNNs, LSTMs, Temporal CNNs, Transformers\",\n",
    "        \"Multi-Modal\": \"Fusion architectures, Cross-modal attention\"\n",
    "    },\n",
    "    \n",
    "    \"Task Type\": {\n",
    "        \"Classification\": \"Standard architectures with classification heads\",\n",
    "        \"Regression\": \"Similar to classification but with regression outputs\",\n",
    "        \"Generation\": \"Generative models (GANs, VAEs, Autoregressive)\",\n",
    "        \"Detection\": \"Object detection architectures (YOLO, R-CNN)\",\n",
    "        \"Segmentation\": \"U-Net, FCN, DeepLab architectures\"\n",
    "    },\n",
    "    \n",
    "    \"Dataset Size\": {\n",
    "        \"Small (<1K)\": \"Transfer learning, data augmentation, simple models\",\n",
    "        \"Medium (1K-100K)\": \"Fine-tuning, moderate complexity models\",\n",
    "        \"Large (100K-1M)\": \"Custom architectures, from-scratch training\",\n",
    "        \"Very Large (>1M)\": \"Large-scale architectures, distributed training\"\n",
    "    },\n",
    "    \n",
    "    \"Computational Budget\": {\n",
    "        \"Limited\": \"Lightweight models, pruning, quantization\",\n",
    "        \"Moderate\": \"Standard architectures, efficient implementations\",\n",
    "        \"High\": \"Large models, ensemble methods, extensive search\",\n",
    "        \"Unlimited\": \"Massive models, neural architecture search\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for criterion, recommendations in selection_guide.items():\n",
    "    print(f\"\\n{criterion}:\")\n",
    "    for key, value in recommendations.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Remember: The best architecture depends on your specific problem,\")\n",
    "print(\"data characteristics, computational constraints, and performance requirements.\")\n",
    "print(\"Start simple, iterate based on results, and always validate thoroughly!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}