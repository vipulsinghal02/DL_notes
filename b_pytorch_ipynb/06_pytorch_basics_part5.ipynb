{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Basics Part 5: Advanced Topics and Production\n\nAdvanced PyTorch features, model deployment, and production considerations with mathematical foundations\n\n## Mathematical Framework for Advanced Training\n\n**Advanced training techniques** optimize computational efficiency and numerical stability through mathematical innovations:\n\n### Core Mathematical Concepts\n\n**1. Numerical Precision and Stability:**\nFor floating-point representation with mantissa $m$, exponent $e$, and bias $b$:\n$$\\text{FP32}: (-1)^s \\times 1.m \\times 2^{e-127}, \\quad e \\in [0, 255]$$\n$$\\text{FP16}: (-1)^s \\times 1.m \\times 2^{e-15}, \\quad e \\in [0, 31]$$\n\n**2. Mixed Precision Training Mathematics:**\nForward pass in reduced precision: $\\mathbf{y} = f(\\mathbf{x}; \\boldsymbol{\\theta}_{\\text{FP16}})$\nGradient computation with scaling: $\\mathbf{g}_{\\text{scaled}} = S \\cdot \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$\nWhere $S$ is a loss scaling factor to prevent gradient underflow.\n\n**3. Model Compression Theory:**\n**Quantization**: Maps continuous weights $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ to discrete set:\n$$Q(\\mathbf{W}) = \\text{round}\\left(\\frac{\\mathbf{W} - Z}{S}\\right) \\cdot S + Z$$\nWhere $S$ is scale factor and $Z$ is zero-point.\n\n**4. Production Optimization:**\n**Inference Efficiency**: For model $f$ with parameters $\\boldsymbol{\\theta}$:\n- **Latency**: $T_{\\text{inference}} = T_{\\text{compute}} + T_{\\text{memory}} + T_{\\text{io}}$\n- **Throughput**: $\\tau = \\frac{\\text{batch\\_size}}{T_{\\text{inference}}}$\n- **Memory usage**: $M = M_{\\text{model}} + M_{\\text{activations}} + M_{\\text{gradients}}$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.jit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Mixed Precision Training\n\n### Mathematical Foundation of Mixed Precision\n\n**Mixed Precision Training** uses both FP16 and FP32 precision to optimize memory and speed:\n\n**Numerical Stability Analysis:**\n- **FP16 range**: $\\approx [6 \\times 10^{-5}, 65504]$\n- **FP32 range**: $\\approx [1.4 \\times 10^{-45}, 3.4 \\times 10^{38}]$\n\n**Gradient Scaling Mathematics:**\nTo prevent gradient underflow in FP16:\n$$\\mathbf{g}_{\\text{scaled}} = S \\cdot \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta})$$\n$$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\cdot \\frac{\\mathbf{g}_{\\text{scaled}}}{S}$$\n\n**Dynamic Loss Scaling:**\n$$S_{t+1} = \\begin{cases} \nS_t \\times 2 & \\text{if no overflow for } N \\text{ steps} \\\\\nS_t / 2 & \\text{if overflow detected} \\\\\nS_t & \\text{otherwise}\n\\end{cases}$$\n\n**Memory Reduction:**\n- **Activations**: FP16 storage reduces memory by ~50%\n- **Gradients**: FP16 communication reduces bandwidth\n- **Master weights**: FP32 maintains training stability\n\n**Performance Benefits:**\n- **Tensor Cores**: Specialized hardware for FP16 matrix operations\n- **Bandwidth**: Reduced memory transfers\n- **Compute**: Faster arithmetic operations on modern GPUs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision training (requires CUDA)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Simple model for demonstration\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model and data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create synthetic data\n",
    "x = torch.randn(1000, 100)\n",
    "y = torch.randint(0, 10, (1000,))\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Mixed precision training will be demonstrated\")\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    print(\"Mixed precision requires CUDA, will show regular training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with mixed precision (if CUDA available)\n",
    "def train_with_mixed_precision(model, dataloader, optimizer, criterion, scaler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Mixed precision backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Regular training function\n",
    "def train_regular(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Train for a few epochs\n",
    "if torch.cuda.is_available():\n",
    "    for epoch in range(3):\n",
    "        loss = train_with_mixed_precision(model, dataloader, optimizer, criterion, scaler, device)\n",
    "        print(f\"Epoch {epoch+1}, Mixed Precision Loss: {loss:.4f}\")\nelse:\n",
    "    for epoch in range(3):\n",
    "        loss = train_regular(model, dataloader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}, Regular Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Quantization\n\n### Mathematical Theory of Neural Network Quantization\n\n**Quantization** reduces model precision while maintaining accuracy through careful mathematical mapping:\n\n**Uniform Quantization Mathematics:**\nFor weights $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$, quantization maps to discrete set:\n$$Q(\\mathbf{W}) = \\text{clip}\\left(\\text{round}\\left(\\frac{\\mathbf{W}}{S}\\right), q_{\\min}, q_{\\max}\\right)$$\n\nWhere:\n- **Scale factor**: $S = \\frac{\\max(\\mathbf{W}) - \\min(\\mathbf{W})}{q_{\\max} - q_{\\min}}$\n- **Zero point**: $Z = q_{\\min} - \\text{round}\\left(\\frac{\\min(\\mathbf{W})}{S}\\right)$\n\n**Asymmetric Quantization:**\n$$Q(\\mathbf{W}) = \\text{round}\\left(\\frac{\\mathbf{W}}{S} + Z\\right)$$\n$$\\text{Dequantize}: \\tilde{\\mathbf{W}} = S(Q(\\mathbf{W}) - Z)$$\n\n**Quantization Error Analysis:**\nApproximation error: $\\epsilon = \\mathbf{W} - \\tilde{\\mathbf{W}}$\nMean squared error: $\\text{MSE} = \\mathbb{E}[(\\mathbf{W} - \\tilde{\\mathbf{W}})^2]$\n\n**Post-Training Quantization:**\n1. **Calibration**: Use representative data to compute optimal scales\n2. **Statistical analysis**: $S = \\frac{\\text{percentile}(\\mathbf{W}, 99.9)}{\\text{MAX\\_INT}}$\n3. **Layer-wise optimization**: Minimize $\\|\\mathbf{W} - Q(\\mathbf{W})\\|_2^2$\n\n**Quantization-Aware Training:**\nInclude quantization in forward pass:\n$$\\mathbf{y} = f(\\mathbf{x}; Q(\\boldsymbol{\\theta}))$$\nStraight-through estimator for gradients:\n$$\\frac{\\partial L}{\\partial \\boldsymbol{\\theta}} = \\frac{\\partial L}{\\partial Q(\\boldsymbol{\\theta})} \\cdot \\mathbf{1}$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-training quantization\n",
    "import torch.quantization as quantization\n",
    "\n",
    "# Create a model for quantization\n",
    "class QuantizableNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizableNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 10)\n",
    "        self.fc2 = nn.Linear(10, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create and train a simple model\n",
    "model_fp32 = QuantizableNet()\n",
    "model_fp32.eval()\n",
    "\n",
    "# Create calibration data\n",
    "calibration_data = torch.randn(100, 20)\n",
    "\n",
    "# Post-training quantization\n",
    "model_fp32.qconfig = quantization.get_default_qconfig('fbgemm')\n",
    "model_prepared = quantization.prepare(model_fp32)\n",
    "\n",
    "# Calibrate with sample data\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        model_prepared(calibration_data[i:i+10])\n",
    "\n",
    "# Convert to quantized model\n",
    "model_quantized = quantization.convert(model_prepared)\n",
    "\n",
    "print(\"Quantization completed\")\n",
    "print(f\"Original model size: {sum(p.numel() * 4 for p in model_fp32.parameters())} bytes (FP32)\")\n",
    "print(f\"Quantized model created (approximately 4x smaller)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inference speed and accuracy\n",
    "test_input = torch.randn(100, 20)\n",
    "\n",
    "# Measure FP32 model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    output_fp32 = model_fp32(test_input)\n",
    "fp32_time = time.time() - start_time\n",
    "\n",
    "# Measure quantized model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    output_quantized = model_quantized(test_input)\n",
    "quantized_time = time.time() - start_time\n",
    "\n",
    "# Compare outputs\n",
    "mse_error = F.mse_loss(output_fp32, output_quantized)\n",
    "\n",
    "print(f\"FP32 inference time: {fp32_time:.4f} seconds\")\n",
    "print(f\"Quantized inference time: {quantized_time:.4f} seconds\")\n",
    "print(f\"Speedup: {fp32_time/quantized_time:.2f}x\")\n",
    "print(f\"MSE between outputs: {mse_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## TorchScript and Model Serialization\n\n### Mathematical Foundation of Graph Compilation\n\n**TorchScript** converts dynamic PyTorch models into static computational graphs for optimization:\n\n**Computational Graph Representation:**\nA neural network as directed acyclic graph $G = (V, E)$ where:\n- **Vertices**: $V = \\{v_1, v_2, \\ldots, v_n\\}$ (operations/tensors)\n- **Edges**: $E \\subseteq V \\times V$ (data dependencies)\n- **Execution order**: Topological sort of $G$\n\n**Tracing Mathematics:**\nGiven input $\\mathbf{x}_0$, record operations:\n$$\\mathbf{x}_1 = f_1(\\mathbf{x}_0), \\quad \\mathbf{x}_2 = f_2(\\mathbf{x}_1), \\quad \\ldots, \\quad \\mathbf{y} = f_n(\\mathbf{x}_{n-1})$$\n\nBuild graph: $G_{\\text{trace}} = \\{(f_i, \\text{input\\_shapes}_i, \\text{output\\_shapes}_i)\\}$\n\n**Scripting vs Tracing:**\n\n**Tracing**: Records actual execution path\n- **Pros**: Captures exact computation for given input\n- **Cons**: Cannot handle dynamic control flow\n\n**Scripting**: Analyzes Python AST\n- **Pros**: Preserves control flow (if/else, loops)\n- **Cons**: Limited Python subset support\n\n**Optimization Passes:**\n1. **Dead code elimination**: Remove unused operations\n2. **Constant propagation**: Pre-compute constant expressions\n3. **Operator fusion**: Combine multiple ops into efficient kernels\n4. **Memory optimization**: Reduce intermediate allocations\n\n**Serialization Format:**\nTorchScript uses Protocol Buffers for:\n$$\\text{Model} = (\\text{Graph}, \\text{Parameters}, \\text{Metadata})$$\n\n**Just-In-Time Compilation:**\n- **Graph optimization**: $G_{\\text{optimized}} = \\text{optimize}(G_{\\text{original}})$\n- **Kernel selection**: Choose optimal implementation for hardware\n- **Memory planning**: Minimize memory footprint"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchScript for model serialization and optimization\n",
    "class ScriptableNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScriptableNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc = nn.Linear(32 * 8 * 8, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = ScriptableNet()\n",
    "model.eval()\n",
    "\n",
    "# Method 1: Tracing\n",
    "example_input = torch.randn(1, 3, 32, 32)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "# Method 2: Scripting\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "print(\"TorchScript models created\")\n",
    "print(f\"Original model type: {type(model)}\")\n",
    "print(f\"Traced model type: {type(traced_model)}\")\n",
    "print(f\"Scripted model type: {type(scripted_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load TorchScript models\n",
    "traced_model.save('traced_model.pt')\n",
    "scripted_model.save('scripted_model.pt')\n",
    "\n",
    "# Load models\n",
    "loaded_traced = torch.jit.load('traced_model.pt')\n",
    "loaded_scripted = torch.jit.load('scripted_model.pt')\n",
    "\n",
    "# Test inference\n",
    "test_input = torch.randn(2, 3, 32, 32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_output = model(test_input)\n",
    "    traced_output = loaded_traced(test_input)\n",
    "    scripted_output = loaded_scripted(test_input)\n",
    "\n",
    "print(\"Inference test:\")\n",
    "print(f\"Original output shape: {original_output.shape}\")\n",
    "print(f\"Traced output shape: {traced_output.shape}\")\n",
    "print(f\"Scripted output shape: {scripted_output.shape}\")\n",
    "print(f\"Outputs match (traced): {torch.allclose(original_output, traced_output)}\")\n",
    "print(f\"Outputs match (scripted): {torch.allclose(original_output, scripted_output)}\")\n",
    "\n",
    "# Clean up files\n",
    "import os\n",
    "os.remove('traced_model.pt')\n",
    "os.remove('scripted_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Custom CUDA Kernels and Autograd\n\n### Mathematical Foundation of Custom Operations\n\n**Custom autograd functions** extend PyTorch's automatic differentiation for specialized operations:\n\n**Autograd System Mathematics:**\nFor function $y = f(x)$, autograd maintains:\n- **Forward pass**: $y = f(x)$\n- **Backward pass**: $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$\n\n**Chain Rule Implementation:**\nFor composite function $L = g(f(x))$:\n$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial g} \\cdot \\frac{\\partial g}{\\partial f} \\cdot \\frac{\\partial f}{\\partial x}$$\n\n**Custom Function Design:**\nDefine $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with:\n$$\\text{forward}(\\mathbf{x}) = f(\\mathbf{x})$$\n$$\\text{backward}(\\mathbf{g}) = \\mathbf{J}^T \\mathbf{g}$$\n\nWhere $\\mathbf{J} = \\frac{\\partial f}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{m \\times n}$ is the Jacobian.\n\n**CUDA Kernel Mathematics:**\nFor element-wise operation $y_i = f(x_i)$:\n- **Thread mapping**: $\\text{thread\\_id} \\rightarrow \\text{array\\_index}$\n- **Memory coalescing**: Adjacent threads access contiguous memory\n- **Parallel execution**: $N$ threads compute $N$ elements simultaneously\n\n**Memory Hierarchy:**\n- **Global memory**: Large, high latency\n- **Shared memory**: Fast, limited size per block\n- **Registers**: Fastest, limited per thread\n\n**Performance Optimization:**\n- **Occupancy**: $\\frac{\\text{active\\_warps}}{\\text{max\\_warps}}$\n- **Memory bandwidth**: $\\frac{\\text{bytes\\_accessed}}{\\text{time}}$\n- **Arithmetic intensity**: $\\frac{\\text{FLOPs}}{\\text{bytes\\_accessed}}$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom autograd function\n",
    "class SquareFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # Save input for backward pass\n",
    "        ctx.save_for_backward(input)\n",
    "        return input ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved input\n",
    "        input, = ctx.saved_tensors\n",
    "        # Gradient of x^2 is 2x\n",
    "        return grad_output * 2 * input\n",
    "\n",
    "# Use custom function\n",
    "def custom_square(x):\n",
    "    return SquareFunction.apply(x)\n",
    "\n",
    "# Test custom function\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "y = custom_square(x)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {y}\")\n",
    "print(f\"Gradient: {x.grad}\")\n",
    "print(f\"Expected gradient (2x): {2 * x}\")\n",
    "print(f\"Gradients match: {torch.allclose(x.grad, 2 * x.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Checkpointing and State Management\n\n### Mathematical Framework for Training State\n\n**Model checkpointing** preserves complete training state for reproducible deep learning:\n\n**Training State Representation:**\nComplete state $S_t$ at epoch $t$ includes:\n$$S_t = (\\boldsymbol{\\theta}_t, \\mathbf{m}_t, \\mathbf{v}_t, \\eta_t, \\mathcal{R}_t)$$\n\nWhere:\n- $\\boldsymbol{\\theta}_t$: Model parameters\n- $\\mathbf{m}_t, \\mathbf{v}_t$: Optimizer momentum/velocity states  \n- $\\eta_t$: Learning rate schedule state\n- $\\mathcal{R}_t$: Random number generator state\n\n**Optimizer State Mathematics:**\n\n**Adam optimizer** maintains:\n$$\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1)\\mathbf{g}_t$$\n$$\\mathbf{v}_t = \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2)\\mathbf{g}_t^2$$\n$$\\hat{\\mathbf{m}}_t = \\frac{\\mathbf{m}_t}{1-\\beta_1^t}, \\quad \\hat{\\mathbf{v}}_t = \\frac{\\mathbf{v}_t}{1-\\beta_2^t}$$\n\n**Learning Rate Scheduling:**\nStep decay: $\\eta_t = \\eta_0 \\gamma^{\\lfloor t/T \\rfloor}$\nExponential decay: $\\eta_t = \\eta_0 e^{-\\lambda t}$\nCosine annealing: $\\eta_t = \\eta_{\\min} + (\\eta_{\\max} - \\eta_{\\min})\\frac{1 + \\cos(\\pi t/T)}{2}$\n\n**Checkpoint Validation:**\nEnsure mathematical consistency:\n1. **Parameter integrity**: $\\|\\boldsymbol{\\theta}_{\\text{loaded}} - \\boldsymbol{\\theta}_{\\text{saved}}\\|_2 < \\epsilon$\n2. **Optimizer state**: Verify momentum terms match\n3. **Reproducibility**: Same random seed → same results\n\n**Distributed Checkpointing:**\nFor model parallelism across $N$ devices:\n$$\\boldsymbol{\\theta} = [\\boldsymbol{\\theta}_1, \\boldsymbol{\\theta}_2, \\ldots, \\boldsymbol{\\theta}_N]$$\nEach device saves: $\\{\\boldsymbol{\\theta}_i, \\text{rank}_i, \\text{world\\_size}\\}$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive checkpointing\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss, filename):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        'loss': loss,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def load_checkpoint(filename, model, optimizer=None, scheduler=None):\n",
    "    checkpoint = torch.load(filename, map_location='cpu')\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Checkpoint loaded: epoch {checkpoint['epoch']}, loss: {checkpoint['loss']:.4f}\")\n",
    "    return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "# Example usage\n",
    "model = SimpleNet()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Save checkpoint\n",
    "save_checkpoint(model, optimizer, scheduler, epoch=5, loss=0.123, filename='checkpoint.pth')\n",
    "\n",
    "# Create new instances\n",
    "new_model = SimpleNet()\n",
    "new_optimizer = optim.Adam(new_model.parameters(), lr=0.001)\n",
    "new_scheduler = optim.lr_scheduler.StepLR(new_optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Load checkpoint\n",
    "epoch, loss = load_checkpoint('checkpoint.pth', new_model, new_optimizer, new_scheduler)\n",
    "\n",
    "# Clean up\n",
    "os.remove('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Profiling and Performance Analysis\n\n### Mathematical Framework for Performance Optimization\n\n**Performance profiling** quantifies computational bottlenecks through systematic measurement:\n\n**Performance Metrics:**\n\n**Computational Complexity:**\n- **Time complexity**: $T(n) = O(f(n))$ for input size $n$\n- **Space complexity**: $S(n) = O(g(n))$ for memory usage\n- **FLOPs count**: Floating-point operations per forward/backward pass\n\n**Hardware Utilization:**\n- **GPU occupancy**: $\\text{Occupancy} = \\frac{\\text{Active warps}}{\\text{Max warps per SM}}$\n- **Memory bandwidth**: $\\text{BW} = \\frac{\\text{Bytes transferred}}{\\text{Time}}$\n- **Arithmetic intensity**: $\\text{AI} = \\frac{\\text{FLOPs}}{\\text{Bytes accessed}}$\n\n**Roofline Model:**\nPerformance bound: $\\text{Performance} \\leq \\min(\\text{Peak FLOPS}, \\text{AI} \\times \\text{Peak BW})$\n\n**Profiling Mathematics:**\n\n**Statistical Profiling:**\nSample execution times $\\{t_1, t_2, \\ldots, t_n\\}$:\n- **Mean**: $\\bar{t} = \\frac{1}{n}\\sum_{i=1}^n t_i$\n- **Variance**: $\\sigma^2 = \\frac{1}{n-1}\\sum_{i=1}^n (t_i - \\bar{t})^2$\n- **Confidence interval**: $\\bar{t} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$\n\n**Bottleneck Analysis:**\nFor operations $\\{O_1, O_2, \\ldots, O_k\\}$ with times $\\{t_1, t_2, \\ldots, t_k\\}$:\n- **Critical path**: $T_{\\text{total}} = \\max_i(t_i)$ (parallel) or $\\sum_i t_i$ (serial)\n- **Utilization**: $U_i = \\frac{t_i}{T_{\\text{total}}}$\n- **Amdahl's law**: $S = \\frac{1}{(1-P) + \\frac{P}{N}}$ where $P$ is parallelizable fraction\n\n**Memory Analysis:**\n- **Cache hit ratio**: $H = \\frac{\\text{Cache hits}}{\\text{Total accesses}}$\n- **Memory throughput**: $\\text{Throughput} = \\frac{\\text{Data size} \\times \\text{Batch size}}{\\text{Time}}$\n- **Memory efficiency**: $\\eta_{\\text{mem}} = \\frac{\\text{Useful bytes}}{\\text{Total bytes accessed}}$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling PyTorch operations\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Model for profiling\n",
    "class ProfileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProfileNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(64 * 16 * 16, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with record_function(\"conv_layer\"):\n",
    "            x = F.relu(self.conv(x))\n",
    "        \n",
    "        with record_function(\"pooling\"):\n",
    "            x = self.pool(x)\n",
    "            \n",
    "        with record_function(\"flatten\"):\n",
    "            x = x.view(x.size(0), -1)\n",
    "            \n",
    "        with record_function(\"fc_layer\"):\n",
    "            x = self.fc(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "model = ProfileNet()\n",
    "inputs = torch.randn(8, 3, 32, 32)\n",
    "\n",
    "# Profile the model\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        output = model(inputs)\n",
    "\n",
    "# Print profiling results\n",
    "print(\"Profiling Results:\")\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Distributed Training Mathematics\n\n### Mathematical Foundation of Parallel Deep Learning\n\n**Distributed training** scales deep learning across multiple devices through mathematical parallelization strategies:\n\n**Data Parallelism:**\nModel replicated across $N$ devices, batch split:\n$$\\mathcal{B} = \\mathcal{B}_1 \\cup \\mathcal{B}_2 \\cup \\cdots \\cup \\mathcal{B}_N, \\quad |\\mathcal{B}_i| = \\frac{|\\mathcal{B}|}{N}$$\n\nLocal gradients: $\\mathbf{g}_i = \\nabla_{\\boldsymbol{\\theta}} L(\\mathcal{B}_i; \\boldsymbol{\\theta})$\nGlobal gradient: $\\mathbf{g} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{g}_i$\n\n**Model Parallelism:**\nParameters split across devices: $\\boldsymbol{\\theta} = [\\boldsymbol{\\theta}_1, \\boldsymbol{\\theta}_2, \\ldots, \\boldsymbol{\\theta}_N]$\nSequential computation: $\\mathbf{h}_i = f_i(\\mathbf{h}_{i-1}; \\boldsymbol{\\theta}_i)$\n\n**Communication Patterns:**\n\n**AllReduce Algorithm:**\nReduces communication complexity from $O(N^2)$ to $O(N \\log N)$:\n1. **Reduce-scatter**: Each device gets partial sum\n2. **AllGather**: Broadcast complete result\n\n**Ring AllReduce:**\nCommunication time: $T_{\\text{comm}} = \\alpha (N-1) + \\frac{\\beta M (N-1)}{N}$\nWhere $\\alpha$ is latency, $\\beta$ is bandwidth, $M$ is message size.\n\n**Gradient Compression:**\nReduce communication by compressing gradients:\n$$\\tilde{\\mathbf{g}} = Q(\\mathbf{g}) \\text{ where } \\mathbb{E}[Q(\\mathbf{g})] = \\mathbf{g}$$\n\n**Scaling Analysis:**\n\n**Linear Speedup**: $S(N) = N$ (ideal case)\n**Strong scaling**: Fixed problem size, increase processors\n**Weak scaling**: Fixed problem size per processor\n\n**Efficiency**: $E(N) = \\frac{S(N)}{N} = \\frac{T(1)}{N \\cdot T(N)}$\n\n**Communication-Computation Overlap:**\nOverlap gradient computation with communication:\n$$T_{\\text{total}} = \\max(T_{\\text{compute}}, T_{\\text{communication}})$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed training setup (conceptual - requires multiple GPUs/nodes)\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup_distributed(rank, world_size):\n",
    "    \"\"\"Initialize distributed training\"\"\"\n",
    "    # This is conceptual - actual implementation depends on your setup\n",
    "    print(f\"Setting up distributed training: rank {rank}, world_size {world_size}\")\n",
    "    # dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup_distributed():\n",
    "    \"\"\"Clean up distributed training\"\"\"\n",
    "    # dist.destroy_process_group()\n",
    "    pass\n",
    "\n",
    "# Data parallel training (single machine, multiple GPUs)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "else:\n",
    "    print(\"Single GPU/CPU training\")\n",
    "\n",
    "print(\"Distributed training concepts demonstrated\")\n",
    "print(\"For actual multi-GPU training:\")\n",
    "print(\"1. Use torch.nn.DataParallel for single-machine multi-GPU\")\n",
    "print(\"2. Use torch.nn.parallel.DistributedDataParallel for multi-machine\")\n",
    "print(\"3. Initialize process groups with torch.distributed\")\n",
    "print(\"4. Use torch.multiprocessing for spawning processes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Production Deployment Mathematics\n\n### Mathematical Framework for Model Serving\n\n**Model deployment** requires mathematical analysis of performance, scalability, and reliability:\n\n**Inference Serving Mathematics:**\n\n**Latency Analysis:**\nTotal inference time: $T_{\\text{inference}} = T_{\\text{preprocess}} + T_{\\text{forward}} + T_{\\text{postprocess}}$\n\n**Batch Processing:**\nFor batch size $B$:\n- **Throughput**: $\\tau(B) = \\frac{B}{T_{\\text{batch}}(B)}$\n- **Optimal batch size**: $B^* = \\arg\\max_B \\tau(B)$ subject to memory constraints\n\n**Queue Theory:**\nUsing M/M/1 queue model for request serving:\n- **Arrival rate**: $\\lambda$ requests/second\n- **Service rate**: $\\mu$ requests/second\n- **Utilization**: $\\rho = \\frac{\\lambda}{\\mu} < 1$ for stability\n- **Average latency**: $L = \\frac{1}{\\mu - \\lambda}$\n\n**Load Balancing:**\nFor $N$ servers with capacities $\\{\\mu_1, \\mu_2, \\ldots, \\mu_N\\}$:\nOptimal routing probabilities: $p_i = \\frac{\\mu_i}{\\sum_{j=1}^N \\mu_j}$\n\n**Resource Allocation:**\n\n**Auto-scaling Mathematics:**\nScale replicas based on metrics:\n$$R(t+1) = \\max\\left(R_{\\min}, \\min\\left(R_{\\max}, R(t) \\cdot \\frac{M(t)}{M_{\\text{target}}}\\right)\\right)$$\n\nWhere $M(t)$ is current metric (CPU, memory, latency).\n\n**Cost Optimization:**\nTotal cost: $C = C_{\\text{compute}} + C_{\\text{storage}} + C_{\\text{network}}$\nMinimize: $\\min C$ subject to SLA constraints $L \\leq L_{\\max}, A \\geq A_{\\min}$\n\n**A/B Testing Mathematics:**\n\n**Statistical Significance:**\nFor metrics $X_A, X_B$ with means $\\mu_A, \\mu_B$:\n$$t = \\frac{\\bar{X}_A - \\bar{X}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}}$$\n\n**Sample Size Calculation:**\nRequired samples: $n = \\frac{2(z_{\\alpha/2} + z_\\beta)^2 \\sigma^2}{(\\mu_1 - \\mu_2)^2}$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model versioning and metadata\n",
    "class ModelMetadata:\n",
    "    def __init__(self, model_name, version, input_shape, output_shape, classes=None):\n",
    "        self.model_name = model_name\n",
    "        self.version = version\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.classes = classes\n",
    "        self.created_at = time.time()\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'model_name': self.model_name,\n",
    "            'version': self.version,\n",
    "            'input_shape': self.input_shape,\n",
    "            'output_shape': self.output_shape,\n",
    "            'classes': self.classes,\n",
    "            'created_at': self.created_at\n",
    "        }\n",
    "    \n",
    "    def save(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "\n",
    "# Create model package\n",
    "def package_model(model, metadata, model_path='model_package'):\n",
    "    \"\"\"\n",
    "    Package model with metadata for deployment\n",
    "    \"\"\"\n",
    "    Path(model_path).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'{model_path}/model.pth')\n",
    "    \n",
    "    # Save TorchScript version\n",
    "    scripted = torch.jit.script(model)\n",
    "    scripted.save(f'{model_path}/model_scripted.pt')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata.save(f'{model_path}/metadata.json')\n",
    "    \n",
    "    print(f\"Model packaged in {model_path}/\")\n",
    "    print(f\"Contents: model.pth, model_scripted.pt, metadata.json\")\n",
    "\n",
    "# Example packaging\n",
    "model = SimpleNet()\n",
    "metadata = ModelMetadata(\n",
    "    model_name='SimpleNet',\n",
    "    version='1.0.0',\n",
    "    input_shape=[None, 100],\n",
    "    output_shape=[None, 10],\n",
    "    classes=list(range(10))\n",
    ")\n",
    "\n",
    "package_model(model, metadata)\n",
    "\n",
    "# Clean up\n",
    "import shutil\n",
    "shutil.rmtree('model_package')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Serving Infrastructure\n\n### Mathematical Foundation of Scalable Inference\n\n**Model serving infrastructure** requires mathematical optimization of throughput, latency, and resource utilization:\n\n**Batching Mathematics:**\n\n**Dynamic Batching:**\nCollect requests for time $T$ or until batch size $B$:\n$$\\text{Batch ready when: } |\\mathcal{B}(t)| \\geq B \\text{ OR } t - t_{\\text{first}} \\geq T$$\n\n**Optimal Batch Size:**\nBalance latency vs throughput:\n$$B^* = \\arg\\max_B \\frac{B \\cdot \\text{QPS}(B)}{\\text{Latency}(B)}$$\n\n**Memory Management:**\nGPU memory usage: $M_{\\text{GPU}} = M_{\\text{model}} + B \\times M_{\\text{activation}}$\nConstraint: $M_{\\text{GPU}} \\leq M_{\\text{available}}$\n\n**Performance Modeling:**\n\n**Little's Law:**\nAverage latency related to throughput and concurrency:\n$$L = \\lambda \\times W$$\nWhere $L$ = average requests in system, $\\lambda$ = arrival rate, $W$ = average response time.\n\n**Utilization Theory:**\nFor server with capacity $C$ and load $\\rho$:\n- **Response time**: $T = \\frac{T_{\\text{service}}}{1-\\rho}$ where $\\rho = \\frac{\\lambda}{C}$\n- **Queue length**: $E[N] = \\frac{\\rho^2}{1-\\rho}$\n\n**Caching Mathematics:**\n\n**Cache Hit Rate:**\nFor LRU cache with capacity $C$ and request pattern:\n$$\\text{Hit Rate} = 1 - \\frac{\\text{Unique requests in window}}{\\text{Total requests}}$$\n\n**Cache Performance:**\nEffective latency: $L_{\\text{eff}} = h \\times L_{\\text{cache}} + (1-h) \\times L_{\\text{compute}}$\nWhere $h$ is hit rate.\n\n**Multi-Level Serving:**\n\n**Cascade Architecture:**\nRoute requests based on complexity:\n$$f(\\mathbf{x}) = \\begin{cases}\nf_{\\text{fast}}(\\mathbf{x}) & \\text{if confidence} > \\theta \\\\\nf_{\\text{accurate}}(\\mathbf{x}) & \\text{otherwise}\n\\end{cases}$$\n\n**Cost-Performance Trade-off:**\nTotal cost: $C = C_1 \\times p_1 + C_2 \\times (1-p_1)$\nWhere $p_1$ is fraction routed to fast model, $C_i$ is cost per request."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model server class\n",
    "class ModelServer:\n",
    "    def __init__(self, model_path, device='cpu'):\n",
    "        self.device = device\n",
    "        self.model = torch.jit.load(model_path, map_location=device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def preprocess(self, input_data):\n",
    "        \"\"\"Preprocess input data\"\"\"\n",
    "        if isinstance(input_data, np.ndarray):\n",
    "            input_data = torch.from_numpy(input_data).float()\n",
    "        return input_data.to(self.device)\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"Make prediction\"\"\"\n",
    "        processed_input = self.preprocess(input_data)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(processed_input)\n",
    "            \n",
    "        return output.cpu().numpy()\n",
    "    \n",
    "    def predict_batch(self, batch_data):\n",
    "        \"\"\"Batch prediction\"\"\"\n",
    "        results = []\n",
    "        for data in batch_data:\n",
    "            result = self.predict(data)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "# Example usage (conceptual)\n",
    "print(\"Model Server Example:\")\n",
    "print(\"\"\"# Usage:\n",
    "server = ModelServer('model.pt', device='cuda')\n",
    "predictions = server.predict(input_data)\n",
    "batch_predictions = server.predict_batch([data1, data2, data3])\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDeployment considerations:\")\n",
    "print(\"1. Model format: TorchScript for production\")\n",
    "print(\"2. Device management: CPU vs GPU\")\n",
    "print(\"3. Batch processing for efficiency\")\n",
    "print(\"4. Input validation and preprocessing\")\n",
    "print(\"5. Error handling and logging\")\n",
    "print(\"6. Model versioning and A/B testing\")\n",
    "print(\"7. Monitoring and metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Performance Optimization Theory\n\n### Mathematical Principles of Deep Learning Optimization\n\n**Performance optimization** applies mathematical analysis to identify and eliminate computational bottlenecks:\n\n**Complexity Analysis:**\n\n**Computational Complexity:**\nFor neural network layers:\n- **Linear**: $O(n \\times m)$ for $n \\times m$ weight matrix\n- **Convolution**: $O(C_{\\text{in}} \\times C_{\\text{out}} \\times K^2 \\times H \\times W)$\n- **Attention**: $O(n^2 \\times d)$ for sequence length $n$, dimension $d$\n\n**Memory Complexity:**\nTotal memory: $M = M_{\\text{parameters}} + M_{\\text{activations}} + M_{\\text{gradients}} + M_{\\text{optimizer}}$\n\n**Roofline Performance Model:**\nPerformance bound by compute or memory:\n$$\\text{Performance} \\leq \\min\\left(\\text{Peak FLOPS}, \\text{Arithmetic Intensity} \\times \\text{Peak Bandwidth}\\right)$$\n\n**Optimization Strategies:**\n\n**Operator Fusion:**\nCombine operations to reduce memory transfers:\n$$y = \\text{ReLU}(\\text{BatchNorm}(\\text{Conv}(x))) \\rightarrow y = \\text{FusedConvBNReLU}(x)$$\n\n**Memory Pool Optimization:**\nMinimize fragmentation through optimal allocation:\n$$\\text{Total Memory} = \\max_{t} \\sum_{i \\text{ alive at } t} \\text{size}(i)$$\n\n**Kernel Optimization:**\nOptimize CUDA kernels for:\n- **Memory coalescing**: Adjacent threads access contiguous memory\n- **Occupancy**: $\\frac{\\text{Active warps}}{\\text{Max warps per SM}}$\n- **Register usage**: Balance between occupancy and performance\n\n**Algorithmic Optimizations:**\n\n**Gradient Accumulation:**\nEffective batch size $B_{\\text{eff}} = B \\times N_{\\text{acc}}$:\n$$\\mathbf{g}_{\\text{eff}} = \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} \\mathbf{g}_i$$\n\n**Checkpointing:**\nTrade compute for memory:\n$$\\text{Memory} = O(\\sqrt{n}), \\text{Compute} = O(n) + O(\\sqrt{n})$$\nwhere $n$ is number of layers.\n\n**Mixed Precision Benefits:**\n- **Memory**: ~50% reduction in activation storage\n- **Bandwidth**: 2× improvement in data transfer\n- **Compute**: Tensor Core acceleration for matrix operations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization demonstrations\n",
    "def benchmark_operations():\n",
    "    \"\"\"Benchmark different operations\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create test data\n",
    "    x = torch.randn(1000, 1000, device=device)\n",
    "    y = torch.randn(1000, 1000, device=device)\n",
    "    \n",
    "    # Benchmark matrix multiplication\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        z = torch.mm(x, y)\n",
    "    mm_time = time.time() - start_time\n",
    "    \n",
    "    # Benchmark element-wise operations\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        z = x * y\n",
    "    ew_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Matrix multiplication (100 iterations): {mm_time:.4f} seconds\")\n",
    "    print(f\"Element-wise multiplication (100 iterations): {ew_time:.4f} seconds\")\n",
    "\n",
    "benchmark_operations()\n",
    "\n",
    "print(\"\\nPerformance Optimization Tips:\")\n",
    "print(\"\\n1. Memory Management:\")\n",
    "print(\"   - Use torch.no_grad() for inference\")\n",
    "print(\"   - Clear cache with torch.cuda.empty_cache()\")\n",
    "print(\"   - Use inplace operations when possible\")\n",
    "\n",
    "print(\"\\n2. Data Loading:\")\n",
    "print(\"   - Use multiple workers in DataLoader\")\n",
    "print(\"   - Pin memory for GPU transfers\")\n",
    "print(\"   - Preprocess data offline when possible\")\n",
    "\n",
    "print(\"\\n3. Model Optimization:\")\n",
    "print(\"   - Use TorchScript for production\")\n",
    "print(\"   - Consider model quantization\")\n",
    "print(\"   - Use mixed precision training\")\n",
    "print(\"   - Optimize batch sizes\")\n",
    "\n",
    "print(\"\\n4. GPU Utilization:\")\n",
    "print(\"   - Ensure sufficient batch sizes\")\n",
    "print(\"   - Use tensor cores (mixed precision)\")\n",
    "print(\"   - Minimize CPU-GPU transfers\")\n",
    "print(\"   - Use asynchronous operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Production Deployment Checklist:\")\n",
    "print(\"\\n✓ Model Development:\")\n",
    "print(\"  □ Model architecture finalized\")\n",
    "print(\"  □ Training pipeline validated\")\n",
    "print(\"  □ Model performance meets requirements\")\n",
    "print(\"  □ Hyperparameters tuned\")\n",
    "\n",
    "print(\"\\n✓ Model Optimization:\")\n",
    "print(\"  □ TorchScript conversion tested\")\n",
    "print(\"  □ Quantization applied (if needed)\")\n",
    "print(\"  □ Model size optimized\")\n",
    "print(\"  □ Inference speed benchmarked\")\n",
    "\n",
    "print(\"\\n✓ Data Pipeline:\")\n",
    "print(\"  □ Input validation implemented\")\n",
    "print(\"  □ Preprocessing pipeline tested\")\n",
    "print(\"  □ Data format standardized\")\n",
    "print(\"  □ Error handling for malformed inputs\")\n",
    "\n",
    "print(\"\\n✓ Infrastructure:\")\n",
    "print(\"  □ Serving infrastructure chosen\")\n",
    "print(\"  □ Scaling strategy defined\")\n",
    "print(\"  □ Load balancing configured\")\n",
    "print(\"  □ Health checks implemented\")\n",
    "\n",
    "print(\"\\n✓ Monitoring & Logging:\")\n",
    "print(\"  □ Model performance metrics\")\n",
    "print(\"  □ Inference latency monitoring\")\n",
    "print(\"  □ Error rate tracking\")\n",
    "print(\"  □ Resource utilization monitoring\")\n",
    "\n",
    "print(\"\\n✓ Testing & Validation:\")\n",
    "print(\"  □ Unit tests for all components\")\n",
    "print(\"  □ Integration tests\")\n",
    "print(\"  □ Load testing completed\")\n",
    "print(\"  □ Model drift detection\")\n",
    "\n",
    "print(\"\\n✓ Deployment & Maintenance:\")\n",
    "print(\"  □ Model versioning strategy\")\n",
    "print(\"  □ Rollback procedures\")\n",
    "print(\"  □ A/B testing framework\")\n",
    "print(\"  □ Continuous integration setup\")\n",
    "print(\"  □ Documentation completed\")\n",
    "\n",
    "print(\"\\n✓ Security & Compliance:\")\n",
    "print(\"  □ Input sanitization\")\n",
    "print(\"  □ Model security assessment\")\n",
    "print(\"  □ Data privacy compliance\")\n",
    "print(\"  □ Access control implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Learning Journey Summary:\")\n",
    "print(\"\\nPart 1: Tensors and Fundamentals\")\n",
    "print(\"  - Tensor creation, operations, and manipulation\")\n",
    "print(\"  - GPU acceleration and device management\")\n",
    "print(\"  - NumPy integration\")\n",
    "\n",
    "print(\"\\nPart 2: Autograd and Neural Networks\")\n",
    "print(\"  - Automatic differentiation\")\n",
    "print(\"  - Building neural networks with nn.Module\")\n",
    "print(\"  - Loss functions and optimizers\")\n",
    "print(\"  - Training loops\")\n",
    "\n",
    "print(\"\\nPart 3: Data Loading and Datasets\")\n",
    "print(\"  - Custom datasets and data loaders\")\n",
    "print(\"  - Data preprocessing and augmentation\")\n",
    "print(\"  - Train/validation/test splits\")\n",
    "print(\"  - Best practices for data handling\")\n",
    "\n",
    "print(\"\\nPart 4: CNNs and Computer Vision\")\n",
    "print(\"  - Convolutional layers and operations\")\n",
    "print(\"  - CNN architectures and training\")\n",
    "print(\"  - Image preprocessing and augmentation\")\n",
    "print(\"  - Transfer learning\")\n",
    "\n",
    "print(\"\\nPart 5: Advanced Topics and Production\")\n",
    "print(\"  - Mixed precision training\")\n",
    "print(\"  - Model quantization and optimization\")\n",
    "print(\"  - TorchScript and deployment\")\n",
    "print(\"  - Production considerations\")\n",
    "\n",
    "print(\"\\nNext Steps for Advanced PyTorch:\")\n",
    "print(\"1. Explore specific domains (NLP, RL, GNNs)\")\n",
    "print(\"2. Learn distributed training for large models\")\n",
    "print(\"3. Master deployment frameworks (TorchServe, ONNX)\")\n",
    "print(\"4. Dive into PyTorch ecosystem libraries\")\n",
    "print(\"5. Contribute to open-source PyTorch projects\")\n",
    "print(\"6. Stay updated with latest PyTorch releases\")\n",
    "\n",
    "print(\"\\nUseful Resources:\")\n",
    "print(\"- PyTorch Documentation: pytorch.org/docs\")\n",
    "print(\"- PyTorch Tutorials: pytorch.org/tutorials\")\n",
    "print(\"- Papers With Code: paperswithcode.com\")\n",
    "print(\"- PyTorch Forums: discuss.pytorch.org\")\n",
    "print(\"- GitHub: github.com/pytorch/pytorch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}