{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Basics Part 6: Recurrent Neural Networks and NLP\n\nIntroduction to sequence modeling, RNNs, LSTMs, and natural language processing with mathematical foundations\n\n## Mathematical Framework for Sequence Modeling\n\n**Recurrent Neural Networks (RNNs)** process sequential data by maintaining hidden states that evolve over time, enabling models to capture temporal dependencies and variable-length patterns.\n\n### Core Mathematical Concepts\n\n**1. Sequential Data Representation:**\nA sequence $\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_T)$ where $\\mathbf{x}_t \\in \\mathbb{R}^d$ at time step $t$.\n\n**2. Temporal Dependencies:**\nCurrent output depends on current input AND previous states:\n$$\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1})$$\n$$\\mathbf{y}_t = g(\\mathbf{h}_t)$$\n\n**3. RNN Recurrence Relation:**\n$$\\mathbf{h}_t = \\tanh(\\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h)$$\n$$\\mathbf{y}_t = \\mathbf{W}_{hy}\\mathbf{h}_t + \\mathbf{b}_y$$\n\nWhere:\n- $\\mathbf{W}_{xh} \\in \\mathbb{R}^{H \\times D}$: Input-to-hidden weights\n- $\\mathbf{W}_{hh} \\in \\mathbb{R}^{H \\times H}$: Hidden-to-hidden (recurrent) weights  \n- $\\mathbf{W}_{hy} \\in \\mathbb{R}^{O \\times H}$: Hidden-to-output weights\n- $\\mathbf{h}_t \\in \\mathbb{R}^H$: Hidden state at time $t$\n\n**4. Backpropagation Through Time (BPTT):**\nGradients flow backward through the temporal dimension:\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_t} = \\frac{\\partial L}{\\partial \\mathbf{y}_t} \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{h}_t} + \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}} \\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t}$$\n\n**5. Vanishing Gradient Problem:**\nGradient magnitude decays exponentially with sequence length:\n$$\\left\\|\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_k}\\right\\| \\leq \\left\\|\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}}\\right\\| \\times \\cdots \\times \\left\\|\\frac{\\partial \\mathbf{h}_{k+1}}{\\partial \\mathbf{h}_k}\\right\\| \\propto \\gamma^{t-k}$$\n\nWhere $\\gamma < 1$ leads to vanishing gradients for long sequences."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding Sequences and RNN Fundamentals\n\n### Mathematical Foundation of Sequential Processing\n\n**Sequence modeling** captures patterns in temporally or spatially ordered data through mathematical frameworks that preserve dependency structure:\n\n**Markov Property:**\n$$P(\\mathbf{x}_t | \\mathbf{x}_{1:t-1}) = P(\\mathbf{x}_t | \\mathbf{x}_{t-k:t-1})$$\nMany sequences exhibit limited-range dependencies.\n\n**Autoregressive Models:**\n$$P(\\mathbf{x}_{1:T}) = \\prod_{t=1}^T P(\\mathbf{x}_t | \\mathbf{x}_{1:t-1})$$\n\n**RNN as Universal Approximator:**\nRNNs can approximate any measurable sequence-to-sequence mapping given sufficient hidden units and proper activation functions.\n\n**Memory Capacity:**\nInformation storage capacity of RNN hidden state:\n$$I(\\mathbf{h}_t; \\mathbf{x}_{1:t}) \\leq H \\log_2(|\\mathcal{A}|)$$\nWhere $H$ is hidden size and $\\mathcal{A}$ is activation range.\n\n**Sequence Types:**\n- **One-to-one**: $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^o$ (standard feedforward)\n- **One-to-many**: $f: \\mathbb{R}^d \\rightarrow (\\mathbb{R}^o)^T$ (image captioning)\n- **Many-to-one**: $f: (\\mathbb{R}^d)^T \\rightarrow \\mathbb{R}^o$ (sentiment analysis)\n- **Many-to-many**: $f: (\\mathbb{R}^d)^T \\rightarrow (\\mathbb{R}^o)^T$ (machine translation)\n\n**Computational Complexity:**\n- **Time**: $O(T \\cdot H^2 + T \\cdot H \\cdot D)$ per sequence\n- **Space**: $O(T \\cdot H)$ for storing all hidden states (BPTT)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sequence data example\n",
    "# Let's create a toy dataset: predicting the next number in a sequence\n",
    "def generate_sequence_data(n_samples=1000, seq_length=10):\n",
    "    \"\"\"Generate simple arithmetic sequences\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Create arithmetic sequence: start + i*step\n",
    "        start = random.randint(1, 10)\n",
    "        step = random.randint(1, 5)\n",
    "        \n",
    "        sequence = [start + i * step for i in range(seq_length)]\n",
    "        target = start + seq_length * step  # Next number in sequence\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return torch.FloatTensor(sequences), torch.FloatTensor(targets)\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_sequence_data(1000, 5)\n",
    "print(f\"Sequence shape: {X.shape}\")  # [batch_size, sequence_length]\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Show examples\n",
    "for i in range(3):\n",
    "    seq = X[i].tolist()\n",
    "    target = y[i].item()\n",
    "    print(f\"Sequence: {seq} -> Next: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Basic RNN Implementation\n\n### Mathematical Mechanics of Recurrent Computation\n\n**RNN Forward Pass Mathematics:**\n\nFor each time step $t$, the RNN computes:\n$$\\mathbf{h}_t = \\tanh(\\mathbf{W}_{xh}\\mathbf{x}_t + \\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h)$$\n\n**Matrix Dimensions:**\n- Input: $\\mathbf{x}_t \\in \\mathbb{R}^{D}$\n- Hidden state: $\\mathbf{h}_t \\in \\mathbb{R}^{H}$ \n- Input weights: $\\mathbf{W}_{xh} \\in \\mathbb{R}^{H \\times D}$\n- Recurrent weights: $\\mathbf{W}_{hh} \\in \\mathbb{R}^{H \\times H}$\n- Bias: $\\mathbf{b}_h \\in \\mathbb{R}^{H}$\n\n**Activation Function Role:**\n$\\tanh$ activation provides:\n- **Bounded output**: $\\tanh(x) \\in [-1, 1]$\n- **Non-zero gradient**: $\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)$\n- **Zero-centered**: Helps with gradient flow\n\n**Unrolled Computation:**\n$$\\mathbf{h}_1 = \\tanh(\\mathbf{W}_{xh}\\mathbf{x}_1 + \\mathbf{b}_h)$$\n$$\\mathbf{h}_2 = \\tanh(\\mathbf{W}_{xh}\\mathbf{x}_2 + \\mathbf{W}_{hh}\\mathbf{h}_1 + \\mathbf{b}_h)$$\n$$\\mathbf{h}_3 = \\tanh(\\mathbf{W}_{xh}\\mathbf{x}_3 + \\mathbf{W}_{hh}\\mathbf{h}_2 + \\mathbf{b}_h)$$\n$$\\vdots$$\n\n**Parameter Sharing:**\nSame weight matrices $\\mathbf{W}_{xh}, \\mathbf{W}_{hh}$ used at all time steps:\n- **Benefits**: Fewer parameters, translation invariance\n- **Limitations**: Fixed capacity regardless of sequence length\n\n**Hidden State Interpretation:**\n$\\mathbf{h}_t$ serves as a \"memory\" vector encoding information from $\\mathbf{x}_{1:t}$:\n$$\\mathbf{h}_t = f(\\mathbf{x}_{1:t}; \\boldsymbol{\\theta})$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual RNN implementation to understand the mechanics\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # RNN weights\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)  # input to hidden\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)  # hidden to hidden\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)  # hidden to output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.size(0), x.size(1)\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "        \n",
    "        # Process sequence step by step\n",
    "        for t in range(seq_length):\n",
    "            # Get input at time t\n",
    "            input_t = x[:, t].unsqueeze(1)  # [batch_size, 1]\n",
    "            \n",
    "            # Update hidden state: h_t = tanh(W_ih * x_t + W_hh * h_{t-1})\n",
    "            hidden = torch.tanh(self.i2h(input_t) + self.h2h(hidden))\n",
    "        \n",
    "        # Output prediction from final hidden state\n",
    "        output = self.h2o(hidden)\n",
    "        return output.squeeze()\n",
    "\n",
    "# Test the manual RNN\n",
    "manual_rnn = SimpleRNN(input_size=1, hidden_size=20, output_size=1)\n",
    "test_input = X[:5].unsqueeze(-1)  # Add feature dimension\n",
    "output = manual_rnn(test_input)\n",
    "print(f\"Manual RNN output shape: {output.shape}\")\n",
    "print(f\"Sample predictions: {output[:3]}\")\n",
    "print(f\"Actual targets: {y[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using PyTorch's Built-in RNN Layers\n\n### Mathematical Optimization in PyTorch RNNs\n\n**PyTorch RNN Optimizations:**\n\n**1. Efficient Matrix Operations:**\nPyTorch combines operations for computational efficiency:\n$$[\\mathbf{W}_{xh} \\; \\mathbf{W}_{hh}] \\begin{bmatrix} \\mathbf{x}_t \\\\ \\mathbf{h}_{t-1} \\end{bmatrix} + \\mathbf{b}_h$$\n\n**2. Batch Processing:**\nProcess multiple sequences simultaneously:\n$$\\mathbf{H}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh}^T + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}^T + \\mathbf{b}_h)$$\n\nWhere $\\mathbf{X}_t \\in \\mathbb{R}^{B \\times D}$, $\\mathbf{H}_t \\in \\mathbb{R}^{B \\times H}$ for batch size $B$.\n\n**3. Multi-layer RNNs:**\nStack RNN layers for increased representational capacity:\n$$\\mathbf{h}_t^{(1)} = \\text{RNN}^{(1)}(\\mathbf{x}_t, \\mathbf{h}_{t-1}^{(1)})$$\n$$\\mathbf{h}_t^{(2)} = \\text{RNN}^{(2)}(\\mathbf{h}_t^{(1)}, \\mathbf{h}_{t-1}^{(2)})$$\n$$\\vdots$$\n$$\\mathbf{h}_t^{(L)} = \\text{RNN}^{(L)}(\\mathbf{h}_t^{(L-1)}, \\mathbf{h}_{t-1}^{(L)})$$\n\n**4. Parameter Count Analysis:**\nFor $L$-layer RNN with dimensions $(D, H, O)$:\n- Layer 1: $D \\times H + H \\times H + H = H(D + H + 1)$\n- Layers 2 to $L$: $(L-1) \\times H(H + H + 1) = (L-1) \\times H(2H + 1)$\n- Output layer: $H \\times O + O$\n\n**Total**: $H(D + H + 1) + (L-1)H(2H + 1) + O(H + 1)$\n\n**5. Computational Graph:**\nPyTorch automatically builds computational graph for backpropagation:\n$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}}$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's built-in RNN\n",
    "class PyTorchRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(PyTorchRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN returns (output, hidden) where output contains all hidden states\n",
    "        rnn_out, hidden = self.rnn(x)\n",
    "        \n",
    "        # Use the last time step's output\n",
    "        last_output = rnn_out[:, -1, :]\n",
    "        prediction = self.fc(last_output)\n",
    "        return prediction.squeeze()\n",
    "\n",
    "# Create and test PyTorch RNN\n",
    "pytorch_rnn = PyTorchRNN(input_size=1, hidden_size=20, num_layers=2, output_size=1)\n",
    "output = pytorch_rnn(test_input)\n",
    "print(f\"PyTorch RNN output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in pytorch_rnn.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## LSTM: Solving the Vanishing Gradient Problem\n\n### Mathematical Architecture of Long Short-Term Memory\n\n**LSTM** addresses vanishing gradients through gating mechanisms that control information flow:\n\n**LSTM Cell Equations:**\n$$\\mathbf{f}_t = \\sigma(\\mathbf{W}_f \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_f)$$ (Forget gate)\n$$\\mathbf{i}_t = \\sigma(\\mathbf{W}_i \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_i)$$ (Input gate)\n$$\\tilde{\\mathbf{C}}_t = \\tanh(\\mathbf{W}_C \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_C)$$ (Candidate values)\n$$\\mathbf{C}_t = \\mathbf{f}_t * \\mathbf{C}_{t-1} + \\mathbf{i}_t * \\tilde{\\mathbf{C}}_t$$ (Cell state)\n$$\\mathbf{o}_t = \\sigma(\\mathbf{W}_o \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t] + \\mathbf{b}_o)$$ (Output gate)\n$$\\mathbf{h}_t = \\mathbf{o}_t * \\tanh(\\mathbf{C}_t)$$ (Hidden state)\n\n**Gate Functions:**\n- **Forget gate** $\\mathbf{f}_t$: Decides what information to discard from cell state\n- **Input gate** $\\mathbf{i}_t$: Controls which values to update in cell state  \n- **Output gate** $\\mathbf{o}_t$: Controls which parts of cell state to output\n\n**Gradient Flow Analysis:**\nCell state gradient: $\\frac{\\partial \\mathbf{C}_t}{\\partial \\mathbf{C}_{t-1}} = \\mathbf{f}_t$\n\nSince $\\mathbf{f}_t \\in [0,1]$, gradient can flow unchanged when $\\mathbf{f}_t \\approx 1$.\n\n**Long-term Dependencies:**\n$$\\frac{\\partial \\mathbf{C}_t}{\\partial \\mathbf{C}_k} = \\prod_{j=k+1}^t \\mathbf{f}_j$$\n\nUnlike vanilla RNN where gradients always multiply by $\\tanh'$ derivatives.\n\n**GRU (Gated Recurrent Unit) - Simplified LSTM:**\n$$\\mathbf{r}_t = \\sigma(\\mathbf{W}_r \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t])$$ (Reset gate)\n$$\\mathbf{z}_t = \\sigma(\\mathbf{W}_z \\cdot [\\mathbf{h}_{t-1}, \\mathbf{x}_t])$$ (Update gate)  \n$$\\tilde{\\mathbf{h}}_t = \\tanh(\\mathbf{W} \\cdot [\\mathbf{r}_t * \\mathbf{h}_{t-1}, \\mathbf{x}_t])$$ (Candidate)\n$$\\mathbf{h}_t = (1 - \\mathbf{z}_t) * \\mathbf{h}_{t-1} + \\mathbf{z}_t * \\tilde{\\mathbf{h}}_t$$ (Hidden state)\n\n**Parameter Efficiency:**\n- **LSTM**: 4 gates × (input + hidden + bias) = $4(D + H + 1)H$\n- **GRU**: 3 gates × (input + hidden + bias) = $3(D + H + 1)H$\n\nGRU has ~25% fewer parameters than LSTM."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM implementation\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM returns (output, (hidden, cell))\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Use last time step\n",
    "        prediction = self.fc(lstm_out[:, -1, :])\n",
    "        return prediction.squeeze()\n",
    "\n",
    "# GRU: Simplified LSTM\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gru_out, hidden = self.gru(x)\n",
    "        prediction = self.fc(gru_out[:, -1, :])\n",
    "        return prediction.squeeze()\n",
    "\n",
    "# Compare architectures\n",
    "lstm_model = LSTMModel(1, 20, 2, 1)\n",
    "gru_model = GRUModel(1, 20, 2, 1)\n",
    "\n",
    "lstm_params = sum(p.numel() for p in lstm_model.parameters())\n",
    "gru_params = sum(p.numel() for p in gru_model.parameters())\n",
    "\n",
    "print(f\"LSTM parameters: {lstm_params}\")\n",
    "print(f\"GRU parameters: {gru_params}\")\n",
    "print(f\"GRU is {((lstm_params - gru_params) / lstm_params * 100):.1f}% smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training RNN Models\n\n### Mathematical Framework for RNN Training\n\n**Backpropagation Through Time (BPTT):**\n\n**Loss Function:**\nFor sequence-to-sequence task:\n$$L = \\frac{1}{T} \\sum_{t=1}^T \\ell(\\mathbf{y}_t, \\hat{\\mathbf{y}}_t)$$\n\n**Gradient Computation:**\n$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}}$$\n\n**Chain Rule for Recurrent Connections:**\n$$\\frac{\\partial L}{\\partial \\mathbf{h}_k} = \\frac{\\partial L}{\\partial \\mathbf{h}_T} \\prod_{t=k+1}^T \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}}$$\n\n**Exploding Gradient Problem:**\nWhen $\\left\\|\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}}\\right\\| > 1$, gradients grow exponentially.\n\n**Gradient Clipping:**\nRescale gradients when norm exceeds threshold:\n$$\\mathbf{g} \\leftarrow \\mathbf{g} \\cdot \\min\\left(1, \\frac{\\text{clip\\_norm}}{\\|\\mathbf{g}\\|}\\right)$$\n\n**Truncated BPTT:**\nProcess long sequences in chunks of length $K$:\n$$\\frac{\\partial L}{\\partial \\mathbf{W}} \\approx \\sum_{t=T-K+1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}}$$\n\n**Teacher Forcing (for sequence generation):**\nDuring training, use ground truth as input:\n$$\\mathbf{h}_t = \\text{RNN}(\\mathbf{y}_{t-1}^{\\text{true}}, \\mathbf{h}_{t-1})$$\n\n**Exposure Bias:**\nTraining/inference mismatch leads to error accumulation during generation.\n\n**Optimization Considerations:**\n- **Learning rate**: Often lower than feedforward networks (0.001-0.01)\n- **Batch size**: Memory constraints limit batch size\n- **Initialization**: Xavier/He initialization for gates, orthogonal for recurrent weights"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X_train, y_train = generate_sequence_data(8000, 5)\n",
    "X_test, y_test = generate_sequence_data(2000, 5)\n",
    "\n",
    "# Add feature dimension\n",
    "X_train = X_train.unsqueeze(-1)\n",
    "X_test = X_test.unsqueeze(-1)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for RNN models\n",
    "def train_rnn_model(model, train_loader, test_loader, num_epochs=50):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                predictions = model(batch_x)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Train LSTM model\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model = LSTMModel(1, 32, 2, 1)\n",
    "lstm_train_losses, lstm_test_losses = train_rnn_model(lstm_model, train_loader, test_loader, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Text Preprocessing and Tokenization\n\n### Mathematical Foundations of Natural Language Processing\n\n**Text Tokenization** converts raw strings into discrete token sequences for neural network processing:\n\n**Tokenization as Mapping:**\n$$T: \\Sigma^* \\rightarrow \\mathcal{V}^*$$\nWhere $\\Sigma$ is character alphabet and $\\mathcal{V}$ is vocabulary set.\n\n**Vocabulary Construction:**\nGiven corpus $\\mathcal{C} = \\{s_1, s_2, \\ldots, s_N\\}$:\n\n**1. Word Frequency:**\n$$f(w) = \\sum_{s \\in \\mathcal{C}} \\text{count}(w, s)$$\n\n**2. Zipf's Law:**\nWord frequency follows power law:\n$$f(w_r) \\propto \\frac{1}{r^\\alpha}$$\nWhere $r$ is rank and $\\alpha \\approx 1$ for natural languages.\n\n**3. Vocabulary Size vs Coverage:**\n$$\\text{Coverage}(V_k) = \\frac{\\sum_{w \\in V_k} f(w)}{\\sum_{w \\in \\mathcal{V}} f(w)}$$\nWhere $V_k$ contains top-$k$ most frequent words.\n\n**Out-of-Vocabulary (OOV) Handling:**\n**UNK token probability:**\n$$P(\\text{UNK}) = \\frac{\\text{OOV tokens}}{\\text{Total tokens}}$$\n\n**Subword Tokenization:**\n**Byte Pair Encoding (BPE)** learns subword units:\n1. Initialize with character vocabulary\n2. Iteratively merge most frequent pairs\n3. Results in vocabulary balancing word boundary and subword units\n\n**Token-to-Index Mapping:**\n$$\\phi: \\mathcal{V} \\rightarrow \\{0, 1, 2, \\ldots, |\\mathcal{V}|-1\\}$$\n\n**Sequence Representation:**\nText $s = w_1 w_2 \\cdots w_T$ becomes index sequence:\n$$\\mathbf{x} = [\\phi(w_1), \\phi(w_2), \\ldots, \\phi(w_T)]$$\n\n**Special Tokens:**\n- `<PAD>` (0): Padding for batch processing\n- `<UNK>` (1): Unknown/out-of-vocabulary words  \n- `<SOS>` (2): Start of sequence\n- `<EOS>` (3): End of sequence"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing utilities\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
    "        self.idx_to_word = {0: '<PAD>', 1: '<UNK>', 2: '<SOS>', 3: '<EOS>'}\n",
    "        self.vocab_size = 4\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Basic text preprocessing\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove punctuation except periods\n",
    "        text = re.sub(r'[^\\w\\s\\.]', ' ', text)\n",
    "        # Split into words\n",
    "        words = text.split()\n",
    "        return words\n",
    "    \n",
    "    def build_vocab(self, texts, min_freq=2):\n",
    "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self.preprocess_text(text)\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # Add words that appear at least min_freq times\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= min_freq and word not in self.word_to_idx:\n",
    "                self.word_to_idx[word] = self.vocab_size\n",
    "                self.idx_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "    \n",
    "    def text_to_indices(self, text):\n",
    "        \"\"\"Convert text to sequence of indices\"\"\"\n",
    "        words = self.preprocess_text(text)\n",
    "        indices = []\n",
    "        for word in words:\n",
    "            if word in self.word_to_idx:\n",
    "                indices.append(self.word_to_idx[word])\n",
    "            else:\n",
    "                indices.append(self.word_to_idx['<UNK>'])\n",
    "        return indices\n",
    "    \n",
    "    def indices_to_text(self, indices):\n",
    "        \"\"\"Convert indices back to text\"\"\"\n",
    "        words = [self.idx_to_word.get(idx, '<UNK>') for idx in indices]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Example text data\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is transforming how we process data.\",\n",
    "    \"Neural networks can learn complex patterns from examples.\",\n",
    "    \"Deep learning requires large amounts of training data.\",\n",
    "    \"The model learns to predict the next word in a sequence.\"\n",
    "]\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(sample_texts, min_freq=1)\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"The neural network learns patterns.\"\n",
    "indices = tokenizer.text_to_indices(test_text)\n",
    "reconstructed = tokenizer.indices_to_text(indices)\n",
    "\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Indices: {indices}\")\n",
    "print(f\"Reconstructed: {reconstructed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Word Embeddings\n\n### Mathematical Theory of Distributed Word Representations\n\n**Word embeddings** map discrete tokens to continuous vector spaces that capture semantic relationships:\n\n**Embedding Function:**\n$$E: \\mathcal{V} \\rightarrow \\mathbb{R}^d$$\n$$\\mathbf{e}_w = E(w) \\in \\mathbb{R}^d$$\n\n**Distributional Hypothesis:**\nWords with similar contexts have similar meanings:\n$$\\text{similarity}(w_i, w_j) \\propto \\text{similarity}(\\text{context}(w_i), \\text{context}(w_j))$$\n\n**Embedding Matrix:**\n$$\\mathbf{E} \\in \\mathbb{R}^{|\\mathcal{V}| \\times d}$$\nwhere row $i$ corresponds to embedding of word with index $i$.\n\n**One-hot to Dense Mapping:**\n$$\\mathbf{e}_w = \\mathbf{E}^T \\mathbf{v}_w$$\nwhere $\\mathbf{v}_w \\in \\{0,1\\}^{|\\mathcal{V}|}$ is one-hot vector.\n\n**Semantic Properties:**\n**Linear relationships** in embedding space:\n$$\\mathbf{e}_{\\text{king}} - \\mathbf{e}_{\\text{man}} + \\mathbf{e}_{\\text{woman}} \\approx \\mathbf{e}_{\\text{queen}}$$\n\n**Similarity Metrics:**\n**Cosine similarity:**\n$$\\cos(\\mathbf{e}_i, \\mathbf{e}_j) = \\frac{\\mathbf{e}_i \\cdot \\mathbf{e}_j}{\\|\\mathbf{e}_i\\| \\|\\mathbf{e}_j\\|}$$\n\n**Euclidean distance:**\n$$d(\\mathbf{e}_i, \\mathbf{e}_j) = \\|\\mathbf{e}_i - \\mathbf{e}_j\\|_2$$\n\n**Training Objectives:**\n\n**Skip-gram (Word2Vec):**\nPredict context from target word:\n$$\\max \\sum_{w \\in \\mathcal{C}} \\sum_{c \\in \\text{context}(w)} \\log P(c|w)$$\n\n**CBOW (Continuous Bag of Words):**\nPredict target from context:\n$$\\max \\sum_{w \\in \\mathcal{C}} \\log P(w|\\text{context}(w))$$\n\n**Initialization Strategy:**\nRandom initialization: $\\mathbf{e}_w \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I})$ with $\\sigma \\approx 0.1$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding demonstration\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 50\n",
    "\n",
    "# Create embedding layer\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "print(f\"Embedding layer shape: {embedding_layer.weight.shape}\")\n",
    "print(f\"Each word is represented by a {embedding_dim}-dimensional vector\")\n",
    "\n",
    "# Convert sample text to embeddings\n",
    "sample_indices = torch.LongTensor(indices)\n",
    "embeddings = embedding_layer(sample_indices)\n",
    "\n",
    "print(f\"Input indices shape: {sample_indices.shape}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Show that similar words can have similar embeddings (after training)\n",
    "word1_idx = tokenizer.word_to_idx.get('the', 1)\n",
    "word2_idx = tokenizer.word_to_idx.get('neural', 1)\n",
    "\n",
    "word1_emb = embedding_layer(torch.LongTensor([word1_idx]))\n",
    "word2_emb = embedding_layer(torch.LongTensor([word2_idx]))\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = F.cosine_similarity(word1_emb, word2_emb)\n",
    "print(f\"Similarity between 'the' and 'neural': {similarity.item():.4f}\")\n",
    "print(\"(Note: similarity is random before training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Language Modeling: Predicting Next Words\n\n### Mathematical Framework for Statistical Language Modeling\n\n**Language modeling** estimates probability distributions over word sequences:\n\n**Autoregressive Factorization:**\n$$P(\\mathbf{w}_{1:T}) = \\prod_{t=1}^T P(w_t | \\mathbf{w}_{1:t-1})$$\n\n**Neural Language Model:**\n$$P(w_t | \\mathbf{w}_{1:t-1}) = \\text{softmax}(\\mathbf{W}_{\\text{out}} \\mathbf{h}_t + \\mathbf{b})_{w_t}$$\n\nwhere $\\mathbf{h}_t$ encodes context $\\mathbf{w}_{1:t-1}$.\n\n**Cross-Entropy Loss:**\n$$L = -\\frac{1}{T} \\sum_{t=1}^T \\log P(w_t | \\mathbf{w}_{1:t-1})$$\n\n**Perplexity:**\nMeasure of how well model predicts text:\n$$\\text{PPL} = \\exp(L) = \\exp\\left(-\\frac{1}{T} \\sum_{t=1}^T \\log P(w_t | \\mathbf{w}_{1:t-1})\\right)$$\n\nLower perplexity indicates better prediction.\n\n**Teacher Forcing:**\nDuring training, use ground truth previous tokens:\n$$\\mathbf{h}_t = f(\\mathbf{e}_{w_t^*}, \\mathbf{h}_{t-1})$$\n\n**Inference Strategies:**\n\n**Greedy Decoding:**\n$$\\hat{w}_t = \\arg\\max_{w \\in \\mathcal{V}} P(w | \\mathbf{w}_{1:t-1})$$\n\n**Beam Search:**\nMaintain top-$k$ hypotheses at each step:\n$$\\text{score}(\\mathbf{w}_{1:t}) = \\sum_{i=1}^t \\log P(w_i | \\mathbf{w}_{1:i-1})$$\n\n**Sampling Methods:**\n**Temperature sampling:**\n$$P'(w_t | \\mathbf{w}_{1:t-1}) = \\frac{\\exp(\\mathbf{z}_{w_t} / \\tau)}{\\sum_{w'} \\exp(\\mathbf{z}_{w'} / \\tau)}$$\n\nLower $\\tau$ makes distribution more peaked.\n\n**Top-k sampling:** Only sample from $k$ most likely tokens\n**Nucleus (top-p) sampling:** Sample from tokens with cumulative probability $\\geq p$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model implementation\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_size)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Reshape for linear layer\n",
    "        batch_size, seq_len, hidden_size = lstm_out.shape\n",
    "        lstm_out = lstm_out.reshape(-1, hidden_size)\n",
    "        \n",
    "        # Predict next word for each position\n",
    "        output = self.fc(lstm_out)  # (batch_size * seq_len, vocab_size)\n",
    "        return output.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "# Create language model\n",
    "lm = LanguageModel(vocab_size, embedding_dim=32, hidden_size=64, num_layers=2)\n",
    "print(f\"Language model parameters: {sum(p.numel() for p in lm.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_sequence = torch.LongTensor([[2, 4, 5, 6, 7]])  # Sample word indices\n",
    "output = lm(test_sequence)\n",
    "print(f\"Input shape: {test_sequence.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output represents probability distribution over {vocab_size} words for each position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Preparing Text Data for Training\n\n### Mathematical Data Preparation for Sequence Models\n\n**Training Data Construction** for autoregressive language modeling:\n\n**Sliding Window Approach:**\nFor sequence $\\mathbf{w} = [w_1, w_2, \\ldots, w_n]$, create training pairs:\n$$(\\mathbf{x}_i, y_i) = ([w_1, \\ldots, w_i], w_{i+1})$$\n\n**Input-Target Alignment:**\n- **Input**: $\\mathbf{x} = [w_1, w_2, \\ldots, w_{T-1}]$\n- **Target**: $\\mathbf{y} = [w_2, w_3, \\ldots, w_T]$\n\n**Sequence Padding Mathematics:**\n\n**Fixed-Length Sequences:**\nPad sequences to maximum length $L$:\n$$\\mathbf{x}_{\\text{pad}} = \\begin{cases}\n[\\text{PAD}, \\ldots, \\text{PAD}, \\mathbf{x}] & \\text{if } |\\mathbf{x}| < L \\\\\n\\mathbf{x}[1:L] & \\text{if } |\\mathbf{x}| \\geq L\n\\end{cases}$$\n\n**Attention Masks:**\nBinary mask indicating valid positions:\n$$\\mathbf{m}_i = \\begin{cases}\n1 & \\text{if position } i \\text{ is valid} \\\\\n0 & \\text{if position } i \\text{ is padding}\n\\end{cases}$$\n\n**Loss Masking:**\n$$L = -\\frac{1}{\\sum_t m_t} \\sum_{t=1}^T m_t \\log P(w_t | \\mathbf{w}_{1:t-1})$$\n\n**Batch Processing:**\nFor batch of sequences $\\{\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(B)}\\}$:\n- **Input tensor**: $\\mathbf{X} \\in \\mathbb{Z}^{B \\times L}$  \n- **Target tensor**: $\\mathbf{Y} \\in \\mathbb{Z}^{B \\times L}$\n- **Mask tensor**: $\\mathbf{M} \\in \\{0,1\\}^{B \\times L}$\n\n**Memory Efficiency:**\nTotal memory for batch: $O(B \\times L \\times H)$ where $H$ is hidden size.\n\n**Data Augmentation for Text:**\n- **Random masking**: Replace tokens with `<UNK>` with probability $p$\n- **Token dropout**: Skip tokens during processing\n- **Sequence shuffling**: Permute sentence order (for document-level models)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for language modeling\n",
    "def create_language_modeling_data(texts, tokenizer, max_length=20):\n",
    "    \"\"\"Create input-target pairs for language modeling\"\"\"\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "    \n",
    "    for text in texts:\n",
    "        indices = tokenizer.text_to_indices(text)\n",
    "        \n",
    "        # Create sliding window of sequences\n",
    "        for i in range(len(indices) - 1):\n",
    "            # Input: words up to position i\n",
    "            # Target: word at position i+1\n",
    "            if i + 1 < max_length:\n",
    "                input_seq = indices[:i+1]\n",
    "                target_seq = indices[1:i+2]\n",
    "                \n",
    "                # Pad sequences to max_length\n",
    "                while len(input_seq) < max_length:\n",
    "                    input_seq.append(0)  # PAD token\n",
    "                    target_seq.append(0)\n",
    "                \n",
    "                input_sequences.append(input_seq[:max_length])\n",
    "                target_sequences.append(target_seq[:max_length])\n",
    "    \n",
    "    return torch.LongTensor(input_sequences), torch.LongTensor(target_sequences)\n",
    "\n",
    "# Extend our sample texts\n",
    "extended_texts = sample_texts + [\n",
    "    \"Artificial intelligence systems can understand natural language.\",\n",
    "    \"Training deep networks requires careful optimization.\",\n",
    "    \"The model generates text by predicting words sequentially.\",\n",
    "    \"Recurrent networks process sequences one step at a time.\",\n",
    "    \"Word embeddings capture semantic relationships between words.\"\n",
    "]\n",
    "\n",
    "# Rebuild vocabulary with more texts\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(extended_texts, min_freq=1)\n",
    "\n",
    "# Create training data\n",
    "X_lm, y_lm = create_language_modeling_data(extended_texts, tokenizer, max_length=15)\n",
    "\n",
    "print(f\"Language modeling data shape: {X_lm.shape}\")\n",
    "print(f\"Target shape: {y_lm.shape}\")\n",
    "print(f\"Number of training examples: {len(X_lm)}\")\n",
    "\n",
    "# Show example\n",
    "idx = 5\n",
    "input_text = tokenizer.indices_to_text(X_lm[idx].tolist())\n",
    "target_text = tokenizer.indices_to_text(y_lm[idx].tolist())\n",
    "print(f\"\\nExample {idx}:\")\n",
    "print(f\"Input:  {input_text}\")\n",
    "print(f\"Target: {target_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Text Classification with RNNs\n\n### Mathematical Framework for Sequence Classification\n\n**Text classification** maps variable-length sequences to discrete categories:\n\n**Classification Function:**\n$$f: (\\mathcal{V})^* \\rightarrow \\{1, 2, \\ldots, C\\}$$\n\n**RNN-based Classifier Architecture:**\n1. **Encoding**: $\\mathbf{h}_{1:T} = \\text{RNN}(\\mathbf{e}_{w_1}, \\ldots, \\mathbf{e}_{w_T})$\n2. **Aggregation**: $\\mathbf{s} = g(\\mathbf{h}_{1:T})$ \n3. **Classification**: $\\hat{y} = \\text{softmax}(\\mathbf{W}\\mathbf{s} + \\mathbf{b})$\n\n**Sequence Representation Methods:**\n\n**Last Hidden State:**\n$$\\mathbf{s} = \\mathbf{h}_T$$\nSimple but may lose early information.\n\n**Mean Pooling:**\n$$\\mathbf{s} = \\frac{1}{T} \\sum_{t=1}^T \\mathbf{h}_t$$\nEqual weight to all positions.\n\n**Max Pooling:**\n$$\\mathbf{s}_i = \\max_{t=1}^T \\mathbf{h}_{t,i}$$\nCaptures most salient features.\n\n**Attention-based Aggregation:**\n$$\\alpha_t = \\frac{\\exp(\\mathbf{v}^T \\tanh(\\mathbf{W}\\mathbf{h}_t))}{\\sum_{k=1}^T \\exp(\\mathbf{v}^T \\tanh(\\mathbf{W}\\mathbf{h}_k))}$$\n$$\\mathbf{s} = \\sum_{t=1}^T \\alpha_t \\mathbf{h}_t$$\n\n**Multi-class Cross-Entropy Loss:**\n$$L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^C y_{i,c} \\log \\hat{y}_{i,c}$$\n\n**Class Imbalance Handling:**\n\n**Weighted Loss:**\n$$L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^C w_c y_{i,c} \\log \\hat{y}_{i,c}$$\n\nwhere $w_c = \\frac{N}{C \\cdot n_c}$ and $n_c$ is count of class $c$.\n\n**Focal Loss:**\n$$L = -\\alpha_c (1-\\hat{y}_{i,c})^\\gamma \\log \\hat{y}_{i,c}$$\n\nFocuses learning on hard examples ($\\gamma > 0$)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification model\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes, num_layers=2):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=0.3)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embed words\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Process with LSTM\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Use last hidden state for classification\n",
    "        final_hidden = hidden[-1]  # Last layer's hidden state\n",
    "        \n",
    "        # Apply dropout and classify\n",
    "        output = self.dropout(final_hidden)\n",
    "        logits = self.classifier(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create synthetic classification data\n",
    "def create_classification_data():\n",
    "    \"\"\"Create simple sentiment classification data\"\"\"\n",
    "    positive_texts = [\n",
    "        \"This is amazing and wonderful.\",\n",
    "        \"I love this fantastic product.\",\n",
    "        \"Excellent quality and great performance.\",\n",
    "        \"Outstanding results and very happy.\",\n",
    "        \"Perfect solution for my needs.\"\n",
    "    ]\n",
    "    \n",
    "    negative_texts = [\n",
    "        \"This is terrible and disappointing.\",\n",
    "        \"I hate this awful product.\",\n",
    "        \"Poor quality and bad performance.\",\n",
    "        \"Disappointing results and very unhappy.\",\n",
    "        \"Useless solution for my needs.\"\n",
    "    ]\n",
    "    \n",
    "    texts = positive_texts + negative_texts\n",
    "    labels = [1] * len(positive_texts) + [0] * len(negative_texts)  # 1=positive, 0=negative\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# Prepare classification data\n",
    "class_texts, class_labels = create_classification_data()\n",
    "\n",
    "# Convert to sequences\n",
    "class_sequences = []\n",
    "for text in class_texts:\n",
    "    indices = tokenizer.text_to_indices(text)\n",
    "    # Pad to fixed length\n",
    "    while len(indices) < 10:\n",
    "        indices.append(0)\n",
    "    class_sequences.append(indices[:10])\n",
    "\n",
    "X_class = torch.LongTensor(class_sequences)\n",
    "y_class = torch.LongTensor(class_labels)\n",
    "\n",
    "print(f\"Classification data shape: {X_class.shape}\")\n",
    "print(f\"Labels shape: {y_class.shape}\")\n",
    "print(f\"Classes: {torch.unique(y_class)}\")\n",
    "\n",
    "# Create and test classifier\n",
    "classifier = TextClassifier(vocab_size=tokenizer.vocab_size, \n",
    "                          embedding_dim=32, \n",
    "                          hidden_size=64, \n",
    "                          num_classes=2)\n",
    "\n",
    "# Test forward pass\n",
    "test_output = classifier(X_class[:3])\n",
    "probabilities = F.softmax(test_output, dim=1)\n",
    "\n",
    "print(f\"\\nClassifier output shape: {test_output.shape}\")\n",
    "print(f\"Sample probabilities:\\n{probabilities}\")\n",
    "print(f\"Predicted classes: {probabilities.argmax(dim=1)}\")\n",
    "print(f\"True labels: {y_class[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Handling Variable-Length Sequences\n\n### Mathematical Framework for Efficient Sequence Processing\n\n**Variable-length sequences** require special handling for efficient batch processing:\n\n**Sequence Length Distribution:**\nLet $\\ell_i$ be length of sequence $i$. Batch efficiency depends on:\n$$\\text{Efficiency} = \\frac{\\text{min}(\\ell_1, \\ldots, \\ell_B)}{\\text{max}(\\ell_1, \\ldots, \\ell_B)}$$\n\n**Padding Strategies:**\n\n**Static Padding:**\nPad all sequences to maximum length in dataset:\n$$L = \\max_{i} \\ell_i$$\n**Memory**: $O(B \\times L)$\n**Computation**: $O(B \\times L \\times H)$\n\n**Dynamic Padding:**\nPad to maximum length in current batch:\n$$L_{\\text{batch}} = \\max_{i \\in \\text{batch}} \\ell_i$$\n\n**Packed Sequences (PyTorch):**\n\n**PackedSequence Structure:**\n- **data**: $\\mathbb{R}^{\\sum_{i=1}^B \\ell_i \\times H}$ (concatenated valid elements)\n- **batch_sizes**: $\\mathbb{Z}^{L_{\\max}}$ (number of sequences at each position)\n\n**Memory Efficiency:**\nPacked sequences use exactly $\\sum_{i=1}^B \\ell_i$ memory vs $B \\times L_{\\max}$ for padding.\n\n**LSTM with Packed Sequences:**\n$$\\text{PackedLSTM}: \\text{PackedSequence} \\rightarrow \\text{PackedSequence}$$\n\n**Masking Mathematics:**\n\n**Attention Mask:**\n$$\\mathbf{M}_{i,j} = \\begin{cases}\n0 & \\text{if position } j > \\ell_i \\\\\n1 & \\text{otherwise}\n\\end{cases}$$\n\n**Masked Computation:**\n$$\\mathbf{h}_{i,t} = \\begin{cases}\n\\text{RNN}(\\mathbf{x}_{i,t}, \\mathbf{h}_{i,t-1}) & \\text{if } t \\leq \\ell_i \\\\\n\\mathbf{0} & \\text{otherwise}\n\\end{cases}$$\n\n**Loss Masking:**\n$$L = \\frac{\\sum_{i,t} M_{i,t} \\cdot \\ell(y_{i,t}, \\hat{y}_{i,t})}{\\sum_{i,t} M_{i,t}}$$\n\n**Sorting for Efficiency:**\nSort sequences by length (descending) for better GPU utilization:\n$$\\ell_{\\sigma(1)} \\geq \\ell_{\\sigma(2)} \\geq \\cdots \\geq \\ell_{\\sigma(B)}$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of padding and packing\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Custom collate function for variable-length sequences\"\"\"\n",
    "    texts, labels = zip(*batch)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    texts = [torch.LongTensor(text) for text in texts]\n",
    "    labels = torch.LongTensor(labels)\n",
    "    \n",
    "    # Get lengths before padding\n",
    "    lengths = [len(text) for text in texts]\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_texts, labels, lengths\n",
    "\n",
    "# Variable-length sequence classifier\n",
    "class VariableLengthClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super(VariableLengthClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # Embed\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Pack padded sequence for efficiency\n",
    "        packed = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Process with LSTM\n",
    "        packed_output, (hidden, _) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack if needed (we just need final hidden state)\n",
    "        final_hidden = hidden[-1]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(final_hidden)\n",
    "        return logits\n",
    "\n",
    "# Create variable length data\n",
    "variable_texts = [\n",
    "    [4, 5, 6],  # Short sequence\n",
    "    [4, 5, 6, 7, 8, 9, 10],  # Medium sequence\n",
    "    [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],  # Long sequence\n",
    "    [4, 5]  # Very short sequence\n",
    "]\n",
    "variable_labels = [1, 0, 1, 0]\n",
    "\n",
    "# Test padding and packing\n",
    "batch_data = list(zip(variable_texts, variable_labels))\n",
    "padded_texts, labels, lengths = collate_batch(batch_data)\n",
    "\n",
    "print(f\"Original lengths: {[len(text) for text in variable_texts]}\")\n",
    "print(f\"Stored lengths: {lengths}\")\n",
    "print(f\"Padded shape: {padded_texts.shape}\")\n",
    "print(f\"Padded texts:\\n{padded_texts}\")\n",
    "\n",
    "# Test variable length classifier\n",
    "var_classifier = VariableLengthClassifier(vocab_size=tokenizer.vocab_size,\n",
    "                                        embedding_dim=16,\n",
    "                                        hidden_size=32,\n",
    "                                        num_classes=2)\n",
    "\n",
    "output = var_classifier(padded_texts, lengths)\n",
    "print(f\"\\nClassifier output shape: {output.shape}\")\n",
    "print(f\"Predictions: {F.softmax(output, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Bidirectional RNNs\n\n### Mathematical Framework for Bidirectional Processing\n\n**Bidirectional RNNs** process sequences in both temporal directions to capture complete contextual information:\n\n**Forward and Backward Processing:**\n$$\\overrightarrow{\\mathbf{h}}_t = \\text{RNN}_f(\\mathbf{x}_t, \\overrightarrow{\\mathbf{h}}_{t-1})$$\n$$\\overleftarrow{\\mathbf{h}}_t = \\text{RNN}_b(\\mathbf{x}_t, \\overleftarrow{\\mathbf{h}}_{t+1})$$\n\n**Complete Context Representation:**\n$$\\mathbf{h}_t = [\\overrightarrow{\\mathbf{h}}_t; \\overleftarrow{\\mathbf{h}}_t] \\in \\mathbb{R}^{2H}$$\n\n**Mathematical Benefits:**\n- **Forward context**: Information from $\\mathbf{x}_{1:t}$\n- **Backward context**: Information from $\\mathbf{x}_{t:T}$\n- **Full context**: Information from entire sequence $\\mathbf{x}_{1:T}$\n\n**Parameter Count:**\nBidirectional RNN has approximately **2× parameters** of unidirectional:\n$$\\text{Params}_{\\text{BiRNN}} \\approx 2 \\times \\text{Params}_{\\text{UniRNN}}$$\n\n**Computational Complexity:**\n- **Time**: $O(2 \\times T \\times H^2)$ (cannot parallelize forward/backward)\n- **Space**: $O(T \\times 2H)$ for storing both directions\n\n**Applications:**\n\n**Sequence Labeling:**\nFor each position $t$, predict label using full context:\n$$P(y_t | \\mathbf{x}_{1:T}) = \\text{softmax}(\\mathbf{W}[\\overrightarrow{\\mathbf{h}}_t; \\overleftarrow{\\mathbf{h}}_t] + \\mathbf{b})$$\n\n**Sequence Classification:**\nAggregate information from both endpoints:\n$$\\mathbf{s} = [\\overrightarrow{\\mathbf{h}}_T; \\overleftarrow{\\mathbf{h}}_1]$$\n\n**Limitations:**\n- **No streaming**: Requires complete sequence before processing\n- **Increased latency**: Cannot produce output until sequence end\n- **Memory overhead**: Stores hidden states in both directions\n\n**Attention Comparison:**\nBidirectional RNN vs self-attention:\n- **BiRNN**: Sequential processing, linear complexity in sequence length\n- **Self-attention**: Parallel processing, quadratic complexity, better long-range dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM classifier\n",
    "class BidirectionalClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super(BidirectionalClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Classifier takes concatenated forward and backward hidden states\n",
    "        self.classifier = nn.Linear(hidden_size * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embed\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # hidden shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # For bidirectional: (2, batch, hidden_size)\n",
    "        forward_hidden = hidden[0]  # Forward direction\n",
    "        backward_hidden = hidden[1]  # Backward direction\n",
    "        \n",
    "        # Concatenate forward and backward hidden states\n",
    "        combined_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(combined_hidden)\n",
    "        return logits\n",
    "\n",
    "# Compare unidirectional vs bidirectional\n",
    "uni_classifier = TextClassifier(vocab_size=tokenizer.vocab_size,\n",
    "                               embedding_dim=32,\n",
    "                               hidden_size=64,\n",
    "                               num_classes=2)\n",
    "\n",
    "bi_classifier = BidirectionalClassifier(vocab_size=tokenizer.vocab_size,\n",
    "                                       embedding_dim=32,\n",
    "                                       hidden_size=64,\n",
    "                                       num_classes=2)\n",
    "\n",
    "uni_params = sum(p.numel() for p in uni_classifier.parameters())\n",
    "bi_params = sum(p.numel() for p in bi_classifier.parameters())\n",
    "\n",
    "print(f\"Unidirectional classifier parameters: {uni_params:,}\")\n",
    "print(f\"Bidirectional classifier parameters: {bi_params:,}\")\n",
    "print(f\"Bidirectional has {((bi_params - uni_params) / uni_params * 100):.1f}% more parameters\")\n",
    "\n",
    "# Test both models\n",
    "test_input = X_class[:3]\n",
    "uni_output = uni_classifier(test_input)\n",
    "bi_output = bi_classifier(test_input)\n",
    "\n",
    "print(f\"\\nUnidirectional predictions: {F.softmax(uni_output, dim=1).argmax(dim=1)}\")\n",
    "print(f\"Bidirectional predictions: {F.softmax(bi_output, dim=1).argmax(dim=1)}\")\n",
    "print(f\"True labels: {y_class[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RNN Best Practices and Optimization\n\n### Mathematical Principles for Effective RNN Training\n\n**Training Stability and Performance** requires careful mathematical consideration of gradient dynamics:\n\n**1. Gradient Clipping Mathematics:**\n\n**Gradient Norm:**\n$$\\|\\nabla_{\\boldsymbol{\\theta}} L\\| = \\sqrt{\\sum_i \\|\\nabla_{\\boldsymbol{\\theta}_i} L\\|^2}$$\n\n**Clipping Operation:**\n$$\\nabla_{\\boldsymbol{\\theta}} L \\leftarrow \\begin{cases}\n\\nabla_{\\boldsymbol{\\theta}} L & \\text{if } \\|\\nabla_{\\boldsymbol{\\theta}} L\\| \\leq C \\\\\nC \\frac{\\nabla_{\\boldsymbol{\\theta}} L}{\\|\\nabla_{\\boldsymbol{\\theta}} L\\|} & \\text{otherwise}\n\\end{cases}$$\n\n**2. Learning Rate Scheduling:**\n\n**Step Decay:**\n$$\\eta_t = \\eta_0 \\gamma^{\\lfloor t/T \\rfloor}$$\n\n**Exponential Decay:**\n$$\\eta_t = \\eta_0 e^{-\\lambda t}$$\n\n**Cosine Annealing:**\n$$\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})(1 + \\cos(\\frac{\\pi t}{T}))$$\n\n**3. Initialization Strategies:**\n\n**Xavier/Glorot Initialization:**\n$$\\mathbf{W} \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\\right)$$\n\n**Orthogonal Initialization (for recurrent weights):**\nInitialize $\\mathbf{W}_{hh}$ as orthogonal matrix to preserve gradient magnitudes.\n\n**4. Regularization Techniques:**\n\n**Dropout Mathematics:**\n$$\\mathbf{h}_t = \\mathbf{m} \\odot \\text{RNN}(\\mathbf{x}_t, \\mathbf{h}_{t-1})$$\nwhere $\\mathbf{m} \\sim \\text{Bernoulli}(1-p)$ and scale by $\\frac{1}{1-p}$.\n\n**Weight Decay:**\n$$L_{\\text{total}} = L_{\\text{task}} + \\lambda \\|\\boldsymbol{\\theta}\\|_2^2$$\n\n**5. Architecture Considerations:**\n\n**Skip Connections (Highway Networks):**\n$$\\mathbf{h}_t = \\mathbf{g}_t \\odot \\tilde{\\mathbf{h}}_t + (1 - \\mathbf{g}_t) \\odot \\mathbf{h}_{t-1}$$\n\nwhere $\\mathbf{g}_t = \\sigma(\\mathbf{W}_g \\mathbf{x}_t + \\mathbf{U}_g \\mathbf{h}_{t-1})$ is transform gate.\n\n**Layer Normalization:**\n$$\\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\mu}{\\sigma} \\odot \\boldsymbol{\\gamma} + \\boldsymbol{\\beta}$$\n\nApplied to hidden states for training stability."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RNN and NLP Best Practices:\")\n",
    "print(\"\\n1. Architecture Choices:\")\n",
    "print(\"   - Use LSTM/GRU instead of vanilla RNN for longer sequences\")\n",
    "print(\"   - Consider bidirectional RNNs when full context is available\")\n",
    "print(\"   - Start with 1-2 layers, add more if underfitting\")\n",
    "print(\"   - Hidden size typically 128-512 for most tasks\")\n",
    "\n",
    "print(\"\\n2. Training Considerations:\")\n",
    "print(\"   - Use gradient clipping (norm 1.0-5.0) to prevent exploding gradients\")\n",
    "print(\"   - Apply dropout between layers and before final classifier\")\n",
    "print(\"   - Use teacher forcing for sequence generation tasks\")\n",
    "print(\"   - Consider scheduled sampling to reduce exposure bias\")\n",
    "\n",
    "print(\"\\n3. Data Preprocessing:\")\n",
    "print(\"   - Normalize/lowercase text consistently\")\n",
    "print(\"   - Handle out-of-vocabulary words with <UNK> tokens\")\n",
    "print(\"   - Use appropriate padding and packing for efficiency\")\n",
    "print(\"   - Consider subword tokenization (BPE) for better vocabulary coverage\")\n",
    "\n",
    "print(\"\\n4. Optimization:\")\n",
    "print(\"   - Adam optimizer often works well for RNNs\")\n",
    "print(\"   - Learning rate scheduling can improve convergence\")\n",
    "print(\"   - Batch size affects gradient noise and memory usage\")\n",
    "print(\"   - Use packed sequences for variable-length inputs\")\n",
    "\n",
    "print(\"\\n5. Evaluation and Analysis:\")\n",
    "print(\"   - Monitor both training and validation perplexity/loss\")\n",
    "print(\"   - Visualize attention weights (when applicable)\")\n",
    "print(\"   - Analyze model predictions on edge cases\")\n",
    "print(\"   - Check for overfitting with early stopping\")\n",
    "\n",
    "print(\"\\n6. When to Consider Alternatives:\")\n",
    "print(\"   - Very long sequences: Consider Transformers or memory networks\")\n",
    "print(\"   - Parallel processing needs: Transformers are more parallelizable\")\n",
    "print(\"   - Limited data: Pre-trained embeddings or models\")\n",
    "print(\"   - Real-time inference: Consider model compression techniques\")\n",
    "\n",
    "# Demonstrate gradient clipping importance\n",
    "print(\"\\nGradient Clipping Demonstration:\")\n",
    "model = nn.LSTM(10, 20, 2)\n",
    "x = torch.randn(5, 10, 10)\n",
    "target = torch.randn(5, 10, 20)\n",
    "\n",
    "output, _ = model(x)\n",
    "loss = F.mse_loss(output, target)\n",
    "loss.backward()\n",
    "\n",
    "# Check gradient norms before clipping\n",
    "total_norm = 0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "print(f\"Gradient norm before clipping: {total_norm:.4f}\")\n",
    "\n",
    "# Apply gradient clipping\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "# Check gradient norms after clipping\n",
    "total_norm_after = 0\n",
    "for p in model.parameters():\n",
    "    if p.grad is not None:\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm_after += param_norm.item() ** 2\n",
    "total_norm_after = total_norm_after ** (1. / 2)\n",
    "\n",
    "print(f\"Gradient norm after clipping: {total_norm_after:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}