{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Basics Part 10: MLOps and Advanced Production\n\nComprehensive guide to deploying PyTorch models in production with mathematical foundations for monitoring, optimization, and quality assurance\n\n## Mathematical Framework for Production ML Systems\n\nProduction machine learning requires rigorous mathematical foundations for monitoring, optimization, and reliability:\n\n### Core Mathematical Concepts for MLOps\n\n**1. Statistical Process Control:**\n- **Control Charts**: Monitor metric $X_t$ with control limits $\\mu \\pm k\\sigma$\n- **CUSUM**: Cumulative sum $S_t = \\max(0, S_{t-1} + (X_t - \\mu_0) - k)$\n- **EWMA**: Exponentially weighted moving average $Z_t = \\lambda X_t + (1-\\lambda)Z_{t-1}$\n\n**2. Drift Detection:**\n- **Population Stability Index**: $\\text{PSI} = \\sum_{i=1}^{10} (\\text{Expected}_i - \\text{Actual}_i) \\times \\ln\\left(\\frac{\\text{Expected}_i}{\\text{Actual}_i}\\right)$\n- **KL Divergence**: $D_{KL}(P||Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}$\n- **Kolmogorov-Smirnov Test**: $D_{n,m} = \\sup_x |F_{1,n}(x) - F_{2,m}(x)|$\n\n**3. Performance Metrics:**\n- **Service Level Indicators**: $\\text{SLI} = \\frac{\\text{Good Events}}{\\text{Total Events}}$\n- **Availability**: $A = \\frac{\\text{MTBF}}{\\text{MTBF} + \\text{MTTR}}$ where MTBF = Mean Time Between Failures\n- **Throughput**: $\\lambda = \\frac{N}{T}$ (requests per unit time)\n- **Latency Percentiles**: $P_{99} = \\inf\\{x : F(x) \\geq 0.99\\}$\n\n**4. A/B Testing Statistics:**\n- **Two-Sample t-test**: $t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$\n- **Effect Size (Cohen's d)**: $d = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_{pooled}}$\n- **Statistical Power**: $\\text{Power} = P(\\text{reject } H_0 | H_1 \\text{ is true})$\n- **Minimum Detectable Effect**: $\\text{MDE} = t_{\\alpha/2} + t_{\\beta} \\times \\frac{\\sigma\\sqrt{2}}{sqrt{n}}$\n\n**5. Model Optimization:**\n- **Quantization**: $q = \\text{round}\\left(\\frac{x}{s}\\right) + z$ where $s$ is scale, $z$ is zero-point\n- **Pruning**: Remove weights where $|w_{ij}| < \\theta$ (threshold-based)\n- **Knowledge Distillation**: $\\mathcal{L} = \\alpha \\mathcal{L}_{CE}(y, \\sigma(z_s)) + (1-\\alpha)\\mathcal{L}_{KL}(\\sigma(z_t/T), \\sigma(z_s/T))$\n\n**6. Resource Allocation:**\n- **Little's Law**: $L = \\lambda W$ (average number in system = arrival rate × average time in system)\n- **Queueing Theory**: $\\rho = \\frac{\\lambda}{\\mu}$ (utilization = arrival rate / service rate)\n- **Auto-scaling**: $\\text{instances} = \\max\\left(\\lceil\\frac{\\text{load}}{\\text{capacity per instance}}\\rceil, \\text{min instances}\\right)$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Experiment Tracking and Management\n\n**Mathematical Foundation for Experiment Design:**\n\nSystematic experiment tracking requires statistical rigor to ensure reproducible and meaningful results:\n\n**1. Experimental Design Principles:**\n- **Randomization**: Ensures $E[\\epsilon_i] = 0$ and reduces bias\n- **Replication**: Increases statistical power through larger sample size $n$\n- **Factorial Design**: Tests $k$ factors with $2^k$ treatments\n\n**2. Significance Testing:**\n- **Null Hypothesis**: $H_0: \\mu_1 = \\mu_2$ (no difference between conditions)\n- **Type I Error**: $\\alpha = P(\\text{reject } H_0 | H_0 \\text{ true})$\n- **Type II Error**: $\\beta = P(\\text{accept } H_0 | H_1 \\text{ true})$\n- **Multiple Comparisons**: Bonferroni correction $\\alpha' = \\frac{\\alpha}{m}$ for $m$ tests\n\n**3. Effect Size and Practical Significance:**\n- **Cohen's d**: $d = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_{pooled}}$ where $s_{pooled} = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}$\n- **Confidence Intervals**: $\\bar{X} \\pm t_{\\alpha/2} \\frac{s}{\\sqrt{n}}$\n\nSystematic experiment tracking is crucial for reproducible machine learning and enables teams to compare approaches systematically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment tracking system\n",
    "class ExperimentTracker:\n",
    "    def __init__(self, experiment_name: str, base_dir: str = \"./experiments\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.experiment_dir = self.base_dir / experiment_name / self.run_id\n",
    "        self.experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize tracking structures\n",
    "        self.metrics = {}\n",
    "        self.hyperparameters = {}\n",
    "        self.artifacts = {}\n",
    "        self.logs = []\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.experiment_dir / 'experiment.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(f\"{experiment_name}_{self.run_id}\")\n",
    "        \n",
    "        self.logger.info(f\"Started experiment: {experiment_name}\")\n",
    "        self.logger.info(f\"Run ID: {self.run_id}\")\n",
    "        self.logger.info(f\"Experiment directory: {self.experiment_dir}\")\n",
    "    \n",
    "    def log_hyperparameters(self, hyperparams: Dict[str, Any]):\n",
    "        \"\"\"Log hyperparameters for this experiment run\"\"\"\n",
    "        self.hyperparameters.update(hyperparams)\n",
    "        self.logger.info(f\"Hyperparameters: {hyperparams}\")\n",
    "        \n",
    "        # Save to file\n",
    "        with open(self.experiment_dir / 'hyperparameters.json', 'w') as f:\n",
    "            json.dump(self.hyperparameters, f, indent=2)\n",
    "    \n",
    "    def log_metric(self, name: str, value: float, step: Optional[int] = None):\n",
    "        \"\"\"Log a metric value\"\"\"\n",
    "        if name not in self.metrics:\n",
    "            self.metrics[name] = []\n",
    "        \n",
    "        metric_entry = {\n",
    "            'value': value,\n",
    "            'step': step,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        self.metrics[name].append(metric_entry)\n",
    "        self.logger.info(f\"Metric - {name}: {value} (step: {step})\")\n",
    "    \n",
    "    def log_artifact(self, name: str, artifact: Any, artifact_type: str = 'pickle'):\n",
    "        \"\"\"Log an artifact (model, plot, etc.)\"\"\"\n",
    "        artifact_path = self.experiment_dir / f\"{name}.{artifact_type}\"\n",
    "        \n",
    "        if artifact_type == 'pickle':\n",
    "            with open(artifact_path, 'wb') as f:\n",
    "                pickle.dump(artifact, f)\n",
    "        elif artifact_type == 'torch':\n",
    "            torch.save(artifact, artifact_path)\n",
    "        elif artifact_type == 'json':\n",
    "            with open(artifact_path, 'w') as f:\n",
    "                json.dump(artifact, f, indent=2)\n",
    "        \n",
    "        self.artifacts[name] = str(artifact_path)\n",
    "        self.logger.info(f\"Artifact saved: {name} -> {artifact_path}\")\n",
    "    \n",
    "    def save_model(self, model: nn.Module, name: str = 'model'):\n",
    "        \"\"\"Save PyTorch model\"\"\"\n",
    "        model_path = self.experiment_dir / f\"{name}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'hyperparameters': self.hyperparameters,\n",
    "            'run_id': self.run_id\n",
    "        }, model_path)\n",
    "        \n",
    "        self.artifacts[f\"{name}_model\"] = str(model_path)\n",
    "        self.logger.info(f\"Model saved: {model_path}\")\n",
    "    \n",
    "    def finish_experiment(self):\n",
    "        \"\"\"Finalize experiment and save summary\"\"\"\n",
    "        summary = {\n",
    "            'experiment_name': self.experiment_name,\n",
    "            'run_id': self.run_id,\n",
    "            'hyperparameters': self.hyperparameters,\n",
    "            'final_metrics': {name: values[-1] for name, values in self.metrics.items()},\n",
    "            'artifacts': self.artifacts,\n",
    "            'duration': time.time() - self.metrics[list(self.metrics.keys())[0]][0]['timestamp'] if self.metrics else 0\n",
    "        }\n",
    "        \n",
    "        with open(self.experiment_dir / 'summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Save detailed metrics\n",
    "        with open(self.experiment_dir / 'metrics.json', 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        \n",
    "        self.logger.info(\"Experiment finished\")\n",
    "        return summary\n",
    "\n",
    "# Example usage of experiment tracking\n",
    "# Create a simple model for demonstration\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Demonstrate experiment tracking\n",
    "tracker = ExperimentTracker(\"classification_experiment\")\n",
    "\n",
    "# Log hyperparameters\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'hidden_size': 64,\n",
    "    'num_epochs': 10,\n",
    "    'dropout_rate': 0.2\n",
    "}\n",
    "tracker.log_hyperparameters(hyperparams)\n",
    "\n",
    "# Create synthetic data\n",
    "X = torch.randn(1000, 20)\n",
    "y = torch.randint(0, 3, (1000,))\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "\n",
    "# Create model\n",
    "model = SimpleClassifier(20, hyperparams['hidden_size'], 3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop with tracking\n",
    "print(\"Training with experiment tracking...\")\n",
    "for epoch in range(hyperparams['num_epochs']):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # Log metrics\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    tracker.log_metric('train_loss', avg_loss, epoch)\n",
    "    tracker.log_metric('train_accuracy', accuracy, epoch)\n",
    "    \n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2f}%\")\n",
    "\n",
    "# Save model and finish experiment\n",
    "tracker.save_model(model)\n",
    "tracker.log_artifact('training_data_stats', {'mean': X.mean().item(), 'std': X.std().item()})\n",
    "\n",
    "summary = tracker.finish_experiment()\n",
    "print(f\"\\nExperiment completed. Run ID: {tracker.run_id}\")\n",
    "print(f\"Final accuracy: {summary['final_metrics']['train_accuracy']['value']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry and Versioning\n",
    "\n",
    "A model registry manages different versions of trained models, tracks their performance, and facilitates model deployment and rollback. This is essential for maintaining model quality and enabling safe deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Registry System\n",
    "class ModelRegistry:\n",
    "    def __init__(self, registry_dir: str = \"./model_registry\"):\n",
    "        self.registry_dir = Path(registry_dir)\n",
    "        self.registry_dir.mkdir(exist_ok=True)\n",
    "        self.metadata_file = self.registry_dir / 'registry_metadata.json'\n",
    "        \n",
    "        # Load existing registry or create new\n",
    "        if self.metadata_file.exists():\n",
    "            with open(self.metadata_file, 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "        else:\n",
    "            self.metadata = {'models': {}}\n",
    "    \n",
    "    def register_model(self, \n",
    "                      model: nn.Module,\n",
    "                      model_name: str,\n",
    "                      version: str,\n",
    "                      metrics: Dict[str, float],\n",
    "                      metadata: Dict[str, Any] = None,\n",
    "                      stage: str = 'staging'):\n",
    "        \"\"\"Register a new model version\"\"\"\n",
    "        \n",
    "        if model_name not in self.metadata['models']:\n",
    "            self.metadata['models'][model_name] = {'versions': {}}\n",
    "        \n",
    "        # Create version directory\n",
    "        version_dir = self.registry_dir / model_name / version\n",
    "        version_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = version_dir / 'model.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_architecture': model.__class__.__name__,\n",
    "            'model_config': self._extract_model_config(model)\n",
    "        }, model_path)\n",
    "        \n",
    "        # Save metadata\n",
    "        version_metadata = {\n",
    "            'version': version,\n",
    "            'stage': stage,\n",
    "            'metrics': metrics,\n",
    "            'metadata': metadata or {},\n",
    "            'created_at': datetime.datetime.now().isoformat(),\n",
    "            'model_path': str(model_path),\n",
    "            'model_size_mb': model_path.stat().st_size / (1024*1024) if model_path.exists() else 0\n",
    "        }\n",
    "        \n",
    "        self.metadata['models'][model_name]['versions'][version] = version_metadata\n",
    "        self._save_metadata()\n",
    "        \n",
    "        print(f\"Registered {model_name} version {version} in {stage} stage\")\n",
    "        return version_metadata\n",
    "    \n",
    "    def promote_model(self, model_name: str, version: str, target_stage: str):\n",
    "        \"\"\"Promote model to a different stage (staging -> production)\"\"\"\n",
    "        if model_name not in self.metadata['models']:\n",
    "            raise ValueError(f\"Model {model_name} not found\")\n",
    "        \n",
    "        if version not in self.metadata['models'][model_name]['versions']:\n",
    "            raise ValueError(f\"Version {version} not found for model {model_name}\")\n",
    "        \n",
    "        # Update stage\n",
    "        self.metadata['models'][model_name]['versions'][version]['stage'] = target_stage\n",
    "        self.metadata['models'][model_name]['versions'][version]['promoted_at'] = datetime.datetime.now().isoformat()\n",
    "        \n",
    "        self._save_metadata()\n",
    "        print(f\"Promoted {model_name} version {version} to {target_stage}\")\n",
    "    \n",
    "    def get_model_by_stage(self, model_name: str, stage: str = 'production'):\n",
    "        \"\"\"Get the latest model version for a given stage\"\"\"\n",
    "        if model_name not in self.metadata['models']:\n",
    "            raise ValueError(f\"Model {model_name} not found\")\n",
    "        \n",
    "        versions = self.metadata['models'][model_name]['versions']\n",
    "        stage_versions = [(v, data) for v, data in versions.items() if data['stage'] == stage]\n",
    "        \n",
    "        if not stage_versions:\n",
    "            raise ValueError(f\"No models found in {stage} stage for {model_name}\")\n",
    "        \n",
    "        # Return the latest version (by creation time)\n",
    "        latest_version = max(stage_versions, key=lambda x: x[1]['created_at'])\n",
    "        return latest_version[0], latest_version[1]\n",
    "    \n",
    "    def load_model(self, model_name: str, version: str, model_class):\n",
    "        \"\"\"Load a specific model version\"\"\"\n",
    "        if model_name not in self.metadata['models']:\n",
    "            raise ValueError(f\"Model {model_name} not found\")\n",
    "        \n",
    "        version_data = self.metadata['models'][model_name]['versions'].get(version)\n",
    "        if not version_data:\n",
    "            raise ValueError(f\"Version {version} not found for model {model_name}\")\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = torch.load(version_data['model_path'])\n",
    "        \n",
    "        # Instantiate model (would need more sophisticated config handling in practice)\n",
    "        model = model_class(**version_data['metadata'].get('model_config', {}))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        return model, version_data\n",
    "    \n",
    "    def list_models(self, stage: Optional[str] = None):\n",
    "        \"\"\"List all models, optionally filtered by stage\"\"\"\n",
    "        models_info = []\n",
    "        \n",
    "        for model_name, model_data in self.metadata['models'].items():\n",
    "            for version, version_data in model_data['versions'].items():\n",
    "                if stage is None or version_data['stage'] == stage:\n",
    "                    models_info.append({\n",
    "                        'model_name': model_name,\n",
    "                        'version': version,\n",
    "                        'stage': version_data['stage'],\n",
    "                        'metrics': version_data['metrics'],\n",
    "                        'created_at': version_data['created_at'],\n",
    "                        'size_mb': version_data['model_size_mb']\n",
    "                    })\n",
    "        \n",
    "        return sorted(models_info, key=lambda x: x['created_at'], reverse=True)\n",
    "    \n",
    "    def compare_models(self, model_name: str, versions: List[str], metric: str):\n",
    "        \"\"\"Compare performance of different model versions\"\"\"\n",
    "        comparison = []\n",
    "        \n",
    "        for version in versions:\n",
    "            version_data = self.metadata['models'][model_name]['versions'].get(version)\n",
    "            if version_data:\n",
    "                comparison.append({\n",
    "                    'version': version,\n",
    "                    'stage': version_data['stage'],\n",
    "                    'metric_value': version_data['metrics'].get(metric, 'N/A'),\n",
    "                    'created_at': version_data['created_at']\n",
    "                })\n",
    "        \n",
    "        return sorted(comparison, key=lambda x: x['metric_value'] if x['metric_value'] != 'N/A' else -1, reverse=True)\n",
    "    \n",
    "    def _extract_model_config(self, model: nn.Module):\n",
    "        \"\"\"Extract model configuration (simplified)\"\"\"\n",
    "        # This would be more sophisticated in practice\n",
    "        if hasattr(model, 'fc1') and hasattr(model, 'fc2'):\n",
    "            return {\n",
    "                'input_size': model.fc1.in_features,\n",
    "                'hidden_size': model.fc1.out_features,\n",
    "                'num_classes': model.fc2.out_features\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Save registry metadata to disk\"\"\"\n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "\n",
    "# Demonstrate model registry\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Register different model versions\n",
    "models_to_register = [\n",
    "    {'version': 'v1.0', 'metrics': {'accuracy': 0.85, 'precision': 0.82}, 'stage': 'staging'},\n",
    "    {'version': 'v1.1', 'metrics': {'accuracy': 0.87, 'precision': 0.84}, 'stage': 'staging'},\n",
    "    {'version': 'v1.2', 'metrics': {'accuracy': 0.89, 'precision': 0.86}, 'stage': 'staging'}\n",
    "]\n",
    "\n",
    "print(\"Registering model versions...\")\n",
    "for model_info in models_to_register:\n",
    "    # Create a slightly different model for each version\n",
    "    test_model = SimpleClassifier(20, 64 + int(model_info['version'][-1]) * 10, 3)\n",
    "    \n",
    "    registry.register_model(\n",
    "        model=test_model,\n",
    "        model_name=\"text_classifier\",\n",
    "        version=model_info['version'],\n",
    "        metrics=model_info['metrics'],\n",
    "        metadata={'description': f'Model version {model_info[\"version\"]} with improved accuracy'},\n",
    "        stage=model_info['stage']\n",
    "    )\n",
    "\n",
    "# Promote best model to production\n",
    "registry.promote_model('text_classifier', 'v1.2', 'production')\n",
    "\n",
    "# List all models\n",
    "print(\"\\nAll registered models:\")\n",
    "all_models = registry.list_models()\n",
    "for model_info in all_models:\n",
    "    print(f\"  {model_info['model_name']} {model_info['version']} ({model_info['stage']}) - \"\n",
    "          f\"Accuracy: {model_info['metrics']['accuracy']:.3f}\")\n",
    "\n",
    "# Get production model\n",
    "prod_version, prod_data = registry.get_model_by_stage('text_classifier', 'production')\n",
    "print(f\"\\nProduction model: {prod_version} with accuracy {prod_data['metrics']['accuracy']:.3f}\")\n",
    "\n",
    "# Compare models\n",
    "print(\"\\nModel comparison by accuracy:\")\n",
    "comparison = registry.compare_models('text_classifier', ['v1.0', 'v1.1', 'v1.2'], 'accuracy')\n",
    "for comp in comparison:\n",
    "    print(f\"  Version {comp['version']}: {comp['metric_value']:.3f} ({comp['stage']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Monitoring and Drift Detection\n",
    "\n",
    "Production models need continuous monitoring to detect performance degradation, data drift, and other issues. This system tracks model health and alerts when intervention is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Monitoring System\n",
    "class ModelMonitor:\n",
    "    def __init__(self, model_name: str, monitoring_dir: str = \"./monitoring\"):\n",
    "        self.model_name = model_name\n",
    "        self.monitoring_dir = Path(monitoring_dir) / model_name\n",
    "        self.monitoring_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Initialize monitoring data\n",
    "        self.performance_log = []\n",
    "        self.feature_stats = {'mean': {}, 'std': {}, 'min': {}, 'max': {}}\n",
    "        self.prediction_stats = {'distribution': [], 'confidence_scores': []}\n",
    "        self.alerts = []\n",
    "        \n",
    "        # Drift detection parameters\n",
    "        self.drift_threshold = 0.05  # Statistical significance level\n",
    "        self.performance_threshold = 0.1  # Acceptable performance drop\n",
    "        \n",
    "    def log_prediction(self, features: torch.Tensor, predictions: torch.Tensor, \n",
    "                      confidences: torch.Tensor, true_labels: Optional[torch.Tensor] = None):\n",
    "        \"\"\"Log model predictions and compute statistics\"\"\"\n",
    "        timestamp = time.time()\n",
    "        \n",
    "        # Feature statistics\n",
    "        batch_stats = self._compute_feature_stats(features)\n",
    "        \n",
    "        # Prediction statistics\n",
    "        pred_dist = torch.bincount(predictions, minlength=3).float() / len(predictions)\n",
    "        avg_confidence = confidences.mean().item()\n",
    "        \n",
    "        # Performance metrics (if ground truth available)\n",
    "        performance = None\n",
    "        if true_labels is not None:\n",
    "            accuracy = (predictions == true_labels).float().mean().item()\n",
    "            performance = {'accuracy': accuracy, 'count': len(predictions)}\n",
    "        \n",
    "        # Log entry\n",
    "        log_entry = {\n",
    "            'timestamp': timestamp,\n",
    "            'batch_size': len(predictions),\n",
    "            'feature_stats': batch_stats,\n",
    "            'prediction_distribution': pred_dist.tolist(),\n",
    "            'average_confidence': avg_confidence,\n",
    "            'performance': performance\n",
    "        }\n",
    "        \n",
    "        self.performance_log.append(log_entry)\n",
    "        \n",
    "        # Check for drift and anomalies\n",
    "        self._check_drift(batch_stats, pred_dist)\n",
    "        if performance:\n",
    "            self._check_performance_degradation(performance['accuracy'])\n",
    "    \n",
    "    def _compute_feature_stats(self, features: torch.Tensor):\n",
    "        \"\"\"Compute statistics for input features\"\"\"\n",
    "        return {\n",
    "            'mean': features.mean(dim=0).tolist(),\n",
    "            'std': features.std(dim=0).tolist(),\n",
    "            'min': features.min(dim=0)[0].tolist(),\n",
    "            'max': features.max(dim=0)[0].tolist()\n",
    "        }\n",
    "    \n",
    "    def set_baseline(self, baseline_features: torch.Tensor, baseline_predictions: torch.Tensor):\n",
    "        \"\"\"Set baseline statistics for drift detection\"\"\"\n",
    "        self.baseline_stats = self._compute_feature_stats(baseline_features)\n",
    "        self.baseline_pred_dist = torch.bincount(baseline_predictions, minlength=3).float() / len(baseline_predictions)\n",
    "        print(f\"Baseline set with {len(baseline_features)} samples\")\n",
    "    \n",
    "    def _check_drift(self, current_stats: Dict, current_pred_dist: torch.Tensor):\n",
    "        \"\"\"Check for data and prediction drift\"\"\"\n",
    "        if not hasattr(self, 'baseline_stats'):\n",
    "            return\n",
    "        \n",
    "        # Feature drift detection (simplified KL divergence approximation)\n",
    "        feature_drift_scores = []\n",
    "        for i, (baseline_mean, current_mean) in enumerate(zip(self.baseline_stats['mean'], current_stats['mean'])):\n",
    "            baseline_std = self.baseline_stats['std'][i]\n",
    "            current_std = current_stats['std'][i]\n",
    "            \n",
    "            # Simple drift score based on mean and std deviation\n",
    "            if baseline_std > 0:\n",
    "                drift_score = abs(baseline_mean - current_mean) / baseline_std\n",
    "                feature_drift_scores.append(drift_score)\n",
    "        \n",
    "        max_feature_drift = max(feature_drift_scores) if feature_drift_scores else 0\n",
    "        \n",
    "        # Prediction drift (KL divergence)\n",
    "        pred_drift = self._kl_divergence(self.baseline_pred_dist, current_pred_dist)\n",
    "        \n",
    "        # Check thresholds\n",
    "        if max_feature_drift > 2.0:  # 2 standard deviations\n",
    "            self._create_alert('feature_drift', f'Feature drift detected: {max_feature_drift:.3f}')\n",
    "        \n",
    "        if pred_drift > self.drift_threshold:\n",
    "            self._create_alert('prediction_drift', f'Prediction drift detected: {pred_drift:.3f}')\n",
    "    \n",
    "    def _kl_divergence(self, p: torch.Tensor, q: torch.Tensor):\n",
    "        \"\"\"Compute KL divergence between two distributions\"\"\"\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-8\n",
    "        p = p + epsilon\n",
    "        q = q + epsilon\n",
    "        \n",
    "        return torch.sum(p * torch.log(p / q)).item()\n",
    "    \n",
    "    def _check_performance_degradation(self, current_accuracy: float):\n",
    "        \"\"\"Check for performance degradation\"\"\"\n",
    "        # Get recent performance\n",
    "        recent_performance = [entry['performance']['accuracy'] \n",
    "                            for entry in self.performance_log[-10:] \n",
    "                            if entry['performance']]\n",
    "        \n",
    "        if len(recent_performance) >= 5:\n",
    "            baseline_accuracy = np.mean(recent_performance[:5])\n",
    "            if current_accuracy < baseline_accuracy - self.performance_threshold:\n",
    "                self._create_alert('performance_degradation', \n",
    "                                 f'Accuracy dropped from {baseline_accuracy:.3f} to {current_accuracy:.3f}')\n",
    "    \n",
    "    def _create_alert(self, alert_type: str, message: str):\n",
    "        \"\"\"Create and log an alert\"\"\"\n",
    "        alert = {\n",
    "            'timestamp': time.time(),\n",
    "            'type': alert_type,\n",
    "            'message': message,\n",
    "            'severity': self._get_alert_severity(alert_type)\n",
    "        }\n",
    "        \n",
    "        self.alerts.append(alert)\n",
    "        print(f\"ALERT [{alert['severity']}]: {alert_type} - {message}\")\n",
    "    \n",
    "    def _get_alert_severity(self, alert_type: str):\n",
    "        \"\"\"Determine alert severity\"\"\"\n",
    "        severity_map = {\n",
    "            'feature_drift': 'MEDIUM',\n",
    "            'prediction_drift': 'MEDIUM',\n",
    "            'performance_degradation': 'HIGH'\n",
    "        }\n",
    "        return severity_map.get(alert_type, 'LOW')\n",
    "    \n",
    "    def get_monitoring_report(self):\n",
    "        \"\"\"Generate monitoring report\"\"\"\n",
    "        if not self.performance_log:\n",
    "            return \"No monitoring data available\"\n",
    "        \n",
    "        # Recent performance\n",
    "        recent_entries = self.performance_log[-10:]\n",
    "        recent_accuracy = [e['performance']['accuracy'] for e in recent_entries if e['performance']]\n",
    "        \n",
    "        # Alert summary\n",
    "        alert_counts = {}\n",
    "        for alert in self.alerts:\n",
    "            alert_counts[alert['type']] = alert_counts.get(alert['type'], 0) + 1\n",
    "        \n",
    "        report = {\n",
    "            'model_name': self.model_name,\n",
    "            'monitoring_period': f\"{len(self.performance_log)} batches\",\n",
    "            'recent_accuracy': {\n",
    "                'mean': np.mean(recent_accuracy) if recent_accuracy else 'N/A',\n",
    "                'std': np.std(recent_accuracy) if recent_accuracy else 'N/A',\n",
    "                'samples': len(recent_accuracy)\n",
    "            },\n",
    "            'alerts': {\n",
    "                'total_alerts': len(self.alerts),\n",
    "                'by_type': alert_counts,\n",
    "                'recent_alerts': self.alerts[-5:] if self.alerts else []\n",
    "            },\n",
    "            'health_status': self._get_health_status()\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _get_health_status(self):\n",
    "        \"\"\"Determine overall model health\"\"\"\n",
    "        recent_alerts = [a for a in self.alerts if time.time() - a['timestamp'] < 3600]  # Last hour\n",
    "        high_severity_alerts = [a for a in recent_alerts if a['severity'] == 'HIGH']\n",
    "        \n",
    "        if high_severity_alerts:\n",
    "            return 'CRITICAL'\n",
    "        elif len(recent_alerts) > 5:\n",
    "            return 'WARNING'\n",
    "        else:\n",
    "            return 'HEALTHY'\n",
    "    \n",
    "    def save_monitoring_data(self):\n",
    "        \"\"\"Save monitoring data to disk\"\"\"\n",
    "        monitoring_data = {\n",
    "            'performance_log': self.performance_log,\n",
    "            'alerts': self.alerts,\n",
    "            'baseline_stats': getattr(self, 'baseline_stats', None)\n",
    "        }\n",
    "        \n",
    "        with open(self.monitoring_dir / 'monitoring_data.json', 'w') as f:\n",
    "            json.dump(monitoring_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Monitoring data saved to {self.monitoring_dir}\")\n",
    "\n",
    "# Demonstrate model monitoring\n",
    "monitor = ModelMonitor(\"text_classifier\")\n",
    "\n",
    "# Set baseline with normal data\n",
    "baseline_features = torch.randn(1000, 20)\n",
    "baseline_predictions = torch.randint(0, 3, (1000,))\n",
    "monitor.set_baseline(baseline_features, baseline_predictions)\n",
    "\n",
    "# Simulate normal operation\n",
    "print(\"\\nSimulating normal model operation...\")\n",
    "for i in range(5):\n",
    "    # Normal data similar to baseline\n",
    "    features = torch.randn(100, 20) * 1.1  # Slight variation\n",
    "    predictions = torch.randint(0, 3, (100,))\n",
    "    confidences = torch.rand(100) * 0.3 + 0.7  # High confidence\n",
    "    true_labels = torch.randint(0, 3, (100,))\n",
    "    \n",
    "    monitor.log_prediction(features, predictions, confidences, true_labels)\n",
    "\n",
    "# Simulate data drift\n",
    "print(\"\\nSimulating data drift...\")\n",
    "for i in range(3):\n",
    "    # Shifted data distribution\n",
    "    features = torch.randn(100, 20) + 2.0  # Mean shift\n",
    "    predictions = torch.randint(0, 3, (100,))\n",
    "    confidences = torch.rand(100) * 0.4 + 0.5  # Lower confidence\n",
    "    true_labels = torch.randint(0, 3, (100,))\n",
    "    \n",
    "    monitor.log_prediction(features, predictions, confidences, true_labels)\n",
    "\n",
    "# Simulate performance degradation\n",
    "print(\"\\nSimulating performance degradation...\")\n",
    "for i in range(3):\n",
    "    features = torch.randn(100, 20)\n",
    "    predictions = torch.randint(0, 3, (100,))\n",
    "    confidences = torch.rand(100) * 0.3 + 0.4  # Lower confidence\n",
    "    # Generate labels with lower accuracy\n",
    "    true_labels = torch.randint(0, 3, (100,))\n",
    "    # Artificially create poor performance\n",
    "    incorrect_mask = torch.rand(100) < 0.4  # 40% incorrect\n",
    "    true_labels[incorrect_mask] = (predictions[incorrect_mask] + 1) % 3\n",
    "    \n",
    "    monitor.log_prediction(features, predictions, confidences, true_labels)\n",
    "\n",
    "# Generate monitoring report\n",
    "report = monitor.get_monitoring_report()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MONITORING REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: {report['model_name']}\")\n",
    "print(f\"Health Status: {report['health_status']}\")\n",
    "print(f\"Monitoring Period: {report['monitoring_period']}\")\n",
    "\n",
    "if report['recent_accuracy']['samples'] > 0:\n",
    "    print(f\"Recent Accuracy: {report['recent_accuracy']['mean']:.3f} ± {report['recent_accuracy']['std']:.3f}\")\n",
    "\n",
    "print(f\"Total Alerts: {report['alerts']['total_alerts']}\")\n",
    "for alert_type, count in report['alerts']['by_type'].items():\n",
    "    print(f\"  - {alert_type}: {count}\")\n",
    "\n",
    "if report['alerts']['recent_alerts']:\n",
    "    print(\"\\nRecent Alerts:\")\n",
    "    for alert in report['alerts']['recent_alerts']:\n",
    "        alert_time = datetime.datetime.fromtimestamp(alert['timestamp']).strftime('%H:%M:%S')\n",
    "        print(f\"  [{alert['severity']}] {alert_time}: {alert['message']}\")\n",
    "\n",
    "# Save monitoring data\n",
    "monitor.save_monitoring_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing Framework for Models\n",
    "\n",
    "A/B testing allows safe deployment of new models by comparing their performance against existing models in production. This framework manages traffic splitting, statistical analysis, and rollback capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Testing Framework\n",
    "class ABTestingFramework:\n",
    "    def __init__(self, test_name: str, results_dir: str = \"./ab_tests\"):\n",
    "        self.test_name = test_name\n",
    "        self.results_dir = Path(results_dir) / test_name\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Test configuration\n",
    "        self.variants = {}  # variant_name -> model info\n",
    "        self.traffic_allocation = {}  # variant_name -> percentage\n",
    "        self.metrics = {}  # variant_name -> list of metrics\n",
    "        self.test_active = False\n",
    "        self.start_time = None\n",
    "        \n",
    "        # Statistical parameters\n",
    "        self.significance_level = 0.05\n",
    "        self.minimum_sample_size = 100\n",
    "        \n",
    "    def add_variant(self, variant_name: str, model: nn.Module, traffic_percentage: float, \n",
    "                   is_control: bool = False):\n",
    "        \"\"\"Add a variant to the A/B test\"\"\"\n",
    "        self.variants[variant_name] = {\n",
    "            'model': model,\n",
    "            'is_control': is_control,\n",
    "            'model_path': self.results_dir / f\"{variant_name}_model.pth\"\n",
    "        }\n",
    "        \n",
    "        self.traffic_allocation[variant_name] = traffic_percentage\n",
    "        self.metrics[variant_name] = []\n",
    "        \n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), self.variants[variant_name]['model_path'])\n",
    "        \n",
    "        print(f\"Added variant '{variant_name}' with {traffic_percentage}% traffic allocation\")\n",
    "        print(f\"  Control variant: {is_control}\")\n",
    "        print(f\"  Model saved to: {self.variants[variant_name]['model_path']}\")\n",
    "    \n",
    "    def start_test(self, duration_hours: float = 24):\n",
    "        \"\"\"Start the A/B test\"\"\"\n",
    "        # Validate configuration\n",
    "        total_traffic = sum(self.traffic_allocation.values())\n",
    "        if abs(total_traffic - 100) > 0.01:\n",
    "            raise ValueError(f\"Traffic allocation must sum to 100%, got {total_traffic}%\")\n",
    "        \n",
    "        control_variants = [name for name, info in self.variants.items() if info['is_control']]\n",
    "        if len(control_variants) != 1:\n",
    "            raise ValueError(\"Exactly one control variant must be specified\")\n",
    "        \n",
    "        self.test_active = True\n",
    "        self.start_time = time.time()\n",
    "        self.duration = duration_hours * 3600  # Convert to seconds\n",
    "        \n",
    "        print(f\"A/B test '{self.test_name}' started\")\n",
    "        print(f\"Duration: {duration_hours} hours\")\n",
    "        print(f\"Variants: {list(self.variants.keys())}\")\n",
    "        print(f\"Traffic allocation: {self.traffic_allocation}\")\n",
    "    \n",
    "    def assign_variant(self, user_id: str) -> str:\n",
    "        \"\"\"Assign a user to a variant based on traffic allocation\"\"\"\n",
    "        if not self.test_active:\n",
    "            raise ValueError(\"A/B test is not active\")\n",
    "        \n",
    "        # Simple hash-based assignment for consistency\n",
    "        hash_value = hash(user_id + self.test_name) % 100\n",
    "        \n",
    "        cumulative_percentage = 0\n",
    "        for variant_name, percentage in self.traffic_allocation.items():\n",
    "            cumulative_percentage += percentage\n",
    "            if hash_value < cumulative_percentage:\n",
    "                return variant_name\n",
    "        \n",
    "        # Fallback to first variant\n",
    "        return list(self.variants.keys())[0]\n",
    "    \n",
    "    def get_model_for_variant(self, variant_name: str) -> nn.Module:\n",
    "        \"\"\"Get the model for a specific variant\"\"\"\n",
    "        if variant_name not in self.variants:\n",
    "            raise ValueError(f\"Variant '{variant_name}' not found\")\n",
    "        \n",
    "        return self.variants[variant_name]['model']\n",
    "    \n",
    "    def log_result(self, user_id: str, variant_name: str, metrics: Dict[str, float]):\n",
    "        \"\"\"Log a result for the A/B test\"\"\"\n",
    "        if not self.test_active:\n",
    "            return\n",
    "        \n",
    "        result = {\n",
    "            'timestamp': time.time(),\n",
    "            'user_id': user_id,\n",
    "            'variant': variant_name,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        self.metrics[variant_name].append(result)\n",
    "    \n",
    "    def get_test_status(self):\n",
    "        \"\"\"Get current test status and preliminary results\"\"\"\n",
    "        if not self.test_active:\n",
    "            return {'status': 'inactive'}\n",
    "        \n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        remaining_time = max(0, self.duration - elapsed_time)\n",
    "        \n",
    "        # Sample sizes\n",
    "        sample_sizes = {variant: len(results) for variant, results in self.metrics.items()}\n",
    "        \n",
    "        # Preliminary results\n",
    "        variant_stats = {}\n",
    "        for variant_name, results in self.metrics.items():\n",
    "            if results:\n",
    "                # Calculate statistics for each metric\n",
    "                metrics_summary = {}\n",
    "                for metric_name in results[0]['metrics'].keys():\n",
    "                    values = [r['metrics'][metric_name] for r in results]\n",
    "                    metrics_summary[metric_name] = {\n",
    "                        'mean': np.mean(values),\n",
    "                        'std': np.std(values),\n",
    "                        'count': len(values)\n",
    "                    }\n",
    "                \n",
    "                variant_stats[variant_name] = metrics_summary\n",
    "        \n",
    "        return {\n",
    "            'status': 'active',\n",
    "            'elapsed_hours': elapsed_time / 3600,\n",
    "            'remaining_hours': remaining_time / 3600,\n",
    "            'sample_sizes': sample_sizes,\n",
    "            'variant_stats': variant_stats,\n",
    "            'ready_for_analysis': all(size >= self.minimum_sample_size for size in sample_sizes.values())\n",
    "        }\n",
    "    \n",
    "    def run_statistical_analysis(self, metric_name: str = 'accuracy'):\n",
    "        \"\"\"Run statistical analysis comparing variants\"\"\"\n",
    "        from scipy import stats\n",
    "        \n",
    "        if not self.test_active:\n",
    "            raise ValueError(\"Test must be active to run analysis\")\n",
    "        \n",
    "        # Get control variant\n",
    "        control_variant = next(name for name, info in self.variants.items() if info['is_control'])\n",
    "        \n",
    "        # Extract metric values\n",
    "        results = {}\n",
    "        for variant_name, metrics_list in self.metrics.items():\n",
    "            if metrics_list and metric_name in metrics_list[0]['metrics']:\n",
    "                values = [m['metrics'][metric_name] for m in metrics_list]\n",
    "                results[variant_name] = values\n",
    "        \n",
    "        if control_variant not in results:\n",
    "            raise ValueError(f\"No data for control variant '{control_variant}'\")\n",
    "        \n",
    "        # Statistical tests\n",
    "        analysis_results = {\n",
    "            'metric': metric_name,\n",
    "            'control_variant': control_variant,\n",
    "            'comparisons': {}\n",
    "        }\n",
    "        \n",
    "        control_values = results[control_variant]\n",
    "        \n",
    "        for variant_name, variant_values in results.items():\n",
    "            if variant_name == control_variant:\n",
    "                continue\n",
    "            \n",
    "            # Two-sample t-test\n",
    "            t_stat, p_value = stats.ttest_ind(variant_values, control_values)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            pooled_std = np.sqrt(((len(variant_values) - 1) * np.var(variant_values) + \n",
    "                                 (len(control_values) - 1) * np.var(control_values)) / \n",
    "                                (len(variant_values) + len(control_values) - 2))\n",
    "            \n",
    "            effect_size = (np.mean(variant_values) - np.mean(control_values)) / pooled_std if pooled_std > 0 else 0\n",
    "            \n",
    "            # Confidence interval for difference\n",
    "            diff_mean = np.mean(variant_values) - np.mean(control_values)\n",
    "            diff_se = np.sqrt(np.var(variant_values)/len(variant_values) + np.var(control_values)/len(control_values))\n",
    "            ci_lower = diff_mean - 1.96 * diff_se\n",
    "            ci_upper = diff_mean + 1.96 * diff_se\n",
    "            \n",
    "            analysis_results['comparisons'][variant_name] = {\n",
    "                'sample_size': len(variant_values),\n",
    "                'mean': np.mean(variant_values),\n",
    "                'std': np.std(variant_values),\n",
    "                'vs_control': {\n",
    "                    'difference': diff_mean,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': p_value < self.significance_level,\n",
    "                    'effect_size': effect_size,\n",
    "                    'confidence_interval': [ci_lower, ci_upper]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Control variant stats\n",
    "        analysis_results['control_stats'] = {\n",
    "            'sample_size': len(control_values),\n",
    "            'mean': np.mean(control_values),\n",
    "            'std': np.std(control_values)\n",
    "        }\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    def make_decision(self, analysis_results: Dict, min_effect_size: float = 0.1):\n",
    "        \"\"\"Make a decision based on statistical analysis\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        best_variant = None\n",
    "        best_improvement = -float('inf')\n",
    "        \n",
    "        for variant_name, comparison in analysis_results['comparisons'].items():\n",
    "            vs_control = comparison['vs_control']\n",
    "            \n",
    "            if vs_control['significant'] and vs_control['effect_size'] > min_effect_size:\n",
    "                if vs_control['difference'] > best_improvement:\n",
    "                    best_improvement = vs_control['difference']\n",
    "                    best_variant = variant_name\n",
    "                \n",
    "                recommendations.append({\n",
    "                    'variant': variant_name,\n",
    "                    'action': 'PROMOTE',\n",
    "                    'reason': f'Statistically significant improvement of {vs_control[\"difference\"]:.4f}',\n",
    "                    'confidence': f'p-value: {vs_control[\"p_value\"]:.4f}'\n",
    "                })\n",
    "            \n",
    "            elif vs_control['significant'] and vs_control['difference'] < -min_effect_size:\n",
    "                recommendations.append({\n",
    "                    'variant': variant_name,\n",
    "                    'action': 'REJECT',\n",
    "                    'reason': f'Statistically significant degradation of {vs_control[\"difference\"]:.4f}',\n",
    "                    'confidence': f'p-value: {vs_control[\"p_value\"]:.4f}'\n",
    "                })\n",
    "            \n",
    "            else:\n",
    "                recommendations.append({\n",
    "                    'variant': variant_name,\n",
    "                    'action': 'INCONCLUSIVE',\n",
    "                    'reason': 'No significant difference detected or effect size too small',\n",
    "                    'confidence': f'p-value: {vs_control[\"p_value\"]:.4f}'\n",
    "                })\n",
    "        \n",
    "        decision = {\n",
    "            'best_variant': best_variant or analysis_results['control_variant'],\n",
    "            'recommendations': recommendations,\n",
    "            'overall_recommendation': 'PROMOTE' if best_variant else 'KEEP_CONTROL'\n",
    "        }\n",
    "        \n",
    "        return decision\n",
    "    \n",
    "    def stop_test(self):\n",
    "        \"\"\"Stop the A/B test\"\"\"\n",
    "        self.test_active = False\n",
    "        \n",
    "        # Save final results\n",
    "        final_results = {\n",
    "            'test_name': self.test_name,\n",
    "            'variants': {name: {'is_control': info['is_control']} for name, info in self.variants.items()},\n",
    "            'traffic_allocation': self.traffic_allocation,\n",
    "            'start_time': self.start_time,\n",
    "            'end_time': time.time(),\n",
    "            'duration_hours': (time.time() - self.start_time) / 3600,\n",
    "            'final_metrics': self.metrics\n",
    "        }\n",
    "        \n",
    "        with open(self.results_dir / 'final_results.json', 'w') as f:\n",
    "            # Convert numpy types for JSON serialization\n",
    "            def convert_numpy(obj):\n",
    "                if isinstance(obj, np.integer):\n",
    "                    return int(obj)\n",
    "                elif isinstance(obj, np.floating):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                return obj\n",
    "            \n",
    "            # Deep convert the nested structure\n",
    "            def deep_convert(obj):\n",
    "                if isinstance(obj, dict):\n",
    "                    return {k: deep_convert(v) for k, v in obj.items()}\n",
    "                elif isinstance(obj, list):\n",
    "                    return [deep_convert(v) for v in obj]\n",
    "                else:\n",
    "                    return convert_numpy(obj)\n",
    "            \n",
    "            json.dump(deep_convert(final_results), f, indent=2)\n",
    "        \n",
    "        print(f\"A/B test '{self.test_name}' stopped and results saved\")\n",
    "\n",
    "# Demonstrate A/B testing\n",
    "try:\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Create A/B test\n",
    "    ab_test = ABTestingFramework(\"model_improvement_test\")\n",
    "    \n",
    "    # Create two model variants\n",
    "    control_model = SimpleClassifier(20, 64, 3)\n",
    "    treatment_model = SimpleClassifier(20, 96, 3)  # Larger model\n",
    "    \n",
    "    # Add variants to test\n",
    "    ab_test.add_variant('control', control_model, traffic_percentage=50, is_control=True)\n",
    "    ab_test.add_variant('treatment', treatment_model, traffic_percentage=50, is_control=False)\n",
    "    \n",
    "    # Start test\n",
    "    ab_test.start_test(duration_hours=0.1)  # Short test for demo\n",
    "    \n",
    "    # Simulate user interactions\n",
    "    print(\"\\nSimulating user interactions...\")\n",
    "    \n",
    "    # Generate realistic performance differences\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(200):  # Simulate 200 users\n",
    "        user_id = f\"user_{i}\"\n",
    "        variant = ab_test.assign_variant(user_id)\n",
    "        \n",
    "        # Simulate different performance for variants\n",
    "        if variant == 'control':\n",
    "            # Control has baseline performance\n",
    "            accuracy = np.random.normal(0.85, 0.05)\n",
    "            latency = np.random.normal(50, 10)  # milliseconds\n",
    "        else:\n",
    "            # Treatment has slightly better accuracy but higher latency\n",
    "            accuracy = np.random.normal(0.88, 0.05)  # 3% improvement\n",
    "            latency = np.random.normal(65, 12)  # 15ms higher latency\n",
    "        \n",
    "        # Ensure valid ranges\n",
    "        accuracy = np.clip(accuracy, 0, 1)\n",
    "        latency = max(latency, 10)\n",
    "        \n",
    "        ab_test.log_result(user_id, variant, {\n",
    "            'accuracy': accuracy,\n",
    "            'latency_ms': latency\n",
    "        })\n",
    "    \n",
    "    # Check test status\n",
    "    status = ab_test.get_test_status()\n",
    "    print(f\"\\nTest Status: {status['status']}\")\n",
    "    print(f\"Sample sizes: {status['sample_sizes']}\")\n",
    "    print(f\"Ready for analysis: {status['ready_for_analysis']}\")\n",
    "    \n",
    "    if status['ready_for_analysis']:\n",
    "        # Run statistical analysis\n",
    "        print(\"\\nRunning statistical analysis...\")\n",
    "        analysis = ab_test.run_statistical_analysis('accuracy')\n",
    "        \n",
    "        print(f\"\\nAccuracy Analysis Results:\")\n",
    "        print(f\"Control ({analysis['control_variant']}): {analysis['control_stats']['mean']:.4f} ± {analysis['control_stats']['std']:.4f}\")\n",
    "        \n",
    "        for variant_name, comparison in analysis['comparisons'].items():\n",
    "            vs_control = comparison['vs_control']\n",
    "            print(f\"{variant_name}: {comparison['mean']:.4f} ± {comparison['std']:.4f}\")\n",
    "            print(f\"  Difference: {vs_control['difference']:.4f} (p-value: {vs_control['p_value']:.4f})\")\n",
    "            print(f\"  Significant: {vs_control['significant']} (effect size: {vs_control['effect_size']:.3f})\")\n",
    "            print(f\"  95% CI: [{vs_control['confidence_interval'][0]:.4f}, {vs_control['confidence_interval'][1]:.4f}]\")\n",
    "        \n",
    "        # Make decision\n",
    "        decision = ab_test.make_decision(analysis, min_effect_size=0.1)\n",
    "        \n",
    "        print(f\"\\nDecision: {decision['overall_recommendation']}\")\n",
    "        print(f\"Best variant: {decision['best_variant']}\")\n",
    "        \n",
    "        for rec in decision['recommendations']:\n",
    "            print(f\"\\n{rec['variant']}: {rec['action']}\")\n",
    "            print(f\"  Reason: {rec['reason']}\")\n",
    "            print(f\"  Confidence: {rec['confidence']}\")\n",
    "        \n",
    "        # Analyze latency as well\n",
    "        latency_analysis = ab_test.run_statistical_analysis('latency_ms')\n",
    "        print(f\"\\nLatency Analysis Results:\")\n",
    "        print(f\"Control: {latency_analysis['control_stats']['mean']:.1f}ms ± {latency_analysis['control_stats']['std']:.1f}ms\")\n",
    "        \n",
    "        for variant_name, comparison in latency_analysis['comparisons'].items():\n",
    "            vs_control = comparison['vs_control']\n",
    "            print(f\"{variant_name}: {comparison['mean']:.1f}ms (difference: {vs_control['difference']:.1f}ms, p-value: {vs_control['p_value']:.4f})\")\n",
    "    \n",
    "    # Stop test\n",
    "    ab_test.stop_test()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"scipy not available. Install with: pip install scipy\")\n",
    "    print(\"Showing A/B testing framework structure instead:\")\n",
    "    \n",
    "    # Show framework without statistical analysis\n",
    "    ab_test = ABTestingFramework(\"demo_test\")\n",
    "    control_model = SimpleClassifier(20, 64, 3)\n",
    "    ab_test.add_variant('control', control_model, traffic_percentage=100, is_control=True)\n",
    "    \n",
    "    print(\"\\nA/B Testing Framework Features:\")\n",
    "    print(\"  • Traffic allocation and variant assignment\")\n",
    "    print(\"  • Statistical significance testing\")\n",
    "    print(\"  • Effect size calculation\")\n",
    "    print(\"  • Automated decision making\")\n",
    "    print(\"  • Experiment tracking and logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Deployment Strategies\n",
    "\n",
    "Modern ML deployment requires sophisticated strategies for scaling, reliability, and performance. This includes edge deployment, model serving optimization, and distributed inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Serving and Deployment Strategies\n",
    "class ModelServer:\n",
    "    def __init__(self, model_name: str, model_version: str):\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.model = None\n",
    "        self.model_config = {}\n",
    "        self.deployment_config = {}\n",
    "        self.performance_stats = {\n",
    "            'requests_served': 0,\n",
    "            'total_inference_time': 0,\n",
    "            'errors': 0,\n",
    "            'start_time': time.time()\n",
    "        }\n",
    "    \n",
    "    def load_model(self, model: nn.Module, config: Dict[str, Any] = None):\n",
    "        \"\"\"Load model for serving\"\"\"\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model_config = config or {}\n",
    "        \n",
    "        # Warm up model\n",
    "        self._warmup_model()\n",
    "        print(f\"Model {self.model_name} v{self.model_version} loaded and warmed up\")\n",
    "    \n",
    "    def _warmup_model(self, warmup_batches: int = 10):\n",
    "        \"\"\"Warm up model with dummy requests\"\"\"\n",
    "        if self.model is None:\n",
    "            return\n",
    "        \n",
    "        # Determine input shape from model config or use default\n",
    "        input_shape = self.model_config.get('input_shape', [1, 20])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(warmup_batches):\n",
    "                dummy_input = torch.randn(*input_shape)\n",
    "                _ = self.model(dummy_input)\n",
    "    \n",
    "    def predict(self, input_data: torch.Tensor, return_probabilities: bool = True):\n",
    "        \"\"\"Make prediction with performance tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if self.model is None:\n",
    "                raise RuntimeError(\"Model not loaded\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Ensure input is in correct format\n",
    "                if input_data.dim() == 1:\n",
    "                    input_data = input_data.unsqueeze(0)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = self.model(input_data)\n",
    "                \n",
    "                # Get predictions\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                result = {\n",
    "                    'predictions': predictions.tolist(),\n",
    "                    'input_shape': list(input_data.shape),\n",
    "                    'batch_size': input_data.size(0)\n",
    "                }\n",
    "                \n",
    "                if return_probabilities:\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    result['probabilities'] = probabilities.tolist()\n",
    "                    result['confidence_scores'] = torch.max(probabilities, dim=1)[0].tolist()\n",
    "                \n",
    "                # Update performance stats\n",
    "                inference_time = time.time() - start_time\n",
    "                self.performance_stats['requests_served'] += 1\n",
    "                self.performance_stats['total_inference_time'] += inference_time\n",
    "                \n",
    "                result['inference_time_ms'] = inference_time * 1000\n",
    "                \n",
    "                return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.performance_stats['errors'] += 1\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'inference_time_ms': (time.time() - start_time) * 1000\n",
    "            }\n",
    "    \n",
    "    def batch_predict(self, input_batch: List[torch.Tensor], max_batch_size: int = 32):\n",
    "        \"\"\"Handle batch predictions with automatic batching\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Process in chunks\n",
    "        for i in range(0, len(input_batch), max_batch_size):\n",
    "            batch_chunk = input_batch[i:i + max_batch_size]\n",
    "            \n",
    "            # Stack tensors\n",
    "            stacked_input = torch.stack(batch_chunk)\n",
    "            \n",
    "            # Predict\n",
    "            batch_result = self.predict(stacked_input)\n",
    "            \n",
    "            if 'error' not in batch_result:\n",
    "                # Split results back\n",
    "                for j in range(len(batch_chunk)):\n",
    "                    individual_result = {\n",
    "                        'prediction': batch_result['predictions'][j],\n",
    "                        'inference_time_ms': batch_result['inference_time_ms'] / len(batch_chunk)\n",
    "                    }\n",
    "                    \n",
    "                    if 'probabilities' in batch_result:\n",
    "                        individual_result['probabilities'] = batch_result['probabilities'][j]\n",
    "                        individual_result['confidence_score'] = batch_result['confidence_scores'][j]\n",
    "                    \n",
    "                    results.append(individual_result)\n",
    "            else:\n",
    "                # Handle error for entire chunk\n",
    "                for _ in batch_chunk:\n",
    "                    results.append(batch_result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_performance_stats(self):\n",
    "        \"\"\"Get server performance statistics\"\"\"\n",
    "        uptime = time.time() - self.performance_stats['start_time']\n",
    "        requests_served = self.performance_stats['requests_served']\n",
    "        \n",
    "        stats = {\n",
    "            'model_name': self.model_name,\n",
    "            'model_version': self.model_version,\n",
    "            'uptime_seconds': uptime,\n",
    "            'requests_served': requests_served,\n",
    "            'errors': self.performance_stats['errors'],\n",
    "            'error_rate': self.performance_stats['errors'] / max(requests_served, 1),\n",
    "            'requests_per_second': requests_served / max(uptime, 1)\n",
    "        }\n",
    "        \n",
    "        if requests_served > 0:\n",
    "            stats['average_inference_time_ms'] = (self.performance_stats['total_inference_time'] / requests_served) * 1000\n",
    "        else:\n",
    "            stats['average_inference_time_ms'] = 0\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def health_check(self):\n",
    "        \"\"\"Health check for load balancer\"\"\"\n",
    "        try:\n",
    "            # Quick inference test\n",
    "            test_input = torch.randn(1, self.model_config.get('input_features', 20))\n",
    "            result = self.predict(test_input)\n",
    "            \n",
    "            if 'error' in result:\n",
    "                return {'status': 'unhealthy', 'reason': result['error']}\n",
    "            \n",
    "            stats = self.get_performance_stats()\n",
    "            \n",
    "            # Check various health indicators\n",
    "            if stats['error_rate'] > 0.1:  # More than 10% error rate\n",
    "                return {'status': 'unhealthy', 'reason': 'High error rate'}\n",
    "            \n",
    "            if result['inference_time_ms'] > 1000:  # Slower than 1 second\n",
    "                return {'status': 'degraded', 'reason': 'Slow inference'}\n",
    "            \n",
    "            return {'status': 'healthy', 'stats': stats}\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {'status': 'unhealthy', 'reason': f'Health check failed: {str(e)}'}\n",
    "\n",
    "# Load Balancer for multiple model servers\n",
    "class ModelLoadBalancer:\n",
    "    def __init__(self, balancing_strategy: str = 'round_robin'):\n",
    "        self.servers = []\n",
    "        self.balancing_strategy = balancing_strategy\n",
    "        self.current_index = 0\n",
    "        self.server_weights = {}\n",
    "    \n",
    "    def add_server(self, server: ModelServer, weight: float = 1.0):\n",
    "        \"\"\"Add a server to the load balancer\"\"\"\n",
    "        self.servers.append(server)\n",
    "        self.server_weights[id(server)] = weight\n",
    "        print(f\"Added server {server.model_name} v{server.model_version} with weight {weight}\")\n",
    "    \n",
    "    def remove_server(self, server: ModelServer):\n",
    "        \"\"\"Remove a server from the load balancer\"\"\"\n",
    "        if server in self.servers:\n",
    "            self.servers.remove(server)\n",
    "            del self.server_weights[id(server)]\n",
    "            print(f\"Removed server {server.model_name} v{server.model_version}\")\n",
    "    \n",
    "    def get_next_server(self):\n",
    "        \"\"\"Get next server based on balancing strategy\"\"\"\n",
    "        healthy_servers = [s for s in self.servers if self._is_server_healthy(s)]\n",
    "        \n",
    "        if not healthy_servers:\n",
    "            raise RuntimeError(\"No healthy servers available\")\n",
    "        \n",
    "        if self.balancing_strategy == 'round_robin':\n",
    "            server = healthy_servers[self.current_index % len(healthy_servers)]\n",
    "            self.current_index += 1\n",
    "            return server\n",
    "        \n",
    "        elif self.balancing_strategy == 'least_loaded':\n",
    "            # Choose server with lowest request rate\n",
    "            server_loads = [(s.get_performance_stats()['requests_per_second'], s) for s in healthy_servers]\n",
    "            return min(server_loads, key=lambda x: x[0])[1]\n",
    "        \n",
    "        elif self.balancing_strategy == 'weighted':\n",
    "            # Weighted random selection\n",
    "            weights = [self.server_weights[id(s)] for s in healthy_servers]\n",
    "            total_weight = sum(weights)\n",
    "            r = np.random.random() * total_weight\n",
    "            \n",
    "            cumsum = 0\n",
    "            for server, weight in zip(healthy_servers, weights):\n",
    "                cumsum += weight\n",
    "                if r <= cumsum:\n",
    "                    return server\n",
    "            \n",
    "            return healthy_servers[-1]  # Fallback\n",
    "        \n",
    "        else:\n",
    "            return healthy_servers[0]  # Default fallback\n",
    "    \n",
    "    def _is_server_healthy(self, server: ModelServer):\n",
    "        \"\"\"Check if server is healthy\"\"\"\n",
    "        health = server.health_check()\n",
    "        return health['status'] in ['healthy', 'degraded']\n",
    "    \n",
    "    def predict(self, input_data: torch.Tensor):\n",
    "        \"\"\"Route prediction request to appropriate server\"\"\"\n",
    "        try:\n",
    "            server = self.get_next_server()\n",
    "            result = server.predict(input_data)\n",
    "            result['served_by'] = f\"{server.model_name}_v{server.model_version}\"\n",
    "            return result\n",
    "        except RuntimeError as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def get_cluster_status(self):\n",
    "        \"\"\"Get status of all servers in the cluster\"\"\"\n",
    "        cluster_status = {\n",
    "            'total_servers': len(self.servers),\n",
    "            'balancing_strategy': self.balancing_strategy,\n",
    "            'servers': []\n",
    "        }\n",
    "        \n",
    "        healthy_count = 0\n",
    "        total_requests = 0\n",
    "        total_errors = 0\n",
    "        \n",
    "        for server in self.servers:\n",
    "            health = server.health_check()\n",
    "            stats = server.get_performance_stats()\n",
    "            \n",
    "            server_info = {\n",
    "                'model_name': server.model_name,\n",
    "                'model_version': server.model_version,\n",
    "                'health_status': health['status'],\n",
    "                'weight': self.server_weights[id(server)],\n",
    "                'stats': stats\n",
    "            }\n",
    "            \n",
    "            cluster_status['servers'].append(server_info)\n",
    "            \n",
    "            if health['status'] in ['healthy', 'degraded']:\n",
    "                healthy_count += 1\n",
    "            \n",
    "            total_requests += stats['requests_served']\n",
    "            total_errors += stats['errors']\n",
    "        \n",
    "        cluster_status['healthy_servers'] = healthy_count\n",
    "        cluster_status['total_requests'] = total_requests\n",
    "        cluster_status['total_errors'] = total_errors\n",
    "        cluster_status['cluster_error_rate'] = total_errors / max(total_requests, 1)\n",
    "        \n",
    "        return cluster_status\n",
    "\n",
    "# Demonstrate model serving and load balancing\n",
    "print(\"Setting up model serving cluster...\")\n",
    "\n",
    "# Create multiple model servers\n",
    "server1 = ModelServer(\"classifier\", \"v1.0\")\n",
    "server2 = ModelServer(\"classifier\", \"v1.1\")\n",
    "server3 = ModelServer(\"classifier\", \"v1.2\")\n",
    "\n",
    "# Load models (using different sizes to simulate different performance)\n",
    "model1 = SimpleClassifier(20, 64, 3)\n",
    "model2 = SimpleClassifier(20, 96, 3)\n",
    "model3 = SimpleClassifier(20, 128, 3)\n",
    "\n",
    "server1.load_model(model1, {'input_features': 20, 'input_shape': [1, 20]})\n",
    "server2.load_model(model2, {'input_features': 20, 'input_shape': [1, 20]})\n",
    "server3.load_model(model3, {'input_features': 20, 'input_shape': [1, 20]})\n",
    "\n",
    "# Create load balancer\n",
    "load_balancer = ModelLoadBalancer(balancing_strategy='weighted')\n",
    "load_balancer.add_server(server1, weight=1.0)\n",
    "load_balancer.add_server(server2, weight=2.0)  # Higher weight\n",
    "load_balancer.add_server(server3, weight=1.5)\n",
    "\n",
    "# Simulate load\n",
    "print(\"\\nSimulating production load...\")\n",
    "for i in range(50):\n",
    "    # Generate random input\n",
    "    input_data = torch.randn(20)\n",
    "    \n",
    "    # Route through load balancer\n",
    "    result = load_balancer.predict(input_data)\n",
    "    \n",
    "    if 'error' not in result and i % 10 == 0:\n",
    "        print(f\"Request {i}: Served by {result['served_by']}, \"\n",
    "              f\"Prediction: {result['predictions'][0]}, \"\n",
    "              f\"Confidence: {result['confidence_scores'][0]:.3f}, \"\n",
    "              f\"Latency: {result['inference_time_ms']:.2f}ms\")\n",
    "\n",
    "# Get cluster status\n",
    "cluster_status = load_balancer.get_cluster_status()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTER STATUS REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Servers: {cluster_status['total_servers']}\")\n",
    "print(f\"Healthy Servers: {cluster_status['healthy_servers']}\")\n",
    "print(f\"Balancing Strategy: {cluster_status['balancing_strategy']}\")\n",
    "print(f\"Total Requests: {cluster_status['total_requests']}\")\n",
    "print(f\"Cluster Error Rate: {cluster_status['cluster_error_rate']:.4f}\")\n",
    "\n",
    "print(\"\\nServer Details:\")\n",
    "for server_info in cluster_status['servers']:\n",
    "    stats = server_info['stats']\n",
    "    print(f\"\\n{server_info['model_name']} v{server_info['model_version']}:\")\n",
    "    print(f\"  Health: {server_info['health_status']}\")\n",
    "    print(f\"  Weight: {server_info['weight']}\")\n",
    "    print(f\"  Requests: {stats['requests_served']}\")\n",
    "    print(f\"  RPS: {stats['requests_per_second']:.2f}\")\n",
    "    print(f\"  Avg Latency: {stats['average_inference_time_ms']:.2f}ms\")\n",
    "    print(f\"  Error Rate: {stats['error_rate']:.4f}\")\n",
    "\n",
    "# Demonstrate batch processing\n",
    "print(\"\\nDemonstrating batch processing...\")\n",
    "batch_inputs = [torch.randn(20) for _ in range(25)]  # 25 individual inputs\n",
    "batch_results = server2.batch_predict(batch_inputs, max_batch_size=8)\n",
    "\n",
    "print(f\"Processed batch of {len(batch_inputs)} inputs\")\n",
    "print(f\"Average latency per item: {np.mean([r['inference_time_ms'] for r in batch_results]):.2f}ms\")\n",
    "print(f\"Total batch processing time: {sum(r['inference_time_ms'] for r in batch_results):.2f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEPLOYMENT BEST PRACTICES SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Model versioning and registry\")\n",
    "print(\"✓ Health checks and monitoring\")\n",
    "print(\"✓ Load balancing and auto-scaling\")\n",
    "print(\"✓ Batch processing optimization\")\n",
    "print(\"✓ Performance tracking and alerting\")\n",
    "print(\"✓ Graceful degradation and error handling\")\n",
    "print(\"✓ A/B testing and canary deployments\")\n",
    "print(\"✓ Experiment tracking and reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLOps Best Practices and Production Checklist\n",
    "\n",
    "Successful MLOps requires comprehensive practices covering the entire ML lifecycle. This section provides a complete checklist and best practices for production ML systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MLOps and Production PyTorch: Comprehensive Best Practices Guide\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mlops_checklist = {\n",
    "    \"Data Management\": {\n",
    "        \"Data Quality\": [\n",
    "            \"Data validation and schema enforcement\",\n",
    "            \"Outlier detection and handling\",\n",
    "            \"Missing data imputation strategies\",\n",
    "            \"Data freshness and staleness monitoring\",\n",
    "            \"Data lineage tracking\"\n",
    "        ],\n",
    "        \"Data Pipeline\": [\n",
    "            \"Reproducible data preprocessing\",\n",
    "            \"Feature store implementation\",\n",
    "            \"Data versioning and snapshots\",\n",
    "            \"Pipeline orchestration (Airflow, Kubeflow)\",\n",
    "            \"Real-time and batch processing support\"\n",
    "        ],\n",
    "        \"Data Governance\": [\n",
    "            \"Data privacy and compliance (GDPR, CCPA)\",\n",
    "            \"Access control and audit trails\",\n",
    "            \"Data retention policies\",\n",
    "            \"Sensitive data anonymization\",\n",
    "            \"Cross-region data regulations\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Model Development\": {\n",
    "        \"Experimentation\": [\n",
    "            \"Experiment tracking (MLflow, Weights & Biases)\",\n",
    "            \"Hyperparameter optimization\",\n",
    "            \"Cross-validation strategies\",\n",
    "            \"Reproducible random seeds\",\n",
    "            \"Environment containerization\"\n",
    "        ],\n",
    "        \"Code Quality\": [\n",
    "            \"Version control for all code\",\n",
    "            \"Unit tests for data and model code\",\n",
    "            \"Code review processes\",\n",
    "            \"Linting and formatting standards\",\n",
    "            \"Documentation and comments\"\n",
    "        ],\n",
    "        \"Model Validation\": [\n",
    "            \"Cross-validation and holdout testing\",\n",
    "            \"Statistical significance testing\",\n",
    "            \"Fairness and bias evaluation\",\n",
    "            \"Model interpretability analysis\",\n",
    "            \"Stress testing and edge cases\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Model Registry & Versioning\": {\n",
    "        \"Model Management\": [\n",
    "            \"Centralized model registry\",\n",
    "            \"Semantic versioning (major.minor.patch)\",\n",
    "            \"Model metadata and lineage\",\n",
    "            \"Stage management (dev/staging/prod)\",\n",
    "            \"Model approval workflows\"\n",
    "        ],\n",
    "        \"Artifacts\": [\n",
    "            \"Model weights and architecture\",\n",
    "            \"Training configuration and hyperparameters\",\n",
    "            \"Performance metrics and validation results\",\n",
    "            \"Dependencies and environment specs\",\n",
    "            \"Data preprocessing pipelines\"\n",
    "        ],\n",
    "        \"Lifecycle Management\": [\n",
    "            \"Model promotion criteria\",\n",
    "            \"Rollback procedures\",\n",
    "            \"Deprecation policies\",\n",
    "            \"Model retirement workflows\",\n",
    "            \"Compliance and audit trails\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Deployment & Serving\": {\n",
    "        \"Infrastructure\": [\n",
    "            \"Containerization (Docker, Kubernetes)\",\n",
    "            \"Auto-scaling policies\",\n",
    "            \"Load balancing strategies\",\n",
    "            \"Resource allocation and limits\",\n",
    "            \"Multi-region deployment\"\n",
    "        ],\n",
    "        \"Serving Optimization\": [\n",
    "            \"Model quantization and compression\",\n",
    "            \"Batch prediction optimization\",\n",
    "            \"GPU/CPU resource optimization\",\n",
    "            \"Caching strategies\",\n",
    "            \"Edge deployment considerations\"\n",
    "        ],\n",
    "        \"Deployment Strategies\": [\n",
    "            \"Blue-green deployments\",\n",
    "            \"Canary releases\",\n",
    "            \"A/B testing frameworks\",\n",
    "            \"Feature flags and toggles\",\n",
    "            \"Gradual traffic migration\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Monitoring & Observability\": {\n",
    "        \"Performance Monitoring\": [\n",
    "            \"Latency and throughput metrics\",\n",
    "            \"Error rates and success rates\",\n",
    "            \"Resource utilization (CPU, GPU, memory)\",\n",
    "            \"Queue depths and processing times\",\n",
    "            \"SLA compliance tracking\"\n",
    "        ],\n",
    "        \"Model Health\": [\n",
    "            \"Data drift detection\",\n",
    "            \"Model drift monitoring\",\n",
    "            \"Prediction quality metrics\",\n",
    "            \"Feature importance changes\",\n",
    "            \"Concept drift identification\"\n",
    "        ],\n",
    "        \"Alerting & Response\": [\n",
    "            \"Real-time alerting systems\",\n",
    "            \"Escalation procedures\",\n",
    "            \"Automated response actions\",\n",
    "            \"Dashboard and visualization\",\n",
    "            \"Incident response playbooks\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Security & Compliance\": {\n",
    "        \"Model Security\": [\n",
    "            \"Adversarial attack protection\",\n",
    "            \"Input validation and sanitization\",\n",
    "            \"Model poisoning prevention\",\n",
    "            \"Secure model storage\",\n",
    "            \"API authentication and authorization\"\n",
    "        ],\n",
    "        \"Data Protection\": [\n",
    "            \"Encryption at rest and in transit\",\n",
    "            \"PII detection and masking\",\n",
    "            \"Data access logging\",\n",
    "            \"Secure data transmission\",\n",
    "            \"Right to be forgotten compliance\"\n",
    "        ],\n",
    "        \"Regulatory Compliance\": [\n",
    "            \"Model explainability requirements\",\n",
    "            \"Audit trail maintenance\",\n",
    "            \"Regulatory approval processes\",\n",
    "            \"Documentation standards\",\n",
    "            \"Risk assessment procedures\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Testing & Validation\": {\n",
    "        \"Testing Strategies\": [\n",
    "            \"Unit tests for data processing\",\n",
    "            \"Integration tests for pipelines\",\n",
    "            \"Model validation tests\",\n",
    "            \"Load testing and stress testing\",\n",
    "            \"End-to-end system tests\"\n",
    "        ],\n",
    "        \"Continuous Testing\": [\n",
    "            \"Automated testing in CI/CD\",\n",
    "            \"Regression testing for model changes\",\n",
    "            \"Data quality tests\",\n",
    "            \"Performance benchmarking\",\n",
    "            \"Shadow testing in production\"\n",
    "        ],\n",
    "        \"Validation Frameworks\": [\n",
    "            \"Great Expectations for data validation\",\n",
    "            \"TensorFlow Data Validation (TFDV)\",\n",
    "            \"Custom validation rules\",\n",
    "            \"Statistical testing frameworks\",\n",
    "            \"Model comparison utilities\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print the comprehensive checklist\n",
    "for category, subcategories in mlops_checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * (len(category) + 1))\n",
    "    \n",
    "    for subcategory, items in subcategories.items():\n",
    "        print(f\"\\n  {subcategory}:\")\n",
    "        for item in items:\n",
    "            print(f\"    ☐ {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PYTORCH-SPECIFIC PRODUCTION CONSIDERATIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pytorch_specifics = {\n",
    "    \"Model Optimization\": [\n",
    "        \"TorchScript for deployment (torch.jit.script/trace)\",\n",
    "        \"Quantization with torch.quantization\",\n",
    "        \"ONNX export for cross-platform deployment\",\n",
    "        \"TensorRT integration for NVIDIA GPUs\",\n",
    "        \"Mobile deployment with PyTorch Mobile\"\n",
    "    ],\n",
    "    \n",
    "    \"Performance Tuning\": [\n",
    "        \"DataLoader optimization (num_workers, pin_memory)\",\n",
    "        \"Mixed precision training/inference (torch.amp)\",\n",
    "        \"Gradient checkpointing for memory efficiency\",\n",
    "        \"Model parallelism for large models\",\n",
    "        \"Efficient data preprocessing pipelines\"\n",
    "    ],\n",
    "    \n",
    "    \"Deployment Formats\": [\n",
    "        \"PyTorch Lightning for structured training\",\n",
    "        \"TorchServe for model serving\",\n",
    "        \"Ray Serve for distributed serving\",\n",
    "        \"Triton Inference Server integration\",\n",
    "        \"Custom Flask/FastAPI serving endpoints\"\n",
    "    ],\n",
    "    \n",
    "    \"Monitoring Tools\": [\n",
    "        \"PyTorch Profiler for performance analysis\",\n",
    "        \"TensorBoard for metrics visualization\",\n",
    "        \"Weights & Biases for experiment tracking\",\n",
    "        \"MLflow for model lifecycle management\",\n",
    "        \"Custom logging with Python logging module\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in pytorch_specifics.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  • {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PRODUCTION READINESS ASSESSMENT:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "readiness_criteria = {\n",
    "    \"Critical (Must Have)\": [\n",
    "        \"Model performance meets business requirements\",\n",
    "        \"Comprehensive testing coverage\",\n",
    "        \"Monitoring and alerting in place\",\n",
    "        \"Rollback procedures tested\",\n",
    "        \"Security measures implemented\",\n",
    "        \"Data quality validation active\",\n",
    "        \"Disaster recovery plan exists\"\n",
    "    ],\n",
    "    \n",
    "    \"Important (Should Have)\": [\n",
    "        \"A/B testing capability\",\n",
    "        \"Automated retraining pipeline\",\n",
    "        \"Model interpretability tools\",\n",
    "        \"Performance optimization applied\",\n",
    "        \"Documentation complete\",\n",
    "        \"Team training on operations\",\n",
    "        \"Compliance requirements met\"\n",
    "    ],\n",
    "    \n",
    "    \"Nice to Have (Could Have)\": [\n",
    "        \"Advanced drift detection algorithms\",\n",
    "        \"Automated model selection\",\n",
    "        \"Multi-region deployment\",\n",
    "        \"Real-time feature engineering\",\n",
    "        \"Advanced model compression\",\n",
    "        \"Custom hardware optimization\",\n",
    "        \"MLOps platform integration\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for priority, criteria in readiness_criteria.items():\n",
    "    print(f\"\\n{priority}:\")\n",
    "    for criterion in criteria:\n",
    "        print(f\"  ☐ {criterion}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDED TOOLS AND FRAMEWORKS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "recommended_tools = {\n",
    "    \"Experiment Tracking\": [\n",
    "        \"MLflow - Open-source ML lifecycle management\",\n",
    "        \"Weights & Biases - Comprehensive experiment tracking\",\n",
    "        \"Neptune - Collaborative ML experiment management\",\n",
    "        \"Comet - ML experiment tracking and monitoring\"\n",
    "    ],\n",
    "    \n",
    "    \"Model Serving\": [\n",
    "        \"TorchServe - PyTorch native model serving\",\n",
    "        \"BentoML - ML model serving framework\",\n",
    "        \"Seldon Core - Kubernetes-native ML deployment\",\n",
    "        \"KFServing - Kubernetes-based model serving\"\n",
    "    ],\n",
    "    \n",
    "    \"Pipeline Orchestration\": [\n",
    "        \"Apache Airflow - Workflow orchestration\",\n",
    "        \"Kubeflow - ML workflows on Kubernetes\",\n",
    "        \"Prefect - Modern workflow orchestration\",\n",
    "        \"DVC - Data Version Control for ML pipelines\"\n",
    "    ],\n",
    "    \n",
    "    \"Monitoring & Observability\": [\n",
    "        \"Prometheus + Grafana - Metrics and dashboards\",\n",
    "        \"ELK Stack - Logging and search\",\n",
    "        \"Jaeger - Distributed tracing\",\n",
    "        \"Evidently AI - ML model monitoring\"\n",
    "    ],\n",
    "    \n",
    "    \"Infrastructure\": [\n",
    "        \"Docker + Kubernetes - Containerization and orchestration\",\n",
    "        \"AWS SageMaker - Managed ML platform\",\n",
    "        \"Google Vertex AI - ML platform\",\n",
    "        \"Azure ML - Microsoft's ML platform\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, tools in recommended_tools.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for tool in tools:\n",
    "        print(f\"  • {tool}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RECOMMENDATIONS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"1. Start Simple, Scale Gradually:\")\n",
    "print(\"   • Begin with basic monitoring and logging\")\n",
    "print(\"   • Add complexity as you understand your needs\")\n",
    "print(\"   • Focus on reliability over features initially\")\n",
    "print(\"\")\n",
    "print(\"2. Automate Everything:\")\n",
    "print(\"   • Automate testing, deployment, and monitoring\")\n",
    "print(\"   • Use CI/CD pipelines for consistency\")\n",
    "print(\"   • Implement infrastructure as code\")\n",
    "print(\"\")\n",
    "print(\"3. Monitor Proactively:\")\n",
    "print(\"   • Set up comprehensive monitoring from day one\")\n",
    "print(\"   • Define clear SLAs and alert thresholds\")\n",
    "print(\"   • Monitor both technical and business metrics\")\n",
    "print(\"\")\n",
    "print(\"4. Plan for Failure:\")\n",
    "print(\"   • Design for graceful degradation\")\n",
    "print(\"   • Test rollback procedures regularly\")\n",
    "print(\"   • Have incident response procedures ready\")\n",
    "print(\"\")\n",
    "print(\"5. Maintain Documentation:\")\n",
    "print(\"   • Document architecture decisions\")\n",
    "print(\"   • Keep runbooks up to date\")\n",
    "print(\"   • Train team members on operations\")\n",
    "print(\"\")\n",
    "print(\"Remember: MLOps is a journey, not a destination. Start with the basics\")\n",
    "print(\"and continuously improve your practices as your system matures!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}