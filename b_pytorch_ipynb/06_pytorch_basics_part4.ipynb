{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics Part 4: Convolutional Neural Networks and Computer Vision\n",
    "\n",
    "CNNs, image processing, and computer vision fundamentals with PyTorch\n",
    "\n",
    "## Mathematical Foundation of Convolutional Neural Networks\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)** are specialized neural networks designed for processing grid-like data, particularly images. They leverage mathematical operations that preserve spatial relationships and reduce parameter count through weight sharing.\n",
    "\n",
    "### Core Mathematical Concepts\n",
    "\n",
    "**1. Convolution Operation:**\n",
    "For 2D discrete convolution between input $I$ and kernel $K$:\n",
    "$$S(i,j) = (I * K)(i,j) = \\sum_{m} \\sum_{n} I(m,n) \\cdot K(i-m, j-n)$$\n",
    "\n",
    "**2. Cross-Correlation (CNN Implementation):**\n",
    "$$S(i,j) = \\sum_{m} \\sum_{n} I(i+m, j+n) \\cdot K(m,n)$$\n",
    "\n",
    "**3. Multi-channel Convolution:**\n",
    "For input $\\mathbf{X} \\in \\mathbb{R}^{C \\times H \\times W}$ and filters $\\mathbf{W} \\in \\mathbb{R}^{F \\times C \\times K_H \\times K_W}$:\n",
    "$$Y_{f,i,j} = \\sum_{c=1}^{C} \\sum_{u=0}^{K_H-1} \\sum_{v=0}^{K_W-1} X_{c,i+u,j+v} \\cdot W_{f,c,u,v} + b_f$$\n",
    "\n",
    "**4. Spatial Dimensions:**\n",
    "Output size: $H_{out} = \\lfloor \\frac{H_{in} + 2P - K_H}{S} \\rfloor + 1$, $W_{out} = \\lfloor \\frac{W_{in} + 2P - K_W}{S} \\rfloor + 1$\n",
    "\n",
    "Where $P$ = padding, $S$ = stride, $K$ = kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Convolutions\n",
    "\n",
    "### Mathematical Intuition of Convolution Kernels\n",
    "\n",
    "**Convolution kernels** are learned feature detectors that respond to specific patterns in the input:\n",
    "\n",
    "**Feature Detection Mathematics:**\n",
    "Each kernel $\\mathbf{K} \\in \\mathbb{R}^{K_H \\times K_W}$ computes:\n",
    "$$\\text{response}_{i,j} = \\sum_{u,v} I_{i+u,j+v} \\cdot K_{u,v}$$\n",
    "\n",
    "**Common Kernel Types:**\n",
    "- **Edge Detection**: $\\begin{bmatrix} -1 & -1 & -1 \\\\ -1 & 8 & -1 \\\\ -1 & -1 & -1 \\end{bmatrix}$ (high response to edges)\n",
    "- **Blur**: $\\frac{1}{9}\\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ (local averaging)\n",
    "- **Sharpen**: $\\begin{bmatrix} 0 & -1 & 0 \\\\ -1 & 5 & -1 \\\\ 0 & -1 & 0 \\end{bmatrix}$ (emphasizes differences)\n",
    "\n",
    "**Translation Equivariance:**\n",
    "If $f$ is the convolution operation: $f(T_{\\mathbf{v}}(\\mathbf{x})) = T_{\\mathbf{v}}(f(\\mathbf{x}))$\n",
    "where $T_{\\mathbf{v}}$ is translation by vector $\\mathbf{v}$.\n",
    "\n",
    "This means convolution preserves spatial relationships - a pattern detected at one location will be detected if it appears elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D convolution\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "\n",
    "# Create a sample image (1 channel, 5x5)\n",
    "image = torch.tensor([[[[1, 0, 1, 0, 1],\n",
    "                        [0, 1, 0, 1, 0],\n",
    "                        [1, 0, 1, 0, 1],\n",
    "                        [0, 1, 0, 1, 0],\n",
    "                        [1, 0, 1, 0, 1]]]], dtype=torch.float32)\n",
    "\n",
    "print(f\"Input shape: {image.shape}\")\n",
    "print(f\"Input image:\\n{image.squeeze()}\")\n",
    "\n",
    "# Apply convolution\n",
    "output = conv_layer(image)\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Conv weight shape: {conv_layer.weight.shape}\")\n",
    "print(f\"Conv bias shape: {conv_layer.bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different kernel effects\n",
    "def apply_kernel(image, kernel):\n",
    "    \"\"\"Apply a custom kernel to an image\"\"\"\n",
    "    conv = nn.Conv2d(1, 1, kernel_size=kernel.shape[0], padding=1, bias=False)\n",
    "    with torch.no_grad():\n",
    "        conv.weight[0, 0] = kernel\n",
    "    return conv(image)\n",
    "\n",
    "# Create a larger test image\n",
    "test_image = torch.zeros(1, 1, 10, 10)\n",
    "test_image[0, 0, 3:7, 3:7] = 1  # White square in the middle\n",
    "\n",
    "# Define different kernels\n",
    "edge_kernel = torch.tensor([[-1, -1, -1],\n",
    "                           [-1,  8, -1],\n",
    "                           [-1, -1, -1]], dtype=torch.float32)\n",
    "\n",
    "blur_kernel = torch.ones(3, 3) / 9\n",
    "\n",
    "sharpen_kernel = torch.tensor([[0, -1, 0],\n",
    "                              [-1, 5, -1],\n",
    "                              [0, -1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Apply kernels\n",
    "edge_result = apply_kernel(test_image, edge_kernel)\n",
    "blur_result = apply_kernel(test_image, blur_kernel)\n",
    "sharpen_result = apply_kernel(test_image, sharpen_kernel)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes[0, 0].imshow(test_image.squeeze(), cmap='gray')\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 1].imshow(edge_result.squeeze(), cmap='gray')\n",
    "axes[0, 1].set_title('Edge Detection')\n",
    "axes[1, 0].imshow(blur_result.squeeze(), cmap='gray')\n",
    "axes[1, 0].set_title('Blur')\n",
    "axes[1, 1].imshow(sharpen_result.squeeze(), cmap='gray')\n",
    "axes[1, 1].set_title('Sharpen')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Layers and Operations\n",
    "\n",
    "### Mathematical Framework of CNN Components\n",
    "\n",
    "**1. Convolution Layer Parameters:**\n",
    "For `Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=K, stride=S, padding=P)`:\n",
    "\n",
    "- **Weight tensor**: $\\mathbf{W} \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times K \\times K}$\n",
    "- **Bias vector**: $\\mathbf{b} \\in \\mathbb{R}^{C_{out}}$\n",
    "- **Parameter count**: $C_{out} \\times C_{in} \\times K^2 + C_{out}$\n",
    "\n",
    "**2. Stride Effect:**\n",
    "Stride $S$ subsamples the output by factor $S$:\n",
    "$$Y_{i,j} = \\sum_{c,u,v} X_{c, S \\cdot i + u, S \\cdot j + v} \\cdot W_{c,u,v}$$\n",
    "\n",
    "**3. Padding Mathematics:**\n",
    "Zero-padding extends input dimensions:\n",
    "- **'SAME' padding**: $P = \\lfloor K/2 \\rfloor$ (output size = input size when $S=1$)\n",
    "- **'VALID' padding**: $P = 0$ (no padding)\n",
    "\n",
    "**4. Receptive Field:**\n",
    "The receptive field grows with depth:\n",
    "$$RF_{\\ell} = RF_{\\ell-1} + (K_{\\ell} - 1) \\prod_{i=1}^{\\ell-1} S_i$$\n",
    "\n",
    "Starting with $RF_0 = 1$ and accumulating kernel sizes weighted by stride products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different convolution parameters\n",
    "input_tensor = torch.randn(1, 3, 32, 32)  # Batch=1, Channels=3, Height=32, Width=32\n",
    "\n",
    "# Different kernel sizes\n",
    "conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "conv2 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
    "conv3 = nn.Conv2d(3, 16, kernel_size=7, padding=3)\n",
    "\n",
    "out1 = conv1(input_tensor)\n",
    "out2 = conv2(input_tensor)\n",
    "out3 = conv3(input_tensor)\n",
    "\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Conv 3x3 output: {out1.shape}\")\n",
    "print(f\"Conv 5x5 output: {out2.shape}\")\n",
    "print(f\"Conv 7x7 output: {out3.shape}\")\n",
    "\n",
    "# Stride and padding effects\n",
    "conv_stride = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)\n",
    "out_stride = conv_stride(input_tensor)\n",
    "print(f\"Conv with stride=2: {out_stride.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Operations - Mathematical Foundation\n",
    "\n",
    "**Pooling** provides spatial downsampling and translation invariance:\n",
    "\n",
    "**1. Max Pooling:**\n",
    "$$P_{\\text{max}}(i,j) = \\max_{(u,v) \\in \\mathcal{R}_{i,j}} X(u,v)$$\n",
    "\n",
    "Where $\\mathcal{R}_{i,j}$ is the pooling window at position $(i,j)$.\n",
    "\n",
    "**2. Average Pooling:**\n",
    "$$P_{\\text{avg}}(i,j) = \\frac{1}{|\\mathcal{R}_{i,j}|} \\sum_{(u,v) \\in \\mathcal{R}_{i,j}} X(u,v)$$\n",
    "\n",
    "**3. Adaptive Pooling:**\n",
    "Forces output to specific size regardless of input dimensions:\n",
    "$$\\text{AdaptivePool}: \\mathbb{R}^{C \\times H_{in} \\times W_{in}} \\rightarrow \\mathbb{R}^{C \\times H_{out} \\times W_{out}}$$\n",
    "\n",
    "**Mathematical Benefits:**\n",
    "- **Translation invariance**: Small translations don't affect pooled output\n",
    "- **Dimensionality reduction**: Reduces spatial dimensions by factor of kernel size\n",
    "- **Computational efficiency**: Fewer parameters in subsequent layers\n",
    "\n",
    "\n",
    "## Building a Simple CNN\n",
    "\n",
    "### Mathematical Architecture Design\n",
    "\n",
    "**CNN Architecture** combines convolution, activation, and pooling layers:\n",
    "\n",
    "$$\\mathbf{h}^{(\\ell+1)} = \\text{Pool}(\\sigma(\\text{Conv}(\\mathbf{h}^{(\\ell)})))$$\n",
    "\n",
    "**Layer-wise Transformations:**\n",
    "1. **Convolution**: $\\mathbf{z}^{(\\ell)} = \\mathbf{W}^{(\\ell)} * \\mathbf{h}^{(\\ell-1)} + \\mathbf{b}^{(\\ell)}$\n",
    "2. **Activation**: $\\mathbf{a}^{(\\ell)} = \\sigma(\\mathbf{z}^{(\\ell)})$\n",
    "3. **Pooling**: $\\mathbf{h}^{(\\ell)} = \\text{Pool}(\\mathbf{a}^{(\\ell)})$\n",
    "\n",
    "**Spatial Dimension Tracking:**\n",
    "For input $(32 \\times 32)$ through our CNN:\n",
    "- Conv1 (3×3, pad=1): $32 \\times 32 \\rightarrow 32 \\times 32$\n",
    "- Pool1 (2×2): $32 \\times 32 \\rightarrow 16 \\times 16$\n",
    "- Conv2 (3×3, pad=1): $16 \\times 16 \\rightarrow 16 \\times 16$\n",
    "- Pool2 (2×2): $16 \\times 16 \\rightarrow 8 \\times 8$\n",
    "- Conv3 (3×3, pad=1): $8 \\times 8 \\rightarrow 8 \\times 8$\n",
    "- Pool3 (2×2): $8 \\times 8 \\rightarrow 4 \\times 4$\n",
    "\n",
    "**Parameter Count Analysis:**\n",
    "$$\\text{Total params} = \\sum_{\\ell} (C_{in}^{(\\ell)} \\times C_{out}^{(\\ell)} \\times K^2 + C_{out}^{(\\ell)}) + \\text{FC params}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling operations\n",
    "pooling_input = torch.randn(1, 16, 32, 32)\n",
    "\n",
    "# Max pooling\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "max_pooled = max_pool(pooling_input)\n",
    "\n",
    "# Average pooling\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "avg_pooled = avg_pool(pooling_input)\n",
    "\n",
    "# Adaptive pooling (output size fixed regardless of input)\n",
    "adaptive_pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "adaptive_pooled = adaptive_pool(pooling_input)\n",
    "\n",
    "print(f\"Input shape: {pooling_input.shape}\")\n",
    "print(f\"Max pooled: {max_pooled.shape}\")\n",
    "print(f\"Avg pooled: {avg_pooled.shape}\")\n",
    "print(f\"Adaptive pooled: {adaptive_pooled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # After 3 pooling operations: 32->16->8->4\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleCNN(num_classes=10)\n",
    "\n",
    "# Check model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "test_input = torch.randn(4, 3, 32, 32)  # Batch of 4 images\n",
    "output = model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output logits for first sample: {output[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with CIFAR-10 Dataset\n",
    "\n",
    "### Mathematical Dataset Characteristics\n",
    "\n",
    "**CIFAR-10 Dataset Structure:**\n",
    "- **Input space**: $\\mathbf{X} \\in \\mathbb{R}^{3 \\times 32 \\times 32}$ (RGB images)\n",
    "- **Label space**: $y \\in \\{0, 1, 2, \\ldots, 9\\}$ (10 classes)\n",
    "- **Dataset size**: $|\\mathcal{D}| = 60,000$ (50k train + 10k test)\n",
    "\n",
    "**Pixel Value Normalization:**\n",
    "$$\\tilde{x}_{i,j,c} = \\frac{x_{i,j,c} - \\mu_c}{\\sigma_c}$$\n",
    "\n",
    "Where $\\mu_c, \\sigma_c$ are channel-wise mean and standard deviation.\n",
    "\n",
    "**Classification Objective:**\n",
    "$$P(y = k | \\mathbf{x}) = \\frac{e^{f_k(\\mathbf{x})}}{\\sum_{j=1}^{10} e^{f_j(\\mathbf{x})}}$$\n",
    "\n",
    "Where $f(\\mathbf{x}) \\in \\mathbb{R}^{10}$ are the network logits.\n",
    "\n",
    "**Cross-Entropy Loss:**\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(y_i | \\mathbf{x}_i) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{e^{f_{y_i}(\\mathbf{x}_i)}}{\\sum_{k=1}^{10} e^{f_k(\\mathbf{x}_i)}}$$\n",
    "\n",
    "This loss encourages the correct class logit to be larger than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic CIFAR-10-like data for demonstration\n",
    "# (In practice, you would use torchvision.datasets.CIFAR10)\n",
    "\n",
    "def create_synthetic_cifar():\n",
    "    # Create random images and labels\n",
    "    images = torch.randn(1000, 3, 32, 32)\n",
    "    labels = torch.randint(0, 10, (1000,))\n",
    "    return images, labels\n",
    "\n",
    "# Create datasets\n",
    "train_images, train_labels = create_synthetic_cifar()\n",
    "test_images, test_labels = create_synthetic_cifar()\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# CIFAR-10 class names\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "          'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation for Images\n",
    "\n",
    "### Mathematical Foundation of Data Augmentation\n",
    "\n",
    "**Data augmentation** applies transformation $T$ to create additional training examples:\n",
    "$$\\mathcal{D}_{\\text{aug}} = \\mathcal{D} \\cup \\{(T(\\mathbf{x}_i), y_i) : (\\mathbf{x}_i, y_i) \\in \\mathcal{D}, T \\in \\mathcal{T}\\}$$\n",
    "\n",
    "**Common Transformations:**\n",
    "\n",
    "**1. Geometric Transformations:**\n",
    "- **Horizontal Flip**: $T_{\\text{flip}}(x_{i,j}) = x_{i, W-1-j}$\n",
    "- **Rotation**: $T_{\\text{rot}}(\\mathbf{x})$ using rotation matrix $\\mathbf{R}(\\theta)$\n",
    "- **Translation**: $T_{\\text{trans}}(x_{i,j}) = x_{i-\\Delta_y, j-\\Delta_x}$\n",
    "\n",
    "**2. Photometric Transformations:**\n",
    "- **Brightness**: $T_{\\text{bright}}(\\mathbf{x}) = \\mathbf{x} + \\beta$\n",
    "- **Contrast**: $T_{\\text{contrast}}(\\mathbf{x}) = \\alpha \\cdot \\mathbf{x}$\n",
    "- **Color Jitter**: Independent channel scaling\n",
    "\n",
    "**Mathematical Benefits:**\n",
    "1. **Increased dataset size**: $|\\mathcal{D}_{\\text{aug}}| > |\\mathcal{D}|$\n",
    "2. **Invariance learning**: Model learns $f(T(\\mathbf{x})) \\approx f(\\mathbf{x})$\n",
    "3. **Regularization**: Prevents overfitting to specific orientations/lighting\n",
    "4. **Better generalization**: Augmented data approximates real-world variations\n",
    "\n",
    "**Normalization Post-Augmentation:**\n",
    "$$\\tilde{\\mathbf{x}} = \\frac{T(\\mathbf{x}) - \\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}$$\n",
    "\n",
    "Applied after geometric transformations but before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Demonstrate augmentation\n",
    "sample_image = torch.rand(3, 32, 32)\n",
    "\n",
    "# Apply transformations multiple times\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes[0, 0].imshow(sample_image.permute(1, 2, 0))\n",
    "axes[0, 0].set_title('Original')\n",
    "\n",
    "for i in range(5):\n",
    "    row = (i + 1) // 3\n",
    "    col = (i + 1) % 3\n",
    "    augmented = transform_train(sample_image)\n",
    "    # Denormalize for display\n",
    "    augmented = augmented * 0.5 + 0.5\n",
    "    axes[row, col].imshow(augmented.permute(1, 2, 0))\n",
    "    axes[row, col].set_title(f'Augmented {i+1}')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN\n",
    "\n",
    "### Mathematical Training Framework\n",
    "\n",
    "**CNN Training** optimizes parameters via stochastic gradient descent:\n",
    "\n",
    "$$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}_t)$$\n",
    "\n",
    "**Learning Rate Scheduling:**\n",
    "$$\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t/T \\rfloor}$$\n",
    "\n",
    "Where $\\gamma < 1$ is decay factor, $T$ is step size.\n",
    "\n",
    "**Backpropagation in CNNs:**\n",
    "Gradients flow backward through:\n",
    "1. **Fully connected layers**: Standard matrix multiplication gradients\n",
    "2. **Pooling layers**: Gradient routing (max pool) or averaging (avg pool)\n",
    "3. **Convolution layers**: Convolution of gradients with flipped kernels\n",
    "\n",
    "**Convolution Gradient:**\n",
    "$$\\frac{\\partial L}{\\partial W_{f,c,u,v}} = \\sum_{i,j} \\frac{\\partial L}{\\partial Y_{f,i,j}} \\cdot X_{c,i+u,j+v}$$\n",
    "\n",
    "**Device Optimization:**\n",
    "- **GPU acceleration**: Parallel convolution computation\n",
    "- **Memory management**: Activation checkpointing for large models\n",
    "- **Mixed precision**: FP16 computation with FP32 accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "\n",
    "print(f\"Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accuracies, label='Train Accuracy')\n",
    "ax2.plot(test_accuracies, label='Test Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-like residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Simple ResNet\n",
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = ResidualBlock(64, 64)\n",
    "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
    "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create and test ResNet\n",
    "resnet = SimpleResNet(num_classes=10)\n",
    "test_input = torch.randn(2, 3, 32, 32)\n",
    "output = resnet(test_input)\n",
    "\n",
    "print(f\"ResNet output shape: {output.shape}\")\n",
    "print(f\"ResNet parameters: {sum(p.numel() for p in resnet.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned filters\n",
    "def visualize_filters(model, layer_name='conv1'):\n",
    "    # Get the first convolutional layer\n",
    "    conv_layer = getattr(model, layer_name)\n",
    "    weights = conv_layer.weight.data\n",
    "    \n",
    "    # Normalize weights for visualization\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "    \n",
    "    # Plot first 8 filters\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    for i in range(8):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        \n",
    "        # For RGB filters, show as RGB or grayscale\n",
    "        if weights.shape[1] == 3:  # RGB input\n",
    "            filter_img = weights[i].permute(1, 2, 0)\n",
    "        else:  # Grayscale or single channel\n",
    "            filter_img = weights[i, 0]\n",
    "            \n",
    "        axes[row, col].imshow(filter_img, cmap='gray' if len(filter_img.shape) == 2 else None)\n",
    "        axes[row, col].set_title(f'Filter {i+1}')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Learned Filters in {layer_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize filters from the trained model\n",
    "visualize_filters(model, 'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Example\n",
    "\n",
    "### Mathematical Foundation of Transfer Learning\n",
    "\n",
    "**Transfer Learning** leverages pre-trained features for new tasks:\n",
    "\n",
    "$$f_{\\text{new}}(\\mathbf{x}) = g(h_{\\text{pretrained}}(\\mathbf{x}))$$\n",
    "\n",
    "Where:\n",
    "- $h_{\\text{pretrained}}: \\mathbb{R}^{H \\times W \\times C} \\rightarrow \\mathbb{R}^d$ (frozen feature extractor)\n",
    "- $g: \\mathbb{R}^d \\rightarrow \\mathbb{R}^K$ (trainable classifier)\n",
    "\n",
    "**Mathematical Justification:**\n",
    "If source and target domains share low-level features, then:\n",
    "$$P_{\\text{source}}(\\text{features}) \\approx P_{\\text{target}}(\\text{features})$$\n",
    "\n",
    "**Training Strategies:**\n",
    "\n",
    "**1. Feature Extraction:**\n",
    "$$\\boldsymbol{\\theta}_{\\text{features}} \\text{ frozen}, \\quad \\min_{\\boldsymbol{\\theta}_{\\text{classifier}}} L(\\boldsymbol{\\theta}_{\\text{classifier}})$$\n",
    "\n",
    "**2. Fine-tuning:**\n",
    "$$\\min_{\\boldsymbol{\\theta}_{\\text{all}}} L(\\boldsymbol{\\theta}_{\\text{features}}, \\boldsymbol{\\theta}_{\\text{classifier}})$$\n",
    "\n",
    "With reduced learning rate: $\\eta_{\\text{features}} \\ll \\eta_{\\text{classifier}}$\n",
    "\n",
    "**Parameter Efficiency:**\n",
    "- **Full training**: $|\\boldsymbol{\\theta}_{\\text{all}}|$ parameters\n",
    "- **Transfer learning**: $|\\boldsymbol{\\theta}_{\\text{classifier}}| \\ll |\\boldsymbol{\\theta}_{\\text{all}}|$\n",
    "\n",
    "**Mathematical Benefits:**\n",
    "1. **Reduced overfitting**: Fewer trainable parameters\n",
    "2. **Faster convergence**: Good initialization from pre-training\n",
    "3. **Data efficiency**: Effective with small target datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of transfer learning with a pre-trained model\n",
    "# (Using a simplified version - normally you'd use torchvision.models)\n",
    "\n",
    "class PretrainedFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PretrainedFeatureExtractor, self).__init__()\n",
    "        # Simulating pre-trained convolutional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "class TransferLearningModel(nn.Module):\n",
    "    def __init__(self, num_classes=10, freeze_features=True):\n",
    "        super(TransferLearningModel, self).__init__()\n",
    "        \n",
    "        # Pre-trained feature extractor\n",
    "        self.feature_extractor = PretrainedFeatureExtractor()\n",
    "        \n",
    "        # Freeze pre-trained parameters if specified\n",
    "        if freeze_features:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # New classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Create transfer learning model\n",
    "transfer_model = TransferLearningModel(num_classes=10, freeze_features=True)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in transfer_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in transfer_model.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"Fraction trainable: {trainable_params/total_params:.3f}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(2, 3, 32, 32)\n",
    "output = transfer_model(test_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Best Practices\n",
    "\n",
    "### Mathematical Principles Behind Best Practices\n",
    "\n",
    "**1. Data Preprocessing Mathematics:**\n",
    "- **Normalization**: $\\tilde{\\mathbf{x}} = \\frac{\\mathbf{x} - \\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}$ improves optimization conditioning\n",
    "- **Augmentation**: $|\\mathcal{D}_{\\text{aug}}| \\gg |\\mathcal{D}|$ provides implicit regularization\n",
    "- **Batch size**: Trade-off between gradient noise ($\\propto 1/\\sqrt{B}$) and memory\n",
    "\n",
    "**2. Architecture Design Mathematics:**\n",
    "- **Batch Normalization**: $\\text{BN}(\\mathbf{x}) = \\gamma \\frac{\\mathbf{x} - \\mu_{\\mathcal{B}}}{\\sigma_{\\mathcal{B}}} + \\beta$ stabilizes training\n",
    "- **Dropout**: $\\mathbf{h}_{\\text{drop}} = \\mathbf{h} \\odot \\mathbf{m}$, $\\mathbf{m} \\sim \\text{Bernoulli}(p)$ prevents overfitting\n",
    "- **Skip connections**: $\\mathbf{h}^{(\\ell+1)} = \\mathbf{h}^{(\\ell)} + \\mathbf{F}(\\mathbf{h}^{(\\ell)})$ enables deep training\n",
    "\n",
    "**3. Optimization Mathematics:**\n",
    "- **Adam**: Combines momentum and adaptive learning rates for stable convergence\n",
    "- **Learning rate scheduling**: $\\eta_t = \\eta_0 \\gamma^{t/T}$ prevents overshooting minima\n",
    "- **Early stopping**: Monitors $L_{\\text{val}}$ to prevent overfitting\n",
    "\n",
    "**4. Evaluation Mathematics:**\n",
    "- **Multiple metrics**: Accuracy, precision, recall provide comprehensive assessment\n",
    "- **Confusion matrix**: $C_{ij} = |\\{k: y_k = i, \\hat{y}_k = j\\}|$ reveals failure modes\n",
    "- **Cross-validation**: $\\text{CV} = \\frac{1}{K} \\sum_{k=1}^K L_{\\text{fold}_k}$ reduces evaluation variance\n",
    "\n",
    "**Theoretical Guarantees:**\n",
    "- **Universal approximation**: CNNs can approximate any continuous function\n",
    "- **Translation equivariance**: $T(f(\\mathbf{x})) = f(T(\\mathbf{x}))$ for translations $T$\n",
    "- **Parameter sharing**: Reduces effective parameter count from $O(H \\cdot W)$ to $O(K^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CNN Best Practices:\")\n",
    "print(\"\\n1. Data Preprocessing:\")\n",
    "print(\"   - Normalize pixel values (0-1 or standardize)\")\n",
    "print(\"   - Use data augmentation to increase dataset diversity\")\n",
    "print(\"   - Consider image size and computational constraints\")\n",
    "\n",
    "print(\"\\n2. Architecture Design:\")\n",
    "print(\"   - Start with smaller networks and gradually increase complexity\")\n",
    "print(\"   - Use batch normalization for training stability\")\n",
    "print(\"   - Add dropout for regularization\")\n",
    "print(\"   - Consider skip connections (ResNet-style) for deeper networks\")\n",
    "\n",
    "print(\"\\n3. Training:\")\n",
    "print(\"   - Use Adam or AdamW optimizers as starting point\")\n",
    "print(\"   - Implement learning rate scheduling\")\n",
    "print(\"   - Monitor both training and validation metrics\")\n",
    "print(\"   - Use early stopping to prevent overfitting\")\n",
    "\n",
    "print(\"\\n4. Transfer Learning:\")\n",
    "print(\"   - Use pre-trained models when possible\")\n",
    "print(\"   - Fine-tune carefully (lower learning rates)\")\n",
    "print(\"   - Consider which layers to freeze/unfreeze\")\n",
    "\n",
    "print(\"\\n5. Evaluation:\")\n",
    "print(\"   - Use multiple metrics (accuracy, precision, recall, F1)\")\n",
    "print(\"   - Visualize predictions and failure cases\")\n",
    "print(\"   - Analyze feature maps and learned filters\")\n",
    "print(\"   - Test on diverse, representative data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarray_dataex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
