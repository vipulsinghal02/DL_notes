{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Basics Part 7: Transformers and Modern NLP\n\nUnderstanding the Transformer architecture, attention mechanisms, and pre-trained language models with mathematical foundations\n\n## Mathematical Framework of Transformer Architecture\n\n**Transformers** revolutionized NLP by replacing recurrence with attention mechanisms, enabling parallel processing and better long-range dependency modeling.\n\n### Core Mathematical Concepts\n\n**1. Self-Attention Mechanism:**\nFor input sequence $\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n] \\in \\mathbb{R}^{n \\times d}$:\n\n**Query, Key, Value Projections:**\n$$\\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}_V$$\n\n**Attention Computation:**\n$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n\n**2. Multi-Head Attention:**\n$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}_O$$\n\nwhere $\\text{head}_i = \\text{Attention}(\\mathbf{Q}\\mathbf{W}_i^Q, \\mathbf{K}\\mathbf{W}_i^K, \\mathbf{V}\\mathbf{W}_i^V)$\n\n**3. Positional Encoding:**\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n\n**4. Layer Architecture:**\nEach Transformer layer applies:\n$$\\mathbf{Z} = \\text{LayerNorm}(\\mathbf{X} + \\text{MultiHeadAttention}(\\mathbf{X}))$$\n$$\\mathbf{Y} = \\text{LayerNorm}(\\mathbf{Z} + \\text{FFN}(\\mathbf{Z}))$$\n\n**5. Computational Complexity:**\n- **Self-attention**: $O(n^2 \\cdot d)$ time, $O(n^2 + n \\cdot d)$ space\n- **Feed-forward**: $O(n \\cdot d^2)$ time and space\n- **Total per layer**: $O(n^2 \\cdot d + n \\cdot d^2)$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding Attention Mechanisms\n\n### Mathematical Foundation of Attention\n\n**Attention mechanisms** allow models to dynamically focus on relevant parts of the input sequence:\n\n**Information-Theoretic View:**\nAttention computes a probability distribution over input positions:\n$$\\alpha_{i,j} = P(\\text{position } j \\text{ is relevant for position } i)$$\n\n**Attention as Soft Database Lookup:**\n- **Keys**: $\\mathbf{K} = [\\mathbf{k}_1, \\mathbf{k}_2, \\ldots, \\mathbf{k}_n]$ (indexable content)\n- **Values**: $\\mathbf{V} = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n]$ (retrievable content)  \n- **Queries**: $\\mathbf{Q} = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_m]$ (what we search for)\n\n**Compatibility Function:**\n$$e_{ij} = f(\\mathbf{q}_i, \\mathbf{k}_j)$$\n\nCommon choices:\n- **Dot-product**: $f(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q}^T\\mathbf{k}$\n- **Scaled dot-product**: $f(\\mathbf{q}, \\mathbf{k}) = \\frac{\\mathbf{q}^T\\mathbf{k}}{\\sqrt{d}}$\n- **Additive**: $f(\\mathbf{q}, \\mathbf{k}) = \\mathbf{v}^T\\tanh(\\mathbf{W}_q\\mathbf{q} + \\mathbf{W}_k\\mathbf{k})$\n\n**Attention Weights:**\n$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^n \\exp(e_{ik})}$$\n\n**Weighted Combination:**\n$$\\mathbf{c}_i = \\sum_{j=1}^n \\alpha_{ij} \\mathbf{v}_j$$\n\n**Advantages over RNNs:**\n1. **Parallel computation**: All positions computed simultaneously\n2. **Direct connections**: Any position can attend to any other position\n3. **Gradient flow**: No vanishing gradient problem for long sequences\n4. **Interpretability**: Attention weights show model focus"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple attention mechanism implementation\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Compute attention scores for each position\n",
    "        attention_scores = self.attention(encoder_outputs)  # (batch_size, seq_len, 1)\n",
    "        attention_scores = attention_scores.squeeze(-1)     # (batch_size, seq_len)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Compute weighted sum (context vector)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, hidden_size)\n",
    "        context = context.squeeze(1)  # (batch_size, hidden_size)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "# Demonstrate attention\n",
    "batch_size, seq_len, hidden_size = 2, 5, 10\n",
    "encoder_outputs = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "attention_layer = SimpleAttention(hidden_size)\n",
    "context, weights = attention_layer(encoder_outputs)\n",
    "\n",
    "print(f\"Encoder outputs shape: {encoder_outputs.shape}\")\n",
    "print(f\"Context vector shape: {context.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nSample attention weights (should sum to 1):\")\n",
    "print(f\"Batch 1: {weights[0]}\")\n",
    "print(f\"Sum: {weights[0].sum():.4f}\")\n",
    "print(f\"Batch 2: {weights[1]}\")\n",
    "print(f\"Sum: {weights[1].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Scaled Dot-Product Attention\n\n### Mathematical Analysis of Scaled Dot-Product Attention\n\n**Scaled dot-product attention** is the core mechanism in Transformers:\n\n**Mathematical Formula:**\n$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n\n**Why Scaling by $\\sqrt{d_k}$?**\n\n**Variance Analysis:**\nIf $\\mathbf{q}$ and $\\mathbf{k}$ have independent components with mean 0 and variance 1:\n$$\\text{Var}[\\mathbf{q}^T\\mathbf{k}] = \\sum_{i=1}^{d_k} \\text{Var}[q_i k_i] = d_k$$\n\n**Without scaling**: As $d_k$ grows, dot products have larger variance\n**With scaling**: $\\text{Var}[\\frac{\\mathbf{q}^T\\mathbf{k}}{\\sqrt{d_k}}] = 1$\n\n**Softmax Saturation:**\nLarge magnitude inputs to softmax cause:\n$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\rightarrow \\begin{cases} 1 & \\text{if } x_i = \\max_j x_j \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\nThis leads to vanishing gradients.\n\n**Matrix Form Computation:**\nFor batch processing with $n$ queries, $m$ key-value pairs:\n- $\\mathbf{Q} \\in \\mathbb{R}^{n \\times d_k}$\n- $\\mathbf{K} \\in \\mathbb{R}^{m \\times d_k}$  \n- $\\mathbf{V} \\in \\mathbb{R}^{m \\times d_v}$\n- Output: $\\mathbb{R}^{n \\times d_v}$\n\n**Computational Cost:**\n1. **$\\mathbf{Q}\\mathbf{K}^T$**: $O(n \\cdot m \\cdot d_k)$ operations\n2. **Softmax**: $O(n \\cdot m)$ operations\n3. **Attention Ã— Values**: $O(n \\cdot m \\cdot d_v)$ operations\n4. **Total**: $O(n \\cdot m \\cdot (d_k + d_v))$\n\n**Memory Requirements:**\n- **Attention matrix**: $O(n \\cdot m)$ \n- **Activations**: $O(n \\cdot d_v + m \\cdot d_k)$\n- **Peak memory**: $O(n \\cdot m + n \\cdot d_v + m \\cdot d_k)$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention implementation\n",
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        query: (batch_size, seq_len, d_k)\n",
    "        key: (batch_size, seq_len, d_k)\n",
    "        value: (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask to prevent attention to certain positions\n",
    "        dropout: Optional dropout layer\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Compute attention scores: Q * K^T / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (set masked positions to large negative value)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply dropout if provided\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "batch_size, seq_len, d_model = 2, 4, 8\n",
    "\n",
    "# For simplicity, use same tensor for Q, K, V (self-attention)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "query = key = value = x\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights for first batch, first query:\")\n",
    "print(attention_weights[0, 0])  # How much each position attends to each position\n",
    "print(f\"Sum: {attention_weights[0, 0].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Multi-Head Attention\n\n### Mathematical Framework for Multi-Head Attention\n\n**Multi-head attention** runs multiple attention functions in parallel to capture different types of relationships:\n\n**Mathematical Definition:**\n$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O$$\n\n**Individual Attention Heads:**\n$$\\text{head}_i = \\text{Attention}(\\mathbf{Q}\\mathbf{W}_i^Q, \\mathbf{K}\\mathbf{W}_i^K, \\mathbf{V}\\mathbf{W}_i^V)$$\n\n**Projection Matrices:**\n- $\\mathbf{W}_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n- $\\mathbf{W}_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$ \n- $\\mathbf{W}_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$\n- $\\mathbf{W}^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$\n\n**Dimension Relationships:**\nTypically: $d_k = d_v = d_{\\text{model}}/h$\n\n**Representational Capacity:**\nEach head can learn different attention patterns:\n- **Head 1**: Syntactic dependencies (subject-verb)\n- **Head 2**: Coreference resolution (pronoun-antecedent)\n- **Head 3**: Semantic relationships (cause-effect)\n- **Head 4**: Positional patterns (adjacent words)\n\n**Parameter Analysis:**\nFor $h$ heads with $d_{\\text{model}} = 512$, $h = 8$:\n- **Per head**: $3 \\times (512 \\times 64) = 98,304$ parameters (Q, K, V projections)\n- **All heads**: $8 \\times 98,304 = 786,432$ parameters\n- **Output projection**: $512 \\times 512 = 262,144$ parameters\n- **Total**: $1,048,576$ parameters per multi-head attention layer\n\n**Computational Complexity:**\n- **Sequential processing**: $h$ times single-head cost\n- **Parallel processing**: Same as single-head (with sufficient hardware)\n- **Memory overhead**: $h$ attention matrices vs. 1\n\n**Information Flow:**\n$$\\mathbf{X} \\rightarrow \\{(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i)\\}_{i=1}^h \\rightarrow \\{\\text{head}_i\\}_{i=1}^h \\rightarrow \\text{Concat} \\rightarrow \\mathbf{W}^O \\rightarrow \\text{Output}$$\n\n**Theoretical Benefits:**\n1. **Subspace specialization**: Each head focuses on different representation subspaces\n2. **Increased model capacity**: More parameters without increasing depth\n3. **Attention diversity**: Reduces risk of attention collapse\n4. **Parallel computation**: Heads computed independently"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention implementation\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        Q = self.w_q(query)  # (batch_size, seq_len, d_model)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        \n",
    "        # 2. Reshape for multi-head attention\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 3. Apply attention\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        # (batch_size, num_heads, seq_len, d_k) -> (batch_size, seq_len, d_model)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        # 5. Final linear projection\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model, num_heads = 64, 8\n",
    "seq_len, batch_size = 10, 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Self-attention (query, key, value are all the same)\n",
    "output, weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "\n",
    "# Visualize attention pattern\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(weights[0, 0].detach().numpy(), cmap='Blues')\n",
    "plt.title('Attention Head 1')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(weights[0, 1].detach().numpy(), cmap='Blues')\n",
    "plt.title('Attention Head 2')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Positional Encoding\n\n### Mathematical Theory of Position Information\n\n**Positional encodings** inject sequence order information into the attention-based Transformer:\n\n**Why Positional Information is Needed:**\nAttention mechanism is **permutation invariant**:\n$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Attention}(\\mathbf{Q}\\mathbf{P}, \\mathbf{K}\\mathbf{P}, \\mathbf{V}\\mathbf{P})$$\nfor any permutation matrix $\\mathbf{P}$.\n\n**Sinusoidal Positional Encoding:**\n$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n\n**Mathematical Properties:**\n\n**1. Unique Position Encoding:**\nEach position $pos$ gets a unique encoding vector $\\mathbf{pe}_{pos} \\in \\mathbb{R}^{d_{\\text{model}}}$.\n\n**2. Relative Position Information:**\nFor any fixed offset $k$:\n$$PE_{pos+k} = \\mathbf{M}_k \\cdot PE_{pos}$$\n\nwhere $\\mathbf{M}_k$ is a linear transformation matrix.\n\n**3. Frequency Spectrum:**\nDifferent dimensions encode different frequency components:\n- **Low frequencies**: Capture long-range position differences  \n- **High frequencies**: Capture fine-grained local position differences\n\n**Wavelength Analysis:**\nThe wavelength for dimension $i$ is:\n$$\\lambda_i = 2\\pi \\cdot 10000^{2i/d_{\\text{model}}}$$\n\n**Range**: From $2\\pi$ (highest frequency) to $2\\pi \\cdot 10000$ (lowest frequency).\n\n**Alternative Approaches:**\n\n**1. Learned Positional Embeddings:**\n$$\\mathbf{PE} \\in \\mathbb{R}^{L_{\\max} \\times d_{\\text{model}}}$$\nTrainable parameters for each position up to maximum length $L_{\\max}$.\n\n**2. Relative Positional Encoding:**\nAttention modified to include relative positions:\n$$e_{ij} = \\frac{(\\mathbf{x}_i \\mathbf{W}_Q)(\\mathbf{x}_j \\mathbf{W}_K + \\mathbf{r}_{i-j})^T}{\\sqrt{d_k}}$$\n\n**3. Rotary Position Embedding (RoPE):**\n$$\\mathbf{q}_m = \\mathbf{R}_{\\Theta,m} \\mathbf{W}_q \\mathbf{x}_m$$\n$$\\mathbf{k}_n = \\mathbf{R}_{\\Theta,n} \\mathbf{W}_k \\mathbf{x}_n$$\n\nwhere $\\mathbf{R}_{\\Theta,m}$ is rotation matrix encoding position $m$.\n\n**Addition vs Concatenation:**\n- **Addition**: $\\mathbf{x}_{pos} = \\mathbf{emb}_{token} + \\mathbf{pe}_{pos}$\n- **Concatenation**: $\\mathbf{x}_{pos} = [\\mathbf{emb}_{token}; \\mathbf{pe}_{pos}]$\n\nAddition preserves model dimension but risks information mixing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding implementation\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create division term for sinusoidal pattern\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pe[:seq_len].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Visualize positional encodings\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "\n",
    "pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Create dummy input to get positional encodings\n",
    "dummy_input = torch.zeros(1, max_len, d_model)\n",
    "pe_values = pos_encoding.pe[:max_len, 0, :].numpy()  # Get positional encodings\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot positional encoding heatmap\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(pe_values.T, cmap='RdBu', aspect='auto')\n",
    "plt.title('Positional Encoding Heatmap')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.colorbar()\n",
    "\n",
    "# Plot some dimensions over positions\n",
    "plt.subplot(1, 3, 2)\n",
    "for i in range(0, d_model, d_model//4):\n",
    "    plt.plot(pe_values[:50, i], label=f'Dim {i}')\n",
    "plt.title('PE Values for Different Dimensions')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('PE Value')\n",
    "plt.legend()\n",
    "\n",
    "# Plot first few positions across all dimensions\n",
    "plt.subplot(1, 3, 3)\n",
    "for pos in [0, 5, 10, 20]:\n",
    "    plt.plot(pe_values[pos, :20], label=f'Pos {pos}')\n",
    "plt.title('PE Across Dimensions for Different Positions')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('PE Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Positional encoding shape: {pe_values.shape}\")\n",
    "print(f\"Range of PE values: [{pe_values.min():.3f}, {pe_values.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Feed-Forward Network and Layer Normalization\n\n### Mathematical Components of Transformer Layers\n\n**Position-wise Feed-Forward Network:**\nApplied independently to each position in the sequence:\n\n**Mathematical Definition:**\n$$\\text{FFN}(\\mathbf{x}) = \\max(0, \\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2$$\n\n**Dimension Transformation:**\n$$\\mathbb{R}^{d_{\\text{model}}} \\rightarrow \\mathbb{R}^{d_{ff}} \\rightarrow \\mathbb{R}^{d_{\\text{model}}}$$\n\nTypically $d_{ff} = 4 \\times d_{\\text{model}}$ (expansion factor of 4).\n\n**Parameter Count:**\nFor $d_{\\text{model}} = 512$, $d_{ff} = 2048$:\n- $\\mathbf{W}_1$: $512 \\times 2048 = 1,048,576$ parameters\n- $\\mathbf{W}_2$: $2048 \\times 512 = 1,048,576$ parameters  \n- Biases: $2048 + 512 = 2,560$ parameters\n- **Total**: $2,099,712$ parameters per layer\n\n**Layer Normalization Mathematics:**\n\n**Standard Layer Norm:**\n$$\\text{LayerNorm}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\mu}{\\sigma} \\odot \\boldsymbol{\\gamma} + \\boldsymbol{\\beta}$$\n\n**Statistics Computation:**\n$$\\mu = \\frac{1}{d} \\sum_{i=1}^d x_i$$\n$$\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^d (x_i - \\mu)^2$$\n\n**Gradient Properties:**\nLayer normalization provides:\n1. **Gradient scaling**: $\\frac{\\partial \\text{LayerNorm}(\\mathbf{x})}{\\partial \\mathbf{x}} \\propto \\frac{1}{\\sigma}$\n2. **Mean-centering**: Removes activation mean shifts\n3. **Variance stabilization**: Normalizes activation magnitudes\n\n**Pre-norm vs Post-norm:**\n\n**Post-norm (Original Transformer):**\n$$\\mathbf{x}' = \\text{LayerNorm}(\\mathbf{x} + \\text{Attention}(\\mathbf{x}))$$\n$$\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x}' + \\text{FFN}(\\mathbf{x}'))$$\n\n**Pre-norm (More stable for deep models):**\n$$\\mathbf{x}' = \\mathbf{x} + \\text{Attention}(\\text{LayerNorm}(\\mathbf{x}))$$\n$$\\mathbf{y} = \\mathbf{x}' + \\text{FFN}(\\text{LayerNorm}(\\mathbf{x}'))$$\n\n**Residual Connection Mathematics:**\n$$\\mathbf{y} = \\mathbf{x} + F(\\mathbf{x})$$\n\n**Benefits:**\n- **Gradient flow**: $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\mathbf{I} + \\frac{\\partial F(\\mathbf{x})}{\\partial \\mathbf{x}}$\n- **Identity mapping**: Network can learn identity if needed\n- **Training stability**: Mitigates vanishing gradients in deep networks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed-Forward Network\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply first linear transformation with ReLU\n",
    "        output = F.relu(self.w_1(x))\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Apply second linear transformation\n",
    "        output = self.w_2(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Layer Normalization (used in Transformer)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "# Test feed-forward network\n",
    "d_model, d_ff = 64, 256\n",
    "batch_size, seq_len = 2, 10\n",
    "\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "layer_norm = LayerNorm(d_model)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "ffn_output = ffn(x)\n",
    "norm_output = layer_norm(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"FFN output shape: {ffn_output.shape}\")\n",
    "print(f\"Layer norm output shape: {norm_output.shape}\")\n",
    "print(f\"FFN parameters: {sum(p.numel() for p in ffn.parameters()):,}\")\n",
    "\n",
    "# Show effect of layer normalization\n",
    "print(f\"\\nBefore layer norm - Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n",
    "print(f\"After layer norm - Mean: {norm_output.mean():.4f}, Std: {norm_output.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Transformer Encoder Layer\n\n### Complete Mathematical Architecture\n\n**Transformer Encoder Layer** combines all components into a coherent processing unit:\n\n**Layer Computation Flow:**\n1. **Input**: $\\mathbf{X} \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$\n2. **Multi-Head Self-Attention**: $\\mathbf{Z} = \\text{MultiHead}(\\mathbf{X}, \\mathbf{X}, \\mathbf{X})$\n3. **Residual + LayerNorm**: $\\mathbf{X}' = \\text{LayerNorm}(\\mathbf{X} + \\mathbf{Z})$\n4. **Feed-Forward**: $\\mathbf{F} = \\text{FFN}(\\mathbf{X}')$\n5. **Residual + LayerNorm**: $\\mathbf{Y} = \\text{LayerNorm}(\\mathbf{X}' + \\mathbf{F})$\n\n**Mathematical Expression:**\n$$\\text{TransformerLayer}(\\mathbf{X}) = \\text{LayerNorm}(\\mathbf{X}' + \\text{FFN}(\\mathbf{X}'))$$\n\nwhere $\\mathbf{X}' = \\text{LayerNorm}(\\mathbf{X} + \\text{MultiHead}(\\mathbf{X}, \\mathbf{X}, \\mathbf{X}))$\n\n**Parameter Analysis per Layer:**\nFor BERT-base configuration ($d_{\\text{model}} = 768$, $h = 12$, $d_{ff} = 3072$):\n\n1. **Multi-Head Attention**:\n   - Q, K, V projections: $3 \\times 768^2 = 1,769,472$\n   - Output projection: $768^2 = 589,824$\n   - **Subtotal**: $2,359,296$ parameters\n\n2. **Feed-Forward Network**:\n   - First layer: $768 \\times 3072 = 2,359,296$\n   - Second layer: $3072 \\times 768 = 2,359,296$\n   - Biases: $3072 + 768 = 3,840$\n   - **Subtotal**: $4,722,432$ parameters\n\n3. **Layer Normalization** (2 layers):\n   - $\\gamma, \\beta$: $2 \\times 768 = 1,536$\n\n**Total per layer**: $7,083,264$ parameters\n\n**Information Flow Analysis:**\n\n**Attention Information Mixing:**\nEach position can attend to all positions:\n$$\\mathbf{h}_i = \\sum_{j=1}^n \\alpha_{ij} \\mathbf{v}_j$$\n\n**Feed-Forward Information Processing:**\nPosition-wise transformation preserves sequence structure while allowing complex non-linear mappings.\n\n**Residual Connections:**\nEnable gradient flow and allow network to learn incremental updates:\n$$\\frac{\\partial L}{\\partial \\mathbf{X}} = \\frac{\\partial L}{\\partial \\mathbf{Y}} \\left(\\mathbf{I} + \\frac{\\partial (\\text{Attention + FFN})}{\\partial \\mathbf{X}}\\right)$$\n\n**Layer Stacking:**\nDeep Transformer with $L$ layers:\n$$\\mathbf{Y} = \\text{Layer}_L(\\text{Layer}_{L-1}(\\cdots \\text{Layer}_1(\\mathbf{X} + \\mathbf{PE}) \\cdots))$$\n\n**Computational Complexity per Layer:**\n- **Self-Attention**: $O(n^2 d + nd^2)$\n- **Feed-Forward**: $O(nd_{ff}d) = O(4nd^2)$\n- **Total**: $O(n^2 d + nd^2)$ (attention dominates for long sequences)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output, attention_weights = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "# Complete Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len=5000, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Token embedding + positional encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        attention_weights = []\n",
    "        \n",
    "        # Pass through each encoder layer\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, mask)\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "# Create and test Transformer encoder\n",
    "vocab_size = 1000\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "num_layers = 6\n",
    "\n",
    "transformer = TransformerEncoder(vocab_size, d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "# Test input\n",
    "batch_size, seq_len = 2, 20\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "output, attention_weights = transformer(input_ids)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention weight tensors: {len(attention_weights)}\")\n",
    "print(f\"Each attention weight shape: {attention_weights[0].shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "\n",
    "# Parameter breakdown\n",
    "embedding_params = transformer.embedding.weight.numel()\n",
    "encoder_params = sum(p.numel() for layer in transformer.layers for p in layer.parameters())\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "print(f\"Embedding: {embedding_params:,}\")\n",
    "print(f\"Encoder layers: {encoder_params:,}\")\n",
    "print(f\"Parameters per layer: {encoder_params // num_layers:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Text Classification with Transformers\n\n### Mathematical Framework for Transformer-Based Classification\n\n**Classification Objective:**\nMap variable-length sequences to discrete labels:\n$$f: \\mathcal{V}^* \\rightarrow \\{1, 2, \\ldots, C\\}$$\n\n**Architecture Components:**\n\n**1. Token + Position Embeddings:**\n$$\\mathbf{X} = \\mathbf{E}_{\\text{token}} + \\mathbf{E}_{\\text{pos}}$$\n\nwhere $\\mathbf{E}_{\\text{token}} \\in \\mathbb{R}^{n \\times d}$ and $\\mathbf{E}_{\\text{pos}} \\in \\mathbb{R}^{n \\times d}$.\n\n**2. Transformer Encoding:**\n$$\\mathbf{H} = \\text{TransformerEncoder}(\\mathbf{X})$$\n\n**3. Sequence Representation:**\nMultiple strategies for aggregating sequence information:\n\n**CLS Token Approach:**\n$$\\mathbf{h}_{\\text{cls}} = \\mathbf{H}[0, :] \\quad \\text{(first position)}$$\n\n**Mean Pooling:**\n$$\\mathbf{h}_{\\text{mean}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{H}[i, :]$$\n\n**Attention Pooling:**\n$$\\alpha_i = \\frac{\\exp(\\mathbf{w}^T \\mathbf{H}[i, :])}{\\sum_{j=1}^n \\exp(\\mathbf{w}^T \\mathbf{H}[j, :])}$$\n$$\\mathbf{h}_{\\text{att}} = \\sum_{i=1}^n \\alpha_i \\mathbf{H}[i, :]$$\n\n**4. Classification Head:**\n$$\\mathbf{y} = \\text{softmax}(\\mathbf{W}_{\\text{cls}} \\mathbf{h} + \\mathbf{b}_{\\text{cls}})$$\n\n**Loss Function:**\n$$L = -\\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^C y_{i,c} \\log \\hat{y}_{i,c}$$\n\n**Fine-tuning Mathematics:**\n\n**Parameter Optimization:**\n$$\\boldsymbol{\\theta} = \\{\\boldsymbol{\\theta}_{\\text{encoder}}, \\boldsymbol{\\theta}_{\\text{classifier}}\\}$$\n\n**Learning Rate Scheduling:**\n- **Encoder layers**: $\\eta_{\\text{enc}} \\in [1e-5, 5e-5]$ (lower LR)\n- **Classifier**: $\\eta_{\\text{cls}} \\in [1e-4, 1e-3]$ (higher LR)\n\n**Gradient Flow:**\n$$\\frac{\\partial L}{\\partial \\boldsymbol{\\theta}_{\\text{encoder}}} = \\frac{\\partial L}{\\partial \\mathbf{h}} \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{H}} \\frac{\\partial \\mathbf{H}}{\\partial \\boldsymbol{\\theta}_{\\text{encoder}}}$$\n\n**Regularization Strategies:**\n- **Dropout**: Applied in classifier and attention layers\n- **Weight decay**: L2 regularization on parameters\n- **Early stopping**: Based on validation performance\n- **Layer freezing**: Freeze lower layers, fine-tune upper layers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer-based text classifier\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, num_classes, max_len=512):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        \n",
    "        self.transformer = TransformerEncoder(\n",
    "            vocab_size, d_model, num_heads, d_ff, num_layers, max_len\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Get transformer output\n",
    "        transformer_output, attention_weights = self.transformer(x, mask)\n",
    "        \n",
    "        # Use [CLS] token (first token) or average pooling for classification\n",
    "        # Here we'll use average pooling\n",
    "        if mask is not None:\n",
    "            # Mask out padding tokens for average\n",
    "            mask_expanded = mask.unsqueeze(-1).expand(transformer_output.size())\n",
    "            sum_embeddings = torch.sum(transformer_output * mask_expanded, dim=1)\n",
    "            sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
    "            pooled_output = sum_embeddings / sum_mask\n",
    "        else:\n",
    "            # Simple average pooling\n",
    "            pooled_output = transformer_output.mean(dim=1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# Create simple vocabulary and tokenizer for demo\n",
    "simple_vocab = {\n",
    "    '<PAD>': 0, '<UNK>': 1, 'good': 2, 'bad': 3, 'great': 4, 'terrible': 5,\n",
    "    'amazing': 6, 'awful': 7, 'love': 8, 'hate': 9, 'this': 10, 'is': 11,\n",
    "    'movie': 12, 'book': 13, 'product': 14, 'the': 15, 'a': 16, 'very': 17\n",
    "}\n",
    "\n",
    "def simple_tokenize(text, vocab, max_len=10):\n",
    "    \"\"\"Simple tokenization for demo\"\"\"\n",
    "    words = text.lower().split()\n",
    "    tokens = [vocab.get(word, vocab['<UNK>']) for word in words]\n",
    "    \n",
    "    # Pad or truncate\n",
    "    if len(tokens) < max_len:\n",
    "        tokens.extend([vocab['<PAD>']] * (max_len - len(tokens)))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Create sample classification data\n",
    "texts = [\n",
    "    \"this movie is great\",\n",
    "    \"terrible book very bad\",\n",
    "    \"amazing product love this\",\n",
    "    \"awful movie hate this\",\n",
    "    \"good book this is\",\n",
    "    \"bad product terrible\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# Tokenize\n",
    "tokenized_texts = [simple_tokenize(text, simple_vocab) for text in texts]\n",
    "X_clf = torch.LongTensor(tokenized_texts)\n",
    "y_clf = torch.LongTensor(labels)\n",
    "\n",
    "print(f\"Sample tokenized text: {tokenized_texts[0]}\")\n",
    "print(f\"Original text: '{texts[0]}'\")\n",
    "print(f\"Classification data shape: {X_clf.shape}\")\n",
    "\n",
    "# Create small Transformer classifier\n",
    "classifier = TransformerClassifier(\n",
    "    vocab_size=len(simple_vocab),\n",
    "    d_model=32,\n",
    "    num_heads=4,\n",
    "    d_ff=128,\n",
    "    num_layers=2,\n",
    "    num_classes=2,\n",
    "    max_len=10\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "logits, attention_weights = classifier(X_clf)\n",
    "probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "print(f\"\\nClassifier output shape: {logits.shape}\")\n",
    "print(f\"Sample probabilities: {probabilities[:3]}\")\n",
    "print(f\"Predicted classes: {probabilities.argmax(dim=1)}\")\n",
    "print(f\"True labels: {y_clf}\")\n",
    "print(f\"Classifier parameters: {sum(p.numel() for p in classifier.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding Pre-trained Models\n\n### Mathematical Foundation of Transfer Learning in NLP\n\n**Pre-trained Language Models** learn rich representations from large-scale unlabeled text:\n\n**Training Paradigm:**\n1. **Pre-training**: Learn general language representations\n2. **Fine-tuning**: Adapt to specific downstream tasks\n\n**Pre-training Objectives:**\n\n**1. Masked Language Modeling (BERT):**\n$$L_{\\text{MLM}} = -\\mathbb{E}_{\\mathbf{x} \\sim D} \\left[ \\sum_{i \\in \\mathcal{M}} \\log P(x_i | \\mathbf{x}_{\\setminus \\mathcal{M}}) \\right]$$\n\nwhere $\\mathcal{M}$ is the set of masked positions.\n\n**2. Next Sentence Prediction (BERT):**\n$$L_{\\text{NSP}} = -\\mathbb{E}_{(A,B)} [\\log P(IsNext(B|A))]$$\n\n**3. Autoregressive Language Modeling (GPT):**\n$$L_{\\text{AR}} = -\\mathbb{E}_{\\mathbf{x}} \\left[ \\sum_{t=1}^T \\log P(x_t | \\mathbf{x}_{<t}) \\right]$$\n\n**Model Scale Mathematics:**\n\n**Parameter Scaling Laws:**\nFor Transformer models, performance scales as:\n$$L(N) \\propto N^{-\\alpha}$$\n\nwhere $N$ is parameter count and $\\alpha \\approx 0.076$.\n\n**BERT Model Configurations:**\n- **BERT-base**: $L=12$, $H=768$, $A=12$, Parameters=110M\n- **BERT-large**: $L=24$, $H=1024$, $A=16$, Parameters=340M\n\n**Parameter Count Formula:**\n$$\\text{Params} = V \\times H + L \\times (4H^2 + 4H \\times \\text{FFN\\_size}) + \\text{Output\\_layer}$$\n\n**Computational Requirements:**\n\n**Training FLOPs:**\nFor sequence length $n$, batch size $B$, and $T$ training steps:\n$$\\text{FLOPs} \\approx 6NBT \\times (n^2H + nH^2)$$\n\n**Inference FLOPs:**\n$$\\text{FLOPs} \\approx 2N \\times (n^2H + nH^2)$$\n\n**Knowledge Transfer Mathematics:**\n\n**Representation Similarity:**\nMeasure similarity between pre-trained and fine-tuned representations:\n$$\\text{CKA}(\\mathbf{X}, \\mathbf{Y}) = \\frac{\\text{tr}(\\mathbf{K}_X \\mathbf{K}_Y)}{\\sqrt{\\text{tr}(\\mathbf{K}_X^2) \\text{tr}(\\mathbf{K}_Y^2)}}$$\n\nwhere $\\mathbf{K}_X = \\mathbf{X}\\mathbf{X}^T$ is the kernel matrix.\n\n**Feature Transferability:**\nLower layers learn general features, upper layers learn task-specific features:\n$$\\mathbf{h}_l = f_l(\\mathbf{h}_{l-1}; \\boldsymbol{\\theta}_l^{\\text{general}} + \\Delta\\boldsymbol{\\theta}_l^{\\text{task}})$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate working with pre-trained models (conceptual)\n",
    "# In practice, you would use libraries like Transformers by Hugging Face\n",
    "\n",
    "class PretrainedModelSimulator:\n",
    "    \"\"\"Simulates the interface of pre-trained models like BERT\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        self.model_name = model_name\n",
    "        self.vocab_size = 30522  # BERT vocab size\n",
    "        self.hidden_size = 768   # BERT hidden size\n",
    "        self.num_layers = 12     # BERT layers\n",
    "        self.num_heads = 12      # BERT attention heads\n",
    "        \n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            '[PAD]': 0,\n",
    "            '[UNK]': 100,\n",
    "            '[CLS]': 101,  # Classification token\n",
    "            '[SEP]': 102,  # Separator token\n",
    "            '[MASK]': 103  # Mask token for MLM\n",
    "        }\n",
    "        \n",
    "        print(f\"Simulated {model_name}:\")\n",
    "        print(f\"  Vocabulary size: {self.vocab_size:,}\")\n",
    "        print(f\"  Hidden size: {self.hidden_size}\")\n",
    "        print(f\"  Layers: {self.num_layers}\")\n",
    "        print(f\"  Attention heads: {self.num_heads}\")\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simulate tokenization (simplified)\"\"\"\n",
    "        # In reality, BERT uses WordPiece tokenization\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Add special tokens\n",
    "        tokens = [self.special_tokens['[CLS]']]\n",
    "        \n",
    "        for word in words:\n",
    "            # Simulate subword tokenization\n",
    "            if len(word) > 6:\n",
    "                # Split long words\n",
    "                tokens.extend([hash(word[:3]) % 1000 + 1000, \n",
    "                              hash(word[3:]) % 1000 + 2000])\n",
    "            else:\n",
    "                tokens.append(hash(word) % 1000 + 3000)\n",
    "        \n",
    "        tokens.append(self.special_tokens['[SEP]'])\n",
    "        return tokens\n",
    "    \n",
    "    def get_embeddings(self, tokens):\n",
    "        \"\"\"Simulate getting contextual embeddings\"\"\"\n",
    "        # In practice, this would be the output of the transformer\n",
    "        batch_size = 1\n",
    "        seq_len = len(tokens)\n",
    "        \n",
    "        # Simulate contextual embeddings\n",
    "        embeddings = torch.randn(batch_size, seq_len, self.hidden_size)\n",
    "        return embeddings\n",
    "    \n",
    "    def estimate_parameters(self):\n",
    "        \"\"\"Estimate number of parameters\"\"\"\n",
    "        # Rough estimation\n",
    "        embedding_params = self.vocab_size * self.hidden_size\n",
    "        \n",
    "        # Each transformer layer\n",
    "        attention_params = 4 * self.hidden_size * self.hidden_size  # Q, K, V, O projections\n",
    "        ffn_params = 2 * self.hidden_size * (4 * self.hidden_size)  # Two linear layers\n",
    "        layer_params = attention_params + ffn_params\n",
    "        \n",
    "        total_params = embedding_params + (self.num_layers * layer_params)\n",
    "        return total_params\n",
    "\n",
    "# Demonstrate pre-trained model concepts\n",
    "bert_sim = PretrainedModelSimulator(\"bert-base-uncased\")\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"The transformer architecture revolutionized natural language processing\"\n",
    "tokens = bert_sim.tokenize(sample_text)\n",
    "embeddings = bert_sim.get_embeddings(tokens)\n",
    "\n",
    "print(f\"\\nSample text: '{sample_text}'\")\n",
    "print(f\"Tokenized length: {len(tokens)}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Estimated parameters: {bert_sim.estimate_parameters():,}\")\n",
    "\n",
    "print(\"\\nCommon Pre-trained Models:\")\n",
    "models_info = {\n",
    "    'BERT-base': {'params': '110M', 'layers': 12, 'heads': 12, 'hidden': 768},\n",
    "    'BERT-large': {'params': '340M', 'layers': 24, 'heads': 16, 'hidden': 1024},\n",
    "    'GPT-2': {'params': '117M-1.5B', 'layers': '12-48', 'heads': '12-25', 'hidden': '768-1600'},\n",
    "    'RoBERTa': {'params': '125M-355M', 'layers': '12-24', 'heads': '12-16', 'hidden': '768-1024'},\n",
    "    'T5-base': {'params': '220M', 'layers': 12, 'heads': 12, 'hidden': 768}\n",
    "}\n",
    "\n",
    "for model, info in models_info.items():\n",
    "    print(f\"  {model}: {info['params']} parameters, {info['layers']} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Fine-tuning Strategies\n\n### Mathematical Framework for Model Adaptation\n\n**Fine-tuning** adapts pre-trained models to downstream tasks through various mathematical strategies:\n\n**1. Full Fine-tuning:**\nUpdate all parameters $\\boldsymbol{\\theta}$:\n$$\\boldsymbol{\\theta}_{\\text{new}} = \\boldsymbol{\\theta}_{\\text{pretrained}} - \\eta \\nabla_{\\boldsymbol{\\theta}} L_{\\text{task}}$$\n\n**2. Feature Extraction:**\nFreeze encoder, train only classifier:\n$$\\boldsymbol{\\theta}_{\\text{encoder}} \\text{ fixed}, \\quad \\boldsymbol{\\theta}_{\\text{classifier}} \\text{ trainable}$$\n\n**3. Layer-wise Learning Rates:**\nDifferent learning rates for different layers:\n$$\\eta_l = \\eta_0 \\cdot \\gamma^{L-l}$$\n\nwhere $l$ is layer index and $\\gamma < 1$ (lower layers get smaller LR).\n\n**4. Gradual Unfreezing:**\nProgressively unfreeze layers during training:\n$$\\boldsymbol{\\theta}_{\\text{trainable}}^{(t)} = \\boldsymbol{\\theta}_{\\text{trainable}}^{(t-1)} \\cup \\{\\boldsymbol{\\theta}_{\\text{layer}_{k(t)}}\\}$$\n\n**Parameter-Efficient Fine-tuning:**\n\n**LoRA (Low-Rank Adaptation):**\nApproximate weight updates with low-rank decomposition:\n$$\\mathbf{W}_{\\text{new}} = \\mathbf{W}_0 + \\Delta\\mathbf{W} = \\mathbf{W}_0 + \\mathbf{B}\\mathbf{A}$$\n\nwhere $\\mathbf{A} \\in \\mathbb{R}^{r \\times d}$, $\\mathbf{B} \\in \\mathbb{R}^{d \\times r}$, and $r \\ll d$.\n\n**Parameter Reduction:**\nOriginal parameters: $d^2$\nLoRA parameters: $2rd$ (typically $r = 8$, so $16d \\ll d^2$)\n\n**Adapter Layers:**\nInsert small feedforward networks between Transformer layers:\n$$\\mathbf{h}_{\\text{adapter}} = \\mathbf{h} + f(\\mathbf{h}; \\boldsymbol{\\theta}_{\\text{adapter}})$$\n\nwhere $f$ is a bottleneck network: $d \\rightarrow d/k \\rightarrow d$.\n\n**Prefix Tuning:**\nPrepend learnable prefix vectors:\n$$[\\mathbf{P}_K; \\mathbf{K}], \\quad [\\mathbf{P}_V; \\mathbf{V}]$$\n\n**BitFit:**\nFine-tune only bias terms:\n$$\\boldsymbol{\\theta}_{\\text{trainable}} = \\{\\mathbf{b} : \\mathbf{b} \\text{ is bias in model}\\}$$\n\n**Fine-tuning Stability:**\n\n**Learning Rate Scheduling:**\n$$\\eta(t) = \\eta_0 \\left(1 - \\frac{t}{T}\\right) \\quad \\text{(linear decay)}$$\n$$\\eta(t) = \\eta_0 \\cos\\left(\\frac{\\pi t}{2T}\\right) \\quad \\text{(cosine decay)}$$\n\n**Warmup Strategy:**\n$$\\eta(t) = \\begin{cases}\n\\eta_0 \\frac{t}{T_{\\text{warmup}}} & \\text{if } t \\leq T_{\\text{warmup}} \\\\\n\\eta_{\\text{scheduled}}(t) & \\text{otherwise}\n\\end{cases}$$\n\n**Regularization Mathematics:**\n$$L_{\\text{total}} = L_{\\text{task}} + \\lambda_1 \\|\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0\\|_2^2 + \\lambda_2 \\|\\boldsymbol{\\theta}\\|_2^2$$\n\nFirst term prevents catastrophic forgetting, second term prevents overfitting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning strategies demonstration\n",
    "class FineTuningStrategies:\n",
    "    \"\"\"Demonstrate different fine-tuning approaches\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_model):\n",
    "        self.pretrained_model = pretrained_model\n",
    "    \n",
    "    def full_fine_tuning(self, num_classes):\n",
    "        \"\"\"Full fine-tuning: update all parameters\"\"\"\n",
    "        print(\"Strategy 1: Full Fine-tuning\")\n",
    "        print(\"- All parameters are updated\")\n",
    "        print(\"- Requires significant computational resources\")\n",
    "        print(\"- Best performance for sufficient data\")\n",
    "        \n",
    "        # Add classification head\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.pretrained_model.d_model, num_classes)\n",
    "        )\n",
    "        \n",
    "        # All parameters trainable\n",
    "        total_params = sum(p.numel() for p in self.pretrained_model.parameters())\n",
    "        trainable_params = total_params\n",
    "        \n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Trainable percentage: 100%\")\n",
    "        \n",
    "        return classifier\n",
    "    \n",
    "    def feature_extraction(self, num_classes):\n",
    "        \"\"\"Feature extraction: freeze pre-trained weights\"\"\"\n",
    "        print(\"\\nStrategy 2: Feature Extraction\")\n",
    "        print(\"- Pre-trained weights are frozen\")\n",
    "        print(\"- Only classification head is trained\")\n",
    "        print(\"- Much faster training\")\n",
    "        \n",
    "        # Freeze all pretrained parameters\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Add trainable classification head\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.pretrained_model.d_model, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.pretrained_model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in classifier.parameters())\n",
    "        \n",
    "        print(f\"  Total parameters: {total_params + trainable_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Trainable percentage: {trainable_params/(total_params + trainable_params)*100:.2f}%\")\n",
    "        \n",
    "        return classifier\n",
    "    \n",
    "    def gradual_unfreezing(self, num_classes, layers_to_unfreeze=2):\n",
    "        \"\"\"Gradual unfreezing: unfreeze top layers\"\"\"\n",
    "        print(f\"\\nStrategy 3: Gradual Unfreezing (top {layers_to_unfreeze} layers)\")\n",
    "        print(\"- Freeze most layers, unfreeze top few layers\")\n",
    "        print(\"- Compromise between speed and performance\")\n",
    "        print(\"- Good for medium-sized datasets\")\n",
    "        \n",
    "        # Freeze all parameters first\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze top layers\n",
    "        unfrozen_params = 0\n",
    "        for layer in self.pretrained_model.layers[-layers_to_unfreeze:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "                unfrozen_params += param.numel()\n",
    "        \n",
    "        # Add classification head\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.pretrained_model.d_model, num_classes)\n",
    "        )\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.pretrained_model.parameters())\n",
    "        trainable_params = unfrozen_params + sum(p.numel() for p in classifier.parameters())\n",
    "        \n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Trainable percentage: {trainable_params/total_params*100:.2f}%\")\n",
    "        \n",
    "        return classifier\n",
    "    \n",
    "    def learning_rate_scheduling(self):\n",
    "        \"\"\"Demonstrate learning rate strategies for fine-tuning\"\"\"\n",
    "        print(\"\\nLearning Rate Strategies:\")\n",
    "        print(\"1. Lower LR for pre-trained layers (1e-5 to 5e-5)\")\n",
    "        print(\"2. Higher LR for new layers (1e-4 to 1e-3)\")\n",
    "        print(\"3. Warmup + decay schedule\")\n",
    "        print(\"4. Discriminative learning rates (different LR per layer)\")\n",
    "        \n",
    "        # Example parameter groups\n",
    "        pretrained_params = list(self.pretrained_model.parameters())\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': pretrained_params, 'lr': 2e-5},\n",
    "            # {'params': classifier.parameters(), 'lr': 1e-4}  # Would add classifier params\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nExample parameter groups:\")\n",
    "        print(f\"  Pre-trained layers: {len(pretrained_params)} parameter tensors, LR=2e-5\")\n",
    "        print(f\"  New classifier: would use LR=1e-4\")\n",
    "        \n",
    "        return param_groups\n",
    "\n",
    "# Demonstrate fine-tuning strategies\n",
    "# Use our previously created transformer\n",
    "finetuning = FineTuningStrategies(transformer)\n",
    "\n",
    "# Show different strategies\n",
    "num_classes = 5\n",
    "classifier1 = finetuning.full_fine_tuning(num_classes)\n",
    "classifier2 = finetuning.feature_extraction(num_classes)\n",
    "classifier3 = finetuning.gradual_unfreezing(num_classes, layers_to_unfreeze=2)\n",
    "param_groups = finetuning.learning_rate_scheduling()\n",
    "\n",
    "print(\"\\nFine-tuning Best Practices:\")\n",
    "print(\"1. Start with feature extraction for small datasets\")\n",
    "print(\"2. Use full fine-tuning for large, task-specific datasets\")\n",
    "print(\"3. Apply gradual unfreezing for medium datasets\")\n",
    "print(\"4. Use lower learning rates for pre-trained parameters\")\n",
    "print(\"5. Monitor validation performance to prevent overfitting\")\n",
    "print(\"6. Consider early stopping and learning rate scheduling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Working with Hugging Face Transformers\n\n### Mathematical Framework for Production NLP Systems\n\n**Hugging Face Transformers** provides standardized interfaces for pre-trained models with mathematical foundations:\n\n**Tokenization Mathematics:**\n\n**WordPiece/BPE Algorithm:**\n1. Initialize vocabulary with characters: $V_0 = \\{c_1, c_2, \\ldots, c_n\\}$\n2. Iteratively merge most frequent pairs:\n   $$V_{t+1} = V_t \\cup \\{\\arg\\max_{(x,y) \\in V_t^2} \\text{count}(xy)\\}$$\n\n**Subword Regularization:**\nSample from multiple tokenizations with probability:\n$$P(\\mathbf{x} | s) = \\prod_{i} \\frac{\\exp(\\text{score}(x_i))}{\\sum_{x' \\in \\text{candidates}} \\exp(\\text{score}(x'))}$$\n\n**Model Loading Mathematics:**\n\n**Weight Initialization:**\nPre-trained weights $\\boldsymbol{\\theta}_{\\text{pretrained}}$ loaded with:\n- **Embedding layers**: Direct mapping from vocabulary\n- **Transformer layers**: Exact architecture match required\n- **Task heads**: Random initialization for new tasks\n\n**Architecture Compatibility:**\nModel dimensions must satisfy:\n- $d_{\\text{model}}$ consistent across layers\n- Attention heads: $h$ divides $d_{\\text{model}}$\n- Vocabulary sizes match for embeddings\n\n**Training Optimization:**\n\n**AdamW Optimizer:**\n$$\\mathbf{m}_t = \\beta_1 \\mathbf{m}_{t-1} + (1-\\beta_1)\\mathbf{g}_t$$\n$$\\mathbf{v}_t = \\beta_2 \\mathbf{v}_{t-1} + (1-\\beta_2)\\mathbf{g}_t^2$$\n$$\\boldsymbol{\\theta}_t = \\boldsymbol{\\theta}_{t-1} - \\eta \\left(\\frac{\\hat{\\mathbf{m}}_t}{\\sqrt{\\hat{\\mathbf{v}}_t} + \\epsilon} + \\lambda \\boldsymbol{\\theta}_{t-1}\\right)$$\n\n**Learning Rate Scheduling:**\nLinear warmup followed by linear decay:\n$$\\eta(t) = \\begin{cases}\n\\eta_{\\max} \\frac{t}{t_{\\text{warmup}}} & \\text{if } t \\leq t_{\\text{warmup}} \\\\\n\\eta_{\\max} \\frac{t_{\\text{total}} - t}{t_{\\text{total}} - t_{\\text{warmup}}} & \\text{otherwise}\n\\end{cases}$$\n\n**Pipeline Architecture:**\n\n**Text Classification Pipeline:**\n$$\\text{Input} \\xrightarrow{\\text{Tokenize}} \\text{Token IDs} \\xrightarrow{\\text{Model}} \\text{Logits} \\xrightarrow{\\text{Softmax}} \\text{Probabilities}$$\n\n**Question Answering Pipeline:**\n$$P(\\text{start}=i) = \\frac{\\exp(\\mathbf{w}_s^T \\mathbf{h}_i)}{\\sum_j \\exp(\\mathbf{w}_s^T \\mathbf{h}_j)}$$\n$$P(\\text{end}=j) = \\frac{\\exp(\\mathbf{w}_e^T \\mathbf{h}_j)}{\\sum_k \\exp(\\mathbf{w}_e^T \\mathbf{h}_k)}$$\n\n**Memory and Compute Scaling:**\n\n**Model Size Estimation:**\nFor model with $N$ parameters:\n- **FP32**: $4N$ bytes\n- **FP16**: $2N$ bytes  \n- **INT8**: $N$ bytes\n\n**Inference Memory:**\n$$\\text{Memory} = \\text{Model weights} + \\text{Activations} + \\text{KV cache}$$\n\n**Batch Processing Efficiency:**\n$$\\text{Throughput} = \\frac{B \\times L}{\\text{Inference time}}$$\n\nTrade-off between batch size $B$, sequence length $L$, and memory constraints."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Hugging Face workflow (conceptual)\n",
    "# In practice, you would: pip install transformers\n",
    "\n",
    "class HuggingFaceWorkflowDemo:\n",
    "    \"\"\"Demonstrate typical Hugging Face patterns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Hugging Face Transformers Workflow:\")\n",
    "        print(\"\\n# Installation:\")\n",
    "        print(\"pip install transformers torch datasets\")\n",
    "        \n",
    "    def show_model_loading(self):\n",
    "        \"\"\"Show how to load pre-trained models\"\"\"\n",
    "        print(\"\\n# Loading Pre-trained Models:\")\n",
    "        print(\"from transformers import AutoModel, AutoTokenizer, AutoConfig\")\n",
    "        print(\"\")\n",
    "        print(\"# Load model and tokenizer\")\n",
    "        print(\"model_name = 'bert-base-uncased'\")\n",
    "        print(\"tokenizer = AutoTokenizer.from_pretrained(model_name)\")\n",
    "        print(\"model = AutoModel.from_pretrained(model_name)\")\n",
    "        print(\"\")\n",
    "        print(\"# For specific tasks\")\n",
    "        print(\"from transformers import AutoModelForSequenceClassification\")\n",
    "        print(\"classifier = AutoModelForSequenceClassification.from_pretrained(\")\n",
    "        print(\"    model_name, num_labels=2\")\n",
    "        \n",
    "    def show_tokenization(self):\n",
    "        \"\"\"Show tokenization patterns\"\"\"\n",
    "        print(\"\\n# Tokenization:\")\n",
    "        print(\"text = 'Hello, how are you?'\")\n",
    "        print(\"\")\n",
    "        print(\"# Basic tokenization\")\n",
    "        print(\"tokens = tokenizer(text, return_tensors='pt')\")\n",
    "        print(\"# Returns: {'input_ids': tensor, 'attention_mask': tensor}\")\n",
    "        print(\"\")\n",
    "        print(\"# Batch processing\")\n",
    "        print(\"texts = ['First text', 'Second text']\")\n",
    "        print(\"batch = tokenizer(texts, padding=True, truncation=True, \")\n",
    "        print(\"                  max_length=512, return_tensors='pt')\")\n",
    "        \n",
    "    def show_training_loop(self):\n",
    "        \"\"\"Show typical training patterns\"\"\"\n",
    "        print(\"\\n# Training Loop:\")\n",
    "        print(\"from transformers import AdamW, get_linear_schedule_with_warmup\")\n",
    "        print(\"\")\n",
    "        print(\"# Setup optimizer\")\n",
    "        print(\"optimizer = AdamW(model.parameters(), lr=2e-5)\")\n",
    "        print(\"\")\n",
    "        print(\"# Learning rate scheduler\")\n",
    "        print(\"scheduler = get_linear_schedule_with_warmup(\")\n",
    "        print(\"    optimizer, num_warmup_steps=100, num_training_steps=1000\")\n",
    "        print(\"\")\n",
    "        print(\"# Training step\")\n",
    "        print(\"outputs = model(**batch)\")\n",
    "        print(\"loss = outputs.loss\")\n",
    "        print(\"loss.backward()\")\n",
    "        print(\"optimizer.step()\")\n",
    "        print(\"scheduler.step()\")\n",
    "        print(\"optimizer.zero_grad()\")\n",
    "        \n",
    "    def show_popular_models(self):\n",
    "        \"\"\"Show popular pre-trained models\"\"\"\n",
    "        print(\"\\n# Popular Pre-trained Models:\")\n",
    "        \n",
    "        models = {\n",
    "            \"Classification\": [\n",
    "                \"bert-base-uncased\",\n",
    "                \"roberta-base\",\n",
    "                \"distilbert-base-uncased\",\n",
    "                \"albert-base-v2\"\n",
    "            ],\n",
    "            \"Generation\": [\n",
    "                \"gpt2\",\n",
    "                \"gpt2-medium\",\n",
    "                \"microsoft/DialoGPT-medium\",\n",
    "                \"facebook/blenderbot-400M-distill\"\n",
    "            ],\n",
    "            \"Question Answering\": [\n",
    "                \"distilbert-base-cased-distilled-squad\",\n",
    "                \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "                \"roberta-base-squad2\"\n",
    "            ],\n",
    "            \"Multilingual\": [\n",
    "                \"bert-base-multilingual-cased\",\n",
    "                \"xlm-roberta-base\",\n",
    "                \"distilbert-base-multilingual-cased\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for category, model_list in models.items():\n",
    "            print(f\"\\n{category}:\")\n",
    "            for model in model_list:\n",
    "                print(f\"  - {model}\")\n",
    "    \n",
    "    def show_pipeline_api(self):\n",
    "        \"\"\"Show the simple pipeline API\"\"\"\n",
    "        print(\"\\n# Pipeline API (Quickstart):\")\n",
    "        print(\"from transformers import pipeline\")\n",
    "        print(\"\")\n",
    "        print(\"# Text classification\")\n",
    "        print(\"classifier = pipeline('sentiment-analysis')\")\n",
    "        print(\"result = classifier('I love this movie!')\")\n",
    "        print(\"\")\n",
    "        print(\"# Question answering\")\n",
    "        print(\"qa = pipeline('question-answering')\")\n",
    "        print(\"answer = qa(question='What is AI?', context='AI is...')\")\n",
    "        print(\"\")\n",
    "        print(\"# Text generation\")\n",
    "        print(\"generator = pipeline('text-generation', model='gpt2')\")\n",
    "        print(\"text = generator('The future of AI is', max_length=50)\")\n",
    "        print(\"\")\n",
    "        print(\"# Named entity recognition\")\n",
    "        print(\"ner = pipeline('ner', aggregation_strategy='simple')\")\n",
    "        print(\"entities = ner('John works at OpenAI in San Francisco')\")\n",
    "\n",
    "# Run the demonstration\n",
    "demo = HuggingFaceWorkflowDemo()\n",
    "demo.show_model_loading()\n",
    "demo.show_tokenization()\n",
    "demo.show_training_loop()\n",
    "demo.show_popular_models()\n",
    "demo.show_pipeline_api()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Key Advantages of Transformers:\")\n",
    "print(\"1. Parallelizable training (unlike RNNs)\")\n",
    "print(\"2. Better handling of long-range dependencies\")\n",
    "print(\"3. Transfer learning capabilities\")\n",
    "print(\"4. State-of-the-art performance on many NLP tasks\")\n",
    "print(\"5. Rich ecosystem of pre-trained models\")\n",
    "\n",
    "print(\"\\nChallenges:\")\n",
    "print(\"1. Quadratic memory complexity with sequence length\")\n",
    "print(\"2. Large computational requirements\")\n",
    "print(\"3. Need for large datasets for training from scratch\")\n",
    "print(\"4. Limited context window (though improving)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}