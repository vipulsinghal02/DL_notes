{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Basics Part 3: Data Loading and Datasets\n\nWorking with datasets, data loaders, and data preprocessing for machine learning\n\n## Mathematical Foundation of Data Loading\n\n**Data loading** is the process of efficiently accessing and preprocessing training examples for machine learning algorithms. The mathematical framework involves:\n\n### Dataset as a Mathematical Object\nA dataset $\\mathcal{D}$ is a collection of input-output pairs:\n$$\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}$$\n\nWhere:\n- $\\mathbf{x}_i \\in \\mathbb{R}^d$ are feature vectors\n- $y_i \\in \\mathcal{Y}$ are targets (labels for classification, real values for regression)\n- $N$ is the dataset size\n\n### Sampling and Batching\n**Mini-batch sampling** creates subsets for efficient training:\n$$\\mathcal{B}_k = \\{(\\mathbf{x}_i, y_i)\\}_{i \\in I_k} \\subset \\mathcal{D}$$\n\nWhere $I_k$ is a random subset of indices with $|I_k| = B$ (batch size).\n\n**Empirical Risk on Mini-batches:**\n$$\\hat{R}_{\\mathcal{B}}(f) = \\frac{1}{|{\\mathcal{B}}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{B}} L(f(\\mathbf{x}), y)$$\n\nThis approximates the full empirical risk while enabling efficient gradient computation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Creating Custom Datasets\n\n### Mathematical Interface Design\nPyTorch's `Dataset` class provides a mathematical interface to data collections:\n\n**Dataset Interface:**\n- `__len__()`: Returns $|\\mathcal{D}| = N$ (cardinality)\n- `__getitem__(i)`: Returns $(\\mathbf{x}_i, y_i)$ (indexed access)\n\n**Synthetic Data Generation:**\nFor regression, we can generate data following:\n$$y_i = f(\\mathbf{x}_i) + \\epsilon_i$$\n\nWhere:\n- $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is the true function\n- $\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ (feature distribution)\n- $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ (noise term)\n\nThis creates a controlled environment for testing algorithms with known ground truth."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple synthetic dataset\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, n_samples=1000, n_features=10, noise=0.1):\n",
    "        # Generate synthetic regression data\n",
    "        X, y = make_regression(n_samples=n_samples, \n",
    "                              n_features=n_features, \n",
    "                              noise=noise, \n",
    "                              random_state=42)\n",
    "        \n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create dataset\n",
    "dataset = SyntheticDataset(n_samples=500, n_features=5)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Feature shape: {dataset[0][0].shape}\")\n",
    "print(f\"Target shape: {dataset[0][1].shape}\")\n",
    "\n",
    "# Access first few samples\n",
    "for i in range(3):\n",
    "    x, y = dataset[i]\n",
    "    print(f\"Sample {i}: x={x[:3]}, y={y.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using TensorDataset for Simple Cases\n\n### Mathematical Convenience\n`TensorDataset` provides a simple wrapper for tensor pairs:\n\n**Tensor Pairs:**\nGiven feature tensor $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ and target tensor $\\mathbf{y} \\in \\mathbb{R}^N$ (or $\\mathbb{Z}^N$ for classification):\n$$\\text{TensorDataset}(\\mathbf{X}, \\mathbf{y}) = \\{(\\mathbf{X}[i], \\mathbf{y}[i])\\}_{i=0}^{N-1}$$\n\n**Classification Targets:**\nFor $K$-class classification:\n$$y_i \\in \\{0, 1, 2, \\ldots, K-1\\}$$\n\nThis integer encoding enables efficient cross-entropy loss computation via one-hot encoding:\n$$\\mathbf{e}_c = [0, \\ldots, 0, 1, 0, \\ldots, 0] \\in \\{0,1\\}^K$$\nwhere the 1 is in position $c$."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data using TensorDataset (easier for simple cases)\n",
    "X = torch.randn(200, 4)\n",
    "y = torch.randint(0, 3, (200,))  # 3 classes\n",
    "\n",
    "tensor_dataset = TensorDataset(X, y)\n",
    "print(f\"TensorDataset size: {len(tensor_dataset)}\")\n",
    "\n",
    "# Access samples\n",
    "x_sample, y_sample = tensor_dataset[0]\n",
    "print(f\"Sample shape: {x_sample.shape}, Label: {y_sample.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## DataLoader: Batching and Shuffling\n\n### Mathematical Foundation of Mini-batch Processing\n\n**DataLoader** implements efficient batch sampling with the following mathematical operations:\n\n**Batch Generation:**\n$$\\text{DataLoader}(\\mathcal{D}, B) \\rightarrow \\{\\mathcal{B}_1, \\mathcal{B}_2, \\ldots, \\mathcal{B}_{\\lceil N/B \\rceil}\\}$$\n\nWhere each batch $\\mathcal{B}_k$ contains $B$ samples (except possibly the last batch).\n\n**Shuffling (Random Permutation):**\nAt each epoch, apply permutation $\\pi: \\{1, 2, \\ldots, N\\} \\rightarrow \\{1, 2, \\ldots, N\\}$:\n$$\\mathcal{D}_{\\text{shuffled}} = \\{(\\mathbf{x}_{\\pi(i)}, y_{\\pi(i)})\\}_{i=1}^{N}$$\n\n**Benefits of Shuffling:**\n1. **Reduces variance** in gradient estimates\n2. **Breaks correlations** between consecutive samples\n3. **Improves convergence** by providing diverse batches\n\n**Batch Size Effects:**\n- **Small batches**: Higher gradient noise, better generalization\n- **Large batches**: Lower gradient noise, faster computation, more memory"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "dataloader = DataLoader(tensor_dataset, \n",
    "                       batch_size=32, \n",
    "                       shuffle=True, \n",
    "                       num_workers=0)  # Use 0 for Windows compatibility\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}: X shape={batch_x.shape}, y shape={batch_y.shape}\")\n",
    "    if batch_idx >= 2:  # Show only first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train/Validation/Test Splits\n\n### Mathematical Foundation of Data Splitting\n\n**Data Partitioning** divides the dataset for proper model evaluation:\n\n$$\\mathcal{D} = \\mathcal{D}_{\\text{train}} \\cup \\mathcal{D}_{\\text{val}} \\cup \\mathcal{D}_{\\text{test}}$$\n\nwhere the sets are **disjoint**: $\\mathcal{D}_{\\text{train}} \\cap \\mathcal{D}_{\\text{val}} \\cap \\mathcal{D}_{\\text{test}} = \\emptyset$\n\n**Common Split Ratios:**\n- Training: $|\\mathcal{D}_{\\text{train}}| = 0.7N$ (70%)\n- Validation: $|\\mathcal{D}_{\\text{val}}| = 0.15N$ (15%)  \n- Test: $|\\mathcal{D}_{\\text{test}}| = 0.15N$ (15%)\n\n**Mathematical Purpose:**\n1. **Training Set** $\\mathcal{D}_{\\text{train}}$: Optimize parameters $\\boldsymbol{\\theta}$\n   $$\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}} \\frac{1}{|\\mathcal{D}_{\\text{train}}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_{\\text{train}}} L(f(\\mathbf{x}; \\boldsymbol{\\theta}), y)$$\n\n2. **Validation Set** $\\mathcal{D}_{\\text{val}}$: Select hyperparameters and early stopping\n   \n3. **Test Set** $\\mathcal{D}_{\\text{test}}$: Unbiased performance estimation\n   $$\\text{Generalization Error} \\approx \\frac{1}{|\\mathcal{D}_{\\text{test}}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{D}_{\\text{test}}} L(f(\\mathbf{x}; \\boldsymbol{\\theta}^*), y)$$\n\n**Statistical Independence** ensures unbiased estimates."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val/test\n",
    "dataset_size = len(tensor_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.15 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    tensor_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Transformations\n\n### Mathematical Foundation of Data Preprocessing\n\n**Data Transformations** are functions $T: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d'}$ that preprocess input features:\n\n**Common Transformations:**\n\n**1. Normalization (Z-score):**\n$$T_{\\text{norm}}(\\mathbf{x}) = \\frac{\\mathbf{x} - \\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}$$\n\nWhere:\n- $\\boldsymbol{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x}_i$ (sample mean)\n- $\\boldsymbol{\\sigma} = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (\\mathbf{x}_i - \\boldsymbol{\\mu})^2}$ (sample std)\n\n**2. Data Augmentation (Noise Addition):**\n$$T_{\\text{noise}}(\\mathbf{x}) = \\mathbf{x} + \\boldsymbol{\\epsilon}$$\n\nWhere $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$\n\n**Mathematical Benefits:**\n1. **Improved conditioning**: Normalized features have similar scales\n2. **Regularization**: Noise injection reduces overfitting\n3. **Data augmentation**: Artificially increases dataset size\n4. **Invariance**: Encourages robustness to small perturbations\n\n**Transform Composition:**\n$$T_{\\text{composed}} = T_2 \\circ T_1 \\text{ where } (T_2 \\circ T_1)(\\mathbf{x}) = T_2(T_1(\\mathbf{x}))$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with transformations\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "# Define transforms\n",
    "def normalize_transform(x):\n",
    "    return (x - x.mean()) / (x.std() + 1e-8)\n",
    "\n",
    "def add_noise_transform(x, noise_std=0.1):\n",
    "    return x + torch.randn_like(x) * noise_std\n",
    "\n",
    "# Create datasets with transforms\n",
    "raw_data = torch.randn(100, 5)\n",
    "targets = torch.randint(0, 2, (100,))\n",
    "\n",
    "# Without transform\n",
    "dataset_raw = TransformDataset(raw_data, targets)\n",
    "\n",
    "# With normalization\n",
    "dataset_norm = TransformDataset(raw_data, targets, transform=normalize_transform)\n",
    "\n",
    "# Compare samples\n",
    "x_raw, _ = dataset_raw[0]\n",
    "x_norm, _ = dataset_norm[0]\n",
    "\n",
    "print(f\"Raw data stats: mean={x_raw.mean():.3f}, std={x_raw.std():.3f}\")\n",
    "print(f\"Normalized data stats: mean={x_norm.mean():.3f}, std={x_norm.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Working with Real Data: CSV Files\n\n### Mathematical Data Pipeline\n\n**Structured Data Loading** involves mapping tabular data to tensor representations:\n\n**CSV to Tensor Mapping:**\n$$\\text{CSV}_{N \\times (d+1)} \\rightarrow (\\mathbf{X} \\in \\mathbb{R}^{N \\times d}, \\mathbf{y} \\in \\mathbb{R}^N)$$\n\nWhere:\n- Each row represents one sample $(\\mathbf{x}_i, y_i)$\n- Feature columns → feature matrix $\\mathbf{X}$\n- Target column → target vector $\\mathbf{y}$\n\n**Data Type Considerations:**\n- **Features**: Usually $\\mathbb{R}^d$ (continuous) or $\\{0,1\\}^d$ (categorical/binary)\n- **Classification targets**: $\\{0, 1, \\ldots, K-1\\}$ (integer labels)\n- **Regression targets**: $\\mathbb{R}$ (continuous values)\n\n**Categorical Encoding:**\nFor categorical variables with $C$ categories:\n$$\\text{OneHot}: \\{1, 2, \\ldots, C\\} \\rightarrow \\{0,1\\}^C$$\n\nThis preserves the assumption that features are numerical for tensor operations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample CSV file\n",
    "np.random.seed(42)\n",
    "sample_data = {\n",
    "    'feature1': np.random.randn(200),\n",
    "    'feature2': np.random.randn(200),\n",
    "    'feature3': np.random.randn(200),\n",
    "    'target': np.random.randint(0, 3, 200)\n",
    "}\n",
    "df = pd.DataFrame(sample_data)\n",
    "df.to_csv('sample_data.csv', index=False)\n",
    "\n",
    "print(\"Sample CSV created:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for CSV data\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, target_column='target'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.target_column = target_column\n",
    "        \n",
    "        # Separate features and targets\n",
    "        self.features = self.data.drop(columns=[target_column]).values\n",
    "        self.targets = self.data[target_column].values\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.targets = torch.LongTensor(self.targets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# Load CSV dataset\n",
    "csv_dataset = CSVDataset('sample_data.csv')\n",
    "print(f\"CSV dataset size: {len(csv_dataset)}\")\n",
    "print(f\"Feature shape: {csv_dataset[0][0].shape}\")\n",
    "print(f\"Number of classes: {len(torch.unique(csv_dataset.targets))}\")\n",
    "\n",
    "# Create data loader\n",
    "csv_loader = DataLoader(csv_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Show a batch\n",
    "for batch_x, batch_y in csv_loader:\n",
    "    print(f\"Batch features shape: {batch_x.shape}\")\n",
    "    print(f\"Batch targets shape: {batch_y.shape}\")\n",
    "    print(f\"Target classes in batch: {torch.unique(batch_y)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Normalization and Standardization\n\n### Mathematical Foundation of Feature Scaling\n\n**Feature Scaling** ensures all features contribute equally to learning algorithms:\n\n**Standard Normalization (Z-score):**\n$$\\tilde{\\mathbf{x}}_j = \\frac{\\mathbf{x}_j - \\mu_j}{\\sigma_j}$$\n\nWhere for feature $j$:\n- $\\mu_j = \\frac{1}{N} \\sum_{i=1}^{N} x_{ij}$ (sample mean)\n- $\\sigma_j = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} (x_{ij} - \\mu_j)^2}$ (sample standard deviation)\n\n**Properties after normalization:**\n- $\\mathbb{E}[\\tilde{\\mathbf{x}}_j] = 0$ (zero mean)\n- $\\text{Var}(\\tilde{\\mathbf{x}}_j) = 1$ (unit variance)\n\n**Critical Rule: Use Training Statistics Only**\n$$\\tilde{\\mathbf{x}}_{\\text{test}} = \\frac{\\mathbf{x}_{\\text{test}} - \\mu_{\\text{train}}}{\\sigma_{\\text{train}}}$$\n\nThis prevents **data leakage** and maintains the i.i.d. assumption.\n\n**Mathematical Benefits:**\n1. **Gradient stability**: Prevents features with large scales from dominating\n2. **Convergence speed**: Improves conditioning of optimization problem\n3. **Numerical stability**: Reduces potential for overflow/underflow"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics from training data\n",
    "def compute_dataset_stats(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=len(dataset))\n",
    "    data, _ = next(iter(loader))\n",
    "    return data.mean(dim=0), data.std(dim=0)\n",
    "\n",
    "# Get statistics from training set\n",
    "train_mean, train_std = compute_dataset_stats(train_dataset)\n",
    "print(f\"Training mean: {train_mean}\")\n",
    "print(f\"Training std: {train_std}\")\n",
    "\n",
    "# Normalization function\n",
    "def normalize_data(x, mean, std):\n",
    "    return (x - mean) / (std + 1e-8)\n",
    "\n",
    "# Apply normalization to a batch\n",
    "for batch_x, batch_y in train_loader:\n",
    "    normalized_x = normalize_data(batch_x, train_mean, train_std)\n",
    "    print(f\"Original batch mean: {batch_x.mean(dim=0)}\")\n",
    "    print(f\"Normalized batch mean: {normalized_x.mean(dim=0)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Complete Training Example with Data Loading\n\n### Mathematical Training Framework\n\nThis example demonstrates the complete pipeline from data loading to model training:\n\n**Classification Problem Setup:**\n- Feature space: $\\mathbf{x} \\in \\mathbb{R}^{10}$\n- Label space: $y \\in \\{0, 1, 2\\}$ (3-class classification)\n- Dataset: $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{1000}$\n\n**Model Architecture:**\n$$f(\\mathbf{x}; \\boldsymbol{\\theta}) = \\mathbf{W}_3 \\sigma(\\mathbf{W}_2 \\sigma(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2) + \\mathbf{b}_3$$\n\nWhere:\n- $\\mathbf{W}_1 \\in \\mathbb{R}^{20 \\times 10}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{20 \\times 20}$, $\\mathbf{W}_3 \\in \\mathbb{R}^{3 \\times 20}$\n- $\\sigma$ is ReLU activation\n- Dropout provides regularization: $\\text{Dropout}(\\mathbf{h}) = \\mathbf{h} \\odot \\mathbf{m}$ where $\\mathbf{m} \\sim \\text{Bernoulli}(p)$\n\n**Objective Function:**\n$$L(\\boldsymbol{\\theta}) = \\frac{1}{|\\mathcal{B}|} \\sum_{(\\mathbf{x}, y) \\in \\mathcal{B}} -\\log p(y|\\mathbf{x}; \\boldsymbol{\\theta})$$\n\nWhere $p(y|\\mathbf{x}; \\boldsymbol{\\theta}) = \\text{softmax}(f(\\mathbf{x}; \\boldsymbol{\\theta}))_y$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=3, \n",
    "                          n_informative=8, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.LongTensor(y)\n",
    "\n",
    "# Create dataset and split\n",
    "full_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = Classifier(input_size=10, hidden_size=20, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with data loader\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Loading Best Practices\n\n### Mathematical and Computational Considerations\n\n**Batch Size Optimization:**\nThe choice of batch size $B$ involves mathematical trade-offs:\n\n**Gradient Estimation Variance:**\n$$\\text{Var}[\\nabla L_{\\mathcal{B}}] = \\frac{1}{B} \\text{Var}[\\nabla L_{(\\mathbf{x}, y)}]$$\n\n- **Larger B**: Lower variance, more stable gradients\n- **Smaller B**: Higher variance, better generalization (implicit regularization)\n\n**Memory Complexity:**\n- **Forward pass**: $\\mathcal{O}(B \\cdot d \\cdot h)$ where $h$ is hidden size\n- **Backward pass**: $\\mathcal{O}(B \\cdot d \\cdot h)$ for gradient computation\n\n**Computational Efficiency:**\n- **Vectorization benefits**: Matrix operations scale efficiently with batch size\n- **Memory hierarchy**: Larger batches better utilize cache and GPU memory\n- **Parallelization**: Larger batches enable better parallel computation\n\n**Optimal Batch Size Selection:**\n$$B^* = \\arg\\min_{B} \\left( \\frac{\\text{computation time}}{B} + \\lambda \\cdot \\text{Var}[\\nabla L_{\\mathcal{B}}] \\right)$$\n\nWhere $\\lambda$ balances computation speed vs. gradient quality.\n\n**Rule of Thumb**: Powers of 2 (16, 32, 64, 128) often work well due to hardware optimization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing data loading with different settings\n",
    "import time\n",
    "\n",
    "def time_dataloader(dataloader, num_batches=10):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        # Simulate some processing\n",
    "        _ = batch_x.mean()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Compare different batch sizes\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "for batch_size in batch_sizes:\n",
    "    loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    elapsed = time_dataloader(loader)\n",
    "    print(f\"Batch size {batch_size}: {elapsed:.3f} seconds\")\n",
    "\n",
    "print(\"\\nTips for efficient data loading:\")\n",
    "print(\"1. Use appropriate batch sizes (powers of 2 often work well)\")\n",
    "print(\"2. Consider num_workers > 0 for larger datasets\")\n",
    "print(\"3. Use pin_memory=True when transferring to GPU\")\n",
    "print(\"4. Precompute expensive transformations when possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up created files\n",
    "import os\n",
    "if os.path.exists('sample_data.csv'):\n",
    "    os.remove('sample_data.csv')\n",
    "    print(\"Cleaned up sample_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}