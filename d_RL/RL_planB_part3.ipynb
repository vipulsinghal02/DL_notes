{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Plan B - Part 3: From Tabular to Deep RL\n",
    "\n",
    "This notebook bridges the gap between tabular RL methods and deep reinforcement learning. We'll explore function approximation, understand why neural networks are necessary for complex environments, and implement the foundation for deep RL algorithms.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the limitations of tabular methods and need for function approximation\n",
    "- Master linear and neural network function approximation theory\n",
    "- Implement the \"deadly triad\" and understand instability issues\n",
    "- Build neural network foundations for value function approximation\n",
    "- Explore experience replay and target networks\n",
    "- Transition from discrete to continuous state spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict, deque\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List, Optional, Union\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import gym\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# CPU optimization for MacBook Air M2\n",
    "torch.set_num_threads(8)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Torch threads: {torch.get_num_threads()}\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Curse of Dimensionality and Need for Function Approximation\n",
    "\n",
    "Tabular methods become impractical when state or action spaces grow large. **Function approximation** allows us to generalize across states using learned representations.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "Instead of storing $V(s)$ for each state $s$, we approximate it with a parameterized function:\n",
    "$$\\hat{V}(s, \\mathbf{w}) \\approx V^\\pi(s)$$\n",
    "\n",
    "Where $\\mathbf{w} \\in \\mathbb{R}^d$ are the **learnable parameters**.\n",
    "\n",
    "### Types of Function Approximation\n",
    "\n",
    "1. **Linear Function Approximation**:\n",
    "   $$\\hat{V}(s, \\mathbf{w}) = \\mathbf{w}^T \\phi(s)$$\n",
    "   Where $\\phi(s)$ are **feature vectors** representing state $s$.\n",
    "\n",
    "2. **Neural Network Approximation**:\n",
    "   $$\\hat{V}(s, \\mathbf{w}) = f_{\\text{neural}}(s; \\mathbf{w})$$\n",
    "   Where $f_{\\text{neural}}$ is a neural network with parameters $\\mathbf{w}$.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "We minimize the **Mean Squared Value Error** (MSVE):\n",
    "$$\\text{MSVE}(\\mathbf{w}) = \\sum_{s \\in \\mathcal{S}} d(s) [V^\\pi(s) - \\hat{V}(s, \\mathbf{w})]^2$$\n",
    "\n",
    "Where $d(s)$ is a state distribution (often the stationary distribution under policy $\\pi$).\n",
    "\n",
    "### Gradient Descent Update\n",
    "\n",
    "The gradient descent update rule is:\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\frac{1}{2}\\alpha \\nabla [V^\\pi(S_t) - \\hat{V}(S_t, \\mathbf{w}_t)]^2$$\n",
    "$$= \\mathbf{w}_t + \\alpha [V^\\pi(S_t) - \\hat{V}(S_t, \\mathbf{w}_t)] \\nabla \\hat{V}(S_t, \\mathbf{w}_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CartPole Environment: Continuous States\n",
    "\n",
    "Let's implement the classic CartPole environment to demonstrate function approximation with continuous state spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper for CartPole environment with additional functionality.\n",
    "    \n",
    "    CartPole has a 4-dimensional continuous state space:\n",
    "    - Cart Position: [-2.4, 2.4]\n",
    "    - Cart Velocity: [-inf, inf]\n",
    "    - Pole Angle: [-0.2095, 0.2095] radians (~12 degrees)\n",
    "    - Pole Angular Velocity: [-inf, inf]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        try:\n",
    "            # Try new gym API first\n",
    "            self.env = gym.make('CartPole-v1', render_mode=render_mode)\n",
    "        except:\n",
    "            # Fallback to old gym API\n",
    "            self.env = gym.make('CartPole-v1')\n",
    "        \n",
    "        self.state_dim = 4\n",
    "        self.action_dim = 2  # Left (0) or Right (1)\n",
    "        \n",
    "        # State bounds for normalization\n",
    "        self.state_bounds = np.array([\n",
    "            [-2.4, 2.4],      # Cart Position\n",
    "            [-3.0, 3.0],      # Cart Velocity (clipped)\n",
    "            [-0.21, 0.21],    # Pole Angle\n",
    "            [-3.0, 3.0]       # Pole Angular Velocity (clipped)\n",
    "        ])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment and return initial state.\"\"\"\n",
    "        try:\n",
    "            # New gym API\n",
    "            state, info = self.env.reset()\n",
    "            return state\n",
    "        except:\n",
    "            # Old gym API\n",
    "            return self.env.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action and return (state, reward, done, info).\"\"\"\n",
    "        try:\n",
    "            # New gym API\n",
    "            state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            return state, reward, done, info\n",
    "        except:\n",
    "            # Old gym API\n",
    "            return self.env.step(action)\n",
    "    \n",
    "    def normalize_state(self, state):\n",
    "        \"\"\"Normalize state to [-1, 1] range.\"\"\"\n",
    "        state = np.clip(state, self.state_bounds[:, 0], self.state_bounds[:, 1])\n",
    "        normalized = 2 * (state - self.state_bounds[:, 0]) / (self.state_bounds[:, 1] - self.state_bounds[:, 0]) - 1\n",
    "        return normalized\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the environment.\"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "# Create CartPole environment\n",
    "cartpole_env = CartPoleWrapper()\n",
    "print(f\"CartPole Environment:\")\n",
    "print(f\"  State dimension: {cartpole_env.state_dim}\")\n",
    "print(f\"  Action dimension: {cartpole_env.action_dim}\")\n",
    "print(f\"  State bounds: {cartpole_env.state_bounds}\")\n",
    "\n",
    "# Test the environment\n",
    "state = cartpole_env.reset()\n",
    "print(f\"\\nSample state: {state}\")\n",
    "print(f\"Normalized: {cartpole_env.normalize_state(state)}\")\n",
    "\n",
    "# Run a short episode to see the dynamics\n",
    "print(\"\\nSample episode (first 10 steps):\")\n",
    "state = cartpole_env.reset()\n",
    "for step in range(10):\n",
    "    action = cartpole_env.env.action_space.sample()  # Random action\n",
    "    next_state, reward, done, info = cartpole_env.step(action)\n",
    "    print(f\"Step {step}: Action={action}, Reward={reward:.1f}, Done={done}\")\n",
    "    print(f\"  State: [{next_state[0]:.3f}, {next_state[1]:.3f}, {next_state[2]:.3f}, {next_state[3]:.3f}]\")\n",
    "    \n",
    "    if done:\n",
    "        print(f\"  Episode ended at step {step + 1}\")\n",
    "        break\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Function Approximation\n",
    "\n",
    "Linear function approximation is the simplest form of function approximation and provides theoretical guarantees under certain conditions.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "For linear approximation:\n",
    "$$\\hat{V}(s, \\mathbf{w}) = \\sum_{i=1}^d w_i \\phi_i(s) = \\mathbf{w}^T \\phi(s)$$\n",
    "\n",
    "The **gradient** is simply:\n",
    "$$\\nabla_\\mathbf{w} \\hat{V}(s, \\mathbf{w}) = \\phi(s)$$\n",
    "\n",
    "### Semi-Gradient TD Update\n",
    "\n",
    "Since we don't have access to the true value $V^\\pi(S_t)$, we use the **TD target** as an approximation:\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\hat{V}(S_{t+1}, \\mathbf{w}_t) - \\hat{V}(S_t, \\mathbf{w}_t)] \\phi(S_t)$$\n",
    "\n",
    "This is called **semi-gradient** because we don't differentiate through the target $\\hat{V}(S_{t+1}, \\mathbf{w}_t)$.\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "Good features $\\phi(s)$ are crucial for linear approximation:\n",
    "- **Polynomial features**: $[1, s_1, s_2, s_1^2, s_1 s_2, s_2^2, \\ldots]$\n",
    "- **RBF features**: Radial basis functions centered at different points\n",
    "- **Fourier features**: Sine and cosine basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFunctionApproximator:\n",
    "    \"\"\"\n",
    "    Linear function approximator for value functions.\n",
    "    \n",
    "    Implements various feature extraction methods and semi-gradient TD learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, feature_type: str = 'polynomial', \n",
    "                 degree: int = 2, n_rbf: int = 10):\n",
    "        self.state_dim = state_dim\n",
    "        self.feature_type = feature_type\n",
    "        self.degree = degree\n",
    "        self.n_rbf = n_rbf\n",
    "        \n",
    "        # Determine feature dimension\n",
    "        if feature_type == 'polynomial':\n",
    "            # Polynomial features up to given degree\n",
    "            from itertools import combinations_with_replacement\n",
    "            self.feature_dim = sum(len(list(combinations_with_replacement(range(state_dim), d))) \n",
    "                                 for d in range(degree + 1))\n",
    "        elif feature_type == 'rbf':\n",
    "            # RBF features\n",
    "            self.feature_dim = n_rbf\n",
    "            # Create random RBF centers\n",
    "            self.rbf_centers = np.random.randn(n_rbf, state_dim)\n",
    "            self.rbf_width = 1.0\n",
    "        else:\n",
    "            # Simple linear features (identity)\n",
    "            self.feature_dim = state_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.zeros(self.feature_dim)\n",
    "        \n",
    "        print(f\"Linear FA: {feature_type} features, dim: {state_dim} -> {self.feature_dim}\")\n",
    "    \n",
    "    def extract_features(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Extract features from state.\"\"\"\n",
    "        if self.feature_type == 'polynomial':\n",
    "            return self._polynomial_features(state)\n",
    "        elif self.feature_type == 'rbf':\n",
    "            return self._rbf_features(state)\n",
    "        else:\n",
    "            return state  # Linear features\n",
    "    \n",
    "    def _polynomial_features(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Generate polynomial features.\"\"\"\n",
    "        from itertools import combinations_with_replacement\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        for d in range(self.degree + 1):\n",
    "            for indices in combinations_with_replacement(range(self.state_dim), d):\n",
    "                if d == 0:\n",
    "                    features.append(1.0)  # Bias term\n",
    "                else:\n",
    "                    feature_val = 1.0\n",
    "                    for idx in indices:\n",
    "                        feature_val *= state[idx]\n",
    "                    features.append(feature_val)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def _rbf_features(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Generate RBF (Radial Basis Function) features.\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for center in self.rbf_centers:\n",
    "            distance = np.linalg.norm(state - center)\n",
    "            feature_val = np.exp(-distance**2 / (2 * self.rbf_width**2))\n",
    "            features.append(feature_val)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def predict(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Predict value for given state.\"\"\"\n",
    "        features = self.extract_features(state)\n",
    "        return np.dot(self.weights, features)\n",
    "    \n",
    "    def update(self, state: np.ndarray, target: float, alpha: float = 0.01) -> float:\n",
    "        \"\"\"Update weights using semi-gradient descent.\"\"\"\n",
    "        features = self.extract_features(state)\n",
    "        prediction = np.dot(self.weights, features)\n",
    "        error = target - prediction\n",
    "        \n",
    "        # Semi-gradient update\n",
    "        self.weights += alpha * error * features\n",
    "        \n",
    "        return abs(error)\n",
    "\n",
    "# Test different feature types\n",
    "test_state = np.array([0.1, -0.2, 0.05, 0.3])\n",
    "\n",
    "print(\"\\nTesting different feature extractors:\")\n",
    "print(f\"Test state: {test_state}\")\n",
    "\n",
    "# Linear features\n",
    "linear_fa = LinearFunctionApproximator(4, 'linear')\n",
    "linear_features = linear_fa.extract_features(test_state)\n",
    "print(f\"\\nLinear features ({len(linear_features)}): {linear_features}\")\n",
    "\n",
    "# Polynomial features\n",
    "poly_fa = LinearFunctionApproximator(4, 'polynomial', degree=2)\n",
    "poly_features = poly_fa.extract_features(test_state)\n",
    "print(f\"\\nPolynomial features (degree 2, {len(poly_features)}): {poly_features[:10]}...\")  # Show first 10\n",
    "\n",
    "# RBF features\n",
    "rbf_fa = LinearFunctionApproximator(4, 'rbf', n_rbf=8)\n",
    "rbf_features = rbf_fa.extract_features(test_state)\n",
    "print(f\"\\nRBF features ({len(rbf_features)}): {rbf_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semi-Gradient TD with Linear Function Approximation\n",
    "\n",
    "Let's implement a complete semi-gradient TD agent for CartPole using linear function approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTDAgent:\n",
    "    \"\"\"\n",
    "    Semi-gradient TD agent with linear function approximation.\n",
    "    \n",
    "    Implements both state-value and action-value function approximation\n",
    "    with various feature extraction methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, feature_type: str = 'polynomial', degree: int = 2, \n",
    "                 alpha: float = 0.01, gamma: float = 0.99, epsilon: float = 0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Create separate function approximators for each action\n",
    "        self.q_functions = {\n",
    "            action: LinearFunctionApproximator(\n",
    "                env.state_dim, feature_type, degree\n",
    "            ) for action in range(env.action_dim)\n",
    "        }\n",
    "        \n",
    "        # Learning statistics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "        self.weights_history = []\n",
    "    \n",
    "    def choose_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"Choose action using ε-greedy policy.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.env.action_dim)\n",
    "        else:\n",
    "            # Greedy action selection\n",
    "            q_values = [self.q_functions[a].predict(state) for a in range(self.env.action_dim)]\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def get_q_values(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get Q-values for all actions in given state.\"\"\"\n",
    "        return np.array([self.q_functions[a].predict(state) for a in range(self.env.action_dim)])\n",
    "    \n",
    "    def train_episode(self) -> Tuple[float, int]:\n",
    "        \"\"\"Train for one episode using semi-gradient TD.\"\"\"\n",
    "        state = self.env.reset()\n",
    "        state = self.env.normalize_state(state)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_td_errors = []\n",
    "        \n",
    "        while True:\n",
    "            # Choose action\n",
    "            action = self.choose_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = self.env.normalize_state(next_state)\n",
    "            \n",
    "            # Calculate TD target\n",
    "            current_q = self.q_functions[action].predict(state)\n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:\n",
    "                next_q_values = self.get_q_values(next_state)\n",
    "                target = reward + self.gamma * np.max(next_q_values)\n",
    "            \n",
    "            # Update Q-function\n",
    "            td_error = self.q_functions[action].update(state, target, self.alpha)\n",
    "            episode_td_errors.append(td_error)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Store statistics\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.td_errors.extend(episode_td_errors)\n",
    "        \n",
    "        return episode_reward, episode_length\n",
    "    \n",
    "    def train(self, num_episodes: int = 1000, verbose: bool = True) -> None:\n",
    "        \"\"\"Train the agent for multiple episodes.\"\"\"\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            reward, length = self.train_episode()\n",
    "            \n",
    "            # Track weight evolution\n",
    "            if episode % 100 == 0:\n",
    "                weights_snapshot = {\n",
    "                    action: fa.weights.copy() \n",
    "                    for action, fa in self.q_functions.items()\n",
    "                }\n",
    "                self.weights_history.append(weights_snapshot)\n",
    "            \n",
    "            # Decay epsilon\n",
    "            if self.epsilon > 0.01:\n",
    "                self.epsilon *= 0.995\n",
    "            \n",
    "            if episode % 100 == 0 and verbose:\n",
    "                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n",
    "                avg_length = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)\n",
    "                avg_td_error = np.mean(self.td_errors[-1000:]) if len(self.td_errors) >= 1000 else np.mean(self.td_errors)\n",
    "                print(f\"Episode {episode}: Avg Reward = {avg_reward:.2f}, Avg Length = {avg_length:.1f}, \"\n",
    "                      f\"TD Error = {avg_td_error:.4f}, ε = {self.epsilon:.4f}\")\n",
    "    \n",
    "    def evaluate(self, num_episodes: int = 10) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate the learned policy.\"\"\"\n",
    "        rewards = []\n",
    "        lengths = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = self.env.normalize_state(state)\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            \n",
    "            while True:\n",
    "                action = self.choose_action(state, training=False)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.env.normalize_state(next_state)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "            lengths.append(episode_length)\n",
    "        \n",
    "        return np.mean(rewards), np.mean(lengths)\n",
    "    \n",
    "    def plot_learning_curves(self) -> None:\n",
    "        \"\"\"Plot learning progress.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Episode rewards\n",
    "        window = min(50, len(self.episode_rewards) // 10)\n",
    "        if window > 1:\n",
    "            smooth_rewards = pd.Series(self.episode_rewards).rolling(window=window).mean()\n",
    "            axes[0, 0].plot(smooth_rewards, 'r-', linewidth=2, label=f'{window}-episode average')\n",
    "        \n",
    "        axes[0, 0].plot(self.episode_rewards, alpha=0.3, color='blue')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Episode Reward')\n",
    "        axes[0, 0].set_title('Learning Progress: Episode Rewards')\n",
    "        if window > 1:\n",
    "            axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Episode lengths\n",
    "        if window > 1:\n",
    "            smooth_lengths = pd.Series(self.episode_lengths).rolling(window=window).mean()\n",
    "            axes[0, 1].plot(smooth_lengths, 'r-', linewidth=2, label=f'{window}-episode average')\n",
    "        \n",
    "        axes[0, 1].plot(self.episode_lengths, alpha=0.3, color='green')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Episode Length')\n",
    "        axes[0, 1].set_title('Learning Progress: Episode Lengths')\n",
    "        if window > 1:\n",
    "            axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # TD Errors\n",
    "        if len(self.td_errors) > 100:\n",
    "            td_window = min(500, len(self.td_errors) // 20)\n",
    "            smooth_td = pd.Series(self.td_errors).rolling(window=td_window).mean()\n",
    "            axes[1, 0].plot(smooth_td, 'orange', linewidth=2)\n",
    "        else:\n",
    "            axes[1, 0].plot(self.td_errors, alpha=0.7, color='orange')\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Update Step')\n",
    "        axes[1, 0].set_ylabel('TD Error')\n",
    "        axes[1, 0].set_title('TD Error Evolution')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Weight evolution (magnitude of weights for action 0)\n",
    "        if self.weights_history:\n",
    "            weight_magnitudes = [np.linalg.norm(w[0]) for w in self.weights_history]\n",
    "            checkpoints = np.arange(0, len(self.episode_rewards), 100)[:len(weight_magnitudes)]\n",
    "            axes[1, 1].plot(checkpoints, weight_magnitudes, 'purple', marker='o', linewidth=2)\n",
    "            axes[1, 1].set_xlabel('Episode')\n",
    "            axes[1, 1].set_ylabel('Weight Magnitude (Action 0)')\n",
    "            axes[1, 1].set_title('Weight Evolution')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Linear TD agent implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear TD agent with different feature types\n",
    "print(\"Training Linear TD agents with different feature types...\")\n",
    "\n",
    "feature_types = ['polynomial', 'rbf']\n",
    "linear_results = {}\n",
    "\n",
    "for feature_type in feature_types:\n",
    "    print(f\"\\n=== Training with {feature_type} features ===\")\n",
    "    \n",
    "    agent = LinearTDAgent(\n",
    "        cartpole_env, \n",
    "        feature_type=feature_type, \n",
    "        degree=2 if feature_type == 'polynomial' else None,\n",
    "        alpha=0.01,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.1\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    agent.train(num_episodes=1000, verbose=True)\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_reward, eval_length = agent.evaluate(num_episodes=20)\n",
    "    \n",
    "    linear_results[feature_type] = {\n",
    "        'agent': agent,\n",
    "        'eval_reward': eval_reward,\n",
    "        'eval_length': eval_length,\n",
    "        'final_training_reward': np.mean(agent.episode_rewards[-100:])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{feature_type.capitalize()} Features Results:\")\n",
    "    print(f\"  Final training reward: {linear_results[feature_type]['final_training_reward']:.2f}\")\n",
    "    print(f\"  Evaluation reward: {eval_reward:.2f}\")\n",
    "    print(f\"  Evaluation length: {eval_length:.2f}\")\n",
    "\n",
    "print(\"\\n=== Linear Function Approximation Comparison ===\")\n",
    "print(f\"{'Feature Type':<15} {'Training Reward':<18} {'Eval Reward':<15} {'Eval Length':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for feature_type, results in linear_results.items():\n",
    "    print(f\"{feature_type:<15} {results['final_training_reward']:<18.2f} \"\n",
    "          f\"{results['eval_reward']:<15.2f} {results['eval_length']:<15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of linear methods\n",
    "best_linear_agent = linear_results['polynomial']['agent']  # Choose best performing\n",
    "best_linear_agent.plot_learning_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Function Approximation\n",
    "\n",
    "Neural networks provide more expressive function approximation but introduce additional challenges.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "For a neural network with parameters $\\mathbf{w} = \\{W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)}, \\ldots\\}$:\n",
    "\n",
    "$$\\hat{Q}(s, a; \\mathbf{w}) = f_{\\text{neural}}(s, a; \\mathbf{w})$$\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "The gradient is computed via **backpropagation**:\n",
    "$$\\nabla_\\mathbf{w} \\hat{Q}(s, a; \\mathbf{w}) = \\frac{\\partial f_{\\text{neural}}(s, a; \\mathbf{w})}{\\partial \\mathbf{w}}$$\n",
    "\n",
    "### Deep Q-Learning Update\n",
    "\n",
    "The neural network update becomes:\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\max_{a'} \\hat{Q}(S_{t+1}, a'; \\mathbf{w}_t) - \\hat{Q}(S_t, A_t; \\mathbf{w}_t)] \\nabla_\\mathbf{w}} \\hat{Q}(S_t, A_t; \\mathbf{w}_t)$$\n",
    "\n",
    "### Challenges with Neural Networks\n",
    "\n",
    "1. **Instability**: Non-linear approximation can lead to oscillations\n",
    "2. **Correlation**: Sequential data violates i.i.d. assumption\n",
    "3. **Moving targets**: The target values change as we update the network\n",
    "4. **Catastrophic forgetting**: New experiences can overwrite old knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network for value function approximation.\n",
    "    \n",
    "    Simple fully-connected network optimized for CPU training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [64, 64]):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        input_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)  # Light dropout for regularization\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_dim, action_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_q_values(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get Q-values for a single state.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.forward(state_tensor)\n",
    "            return q_values.squeeze(0).numpy()\n",
    "    \n",
    "    def get_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Get greedy action for a single state.\"\"\"\n",
    "        q_values = self.get_q_values(state)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "# Test the network\n",
    "test_network = DQNNetwork(state_dim=4, action_dim=2, hidden_dims=[32, 32])\n",
    "print(f\"DQN Network:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in test_network.parameters())}\")\n",
    "print(f\"  Architecture: {test_network}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_state = np.array([0.1, -0.2, 0.05, 0.3])\n",
    "test_q_values = test_network.get_q_values(test_state)\n",
    "print(f\"\\nTest state: {test_state}\")\n",
    "print(f\"Q-values: {test_q_values}\")\n",
    "print(f\"Greedy action: {test_network.get_action(test_state)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Deadly Triad and Instability\n",
    "\n",
    "The **Deadly Triad** consists of three components that can cause instability when combined:\n",
    "\n",
    "1. **Function Approximation**: Using parameterized functions instead of tables\n",
    "2. **Bootstrapping**: Using estimates to update estimates (TD learning)\n",
    "3. **Off-policy learning**: Learning about a different policy than the one being followed\n",
    "\n",
    "### Why Instability Occurs\n",
    "\n",
    "- **Moving targets**: The target $R + \\gamma \\max_{a'} Q(s', a'; \\mathbf{w})$ changes as $\\mathbf{w}$ updates\n",
    "- **Correlation**: Consecutive experiences are highly correlated\n",
    "- **Distribution shift**: The data distribution changes as the policy evolves\n",
    "- **Overestimation**: Function approximation errors can compound\n",
    "\n",
    "### Solutions\n",
    "\n",
    "1. **Experience Replay**: Store and sample experiences randomly\n",
    "2. **Target Networks**: Use separate network for computing targets\n",
    "3. **Gradient Clipping**: Limit the magnitude of gradients\n",
    "4. **Regularization**: Add constraints to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience replay buffer for storing and sampling transitions.\n",
    "    \n",
    "    Breaks the correlation between consecutive experiences by\n",
    "    storing experiences and sampling random mini-batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state: np.ndarray, action: int, reward: float, \n",
    "             next_state: np.ndarray, done: bool) -> None:\n",
    "        \"\"\"Store a transition in the buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Sample a random batch of transitions.\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            batch_size = len(self.buffer)\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Unpack batch\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "class SimpleDQNAgent:\n",
    "    \"\"\"\n",
    "    Simple DQN agent demonstrating the deadly triad and basic solutions.\n",
    "    \n",
    "    Includes experience replay and optional target networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, hidden_dims: List[int] = [32, 32], \n",
    "                 lr: float = 0.001, gamma: float = 0.99, epsilon: float = 0.1,\n",
    "                 buffer_size: int = 10000, batch_size: int = 32,\n",
    "                 use_target_network: bool = True, target_update_freq: int = 100):\n",
    "        \n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.use_target_network = use_target_network\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = DQNNetwork(env.state_dim, env.action_dim, hidden_dims)\n",
    "        if use_target_network:\n",
    "            self.target_network = DQNNetwork(env.state_dim, env.action_dim, hidden_dims)\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "            self.target_network.eval()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.replay_buffer = ExperienceReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Learning statistics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.losses = []\n",
    "        self.update_count = 0\n",
    "        \n",
    "        print(f\"DQN Agent: {sum(p.numel() for p in self.q_network.parameters())} parameters\")\n",
    "        print(f\"Target network: {use_target_network}\")\n",
    "        print(f\"Buffer size: {buffer_size}, Batch size: {batch_size}\")\n",
    "    \n",
    "    def choose_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"Choose action using ε-greedy policy.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.env.action_dim)\n",
    "        else:\n",
    "            return self.q_network.get_action(state)\n",
    "    \n",
    "    def update_target_network(self) -> None:\n",
    "        \"\"\"Update target network by copying weights from main network.\"\"\"\n",
    "        if self.use_target_network:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def train_step(self) -> float:\n",
    "        \"\"\"Perform one training step using experience replay.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Next Q values\n",
    "        with torch.no_grad():\n",
    "            if self.use_target_network:\n",
    "                next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            else:\n",
    "                next_q_values = self.q_network(next_states).max(1)[0]\n",
    "            \n",
    "            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.update_count += 1\n",
    "        if self.use_target_network and self.update_count % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train_episode(self) -> Tuple[float, int]:\n",
    "        \"\"\"Train for one episode.\"\"\"\n",
    "        state = self.env.reset()\n",
    "        state = self.env.normalize_state(state)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        while True:\n",
    "            # Choose action\n",
    "            action = self.choose_action(state, training=True)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = self.env.normalize_state(next_state)\n",
    "            \n",
    "            # Store transition\n",
    "            self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train if enough experiences\n",
    "            if len(self.replay_buffer) >= self.batch_size:\n",
    "                loss = self.train_step()\n",
    "                episode_losses.append(loss)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Store statistics\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        if episode_losses:\n",
    "            self.losses.extend(episode_losses)\n",
    "        \n",
    "        return episode_reward, episode_length\n",
    "    \n",
    "    def train(self, num_episodes: int = 1000, verbose: bool = True) -> None:\n",
    "        \"\"\"Train the agent for multiple episodes.\"\"\"\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            reward, length = self.train_episode()\n",
    "            \n",
    "            # Decay epsilon\n",
    "            if self.epsilon > 0.01:\n",
    "                self.epsilon *= 0.995\n",
    "            \n",
    "            if episode % 100 == 0 and verbose:\n",
    "                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n",
    "                avg_length = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)\n",
    "                avg_loss = np.mean(self.losses[-1000:]) if len(self.losses) >= 1000 else (np.mean(self.losses) if self.losses else 0)\n",
    "                buffer_size = len(self.replay_buffer)\n",
    "                print(f\"Episode {episode}: Avg Reward = {avg_reward:.2f}, Avg Length = {avg_length:.1f}, \"\n",
    "                      f\"Loss = {avg_loss:.4f}, Buffer = {buffer_size}, ε = {self.epsilon:.4f}\")\n",
    "    \n",
    "    def evaluate(self, num_episodes: int = 10) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate the learned policy.\"\"\"\n",
    "        rewards = []\n",
    "        lengths = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            state = self.env.normalize_state(state)\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            \n",
    "            while True:\n",
    "                action = self.choose_action(state, training=False)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.env.normalize_state(next_state)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "            lengths.append(episode_length)\n",
    "        \n",
    "        return np.mean(rewards), np.mean(lengths)\n",
    "\n",
    "print(\"DQN agent implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare DQN with and without target networks\n",
    "print(\"Training DQN agents with different configurations...\")\n",
    "\n",
    "dqn_configs = {\n",
    "    'DQN_basic': {'use_target_network': False},\n",
    "    'DQN_target': {'use_target_network': True, 'target_update_freq': 100}\n",
    "}\n",
    "\n",
    "dqn_results = {}\n",
    "\n",
    "for name, config in dqn_configs.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    \n",
    "    agent = SimpleDQNAgent(\n",
    "        cartpole_env,\n",
    "        hidden_dims=[32, 32],\n",
    "        lr=0.001,\n",
    "        gamma=0.99,\n",
    "        epsilon=0.1,\n",
    "        buffer_size=5000,\n",
    "        batch_size=32,\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    agent.train(num_episodes=800, verbose=True)\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_reward, eval_length = agent.evaluate(num_episodes=20)\n",
    "    \n",
    "    dqn_results[name] = {\n",
    "        'agent': agent,\n",
    "        'eval_reward': eval_reward,\n",
    "        'eval_length': eval_length,\n",
    "        'final_training_reward': np.mean(agent.episode_rewards[-100:])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"  Final training reward: {dqn_results[name]['final_training_reward']:.2f}\")\n",
    "    print(f\"  Evaluation reward: {eval_reward:.2f}\")\n",
    "    print(f\"  Evaluation length: {eval_length:.2f}\")\n",
    "\n",
    "print(\"\\n=== DQN Configuration Comparison ===\")\n",
    "print(f\"{'Configuration':<15} {'Training Reward':<18} {'Eval Reward':<15} {'Eval Length':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for name, results in dqn_results.items():\n",
    "    print(f\"{name:<15} {results['final_training_reward']:<18.2f} \"\n",
    "          f\"{results['eval_reward']:<15.2f} {results['eval_length']:<15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot DQN learning curves and compare with linear methods\n",
    "def plot_comprehensive_comparison():\n",
    "    \"\"\"Compare all implemented methods.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Episode rewards comparison\n",
    "    window = 50\n",
    "    \n",
    "    # Linear methods\n",
    "    for feature_type, results in linear_results.items():\n",
    "        agent = results['agent']\n",
    "        if len(agent.episode_rewards) >= window:\n",
    "            smooth_rewards = pd.Series(agent.episode_rewards).rolling(window=window).mean()\n",
    "            axes[0, 0].plot(smooth_rewards, label=f'Linear ({feature_type})', alpha=0.8)\n",
    "    \n",
    "    # DQN methods\n",
    "    for name, results in dqn_results.items():\n",
    "        agent = results['agent']\n",
    "        if len(agent.episode_rewards) >= window:\n",
    "            smooth_rewards = pd.Series(agent.episode_rewards).rolling(window=window).mean()\n",
    "            axes[0, 0].plot(smooth_rewards, label=name, alpha=0.8, linewidth=2)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Average Episode Reward')\n",
    "    axes[0, 0].set_title('Learning Progress Comparison')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    all_methods = list(linear_results.keys()) + list(dqn_results.keys())\n",
    "    all_rewards = ([linear_results[k]['eval_reward'] for k in linear_results.keys()] + \n",
    "                  [dqn_results[k]['eval_reward'] for k in dqn_results.keys()])\n",
    "    \n",
    "    colors = ['skyblue', 'lightgreen', 'orange', 'lightcoral']\n",
    "    bars = axes[0, 1].bar(range(len(all_methods)), all_rewards, \n",
    "                         color=colors[:len(all_methods)], alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Method')\n",
    "    axes[0, 1].set_ylabel('Evaluation Reward')\n",
    "    axes[0, 1].set_title('Final Performance Comparison')\n",
    "    axes[0, 1].set_xticks(range(len(all_methods)))\n",
    "    axes[0, 1].set_xticklabels(all_methods, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, reward in zip(bars, all_rewards):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                       f'{reward:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Loss evolution for DQN\n",
    "    for name, results in dqn_results.items():\n",
    "        agent = results['agent']\n",
    "        if agent.losses:\n",
    "            loss_window = min(100, len(agent.losses) // 10)\n",
    "            if loss_window > 1:\n",
    "                smooth_loss = pd.Series(agent.losses).rolling(window=loss_window).mean()\n",
    "                axes[1, 0].plot(smooth_loss, label=name, alpha=0.8)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Update Step')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('DQN Loss Evolution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sample complexity comparison\n",
    "    method_episodes = []\n",
    "    for method in all_methods:\n",
    "        if method in linear_results:\n",
    "            episodes = len(linear_results[method]['agent'].episode_rewards)\n",
    "        else:\n",
    "            episodes = len(dqn_results[method]['agent'].episode_rewards)\n",
    "        method_episodes.append(episodes)\n",
    "    \n",
    "    bars2 = axes[1, 1].bar(range(len(all_methods)), method_episodes,\n",
    "                          color=colors[:len(all_methods)], alpha=0.8)\n",
    "    axes[1, 1].set_xlabel('Method')\n",
    "    axes[1, 1].set_ylabel('Training Episodes')\n",
    "    axes[1, 1].set_title('Sample Complexity')\n",
    "    axes[1, 1].set_xticks(range(len(all_methods)))\n",
    "    axes[1, 1].set_xticklabels(all_methods, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_comprehensive_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Theoretical Analysis\n",
    "\n",
    "Let's analyze the results and extract key insights about function approximation in RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_function_approximation_results():\n",
    "    \"\"\"Analyze and summarize insights from function approximation experiments.\"\"\"\n",
    "    \n",
    "    print(\"=== Function Approximation Analysis ===\")\n",
    "    print()\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(\"1. PERFORMANCE COMPARISON:\")\n",
    "    all_results = {**linear_results, **dqn_results}\n",
    "    \n",
    "    best_method = max(all_results.keys(), key=lambda x: all_results[x]['eval_reward'])\n",
    "    worst_method = min(all_results.keys(), key=lambda x: all_results[x]['eval_reward'])\n",
    "    \n",
    "    print(f\"   • Best performing method: {best_method} ({all_results[best_method]['eval_reward']:.2f} reward)\")\n",
    "    print(f\"   • Worst performing method: {worst_method} ({all_results[worst_method]['eval_reward']:.2f} reward)\")\n",
    "    print(f\"   • Performance gap: {all_results[best_method]['eval_reward'] - all_results[worst_method]['eval_reward']:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. LINEAR VS NEURAL NETWORK APPROXIMATION:\")\n",
    "    linear_avg = np.mean([linear_results[k]['eval_reward'] for k in linear_results.keys()])\n",
    "    dqn_avg = np.mean([dqn_results[k]['eval_reward'] for k in dqn_results.keys()])\n",
    "    \n",
    "    print(f\"   • Average linear performance: {linear_avg:.2f}\")\n",
    "    print(f\"   • Average DQN performance: {dqn_avg:.2f}\")\n",
    "    \n",
    "    if dqn_avg > linear_avg:\n",
    "        print(f\"   • Neural networks outperform linear methods by {dqn_avg - linear_avg:.2f}\")\n",
    "    else:\n",
    "        print(f\"   • Linear methods outperform neural networks by {linear_avg - dqn_avg:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. FEATURE ENGINEERING IMPACT:\")\n",
    "    if 'polynomial' in linear_results and 'rbf' in linear_results:\n",
    "        poly_perf = linear_results['polynomial']['eval_reward']\n",
    "        rbf_perf = linear_results['rbf']['eval_reward']\n",
    "        print(f\"   • Polynomial features: {poly_perf:.2f}\")\n",
    "        print(f\"   • RBF features: {rbf_perf:.2f}\")\n",
    "        print(f\"   • Feature choice impact: {abs(poly_perf - rbf_perf):.2f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. TARGET NETWORK IMPACT:\")\n",
    "    if 'DQN_basic' in dqn_results and 'DQN_target' in dqn_results:\n",
    "        basic_perf = dqn_results['DQN_basic']['eval_reward']\n",
    "        target_perf = dqn_results['DQN_target']['eval_reward']\n",
    "        print(f\"   • DQN without target network: {basic_perf:.2f}\")\n",
    "        print(f\"   • DQN with target network: {target_perf:.2f}\")\n",
    "        print(f\"   • Target network improvement: {target_perf - basic_perf:.2f}\")\n",
    "        \n",
    "        if target_perf > basic_perf:\n",
    "            print(\"   • Target networks provide stability and improved performance\")\n",
    "        else:\n",
    "            print(\"   • Target networks show minimal or negative impact (possibly due to environment simplicity)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"5. SAMPLE COMPLEXITY:\")\n",
    "    for method, results in all_results.items():\n",
    "        if method in linear_results:\n",
    "            episodes = len(linear_results[method]['agent'].episode_rewards)\n",
    "        else:\n",
    "            episodes = len(dqn_results[method]['agent'].episode_rewards)\n",
    "        \n",
    "        print(f\"   • {method}: {episodes} episodes to convergence\")\n",
    "    print()\n",
    "    \n",
    "    print(\"6. THEORETICAL INSIGHTS:\")\n",
    "    print(\"   • Linear function approximation:\")\n",
    "    print(\"     - Provides convergence guarantees under certain conditions\")\n",
    "    print(\"     - Limited expressiveness requires good feature engineering\")\n",
    "    print(\"     - Stable learning with proper step sizes\")\n",
    "    print(\"   • Neural network approximation:\")\n",
    "    print(\"     - Higher expressiveness but no convergence guarantees\")\n",
    "    print(\"     - Requires careful hyperparameter tuning\")\n",
    "    print(\"     - Prone to instability (deadly triad)\")\n",
    "    print(\"   • Experience replay:\")\n",
    "    print(\"     - Breaks correlation in sequential data\")\n",
    "    print(\"     - Improves sample efficiency\")\n",
    "    print(\"     - Essential for stable neural network training\")\n",
    "    print(\"   • Target networks:\")\n",
    "    print(\"     - Reduce non-stationarity in target values\")\n",
    "    print(\"     - Provide more stable learning\")\n",
    "    print(\"     - May slow adaptation to environment changes\")\n",
    "    print()\n",
    "    \n",
    "    print(\"7. PRACTICAL RECOMMENDATIONS:\")\n",
    "    print(\"   • For simple environments: Linear methods may suffice\")\n",
    "    print(\"   • For complex environments: Neural networks are necessary\")\n",
    "    print(\"   • Always use experience replay with neural networks\")\n",
    "    print(\"   • Consider target networks for stability\")\n",
    "    print(\"   • Feature engineering is crucial for linear methods\")\n",
    "    print(\"   • Monitor for instability and adjust hyperparameters accordingly\")\n",
    "\n",
    "analyze_function_approximation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preparing for Deep Reinforcement Learning\n",
    "\n",
    "Let's prepare the foundations for the next notebook by testing our implementations on a slightly more complex environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up environments\n",
    "cartpole_env.close()\n",
    "\n",
    "print(\"=== Transition to Deep RL - Key Takeaways ===\")\n",
    "print()\n",
    "print(\"From this notebook, we've learned:\")\n",
    "print()\n",
    "print(\"1. **Function Approximation Necessity**:\")\n",
    "print(\"   - Tabular methods don't scale to large state spaces\")\n",
    "print(\"   - Function approximation enables generalization\")\n",
    "print(\"   - Trade-off between expressiveness and stability\")\n",
    "print()\n",
    "print(\"2. **Linear Function Approximation**:\")\n",
    "print(\"   - Provides theoretical guarantees\")\n",
    "print(\"   - Requires good feature engineering\")\n",
    "print(\"   - Limited expressiveness for complex environments\")\n",
    "print()\n",
    "print(\"3. **Neural Network Approximation**:\")\n",
    "print(\"   - High expressiveness and automatic feature learning\")\n",
    "print(\"   - Introduces instability challenges (deadly triad)\")\n",
    "print(\"   - Requires sophisticated techniques for stable learning\")\n",
    "print()\n",
    "print(\"4. **Key Solutions for Stability**:\")\n",
    "print(\"   - Experience replay breaks correlation\")\n",
    "print(\"   - Target networks reduce non-stationarity\")\n",
    "print(\"   - Gradient clipping prevents exploding gradients\")\n",
    "print(\"   - Regularization prevents overfitting\")\n",
    "print()\n",
    "print(\"5. **Ready for Deep RL**:\")\n",
    "print(\"   - Understand function approximation theory\")\n",
    "print(\"   - Implemented basic DQN components\")\n",
    "print(\"   - Recognize instability issues and solutions\")\n",
    "print(\"   - Have foundation for advanced algorithms\")\n",
    "print()\n",
    "print(\"Next: Deep Q-Learning with advanced techniques and complex environments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive notebook, we've successfully bridged the gap between tabular RL and deep reinforcement learning:\n",
    "\n",
    "### **Function Approximation Theory**\n",
    "- **Curse of Dimensionality**: Understanding why tabular methods fail for large state spaces\n",
    "- **Linear Approximation**: Mathematical foundation with $\\hat{V}(s, \\mathbf{w}) = \\mathbf{w}^T \\phi(s)$\n",
    "- **Semi-Gradient Methods**: TD learning with function approximation\n",
    "- **Feature Engineering**: Polynomial, RBF, and custom feature extraction\n",
    "\n",
    "### **Neural Network Foundations**\n",
    "- **Deep Q-Networks**: Neural network implementation for value function approximation\n",
    "- **Backpropagation**: Gradient computation for complex function approximators\n",
    "- **Network Architecture**: Design choices for RL applications\n",
    "\n",
    "### **The Deadly Triad**\n",
    "- **Three Components**: Function approximation + Bootstrapping + Off-policy learning\n",
    "- **Instability Issues**: Moving targets, correlation, distribution shift\n",
    "- **Practical Solutions**: Experience replay, target networks, gradient clipping\n",
    "\n",
    "### **Experience Replay**\n",
    "- **Breaking Correlation**: Random sampling from stored experiences\n",
    "- **Implementation**: Efficient buffer management and batch sampling\n",
    "- **Benefits**: Improved sample efficiency and stability\n",
    "\n",
    "### **Target Networks**\n",
    "- **Concept**: Separate network for computing target values\n",
    "- **Stability**: Reducing non-stationarity in learning targets\n",
    "- **Update Strategy**: Periodic copying of main network weights\n",
    "\n",
    "### **Comparative Analysis**\n",
    "- **Linear vs Neural**: Trade-offs between simplicity and expressiveness\n",
    "- **Feature Impact**: Importance of representation in linear methods\n",
    "- **Stability Techniques**: Empirical validation of theoretical solutions\n",
    "\n",
    "### **Practical Insights**\n",
    "- **Environment Complexity**: Matching method sophistication to problem difficulty\n",
    "- **Hyperparameter Sensitivity**: Critical parameters for stable learning\n",
    "- **Implementation Details**: CPU optimization for MacBook Air M2\n",
    "\n",
    "This foundation prepares us for deep Q-learning with advanced techniques like Double DQN, Dueling DQN, and Rainbow improvements in the next notebook. We now understand both the theoretical necessity and practical implementation of function approximation in reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}