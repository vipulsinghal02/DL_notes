{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Plan B - Part 1: RL Fundamentals & Tabular Methods\n",
    "\n",
    "This notebook introduces the mathematical foundations of Reinforcement Learning and implements core tabular methods. We'll cover Markov Decision Processes, Bellman equations, and fundamental algorithms like value iteration, policy iteration, Q-learning, and SARSA.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the mathematical framework of MDPs\n",
    "- Derive and implement Bellman equations\n",
    "- Compare dynamic programming vs temporal difference methods\n",
    "- Build intuition through GridWorld environments\n",
    "- Analyze convergence properties and performance trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Markov Decision Processes (MDPs)\n",
    "\n",
    "A **Markov Decision Process** is the mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "An MDP is defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$:\n",
    "\n",
    "- **$\\mathcal{S}$**: State space - the set of all possible states\n",
    "- **$\\mathcal{A}$**: Action space - the set of all possible actions  \n",
    "- **$P$**: Transition probability function: $P(s'|s,a) = \\mathbb{P}[S_{t+1} = s' | S_t = s, A_t = a]$\n",
    "- **$R$**: Reward function: $R(s,a,s') = \\mathbb{E}[R_{t+1} | S_t = s, A_t = a, S_{t+1} = s']$\n",
    "- **$\\gamma$**: Discount factor: $\\gamma \\in [0,1]$\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "**Markov Property**: The future is independent of the past given the present:\n",
    "$$\\mathbb{P}[S_{t+1} = s' | S_t = s, A_t = a, S_{t-1}, A_{t-1}, \\ldots, S_0, A_0] = \\mathbb{P}[S_{t+1} = s' | S_t = s, A_t = a]$$\n",
    "\n",
    "**Return**: The cumulative discounted reward from time $t$:\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "The discount factor $\\gamma$ balances immediate vs future rewards:\n",
    "- $\\gamma = 0$: Only immediate rewards matter (myopic)\n",
    "- $\\gamma = 1$: All future rewards are equally important  \n",
    "- $\\gamma < 1$: Future rewards are discounted exponentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GridWorld Environment Implementation\n",
    "\n",
    "We'll implement a classic GridWorld environment to demonstrate RL concepts. This serves as our testing ground for various algorithms.\n",
    "\n",
    "### Environment Dynamics\n",
    "\n",
    "- **States**: Grid positions $(i,j)$\n",
    "- **Actions**: Up, Down, Left, Right\n",
    "- **Transitions**: Deterministic movement (with boundary handling)\n",
    "- **Rewards**: Goal state gives positive reward, obstacles negative, others zero\n",
    "- **Terminal States**: Goal and obstacle states end episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    GridWorld environment for reinforcement learning experiments.\n",
    "    \n",
    "    The agent navigates a grid to reach a goal while avoiding obstacles.\n",
    "    This environment serves as a perfect testbed for tabular RL methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, height: int = 5, width: int = 5, goal_reward: float = 1.0, \n",
    "                 step_reward: float = -0.01, obstacle_reward: float = -1.0):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.goal_reward = goal_reward\n",
    "        self.step_reward = step_reward\n",
    "        self.obstacle_reward = obstacle_reward\n",
    "        \n",
    "        # Define actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "        self.actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # (dx, dy)\n",
    "        self.action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "        self.num_actions = len(self.actions)\n",
    "        \n",
    "        # Set up grid layout\n",
    "        self.goal_state = (height-1, width-1)  # Bottom-right corner\n",
    "        self.start_state = (0, 0)  # Top-left corner\n",
    "        \n",
    "        # Define obstacles (can be customized)\n",
    "        self.obstacles = {(2, 2), (1, 3)} if height >= 4 and width >= 4 else set()\n",
    "        \n",
    "        # Current state\n",
    "        self.current_state = self.start_state\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        \"\"\"Reset environment to starting state.\"\"\"\n",
    "        self.current_state = self.start_state\n",
    "        self.done = False\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, dict]:\n",
    "        \"\"\"Execute action and return (next_state, reward, done, info).\"\"\"\n",
    "        if self.done:\n",
    "            return self.current_state, 0, True, {}\n",
    "        \n",
    "        # Calculate next state\n",
    "        dx, dy = self.actions[action]\n",
    "        next_x = max(0, min(self.width - 1, self.current_state[0] + dx))\n",
    "        next_y = max(0, min(self.height - 1, self.current_state[1] + dy))\n",
    "        next_state = (next_x, next_y)\n",
    "        \n",
    "        # Calculate reward\n",
    "        if next_state == self.goal_state:\n",
    "            reward = self.goal_reward\n",
    "            self.done = True\n",
    "        elif next_state in self.obstacles:\n",
    "            reward = self.obstacle_reward\n",
    "            self.done = True\n",
    "        else:\n",
    "            reward = self.step_reward\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return next_state, reward, self.done, {}\n",
    "    \n",
    "    def get_all_states(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Return all possible states.\"\"\"\n",
    "        return [(i, j) for i in range(self.width) for j in range(self.height)]\n",
    "    \n",
    "    def is_terminal(self, state: Tuple[int, int]) -> bool:\n",
    "        \"\"\"Check if state is terminal.\"\"\"\n",
    "        return state == self.goal_state or state in self.obstacles\n",
    "    \n",
    "    def get_transition_prob(self, state: Tuple[int, int], action: int, \n",
    "                          next_state: Tuple[int, int]) -> float:\n",
    "        \"\"\"Get transition probability P(s'|s,a).\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return 1.0 if next_state == state else 0.0\n",
    "        \n",
    "        # Calculate expected next state\n",
    "        dx, dy = self.actions[action]\n",
    "        expected_next_x = max(0, min(self.width - 1, state[0] + dx))\n",
    "        expected_next_y = max(0, min(self.height - 1, state[1] + dy))\n",
    "        expected_next_state = (expected_next_x, expected_next_y)\n",
    "        \n",
    "        return 1.0 if next_state == expected_next_state else 0.0\n",
    "    \n",
    "    def get_reward(self, state: Tuple[int, int], action: int, \n",
    "                   next_state: Tuple[int, int]) -> float:\n",
    "        \"\"\"Get reward R(s,a,s').\"\"\"\n",
    "        if next_state == self.goal_state:\n",
    "            return self.goal_reward\n",
    "        elif next_state in self.obstacles:\n",
    "            return self.obstacle_reward\n",
    "        else:\n",
    "            return self.step_reward\n",
    "    \n",
    "    def render(self, values: Optional[Dict] = None, policy: Optional[Dict] = None) -> None:\n",
    "        \"\"\"Visualize the grid world with optional value function or policy.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # Create grid visualization\n",
    "        grid = np.zeros((self.height, self.width))\n",
    "        \n",
    "        if values:\n",
    "            for (x, y), value in values.items():\n",
    "                grid[y, x] = value\n",
    "        \n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(grid, cmap='coolwarm', alpha=0.7)\n",
    "        \n",
    "        # Add grid lines\n",
    "        ax.set_xticks(np.arange(self.width + 1) - 0.5, minor=True)\n",
    "        ax.set_yticks(np.arange(self.height + 1) - 0.5, minor=True)\n",
    "        ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
    "        \n",
    "        # Mark special states\n",
    "        start_y, start_x = self.start_state[1], self.start_state[0]\n",
    "        goal_y, goal_x = self.goal_state[1], self.goal_state[0]\n",
    "        \n",
    "        ax.text(start_x, start_y, 'S', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='green')\n",
    "        ax.text(goal_x, goal_y, 'G', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='red')\n",
    "        \n",
    "        # Mark obstacles\n",
    "        for (obs_x, obs_y) in self.obstacles:\n",
    "            ax.text(obs_x, obs_y, 'X', ha='center', va='center', \n",
    "                    fontsize=16, fontweight='bold', color='black')\n",
    "        \n",
    "        # Add policy arrows if provided\n",
    "        if policy:\n",
    "            arrow_props = dict(arrowstyle='->', lw=2, color='blue')\n",
    "            for (x, y), action in policy.items():\n",
    "                if not self.is_terminal((x, y)):\n",
    "                    dx, dy = self.actions[action]\n",
    "                    ax.annotate('', xy=(x + dx*0.3, y + dy*0.3), xytext=(x, y),\n",
    "                              arrowprops=arrow_props)\n",
    "        \n",
    "        # Add value labels if provided\n",
    "        if values:\n",
    "            for (x, y), value in values.items():\n",
    "                if not self.is_terminal((x, y)):\n",
    "                    ax.text(x, y + 0.3, f'{value:.2f}', ha='center', va='center', \n",
    "                            fontsize=10, color='white', fontweight='bold')\n",
    "        \n",
    "        ax.set_title('GridWorld Environment')\n",
    "        ax.set_xticks(range(self.width))\n",
    "        ax.set_yticks(range(self.height))\n",
    "        \n",
    "        if values:\n",
    "            plt.colorbar(im, ax=ax, label='State Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize the environment\n",
    "env = GridWorld(height=5, width=5)\n",
    "print(f\"GridWorld created: {env.width}x{env.height}\")\n",
    "print(f\"Start: {env.start_state}, Goal: {env.goal_state}\")\n",
    "print(f\"Obstacles: {env.obstacles}\")\n",
    "print(f\"Actions: {env.action_names}\")\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Value Functions and Bellman Equations\n",
    "\n",
    "Value functions are fundamental to RL - they estimate how good it is to be in a particular state or to take a particular action in a state.\n",
    "\n",
    "### State Value Function\n",
    "\n",
    "The **state value function** $V^\\pi(s)$ gives the expected return when starting from state $s$ and following policy $\\pi$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$$\n",
    "\n",
    "### Action Value Function (Q-Function)\n",
    "\n",
    "The **action value function** $Q^\\pi(s,a)$ gives the expected return when starting from state $s$, taking action $a$, then following policy $\\pi$:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a] = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$$\n",
    "\n",
    "### Bellman Equations\n",
    "\n",
    "The **Bellman equation** for $V^\\pi$ expresses the recursive relationship:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "The **Bellman equation** for $Q^\\pi$:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')]$$\n",
    "\n",
    "### Optimal Value Functions\n",
    "\n",
    "The **optimal state value function** is:\n",
    "$$V^*(s) = \\max_\\pi V^\\pi(s)$$\n",
    "\n",
    "The **optimal action value function** is:\n",
    "$$Q^*(s,a) = \\max_\\pi Q^\\pi(s,a)$$\n",
    "\n",
    "### Bellman Optimality Equations\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "$$Q^*(s,a) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic Programming: Value Iteration\n",
    "\n",
    "**Value Iteration** is a dynamic programming algorithm that computes the optimal value function by iteratively applying the Bellman optimality equation.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Initialize $V_0(s)$ arbitrarily for all $s \\in \\mathcal{S}$\n",
    "2. For $k = 1, 2, 3, \\ldots$ until convergence:\n",
    "   $$V_{k+1}(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V_k(s')]$$\n",
    "3. Extract optimal policy: $\\pi^*(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^*(s')]$\n",
    "\n",
    "### Convergence\n",
    "\n",
    "Value iteration converges to $V^*$ under the **contraction mapping theorem**. The Bellman operator $T$ defined by:\n",
    "$$TV(s) = \\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V(s')]$$\n",
    "\n",
    "is a $\\gamma$-contraction, meaning:\n",
    "$$\\|TV_1 - TV_2\\|_\\infty \\leq \\gamma \\|V_1 - V_2\\|_\\infty$$\n",
    "\n",
    "This guarantees convergence when $\\gamma < 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm for solving MDPs.\n",
    "    \n",
    "    This dynamic programming approach iteratively updates value estimates\n",
    "    until convergence to the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: GridWorld, gamma: float = 0.9, theta: float = 1e-6):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta  # Convergence threshold\n",
    "        \n",
    "        self.states = env.get_all_states()\n",
    "        self.num_states = len(self.states)\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        # Initialize value function\n",
    "        self.V = {state: 0.0 for state in self.states}\n",
    "        self.policy = {state: 0 for state in self.states}\n",
    "        \n",
    "        # Track learning progress\n",
    "        self.value_history = []\n",
    "        self.delta_history = []\n",
    "    \n",
    "    def bellman_update(self, state: Tuple[int, int]) -> float:\n",
    "        \"\"\"Perform Bellman update for a single state.\"\"\"\n",
    "        if self.env.is_terminal(state):\n",
    "            return 0.0\n",
    "        \n",
    "        action_values = []\n",
    "        \n",
    "        for action in range(self.num_actions):\n",
    "            action_value = 0.0\n",
    "            \n",
    "            # Sum over all possible next states\n",
    "            for next_state in self.states:\n",
    "                prob = self.env.get_transition_prob(state, action, next_state)\n",
    "                if prob > 0:\n",
    "                    reward = self.env.get_reward(state, action, next_state)\n",
    "                    action_value += prob * (reward + self.gamma * self.V[next_state])\n",
    "            \n",
    "            action_values.append(action_value)\n",
    "        \n",
    "        return max(action_values)\n",
    "    \n",
    "    def solve(self, max_iterations: int = 1000, verbose: bool = True) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Solve MDP using value iteration.\"\"\"\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            # Store current values for convergence check\n",
    "            old_V = self.V.copy()\n",
    "            \n",
    "            # Update all state values\n",
    "            for state in self.states:\n",
    "                self.V[state] = self.bellman_update(state)\n",
    "            \n",
    "            # Check convergence\n",
    "            delta = max(abs(self.V[s] - old_V[s]) for s in self.states)\n",
    "            self.delta_history.append(delta)\n",
    "            self.value_history.append(self.V.copy())\n",
    "            \n",
    "            if verbose and iteration % 10 == 0:\n",
    "                print(f\"Iteration {iteration}, Max value change: {delta:.6f}\")\n",
    "            \n",
    "            if delta < self.theta:\n",
    "                if verbose:\n",
    "                    print(f\"\\nConverged after {iteration + 1} iterations!\")\n",
    "                break\n",
    "        \n",
    "        # Extract optimal policy\n",
    "        self.extract_policy()\n",
    "        \n",
    "        return self.V, self.policy\n",
    "    \n",
    "    def extract_policy(self) -> None:\n",
    "        \"\"\"Extract optimal policy from value function.\"\"\"\n",
    "        for state in self.states:\n",
    "            if self.env.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            action_values = []\n",
    "            \n",
    "            for action in range(self.num_actions):\n",
    "                action_value = 0.0\n",
    "                \n",
    "                for next_state in self.states:\n",
    "                    prob = self.env.get_transition_prob(state, action, next_state)\n",
    "                    if prob > 0:\n",
    "                        reward = self.env.get_reward(state, action, next_state)\n",
    "                        action_value += prob * (reward + self.gamma * self.V[next_state])\n",
    "                \n",
    "                action_values.append(action_value)\n",
    "            \n",
    "            # Choose action with highest value\n",
    "            self.policy[state] = np.argmax(action_values)\n",
    "    \n",
    "    def plot_convergence(self) -> None:\n",
    "        \"\"\"Plot convergence of value iteration.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot max value change over iterations\n",
    "        ax1.plot(self.delta_history)\n",
    "        ax1.set_xlabel('Iteration')\n",
    "        ax1.set_ylabel('Max Value Change')\n",
    "        ax1.set_title('Value Iteration Convergence')\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot value function evolution for a few states\n",
    "        sample_states = [self.env.start_state, (2, 2), (3, 3)]\n",
    "        for state in sample_states:\n",
    "            if state in self.states and not self.env.is_terminal(state):\n",
    "                values = [v_dict[state] for v_dict in self.value_history]\n",
    "                ax2.plot(values, label=f'State {state}')\n",
    "        \n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('State Value')\n",
    "        ax2.set_title('Value Function Evolution')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Solve GridWorld with Value Iteration\n",
    "print(\"Solving GridWorld with Value Iteration...\")\n",
    "vi_solver = ValueIteration(env, gamma=0.9)\n",
    "optimal_values, optimal_policy = vi_solver.solve()\n",
    "\n",
    "print(f\"\\nOptimal Values (sample):\")\n",
    "for i, (state, value) in enumerate(list(optimal_values.items())[:8]):\n",
    "    print(f\"V*{state} = {value:.4f}\")\n",
    "\n",
    "print(f\"\\nOptimal Policy (sample):\")\n",
    "for i, (state, action) in enumerate(list(optimal_policy.items())[:8]):\n",
    "    if not env.is_terminal(state):\n",
    "        print(f\"π*{state} = {env.action_names[action]}\")\n",
    "\n",
    "# Plot convergence\n",
    "vi_solver.plot_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the solution\n",
    "print(\"Optimal Value Function:\")\n",
    "env.render(values=optimal_values)\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "env.render(policy=optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dynamic Programming: Policy Iteration\n",
    "\n",
    "**Policy Iteration** alternates between policy evaluation and policy improvement until convergence to the optimal policy.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. **Initialize** policy $\\pi_0$ arbitrarily\n",
    "2. **Repeat** until policy converges:\n",
    "   - **Policy Evaluation**: Compute $V^{\\pi_k}$ by solving:\n",
    "     $$V^{\\pi_k}(s) = \\sum_a \\pi_k(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^{\\pi_k}(s')]$$\n",
    "   - **Policy Improvement**: Update policy greedily:\n",
    "     $$\\pi_{k+1}(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^{\\pi_k}(s')]$$\n",
    "\n",
    "### Policy Improvement Theorem\n",
    "\n",
    "If $\\pi'$ is the greedy policy with respect to $V^\\pi$, then:\n",
    "$$V^{\\pi'}(s) \\geq V^\\pi(s) \\text{ for all } s$$\n",
    "\n",
    "This guarantees that policy iteration converges to the optimal policy in finite steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \"\"\"\n",
    "    Policy Iteration algorithm for solving MDPs.\n",
    "    \n",
    "    Alternates between policy evaluation and policy improvement\n",
    "    until convergence to the optimal policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: GridWorld, gamma: float = 0.9, theta: float = 1e-6):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        \n",
    "        self.states = env.get_all_states()\n",
    "        self.num_states = len(self.states)\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        # Initialize random policy and zero values\n",
    "        self.V = {state: 0.0 for state in self.states}\n",
    "        self.policy = {state: np.random.randint(self.num_actions) for state in self.states}\n",
    "        \n",
    "        # Track progress\n",
    "        self.policy_history = []\n",
    "        self.value_history = []\n",
    "    \n",
    "    def policy_evaluation(self, max_iterations: int = 1000) -> None:\n",
    "        \"\"\"Evaluate current policy until convergence.\"\"\"\n",
    "        for iteration in range(max_iterations):\n",
    "            old_V = self.V.copy()\n",
    "            \n",
    "            for state in self.states:\n",
    "                if self.env.is_terminal(state):\n",
    "                    self.V[state] = 0.0\n",
    "                    continue\n",
    "                \n",
    "                action = self.policy[state]\n",
    "                value = 0.0\n",
    "                \n",
    "                for next_state in self.states:\n",
    "                    prob = self.env.get_transition_prob(state, action, next_state)\n",
    "                    if prob > 0:\n",
    "                        reward = self.env.get_reward(state, action, next_state)\n",
    "                        value += prob * (reward + self.gamma * self.V[next_state])\n",
    "                \n",
    "                self.V[state] = value\n",
    "            \n",
    "            # Check convergence\n",
    "            delta = max(abs(self.V[s] - old_V[s]) for s in self.states)\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "    \n",
    "    def policy_improvement(self) -> bool:\n",
    "        \"\"\"Improve policy greedily. Returns True if policy changed.\"\"\"\n",
    "        old_policy = self.policy.copy()\n",
    "        \n",
    "        for state in self.states:\n",
    "            if self.env.is_terminal(state):\n",
    "                continue\n",
    "            \n",
    "            action_values = []\n",
    "            \n",
    "            for action in range(self.num_actions):\n",
    "                action_value = 0.0\n",
    "                \n",
    "                for next_state in self.states:\n",
    "                    prob = self.env.get_transition_prob(state, action, next_state)\n",
    "                    if prob > 0:\n",
    "                        reward = self.env.get_reward(state, action, next_state)\n",
    "                        action_value += prob * (reward + self.gamma * self.V[next_state])\n",
    "                \n",
    "                action_values.append(action_value)\n",
    "            \n",
    "            self.policy[state] = np.argmax(action_values)\n",
    "        \n",
    "        # Check if policy changed\n",
    "        policy_changed = any(old_policy[s] != self.policy[s] for s in self.states\n",
    "                           if not self.env.is_terminal(s))\n",
    "        \n",
    "        return policy_changed\n",
    "    \n",
    "    def solve(self, max_iterations: int = 100, verbose: bool = True) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Solve MDP using policy iteration.\"\"\"\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            if verbose:\n",
    "                print(f\"Policy Iteration {iteration + 1}\")\n",
    "            \n",
    "            # Policy Evaluation\n",
    "            self.policy_evaluation()\n",
    "            self.value_history.append(self.V.copy())\n",
    "            \n",
    "            # Policy Improvement\n",
    "            policy_changed = self.policy_improvement()\n",
    "            self.policy_history.append(self.policy.copy())\n",
    "            \n",
    "            if not policy_changed:\n",
    "                if verbose:\n",
    "                    print(f\"\\nPolicy converged after {iteration + 1} iterations!\")\n",
    "                break\n",
    "        \n",
    "        return self.V, self.policy\n",
    "    \n",
    "    def plot_progress(self) -> None:\n",
    "        \"\"\"Plot policy iteration progress.\"\"\"\n",
    "        fig, axes = plt.subplots(1, min(len(self.policy_history), 4), figsize=(16, 4))\n",
    "        \n",
    "        if len(self.policy_history) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, policy in enumerate(self.policy_history[:4]):\n",
    "            ax = axes[i] if len(self.policy_history) > 1 else axes[0]\n",
    "            \n",
    "            # Create policy visualization\n",
    "            policy_grid = np.zeros((self.env.height, self.env.width))\n",
    "            \n",
    "            for (x, y), action in policy.items():\n",
    "                if not self.env.is_terminal((x, y)):\n",
    "                    policy_grid[y, x] = action\n",
    "            \n",
    "            im = ax.imshow(policy_grid, cmap='tab10', alpha=0.7)\n",
    "            \n",
    "            # Add arrows\n",
    "            arrow_props = dict(arrowstyle='->', lw=2, color='blue')\n",
    "            for (x, y), action in policy.items():\n",
    "                if not self.env.is_terminal((x, y)):\n",
    "                    dx, dy = self.env.actions[action]\n",
    "                    ax.annotate('', xy=(x + dx*0.3, y + dy*0.3), xytext=(x, y),\n",
    "                              arrowprops=arrow_props)\n",
    "            \n",
    "            ax.set_title(f'Policy Iteration {i + 1}')\n",
    "            ax.set_xticks(range(self.env.width))\n",
    "            ax.set_yticks(range(self.env.height))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Solve with Policy Iteration\n",
    "print(\"Solving GridWorld with Policy Iteration...\")\n",
    "pi_solver = PolicyIteration(env, gamma=0.9)\n",
    "pi_values, pi_policy = pi_solver.solve()\n",
    "\n",
    "print(f\"\\nPolicy Iteration completed!\")\n",
    "print(f\"Number of policy iterations: {len(pi_solver.policy_history)}\")\n",
    "\n",
    "# Compare with Value Iteration\n",
    "print(\"\\nComparison with Value Iteration:\")\n",
    "print(f\"Value difference (max): {max(abs(optimal_values[s] - pi_values[s]) for s in env.get_all_states()):.8f}\")\n",
    "\n",
    "policy_diff = sum(1 for s in env.get_all_states() \n",
    "                  if not env.is_terminal(s) and optimal_policy[s] != pi_policy[s])\n",
    "print(f\"Policy differences: {policy_diff} states\")\n",
    "\n",
    "# Plot policy evolution\n",
    "pi_solver.plot_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Q-Learning: Model-Free Temporal Difference Learning\n",
    "\n",
    "**Q-Learning** is an off-policy temporal difference learning algorithm that directly learns the optimal action-value function without needing a model of the environment.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Q-Learning uses the following update rule:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha \\in (0,1]$ is the learning rate\n",
    "- $R$ is the immediate reward\n",
    "- $s'$ is the next state\n",
    "- The term $[R + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$ is called the **TD error**\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Off-policy**: Q-learning can learn the optimal policy while following any exploratory policy\n",
    "2. **Model-free**: No need to know transition probabilities or reward function\n",
    "3. **Convergence**: Under certain conditions (visiting all state-action pairs infinitely often, decreasing learning rate), Q-learning converges to $Q^*$\n",
    "\n",
    "### Exploration vs Exploitation\n",
    "\n",
    "Q-learning requires balancing exploration and exploitation. Common strategies:\n",
    "\n",
    "- **ε-greedy**: Choose random action with probability $\\epsilon$, otherwise choose $\\arg\\max_a Q(s,a)$\n",
    "- **ε-decay**: Decrease $\\epsilon$ over time to reduce exploration\n",
    "- **Boltzmann exploration**: Choose actions probabilistically based on Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for model-free reinforcement learning.\n",
    "    \n",
    "    Learns optimal action-value function through temporal difference updates\n",
    "    without requiring knowledge of environment dynamics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: GridWorld, alpha: float = 0.1, gamma: float = 0.9, \n",
    "                 epsilon: float = 0.1, epsilon_decay: float = 0.995, \n",
    "                 epsilon_min: float = 0.01):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.states = env.get_all_states()\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        \n",
    "        # Track learning progress\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "        self.q_value_history = []\n",
    "    \n",
    "    def choose_action(self, state: Tuple[int, int], training: bool = True) -> int:\n",
    "        \"\"\"Choose action using ε-greedy policy.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update_q_value(self, state: Tuple[int, int], action: int, reward: float, \n",
    "                       next_state: Tuple[int, int], done: bool) -> float:\n",
    "        \"\"\"Update Q-value using Q-learning rule.\"\"\"\n",
    "        # Current Q-value\n",
    "        current_q = self.Q[state][action]\n",
    "        \n",
    "        # Target Q-value\n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            target_q = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        \n",
    "        # TD error\n",
    "        td_error = target_q - current_q\n",
    "        \n",
    "        # Q-learning update\n",
    "        self.Q[state][action] = current_q + self.alpha * td_error\n",
    "        \n",
    "        return abs(td_error)\n",
    "    \n",
    "    def train(self, num_episodes: int = 1000, verbose: bool = True) -> None:\n",
    "        \"\"\"Train the Q-learning agent.\"\"\"\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            episode_td_errors = []\n",
    "            \n",
    "            while True:\n",
    "                # Choose action\n",
    "                action = self.choose_action(state, training=True)\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Update Q-value\n",
    "                td_error = self.update_q_value(state, action, reward, next_state, done)\n",
    "                episode_td_errors.append(td_error)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            # Store episode statistics\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_lengths.append(episode_length)\n",
    "            self.td_errors.extend(episode_td_errors)\n",
    "            \n",
    "            # Decay epsilon\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            # Track Q-values periodically\n",
    "            if episode % 100 == 0:\n",
    "                sample_q_values = {state: np.max(self.Q[state]) for state in self.states[:5]}\n",
    "                self.q_value_history.append(sample_q_values)\n",
    "                \n",
    "                if verbose:\n",
    "                    avg_reward = np.mean(self.episode_rewards[-100:])\n",
    "                    avg_length = np.mean(self.episode_lengths[-100:])\n",
    "                    print(f\"Episode {episode}: Avg Reward = {avg_reward:.3f}, \"\n",
    "                          f\"Avg Length = {avg_length:.1f}, ε = {self.epsilon:.3f}\")\n",
    "    \n",
    "    def get_policy(self) -> Dict[Tuple[int, int], int]:\n",
    "        \"\"\"Extract greedy policy from Q-values.\"\"\"\n",
    "        policy = {}\n",
    "        for state in self.states:\n",
    "            if not self.env.is_terminal(state):\n",
    "                policy[state] = np.argmax(self.Q[state])\n",
    "        return policy\n",
    "    \n",
    "    def get_value_function(self) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"Extract value function from Q-values: V(s) = max_a Q(s,a).\"\"\"\n",
    "        return {state: np.max(self.Q[state]) for state in self.states}\n",
    "    \n",
    "    def plot_training_progress(self) -> None:\n",
    "        \"\"\"Plot training progress metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Episode rewards\n",
    "        axes[0, 0].plot(self.episode_rewards, alpha=0.7)\n",
    "        # Moving average\n",
    "        window = min(100, len(self.episode_rewards) // 10)\n",
    "        if window > 1:\n",
    "            moving_avg = pd.Series(self.episode_rewards).rolling(window=window).mean()\n",
    "            axes[0, 0].plot(moving_avg, 'r-', linewidth=2, label=f'{window}-episode average')\n",
    "            axes[0, 0].legend()\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Episode Reward')\n",
    "        axes[0, 0].set_title('Learning Progress: Episode Rewards')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Episode lengths\n",
    "        axes[0, 1].plot(self.episode_lengths, alpha=0.7)\n",
    "        if window > 1:\n",
    "            moving_avg = pd.Series(self.episode_lengths).rolling(window=window).mean()\n",
    "            axes[0, 1].plot(moving_avg, 'r-', linewidth=2, label=f'{window}-episode average')\n",
    "            axes[0, 1].legend()\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Episode Length')\n",
    "        axes[0, 1].set_title('Learning Progress: Episode Lengths')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # TD errors\n",
    "        if len(self.td_errors) > 100:\n",
    "            td_window = min(1000, len(self.td_errors) // 20)\n",
    "            moving_td_avg = pd.Series(self.td_errors).rolling(window=td_window).mean()\n",
    "            axes[1, 0].plot(moving_td_avg)\n",
    "        else:\n",
    "            axes[1, 0].plot(self.td_errors)\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Update Step')\n",
    "        axes[1, 0].set_ylabel('Average TD Error')\n",
    "        axes[1, 0].set_title('Learning Progress: TD Error')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-value evolution\n",
    "        if self.q_value_history:\n",
    "            for state in list(self.q_value_history[0].keys())[:3]:\n",
    "                values = [q_dict[state] for q_dict in self.q_value_history if state in q_dict]\n",
    "                axes[1, 1].plot(values, label=f'State {state}')\n",
    "            \n",
    "            axes[1, 1].set_xlabel('Training Checkpoint')\n",
    "            axes[1, 1].set_ylabel('Max Q-Value')\n",
    "            axes[1, 1].set_title('Q-Value Evolution')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Train Q-Learning agent\n",
    "print(\"Training Q-Learning agent...\")\n",
    "q_agent = QLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.995)\n",
    "q_agent.train(num_episodes=1000, verbose=True)\n",
    "\n",
    "# Extract learned policy and values\n",
    "q_policy = q_agent.get_policy()\n",
    "q_values = q_agent.get_value_function()\n",
    "\n",
    "print(f\"\\nQ-Learning training completed!\")\n",
    "print(f\"Final exploration rate: {q_agent.epsilon:.4f}\")\n",
    "print(f\"Average reward (last 100 episodes): {np.mean(q_agent.episode_rewards[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "q_agent.plot_training_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Q-Learning results with optimal solution\n",
    "print(\"Q-Learning Learned Value Function:\")\n",
    "env.render(values=q_values)\n",
    "\n",
    "print(\"\\nQ-Learning Learned Policy:\")\n",
    "env.render(policy=q_policy)\n",
    "\n",
    "# Quantitative comparison\n",
    "print(\"\\nComparison with Optimal Solution:\")\n",
    "value_diff = max(abs(optimal_values[s] - q_values[s]) for s in env.get_all_states())\n",
    "print(f\"Max value difference: {value_diff:.4f}\")\n",
    "\n",
    "policy_diff = sum(1 for s in env.get_all_states() \n",
    "                  if not env.is_terminal(s) and optimal_policy[s] != q_policy[s])\n",
    "print(f\"Policy differences: {policy_diff} states\")\n",
    "\n",
    "# Show Q-table for start state\n",
    "start_state = env.start_state\n",
    "print(f\"\\nQ-values for start state {start_state}:\")\n",
    "for action, q_val in enumerate(q_agent.Q[start_state]):\n",
    "    print(f\"  {env.action_names[action]}: {q_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SARSA: On-Policy Temporal Difference Learning\n",
    "\n",
    "**SARSA** (State-Action-Reward-State-Action) is an on-policy temporal difference learning algorithm that learns the Q-value for the policy being followed.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "SARSA uses the following update rule:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "Where $a'$ is the action actually taken in state $s'$ (not the maximum as in Q-learning).\n",
    "\n",
    "### Key Differences from Q-Learning\n",
    "\n",
    "1. **On-policy vs Off-policy**: \n",
    "   - SARSA learns about the policy being followed (including exploration)\n",
    "   - Q-learning learns about the optimal policy regardless of behavior\n",
    "\n",
    "2. **Update target**:\n",
    "   - SARSA: $R + \\gamma Q(s',a')$ (uses actual next action)\n",
    "   - Q-learning: $R + \\gamma \\max_{a'} Q(s',a')$ (uses optimal next action)\n",
    "\n",
    "3. **Convergence**:\n",
    "   - SARSA converges to optimal policy if exploration decreases appropriately\n",
    "   - Q-learning converges to optimal Q-function even with fixed exploration\n",
    "\n",
    "### Expected SARSA\n",
    "\n",
    "A variant that uses the expected value under the current policy:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [R + \\gamma \\sum_{a'} \\pi(a'|s') Q(s',a') - Q(s,a)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"\n",
    "    SARSA agent for on-policy temporal difference learning.\n",
    "    \n",
    "    Learns Q-values for the policy being followed, making it more\n",
    "    conservative than Q-learning in stochastic environments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: GridWorld, alpha: float = 0.1, gamma: float = 0.9, \n",
    "                 epsilon: float = 0.1, epsilon_decay: float = 0.995, \n",
    "                 epsilon_min: float = 0.01):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.states = env.get_all_states()\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        \n",
    "        # Track learning progress\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def choose_action(self, state: Tuple[int, int], training: bool = True) -> int:\n",
    "        \"\"\"Choose action using ε-greedy policy.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update_q_value(self, state: Tuple[int, int], action: int, reward: float, \n",
    "                       next_state: Tuple[int, int], next_action: int, done: bool) -> float:\n",
    "        \"\"\"Update Q-value using SARSA rule.\"\"\"\n",
    "        # Current Q-value\n",
    "        current_q = self.Q[state][action]\n",
    "        \n",
    "        # Target Q-value (key difference from Q-learning)\n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            target_q = reward + self.gamma * self.Q[next_state][next_action]\n",
    "        \n",
    "        # TD error\n",
    "        td_error = target_q - current_q\n",
    "        \n",
    "        # SARSA update\n",
    "        self.Q[state][action] = current_q + self.alpha * td_error\n",
    "        \n",
    "        return abs(td_error)\n",
    "    \n",
    "    def train(self, num_episodes: int = 1000, verbose: bool = True) -> None:\n",
    "        \"\"\"Train the SARSA agent.\"\"\"\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            action = self.choose_action(state, training=True)\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            episode_td_errors = []\n",
    "            \n",
    "            while True:\n",
    "                # Take action\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Choose next action (important for SARSA)\n",
    "                if not done:\n",
    "                    next_action = self.choose_action(next_state, training=True)\n",
    "                else:\n",
    "                    next_action = None\n",
    "                \n",
    "                # Update Q-value\n",
    "                td_error = self.update_q_value(state, action, reward, next_state, next_action, done)\n",
    "                episode_td_errors.append(td_error)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                # Move to next state-action pair\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            \n",
    "            # Store episode statistics\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_lengths.append(episode_length)\n",
    "            self.td_errors.extend(episode_td_errors)\n",
    "            \n",
    "            # Decay epsilon\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            if episode % 100 == 0 and verbose:\n",
    "                avg_reward = np.mean(self.episode_rewards[-100:])\n",
    "                avg_length = np.mean(self.episode_lengths[-100:])\n",
    "                print(f\"Episode {episode}: Avg Reward = {avg_reward:.3f}, \"\n",
    "                      f\"Avg Length = {avg_length:.1f}, ε = {self.epsilon:.3f}\")\n",
    "    \n",
    "    def get_policy(self) -> Dict[Tuple[int, int], int]:\n",
    "        \"\"\"Extract greedy policy from Q-values.\"\"\"\n",
    "        policy = {}\n",
    "        for state in self.states:\n",
    "            if not self.env.is_terminal(state):\n",
    "                policy[state] = np.argmax(self.Q[state])\n",
    "        return policy\n",
    "    \n",
    "    def get_value_function(self) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"Extract value function from Q-values.\"\"\"\n",
    "        return {state: np.max(self.Q[state]) for state in self.states}\n",
    "\n",
    "# Train SARSA agent\n",
    "print(\"Training SARSA agent...\")\n",
    "sarsa_agent = SARSAAgent(env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.995)\n",
    "sarsa_agent.train(num_episodes=1000, verbose=True)\n",
    "\n",
    "# Extract learned policy and values\n",
    "sarsa_policy = sarsa_agent.get_policy()\n",
    "sarsa_values = sarsa_agent.get_value_function()\n",
    "\n",
    "print(f\"\\nSARSA training completed!\")\n",
    "print(f\"Final exploration rate: {sarsa_agent.epsilon:.4f}\")\n",
    "print(f\"Average reward (last 100 episodes): {np.mean(sarsa_agent.episode_rewards[-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Algorithm Comparison and Analysis\n",
    "\n",
    "Let's compare all the algorithms we've implemented to understand their strengths, weaknesses, and convergence properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of all algorithms\n",
    "def compare_algorithms():\n",
    "    \"\"\"Compare all implemented algorithms.\"\"\"\n",
    "    \n",
    "    algorithms = {\n",
    "        'Value Iteration': {\n",
    "            'values': optimal_values,\n",
    "            'policy': optimal_policy,\n",
    "            'type': 'Dynamic Programming',\n",
    "            'model_free': False,\n",
    "            'iterations': len(vi_solver.delta_history)\n",
    "        },\n",
    "        'Policy Iteration': {\n",
    "            'values': pi_values,\n",
    "            'policy': pi_policy,\n",
    "            'type': 'Dynamic Programming', \n",
    "            'model_free': False,\n",
    "            'iterations': len(pi_solver.policy_history)\n",
    "        },\n",
    "        'Q-Learning': {\n",
    "            'values': q_values,\n",
    "            'policy': q_policy,\n",
    "            'type': 'Temporal Difference (Off-policy)',\n",
    "            'model_free': True,\n",
    "            'episodes': len(q_agent.episode_rewards)\n",
    "        },\n",
    "        'SARSA': {\n",
    "            'values': sarsa_values,\n",
    "            'policy': sarsa_policy,\n",
    "            'type': 'Temporal Difference (On-policy)',\n",
    "            'model_free': True,\n",
    "            'episodes': len(sarsa_agent.episode_rewards)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=== Algorithm Comparison ===\")\n",
    "    print(f\"{'Algorithm':<18} {'Type':<30} {'Model-Free':<12} {'Iterations/Episodes':<20}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for name, info in algorithms.items():\n",
    "        iters = info.get('iterations', info.get('episodes', 'N/A'))\n",
    "        print(f\"{name:<18} {info['type']:<30} {info['model_free']:<12} {iters:<20}\")\n",
    "    \n",
    "    # Value function comparison\n",
    "    print(\"\\n=== Value Function Comparison ===\")\n",
    "    baseline_values = optimal_values  # Use Value Iteration as baseline\n",
    "    \n",
    "    sample_states = [(0, 0), (1, 1), (2, 2), (3, 3)]\n",
    "    sample_states = [s for s in sample_states if s in env.get_all_states() and not env.is_terminal(s)]\n",
    "    \n",
    "    print(f\"\\n{'State':<12}\", end=\"\")\n",
    "    for name in algorithms.keys():\n",
    "        print(f\"{name:<18}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * (12 + 18 * len(algorithms)))\n",
    "    \n",
    "    for state in sample_states:\n",
    "        print(f\"{str(state):<12}\", end=\"\")\n",
    "        for name, info in algorithms.items():\n",
    "            value = info['values'].get(state, 0.0)\n",
    "            print(f\"{value:<18.4f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Policy agreement\n",
    "    print(\"\\n=== Policy Comparison ===\")\n",
    "    non_terminal_states = [s for s in env.get_all_states() if not env.is_terminal(s)]\n",
    "    \n",
    "    for i, (name1, info1) in enumerate(algorithms.items()):\n",
    "        for j, (name2, info2) in enumerate(algorithms.items()):\n",
    "            if i < j:\n",
    "                agreements = sum(1 for s in non_terminal_states \n",
    "                               if info1['policy'][s] == info2['policy'][s])\n",
    "                total = len(non_terminal_states)\n",
    "                percentage = (agreements / total) * 100\n",
    "                print(f\"{name1} vs {name2}: {agreements}/{total} states agree ({percentage:.1f}%)\")\n",
    "\n",
    "compare_algorithms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves comparison\n",
    "def plot_learning_comparison():\n",
    "    \"\"\"Plot learning curves for temporal difference methods.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Episode rewards\n",
    "    window = 50\n",
    "    q_rewards_smooth = pd.Series(q_agent.episode_rewards).rolling(window=window).mean()\n",
    "    sarsa_rewards_smooth = pd.Series(sarsa_agent.episode_rewards).rolling(window=window).mean()\n",
    "    \n",
    "    axes[0, 0].plot(q_rewards_smooth, label='Q-Learning', alpha=0.8)\n",
    "    axes[0, 0].plot(sarsa_rewards_smooth, label='SARSA', alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Average Episode Reward')\n",
    "    axes[0, 0].set_title('Learning Progress: Episode Rewards')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Episode lengths\n",
    "    q_lengths_smooth = pd.Series(q_agent.episode_lengths).rolling(window=window).mean()\n",
    "    sarsa_lengths_smooth = pd.Series(sarsa_agent.episode_lengths).rolling(window=window).mean()\n",
    "    \n",
    "    axes[0, 1].plot(q_lengths_smooth, label='Q-Learning', alpha=0.8)\n",
    "    axes[0, 1].plot(sarsa_lengths_smooth, label='SARSA', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Average Episode Length')\n",
    "    axes[0, 1].set_title('Learning Progress: Episode Lengths')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Value iteration convergence\n",
    "    axes[1, 0].plot(vi_solver.delta_history)\n",
    "    axes[1, 0].set_xlabel('Iteration')\n",
    "    axes[1, 0].set_ylabel('Max Value Change')\n",
    "    axes[1, 0].set_title('Value Iteration Convergence')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    algorithms = ['Value Iter.', 'Policy Iter.', 'Q-Learning', 'SARSA']\n",
    "    final_rewards = [\n",
    "        1.0,  # Optimal (assumed perfect performance)\n",
    "        1.0,  # Optimal (assumed perfect performance)\n",
    "        np.mean(q_agent.episode_rewards[-100:]),\n",
    "        np.mean(sarsa_agent.episode_rewards[-100:])\n",
    "    ]\n",
    "    \n",
    "    colors = ['blue', 'green', 'red', 'orange']\n",
    "    bars = axes[1, 1].bar(algorithms, final_rewards, color=colors, alpha=0.7)\n",
    "    axes[1, 1].set_ylabel('Average Reward (Last 100 Episodes)')\n",
    "    axes[1, 1].set_title('Final Performance Comparison')\n",
    "    axes[1, 1].set_ylim(min(final_rewards) - 0.1, max(final_rewards) + 0.1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, reward in zip(bars, final_rewards):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                       f'{reward:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways and Insights\n",
    "\n",
    "From our implementation and comparison of tabular RL methods, we can draw several important insights:\n",
    "\n",
    "### Mathematical Foundations\n",
    "1. **Bellman Equations** are the cornerstone of RL, providing both the theoretical foundation and practical algorithms\n",
    "2. **Contraction Mapping** properties ensure convergence under appropriate conditions\n",
    "3. **Optimal Value Functions** satisfy the Bellman optimality equations\n",
    "\n",
    "### Algorithm Characteristics\n",
    "\n",
    "**Dynamic Programming (Value/Policy Iteration)**:\n",
    "- Requires complete model knowledge\n",
    "- Guaranteed optimal solutions\n",
    "- Fast convergence (few iterations)\n",
    "- Computationally expensive per iteration\n",
    "\n",
    "**Q-Learning**:\n",
    "- Model-free and off-policy\n",
    "- Learns optimal policy regardless of behavior\n",
    "- More aggressive exploration can help\n",
    "- Converges to optimal Q-function\n",
    "\n",
    "**SARSA**:\n",
    "- Model-free and on-policy\n",
    "- Learns about the policy being followed\n",
    "- More conservative in risky environments\n",
    "- Better for safety-critical applications\n",
    "\n",
    "### Practical Considerations\n",
    "- **Exploration vs Exploitation**: Critical for TD methods\n",
    "- **Learning Rate**: Affects convergence speed and stability\n",
    "- **Discount Factor**: Controls planning horizon\n",
    "- **Environment Complexity**: Affects sample complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test learned policies by running episodes\n",
    "def test_policy(agent, num_episodes: int = 10, policy_name: str = \"Agent\") -> None:\n",
    "    \"\"\"Test a learned policy by running episodes.\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        \n",
    "        print(f\"\\n{policy_name} Episode {episode + 1}: \", end=\"\")\n",
    "        \n",
    "        while episode_length < 50:  # Prevent infinite loops\n",
    "            if hasattr(agent, 'choose_action'):\n",
    "                action = agent.choose_action(state, training=False)  # No exploration\n",
    "            else:\n",
    "                # For dictionaries (optimal policies from DP)\n",
    "                action = agent[state]\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            print(f\"{env.action_names[action][0]}\", end=\"\")\n",
    "            \n",
    "            if done:\n",
    "                print(f\" -> Goal! Reward: {episode_reward:.3f}, Length: {episode_length}\")\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if not done:\n",
    "            print(f\" -> Timeout. Reward: {episode_reward:.3f}, Length: {episode_length}\")\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "    \n",
    "    print(f\"\\n{policy_name} Average Performance:\")\n",
    "    print(f\"  Average Reward: {np.mean(episode_rewards):.4f} ± {np.std(episode_rewards):.4f}\")\n",
    "    print(f\"  Average Length: {np.mean(episode_lengths):.2f} ± {np.std(episode_lengths):.2f}\")\n",
    "    print(f\"  Success Rate: {sum(1 for r in episode_rewards if r > 0.5) / len(episode_rewards) * 100:.1f}%\")\n",
    "\n",
    "print(\"Testing learned policies...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test optimal policy\n",
    "test_policy(optimal_policy, num_episodes=5, policy_name=\"Optimal (Value Iteration)\")\n",
    "\n",
    "# Test Q-learning policy\n",
    "test_policy(q_agent, num_episodes=5, policy_name=\"Q-Learning\")\n",
    "\n",
    "# Test SARSA policy  \n",
    "test_policy(sarsa_agent, num_episodes=5, policy_name=\"SARSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered the fundamental concepts and algorithms of reinforcement learning:\n",
    "\n",
    "### **Mathematical Framework**\n",
    "- **Markov Decision Processes**: The mathematical foundation of sequential decision making\n",
    "- **Bellman Equations**: Recursive relationships that define optimal value functions\n",
    "- **Value Functions**: Tools for evaluating states and actions\n",
    "\n",
    "### **Dynamic Programming**\n",
    "- **Value Iteration**: Direct application of Bellman optimality equations\n",
    "- **Policy Iteration**: Alternating policy evaluation and improvement\n",
    "- **Convergence Guarantees**: Theoretical foundation for algorithm correctness\n",
    "\n",
    "### **Temporal Difference Learning**\n",
    "- **Q-Learning**: Off-policy learning of optimal action-value function\n",
    "- **SARSA**: On-policy learning with exploration considerations\n",
    "- **Model-Free Learning**: Learning without environment dynamics knowledge\n",
    "\n",
    "### **Key Insights**\n",
    "- **Exploration vs Exploitation**: Fundamental trade-off in RL\n",
    "- **On-Policy vs Off-Policy**: Different learning paradigms with distinct properties\n",
    "- **Sample Complexity**: Model-free methods require more interaction data\n",
    "- **Convergence Properties**: Different algorithms have different theoretical guarantees\n",
    "\n",
    "These tabular methods form the foundation for understanding more advanced RL algorithms. In the next notebook, we'll explore Monte Carlo methods and more sophisticated temporal difference techniques that will bridge us toward function approximation and deep reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}