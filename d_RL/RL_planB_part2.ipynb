{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Plan B - Part 2: Monte Carlo & Temporal Difference Learning\n",
    "\n",
    "This notebook explores Monte Carlo methods and advanced temporal difference learning techniques. We'll dive deep into the mathematical foundations, implement various algorithms, and analyze their convergence properties and practical trade-offs.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand Monte Carlo prediction and control methods\n",
    "- Master eligibility traces and n-step temporal difference learning\n",
    "- Analyze on-policy vs off-policy learning paradigms\n",
    "- Implement importance sampling for off-policy methods\n",
    "- Compare convergence properties and sample efficiency\n",
    "- Build intuition for function approximation preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List, Optional, Union\n",
    "import time\n",
    "import warnings\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Monte Carlo Methods: Mathematical Foundation\n",
    "\n",
    "Monte Carlo (MC) methods learn directly from episodes of experience without bootstrapping. They use the actual returns experienced to estimate value functions.\n",
    "\n",
    "### Monte Carlo Prediction\n",
    "\n",
    "The goal is to estimate $V^\\pi(s)$ given a policy $\\pi$. For each state $s$, we collect returns from episodes that visit $s$:\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$$\n",
    "\n",
    "Where $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ is the return from time $t$.\n",
    "\n",
    "### First-Visit vs Every-Visit MC\n",
    "\n",
    "- **First-visit MC**: Only consider the first visit to state $s$ in each episode\n",
    "- **Every-visit MC**: Consider every visit to state $s$ in each episode\n",
    "\n",
    "Both converge to $V^\\pi(s)$ as the number of visits approaches infinity.\n",
    "\n",
    "### Monte Carlo Backup\n",
    "\n",
    "The MC update rule is:\n",
    "$$V(s) \\leftarrow V(s) + \\alpha[G_t - V(s)]$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate and $G_t$ is the actual return from the episode.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Unbiased**: MC estimates are unbiased since they use actual returns\n",
    "2. **High Variance**: Returns can vary significantly between episodes\n",
    "3. **Model-Free**: No need for environment dynamics\n",
    "4. **Episodic**: Requires complete episodes to compute returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Environments for Advanced Methods\n",
    "\n",
    "We'll create more complex environments to better demonstrate the differences between algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnvironment:\n",
    "    \"\"\"\n",
    "    Cliff Walking environment - a classic RL benchmark.\n",
    "    \n",
    "    The agent must navigate from start to goal while avoiding a cliff.\n",
    "    This environment highlights the difference between on-policy and off-policy methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, height: int = 4, width: int = 12):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # Define actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # (dy, dx)\n",
    "        self.action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "        self.num_actions = len(self.actions)\n",
    "        \n",
    "        # Set up environment\n",
    "        self.start_state = (height-1, 0)  # Bottom-left\n",
    "        self.goal_state = (height-1, width-1)  # Bottom-right\n",
    "        \n",
    "        # Define cliff (bottom row except start and goal)\n",
    "        self.cliff_states = {(height-1, x) for x in range(1, width-1)}\n",
    "        \n",
    "        # Current state\n",
    "        self.current_state = self.start_state\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        \"\"\"Reset to starting state.\"\"\"\n",
    "        self.current_state = self.start_state\n",
    "        self.done = False\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, dict]:\n",
    "        \"\"\"Execute action and return (next_state, reward, done, info).\"\"\"\n",
    "        if self.done:\n",
    "            return self.current_state, 0, True, {}\n",
    "        \n",
    "        # Calculate next state\n",
    "        dy, dx = self.actions[action]\n",
    "        next_y = max(0, min(self.height - 1, self.current_state[0] + dy))\n",
    "        next_x = max(0, min(self.width - 1, self.current_state[1] + dx))\n",
    "        next_state = (next_y, next_x)\n",
    "        \n",
    "        # Calculate reward\n",
    "        if next_state in self.cliff_states:\n",
    "            # Fall off cliff - large negative reward and reset to start\n",
    "            reward = -100\n",
    "            next_state = self.start_state\n",
    "            self.done = False  # Episode continues\n",
    "        elif next_state == self.goal_state:\n",
    "            # Reached goal\n",
    "            reward = 0\n",
    "            self.done = True\n",
    "        else:\n",
    "            # Normal move\n",
    "            reward = -1\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return next_state, reward, self.done, {}\n",
    "    \n",
    "    def get_all_states(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Return all possible states.\"\"\"\n",
    "        return [(i, j) for i in range(self.height) for j in range(self.width)]\n",
    "    \n",
    "    def is_terminal(self, state: Tuple[int, int]) -> bool:\n",
    "        \"\"\"Check if state is terminal.\"\"\"\n",
    "        return state == self.goal_state\n",
    "    \n",
    "    def render(self, values: Optional[Dict] = None, policy: Optional[Dict] = None) -> None:\n",
    "        \"\"\"Visualize the cliff walking environment.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Create grid visualization\n",
    "        grid = np.zeros((self.height, self.width))\n",
    "        \n",
    "        if values:\n",
    "            for (y, x), value in values.items():\n",
    "                grid[y, x] = value\n",
    "        \n",
    "        # Plot heatmap\n",
    "        im = ax.imshow(grid, cmap='RdYlBu_r', alpha=0.7)\n",
    "        \n",
    "        # Add grid lines\n",
    "        ax.set_xticks(np.arange(self.width + 1) - 0.5, minor=True)\n",
    "        ax.set_yticks(np.arange(self.height + 1) - 0.5, minor=True)\n",
    "        ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
    "        \n",
    "        # Mark special states\n",
    "        start_y, start_x = self.start_state\n",
    "        goal_y, goal_x = self.goal_state\n",
    "        \n",
    "        ax.text(start_x, start_y, 'S', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='green')\n",
    "        ax.text(goal_x, goal_y, 'G', ha='center', va='center', \n",
    "                fontsize=16, fontweight='bold', color='red')\n",
    "        \n",
    "        # Mark cliff states\n",
    "        for (cliff_y, cliff_x) in self.cliff_states:\n",
    "            ax.text(cliff_x, cliff_y, 'C', ha='center', va='center', \n",
    "                    fontsize=14, fontweight='bold', color='darkred')\n",
    "            # Add red background for cliff\n",
    "            ax.add_patch(plt.Rectangle((cliff_x-0.4, cliff_y-0.4), 0.8, 0.8, \n",
    "                                     facecolor='red', alpha=0.3))\n",
    "        \n",
    "        # Add policy arrows if provided\n",
    "        if policy:\n",
    "            arrow_props = dict(arrowstyle='->', lw=2, color='blue')\n",
    "            for (y, x), action in policy.items():\n",
    "                if not self.is_terminal((y, x)) and (y, x) not in self.cliff_states:\n",
    "                    dy, dx = self.actions[action]\n",
    "                    ax.annotate('', xy=(x + dx*0.3, y + dy*0.3), xytext=(x, y),\n",
    "                              arrowprops=arrow_props)\n",
    "        \n",
    "        # Add value labels if provided\n",
    "        if values:\n",
    "            for (y, x), value in values.items():\n",
    "                if not self.is_terminal((y, x)) and (y, x) not in self.cliff_states:\n",
    "                    ax.text(x, y + 0.3, f'{value:.1f}', ha='center', va='center', \n",
    "                            fontsize=8, color='white', fontweight='bold')\n",
    "        \n",
    "        ax.set_title('Cliff Walking Environment')\n",
    "        ax.set_xticks(range(self.width))\n",
    "        ax.set_yticks(range(self.height))\n",
    "        \n",
    "        if values:\n",
    "            plt.colorbar(im, ax=ax, label='State Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create cliff walking environment\n",
    "cliff_env = CliffWalkingEnvironment()\n",
    "print(f\"Cliff Walking Environment: {cliff_env.height}x{cliff_env.width}\")\n",
    "print(f\"Start: {cliff_env.start_state}, Goal: {cliff_env.goal_state}\")\n",
    "print(f\"Cliff states: {len(cliff_env.cliff_states)} states\")\n",
    "print(f\"Actions: {cliff_env.action_names}\")\n",
    "\n",
    "# Visualize the environment\n",
    "cliff_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Prediction Implementation\n",
    "\n",
    "Let's implement both first-visit and every-visit Monte Carlo prediction methods.\n",
    "\n",
    "### Algorithm: First-Visit MC Prediction\n",
    "\n",
    "1. Initialize $V(s) = 0$ and $Returns(s) = \\emptyset$ for all $s \\in \\mathcal{S}$\n",
    "2. For each episode:\n",
    "   - Generate episode following policy $\\pi$: $S_0, A_0, R_1, S_1, A_1, \\ldots, S_{T-1}, A_{T-1}, R_T$\n",
    "   - Calculate returns: $G_t = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k$\n",
    "   - For each state $s$ appearing in the episode:\n",
    "     - If this is the first visit to $s$ in this episode:\n",
    "       - Append $G_t$ to $Returns(s)$\n",
    "       - $V(s) \\leftarrow$ average of $Returns(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloAgent:\n",
    "    \"\"\"\n",
    "    Monte Carlo agent for value function estimation and control.\n",
    "    \n",
    "    Implements both first-visit and every-visit MC prediction,\n",
    "    as well as MC control with exploring starts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma: float = 0.9, first_visit: bool = True,\n",
    "                 epsilon: float = 0.1, epsilon_decay: float = 0.99):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.first_visit = first_visit\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.states = env.get_all_states()\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        # Value function and returns\n",
    "        self.V = defaultdict(float)\n",
    "        self.returns = defaultdict(list)\n",
    "        \n",
    "        # Action-value function for control\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        self.q_returns = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        # Policy (for control)\n",
    "        self.policy = defaultdict(lambda: np.ones(self.num_actions) / self.num_actions)\n",
    "        \n",
    "        # Learning statistics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.value_history = []\n",
    "    \n",
    "    def generate_episode(self, policy: Optional[Dict] = None, max_steps: int = 1000) -> List:\n",
    "        \"\"\"Generate an episode following the given policy.\"\"\"\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose action according to policy\n",
    "            if policy is None:\n",
    "                # Use current policy (for control)\n",
    "                action = np.random.choice(self.num_actions, p=self.policy[state])\n",
    "            else:\n",
    "                # Use provided policy (for prediction)\n",
    "                if isinstance(policy, dict):\n",
    "                    action = policy[state]\n",
    "                else:\n",
    "                    action = policy(state)\n",
    "            \n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        return episode\n",
    "    \n",
    "    def calculate_returns(self, episode: List) -> List[float]:\n",
    "        \"\"\"Calculate returns for each time step in the episode.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        # Work backwards through the episode\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            _, _, reward = episode[t]\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def mc_prediction(self, policy, num_episodes: int = 10000, verbose: bool = True) -> Dict:\n",
    "        \"\"\"Monte Carlo prediction to estimate V^π.\"\"\"\n",
    "        \n",
    "        for episode_num in range(num_episodes):\n",
    "            # Generate episode\n",
    "            episode = self.generate_episode(policy)\n",
    "            returns = self.calculate_returns(episode)\n",
    "            \n",
    "            # Track statistics\n",
    "            episode_reward = sum(step[2] for step in episode)\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_lengths.append(len(episode))\n",
    "            \n",
    "            # Update value function\n",
    "            visited_states = set()\n",
    "            \n",
    "            for t, ((state, action, reward), G) in enumerate(zip(episode, returns)):\n",
    "                if self.first_visit and state in visited_states:\n",
    "                    continue\n",
    "                \n",
    "                visited_states.add(state)\n",
    "                self.returns[state].append(G)\n",
    "                self.V[state] = np.mean(self.returns[state])\n",
    "            \n",
    "            # Track value function evolution\n",
    "            if episode_num % 1000 == 0:\n",
    "                self.value_history.append(dict(self.V))\n",
    "                if verbose:\n",
    "                    avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n",
    "                    print(f\"Episode {episode_num}: Avg Reward = {avg_reward:.3f}, Avg Length = {np.mean(self.episode_lengths[-100:]):.1f}\")\n",
    "        \n",
    "        return dict(self.V)\n",
    "    \n",
    "    def epsilon_greedy_policy_from_q(self, state: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Generate ε-greedy policy from Q-values.\"\"\"\n",
    "        policy = np.ones(self.num_actions) * self.epsilon / self.num_actions\n",
    "        best_action = np.argmax(self.Q[state])\n",
    "        policy[best_action] += 1 - self.epsilon\n",
    "        return policy\n",
    "    \n",
    "    def mc_control(self, num_episodes: int = 10000, verbose: bool = True) -> Tuple[Dict, Dict]:\n",
    "        \"\"\"Monte Carlo control to find optimal policy.\"\"\"\n",
    "        \n",
    "        for episode_num in range(num_episodes):\n",
    "            # Update epsilon\n",
    "            self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            # Update policy based on current Q-values\n",
    "            for state in self.states:\n",
    "                if not self.env.is_terminal(state):\n",
    "                    self.policy[state] = self.epsilon_greedy_policy_from_q(state)\n",
    "            \n",
    "            # Generate episode\n",
    "            episode = self.generate_episode(max_steps=500)\n",
    "            returns = self.calculate_returns(episode)\n",
    "            \n",
    "            # Track statistics\n",
    "            episode_reward = sum(step[2] for step in episode)\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_lengths.append(len(episode))\n",
    "            \n",
    "            # Update Q-function\n",
    "            visited_state_actions = set()\n",
    "            \n",
    "            for t, ((state, action, reward), G) in enumerate(zip(episode, returns)):\n",
    "                state_action = (state, action)\n",
    "                \n",
    "                if self.first_visit and state_action in visited_state_actions:\n",
    "                    continue\n",
    "                \n",
    "                visited_state_actions.add(state_action)\n",
    "                self.q_returns[state][action].append(G)\n",
    "                self.Q[state][action] = np.mean(self.q_returns[state][action])\n",
    "            \n",
    "            if episode_num % 1000 == 0 and verbose:\n",
    "                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n",
    "                avg_length = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)\n",
    "                print(f\"Episode {episode_num}: Avg Reward = {avg_reward:.3f}, Avg Length = {avg_length:.1f}, ε = {self.epsilon:.4f}\")\n",
    "        \n",
    "        # Extract final greedy policy\n",
    "        greedy_policy = {}\n",
    "        for state in self.states:\n",
    "            if not self.env.is_terminal(state):\n",
    "                greedy_policy[state] = np.argmax(self.Q[state])\n",
    "        \n",
    "        return dict(self.Q), greedy_policy\n",
    "    \n",
    "    def plot_learning_curves(self) -> None:\n",
    "        \"\"\"Plot learning progress.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Episode rewards\n",
    "        window = min(100, len(self.episode_rewards) // 10)\n",
    "        if window > 1:\n",
    "            moving_avg = pd.Series(self.episode_rewards).rolling(window=window).mean()\n",
    "            axes[0].plot(moving_avg, 'r-', linewidth=2, label=f'{window}-episode average')\n",
    "        \n",
    "        axes[0].plot(self.episode_rewards, alpha=0.3, color='blue')\n",
    "        axes[0].set_xlabel('Episode')\n",
    "        axes[0].set_ylabel('Episode Reward')\n",
    "        axes[0].set_title('Monte Carlo Learning Progress')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Episode lengths\n",
    "        if window > 1:\n",
    "            length_avg = pd.Series(self.episode_lengths).rolling(window=window).mean()\n",
    "            axes[1].plot(length_avg, 'r-', linewidth=2, label=f'{window}-episode average')\n",
    "        \n",
    "        axes[1].plot(self.episode_lengths, alpha=0.3, color='green')\n",
    "        axes[1].set_xlabel('Episode')\n",
    "        axes[1].set_ylabel('Episode Length')\n",
    "        axes[1].set_title('Episode Length Progress')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Monte Carlo agent implementation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monte Carlo Control with Exploring Starts\n",
    "\n",
    "Monte Carlo control finds the optimal policy through **Generalized Policy Iteration** (GPI), which interleaves policy evaluation and policy improvement.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "The goal is to find the optimal policy $\\pi^*$ by alternating between:\n",
    "\n",
    "1. **Policy Evaluation**: Estimate $Q^{\\pi_k}$ using MC methods\n",
    "2. **Policy Improvement**: Update policy greedily: $\\pi_{k+1}(s) = \\arg\\max_a Q^{\\pi_k}(s,a)$\n",
    "\n",
    "### Exploring Starts Assumption\n",
    "\n",
    "To ensure all state-action pairs are visited, we assume **exploring starts**: every state-action pair has nonzero probability of being selected as the starting pair.\n",
    "\n",
    "### MC Control Algorithm\n",
    "\n",
    "1. Initialize $Q(s,a)$ arbitrarily and $\\pi(s)$ arbitrarily for all $s,a$\n",
    "2. Repeat:\n",
    "   - Generate episode with exploring starts\n",
    "   - For each state-action pair $(s,a)$ in the episode:\n",
    "     - $G \\leftarrow$ return following first occurrence of $(s,a)$\n",
    "     - Append $G$ to $Returns(s,a)$\n",
    "     - $Q(s,a) \\leftarrow$ average of $Returns(s,a)$\n",
    "   - For each state $s$ in the episode:\n",
    "     - $\\pi(s) \\leftarrow \\arg\\max_a Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Monte Carlo control on Cliff Walking\n",
    "print(\"Training Monte Carlo Control on Cliff Walking...\")\n",
    "\n",
    "mc_agent = MonteCarloAgent(cliff_env, gamma=0.9, epsilon=0.1, epsilon_decay=0.9995)\n",
    "Q_values, mc_policy = mc_agent.mc_control(num_episodes=20000, verbose=True)\n",
    "\n",
    "print(f\"\\nMonte Carlo Control completed!\")\n",
    "print(f\"Final ε: {mc_agent.epsilon:.6f}\")\n",
    "print(f\"Average reward (last 100 episodes): {np.mean(mc_agent.episode_rewards[-100:]):.3f}\")\n",
    "print(f\"Average length (last 100 episodes): {np.mean(mc_agent.episode_lengths[-100:]):.1f}\")\n",
    "\n",
    "# Extract value function from Q-values\n",
    "mc_values = {state: np.max(Q_values[state]) for state in cliff_env.get_all_states()}\n",
    "\n",
    "# Plot learning progress\n",
    "mc_agent.plot_learning_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Monte Carlo results\n",
    "print(\"Monte Carlo Control - Learned Value Function:\")\n",
    "cliff_env.render(values=mc_values)\n",
    "\n",
    "print(\"\\nMonte Carlo Control - Learned Policy:\")\n",
    "cliff_env.render(policy=mc_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. n-Step Temporal Difference Learning\n",
    "\n",
    "n-step methods bridge Monte Carlo and TD(0) methods by looking ahead $n$ steps instead of just 1.\n",
    "\n",
    "### n-Step Return\n",
    "\n",
    "The **n-step return** from time $t$ is:\n",
    "$$G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n})$$\n",
    "\n",
    "### n-Step TD Update\n",
    "\n",
    "The n-step TD update rule is:\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [G_t^{(n)} - V(S_t)]$$\n",
    "\n",
    "### Special Cases\n",
    "\n",
    "- **n = 1**: Standard TD(0)\n",
    "- **n = ∞**: Monte Carlo (full return)\n",
    "- **n = intermediate**: Balanced bias-variance trade-off\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "\n",
    "- **Smaller n**: Lower variance (less random), higher bias (more approximate)\n",
    "- **Larger n**: Higher variance (more random), lower bias (more accurate)\n",
    "\n",
    "### n-Step SARSA\n",
    "\n",
    "For action-value methods:\n",
    "$$G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q(S_{t+n}, A_{t+n})$$\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [G_t^{(n)} - Q(S_t, A_t)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepTDAgent:\n",
    "    \"\"\"\n",
    "    n-Step Temporal Difference Learning Agent.\n",
    "    \n",
    "    Implements n-step SARSA and n-step Expected SARSA for\n",
    "    action-value function learning with configurable lookahead.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, n: int = 3, alpha: float = 0.1, gamma: float = 0.9, \n",
    "                 epsilon: float = 0.1, epsilon_decay: float = 0.995):\n",
    "        self.env = env\n",
    "        self.n = n  # Number of steps to look ahead\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.states = env.get_all_states()\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        # Initialize Q-function\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        \n",
    "        # Learning statistics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def choose_action(self, state: Tuple[int, int], training: bool = True) -> int:\n",
    "        \"\"\"Choose action using ε-greedy policy.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def calculate_n_step_return(self, rewards: List[float], states: List, \n",
    "                               actions: List, start_idx: int, episode_length: int) -> float:\n",
    "        \"\"\"Calculate n-step return from given starting index.\"\"\"\n",
    "        G = 0.0\n",
    "        end_idx = min(start_idx + self.n, episode_length)\n",
    "        \n",
    "        # Sum discounted rewards\n",
    "        for i in range(start_idx, end_idx):\n",
    "            G += (self.gamma ** (i - start_idx)) * rewards[i]\n",
    "        \n",
    "        # Add bootstrapped value if episode didn't end\n",
    "        if end_idx < episode_length:\n",
    "            bootstrap_state = states[end_idx]\n",
    "            bootstrap_action = actions[end_idx]\n",
    "            G += (self.gamma ** self.n) * self.Q[bootstrap_state][bootstrap_action]\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def train(self, num_episodes: int = 5000, verbose: bool = True) -> None:\n",
    "        \"\"\"Train using n-step SARSA.\"\"\"\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Initialize episode\n",
    "            state = self.env.reset()\n",
    "            action = self.choose_action(state, training=True)\n",
    "            \n",
    "            # Store episode data\n",
    "            states = [state]\n",
    "            actions = [action]\n",
    "            rewards = []\n",
    "            \n",
    "            episode_reward = 0\n",
    "            t = 0\n",
    "            T = float('inf')  # Episode termination time\n",
    "            \n",
    "            while True:\n",
    "                if t < T:\n",
    "                    # Take action and observe result\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    rewards.append(reward)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if done:\n",
    "                        T = t + 1\n",
    "                    else:\n",
    "                        states.append(next_state)\n",
    "                        next_action = self.choose_action(next_state, training=True)\n",
    "                        actions.append(next_action)\n",
    "                        action = next_action\n",
    "                \n",
    "                # Update Q-function for state-action pair t-n+1\n",
    "                update_time = t - self.n + 1\n",
    "                if update_time >= 0:\n",
    "                    # Calculate n-step return\n",
    "                    G = self.calculate_n_step_return(rewards, states, actions, \n",
    "                                                   update_time, len(rewards))\n",
    "                    \n",
    "                    # Update Q-value\n",
    "                    update_state = states[update_time]\n",
    "                    update_action = actions[update_time]\n",
    "                    \n",
    "                    old_q = self.Q[update_state][update_action]\n",
    "                    td_error = G - old_q\n",
    "                    self.Q[update_state][update_action] += self.alpha * td_error\n",
    "                    self.td_errors.append(abs(td_error))\n",
    "                \n",
    "                if update_time == T - 1:\n",
    "                    break\n",
    "                    \n",
    "                t += 1\n",
    "            \n",
    "            # Store episode statistics\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_lengths.append(len(rewards))\n",
    "            \n",
    "            # Decay epsilon\n",
    "            if self.epsilon > 0.01:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            if episode % 1000 == 0 and verbose:\n",
    "                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n",
    "                avg_length = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)\n",
    "                print(f\"Episode {episode}: Avg Reward = {avg_reward:.3f}, Avg Length = {avg_length:.1f}, ε = {self.epsilon:.4f}\")\n",
    "    \n",
    "    def get_policy(self) -> Dict:\n",
    "        \"\"\"Extract greedy policy from Q-values.\"\"\"\n",
    "        policy = {}\n",
    "        for state in self.states:\n",
    "            if not self.env.is_terminal(state):\n",
    "                policy[state] = np.argmax(self.Q[state])\n",
    "        return policy\n",
    "    \n",
    "    def get_value_function(self) -> Dict:\n",
    "        \"\"\"Extract value function from Q-values.\"\"\"\n",
    "        return {state: np.max(self.Q[state]) for state in self.states}\n",
    "\n",
    "print(\"n-Step TD agent implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different n-step values\n",
    "print(\"Comparing n-Step SARSA with different n values...\")\n",
    "\n",
    "n_values = [1, 3, 5, 10]\n",
    "n_step_results = {}\n",
    "\n",
    "for n in n_values:\n",
    "    print(f\"\\n=== Training {n}-Step SARSA ===\")\n",
    "    \n",
    "    agent = NStepTDAgent(cliff_env, n=n, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "    agent.train(num_episodes=10000, verbose=True)\n",
    "    \n",
    "    policy = agent.get_policy()\n",
    "    values = agent.get_value_function()\n",
    "    \n",
    "    n_step_results[n] = {\n",
    "        'agent': agent,\n",
    "        'policy': policy,\n",
    "        'values': values,\n",
    "        'final_reward': np.mean(agent.episode_rewards[-100:]),\n",
    "        'final_length': np.mean(agent.episode_lengths[-100:])\n",
    "    }\n",
    "    \n",
    "    print(f\"{n}-Step SARSA - Final avg reward: {n_step_results[n]['final_reward']:.3f}\")\n",
    "    print(f\"{n}-Step SARSA - Final avg length: {n_step_results[n]['final_length']:.1f}\")\n",
    "\n",
    "print(\"\\n=== n-Step Comparison Summary ===\")\n",
    "print(f\"{'n':<5} {'Final Reward':<15} {'Final Length':<15} {'Convergence':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for n in n_values:\n",
    "    reward = n_step_results[n]['final_reward']\n",
    "    length = n_step_results[n]['final_length']\n",
    "    # Simple convergence metric: reward improvement in last 25% of episodes\n",
    "    agent = n_step_results[n]['agent']\n",
    "    early_reward = np.mean(agent.episode_rewards[len(agent.episode_rewards)//4:len(agent.episode_rewards)//2])\n",
    "    late_reward = np.mean(agent.episode_rewards[-len(agent.episode_rewards)//4:])\n",
    "    improvement = late_reward - early_reward\n",
    "    print(f\"{n:<5} {reward:<15.3f} {length:<15.1f} {improvement:<15.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of n-step methods\n",
    "def plot_n_step_comparison():\n",
    "    \"\"\"Compare learning curves for different n values.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Episode rewards comparison\n",
    "    window = 200\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    for i, (n, results) in enumerate(n_step_results.items()):\n",
    "        agent = results['agent']\n",
    "        if len(agent.episode_rewards) >= window:\n",
    "            smooth_rewards = pd.Series(agent.episode_rewards).rolling(window=window).mean()\n",
    "            axes[0, 0].plot(smooth_rewards, label=f'n={n}', color=colors[i], alpha=0.8)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Average Episode Reward')\n",
    "    axes[0, 0].set_title('n-Step SARSA: Learning Progress')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Episode lengths comparison\n",
    "    for i, (n, results) in enumerate(n_step_results.items()):\n",
    "        agent = results['agent']\n",
    "        if len(agent.episode_lengths) >= window:\n",
    "            smooth_lengths = pd.Series(agent.episode_lengths).rolling(window=window).mean()\n",
    "            axes[0, 1].plot(smooth_lengths, label=f'n={n}', color=colors[i], alpha=0.8)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Average Episode Length')\n",
    "    axes[0, 1].set_title('n-Step SARSA: Episode Lengths')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    n_vals = list(n_step_results.keys())\n",
    "    final_rewards = [n_step_results[n]['final_reward'] for n in n_vals]\n",
    "    \n",
    "    bars = axes[1, 0].bar([str(n) for n in n_vals], final_rewards, \n",
    "                         color=colors[:len(n_vals)], alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('n (steps ahead)')\n",
    "    axes[1, 0].set_ylabel('Final Average Reward')\n",
    "    axes[1, 0].set_title('Final Performance Comparison')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, reward in zip(bars, final_rewards):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                       f'{reward:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # TD Error comparison (first 5000 updates)\n",
    "    for i, (n, results) in enumerate(n_step_results.items()):\n",
    "        agent = results['agent']\n",
    "        if len(agent.td_errors) > 1000:\n",
    "            # Smooth TD errors\n",
    "            td_smooth = pd.Series(agent.td_errors[:5000]).rolling(window=100).mean()\n",
    "            axes[1, 1].plot(td_smooth, label=f'n={n}', color=colors[i], alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Update Step')\n",
    "    axes[1, 1].set_ylabel('Average TD Error')\n",
    "    axes[1, 1].set_title('TD Error Evolution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_n_step_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Eligibility Traces and TD(λ)\n",
    "\n",
    "**Eligibility traces** provide an elegant way to implement n-step methods more efficiently and bridge TD and MC methods.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The **eligibility trace** for state $s$ at time $t$ is:\n",
    "$$e_t(s) = \\begin{cases}\n",
    "\\gamma \\lambda e_{t-1}(s) + \\mathbf{1}_{S_t = s} & \\text{(accumulating traces)} \\\\\n",
    "\\gamma \\lambda e_{t-1}(s) & \\text{if } S_t \\neq s \\\\\n",
    "1 & \\text{if } S_t = s \\text{ (replacing traces)}\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $\\lambda \\in [0,1]$ is the **trace decay parameter**.\n",
    "\n",
    "### TD(λ) Update Rule\n",
    "\n",
    "The TD(λ) update rule is:\n",
    "$$\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$$\n",
    "$$V(s) \\leftarrow V(s) + \\alpha \\delta_t e_t(s) \\text{ for all } s$$\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **λ = 0**: Reduces to TD(0)\n",
    "- **λ = 1**: Equivalent to Monte Carlo (for accumulating traces)\n",
    "- **λ ∈ (0,1)**: Balances bias and variance\n",
    "\n",
    "### Forward vs Backward View\n",
    "\n",
    "- **Forward view**: TD(λ) as weighted average of n-step returns\n",
    "- **Backward view**: Eligibility traces distribute TD error backward in time\n",
    "\n",
    "### SARSA(λ)\n",
    "\n",
    "For action-value functions:\n",
    "$$e_t(s,a) = \\begin{cases}\n",
    "\\gamma \\lambda e_{t-1}(s,a) + 1 & \\text{if } S_t = s, A_t = a \\\\\n",
    "\\gamma \\lambda e_{t-1}(s,a) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta_t e_t(s,a) \\text{ for all } s,a$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSALambdaAgent:\n",
    "    \"\"\"\n",
    "    SARSA(λ) agent with eligibility traces.\n",
    "    \n",
    "    Implements both accumulating and replacing traces,\n",
    "    providing efficient credit assignment across time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, alpha: float = 0.1, gamma: float = 0.9, \n",
    "                 lambda_param: float = 0.9, epsilon: float = 0.1, \n",
    "                 epsilon_decay: float = 0.995, trace_type: str = 'accumulating'):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lambda_param = lambda_param  # λ parameter\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.trace_type = trace_type  # 'accumulating' or 'replacing'\n",
    "        \n",
    "        self.states = env.get_all_states()\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        # Initialize Q-function and eligibility traces\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        self.traces = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        \n",
    "        # Learning statistics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.td_errors = []\n",
    "        self.trace_magnitudes = []  # Track trace decay\n",
    "    \n",
    "    def choose_action(self, state: Tuple[int, int], training: bool = True) -> int:\n",
    "        \"\"\"Choose action using ε-greedy policy.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update_traces(self, state: Tuple[int, int], action: int) -> None:\n",
    "        \"\"\"Update eligibility traces.\"\"\"\n",
    "        # Decay all traces\n",
    "        for s in list(self.traces.keys()):\n",
    "            self.traces[s] *= self.gamma * self.lambda_param\n",
    "            \n",
    "            # Remove traces that are too small (for efficiency)\n",
    "            if np.max(np.abs(self.traces[s])) < 1e-8:\n",
    "                del self.traces[s]\n",
    "        \n",
    "        # Update trace for current state-action\n",
    "        if self.trace_type == 'accumulating':\n",
    "            self.traces[state][action] += 1.0\n",
    "        elif self.trace_type == 'replacing':\n",
    "            # Reset trace for current state to 0, then set to 1\n",
    "            self.traces[state][:] = 0.0\n",
    "            self.traces[state][action] = 1.0\n",
    "    \n",
    "    def train(self, num_episodes: int = 5000, verbose: bool = True) -> None:\n",
    "        \"\"\"Train using SARSA(λ).\"\"\"\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Reset traces at beginning of episode\n",
    "            self.traces.clear()\n",
    "            \n",
    "            # Initialize episode\n",
    "            state = self.env.reset()\n",
    "            action = self.choose_action(state, training=True)\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            \n",
    "            while True:\n",
    "                # Take action\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                # Choose next action\n",
    "                if not done:\n",
    "                    next_action = self.choose_action(next_state, training=True)\n",
    "                else:\n",
    "                    next_action = None\n",
    "                \n",
    "                # Calculate TD error\n",
    "                if done:\n",
    "                    td_error = reward - self.Q[state][action]\n",
    "                else:\n",
    "                    td_error = reward + self.gamma * self.Q[next_state][next_action] - self.Q[state][action]\n",
    "                \n",
    "                self.td_errors.append(abs(td_error))\n",
    "                \n",
    "                # Update eligibility trace for current state-action\n",
    "                self.update_traces(state, action)\n",
    "                \n",
    "                # Update Q-values for all states using eligibility traces\n",
    "                total_trace_magnitude = 0\n",
    "                for s in list(self.traces.keys()):\n",
    "                    for a in range(self.num_actions):\n",
    "                        if abs(self.traces[s][a]) > 1e-8:\n",
    "                            self.Q[s][a] += self.alpha * td_error * self.traces[s][a]\n",
    "                            total_trace_magnitude += abs(self.traces[s][a])\n",
    "                \n",
    "                self.trace_magnitudes.append(total_trace_magnitude)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            \n",
    "            # Store episode statistics\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_lengths.append(episode_length)\n",
    "            \n",
    "            # Decay epsilon\n",
    "            if self.epsilon > 0.01:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            if episode % 1000 == 0 and verbose:\n",
    "                avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)\n",
    "                avg_length = np.mean(self.episode_lengths[-100:]) if len(self.episode_lengths) >= 100 else np.mean(self.episode_lengths)\n",
    "                print(f\"Episode {episode}: Avg Reward = {avg_reward:.3f}, Avg Length = {avg_length:.1f}, ε = {self.epsilon:.4f}\")\n",
    "    \n",
    "    def get_policy(self) -> Dict:\n",
    "        \"\"\"Extract greedy policy from Q-values.\"\"\"\n",
    "        policy = {}\n",
    "        for state in self.states:\n",
    "            if not self.env.is_terminal(state):\n",
    "                policy[state] = np.argmax(self.Q[state])\n",
    "        return policy\n",
    "    \n",
    "    def get_value_function(self) -> Dict:\n",
    "        \"\"\"Extract value function from Q-values.\"\"\"\n",
    "        return {state: np.max(self.Q[state]) for state in self.states}\n",
    "\n",
    "print(\"SARSA(λ) agent implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different λ values\n",
    "print(\"Comparing SARSA(λ) with different λ values...\")\n",
    "\n",
    "lambda_values = [0.0, 0.3, 0.7, 0.9]\n",
    "lambda_results = {}\n",
    "\n",
    "for lambda_val in lambda_values:\n",
    "    print(f\"\\n=== Training SARSA(λ={lambda_val}) ===\")\n",
    "    \n",
    "    agent = SARSALambdaAgent(cliff_env, alpha=0.1, gamma=0.9, \n",
    "                            lambda_param=lambda_val, epsilon=0.1, \n",
    "                            trace_type='accumulating')\n",
    "    agent.train(num_episodes=8000, verbose=True)\n",
    "    \n",
    "    policy = agent.get_policy()\n",
    "    values = agent.get_value_function()\n",
    "    \n",
    "    lambda_results[lambda_val] = {\n",
    "        'agent': agent,\n",
    "        'policy': policy,\n",
    "        'values': values,\n",
    "        'final_reward': np.mean(agent.episode_rewards[-100:]),\n",
    "        'final_length': np.mean(agent.episode_lengths[-100:])\n",
    "    }\n",
    "    \n",
    "    print(f\"SARSA(λ={lambda_val}) - Final avg reward: {lambda_results[lambda_val]['final_reward']:.3f}\")\n",
    "    print(f\"SARSA(λ={lambda_val}) - Final avg length: {lambda_results[lambda_val]['final_length']:.1f}\")\n",
    "\n",
    "print(\"\\n=== λ Value Comparison Summary ===\")\n",
    "print(f\"{'λ':<5} {'Final Reward':<15} {'Final Length':<15} {'Learning Speed':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for lambda_val in lambda_values:\n",
    "    reward = lambda_results[lambda_val]['final_reward']\n",
    "    length = lambda_results[lambda_val]['final_length']\n",
    "    # Measure learning speed as episodes to reach 75% of final performance\n",
    "    agent = lambda_results[lambda_val]['agent']\n",
    "    target_reward = reward * 0.75\n",
    "    episodes_to_target = len(agent.episode_rewards)\n",
    "    \n",
    "    # Find when rolling average first exceeds target\n",
    "    window = 100\n",
    "    for i in range(window, len(agent.episode_rewards)):\n",
    "        if np.mean(agent.episode_rewards[i-window:i]) >= target_reward:\n",
    "            episodes_to_target = i\n",
    "            break\n",
    "    \n",
    "    print(f\"{lambda_val:<5} {reward:<15.3f} {length:<15.1f} {episodes_to_target:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot λ comparison and trace analysis\n",
    "def plot_lambda_comparison():\n",
    "    \"\"\"Compare SARSA(λ) with different λ values.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    window = 150\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    # Episode rewards comparison\n",
    "    for i, (lambda_val, results) in enumerate(lambda_results.items()):\n",
    "        agent = results['agent']\n",
    "        if len(agent.episode_rewards) >= window:\n",
    "            smooth_rewards = pd.Series(agent.episode_rewards).rolling(window=window).mean()\n",
    "            axes[0, 0].plot(smooth_rewards, label=f'λ={lambda_val}', color=colors[i], alpha=0.8)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Average Episode Reward')\n",
    "    axes[0, 0].set_title('SARSA(λ): Learning Progress')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trace magnitude evolution (first 10000 steps)\n",
    "    for i, (lambda_val, results) in enumerate(lambda_results.items()):\n",
    "        agent = results['agent']\n",
    "        if len(agent.trace_magnitudes) > 1000:\n",
    "            trace_smooth = pd.Series(agent.trace_magnitudes[:10000]).rolling(window=100).mean()\n",
    "            axes[0, 1].plot(trace_smooth, label=f'λ={lambda_val}', color=colors[i], alpha=0.8)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Update Step')\n",
    "    axes[0, 1].set_ylabel('Total Trace Magnitude')\n",
    "    axes[0, 1].set_title('Eligibility Trace Magnitudes')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    lambda_vals = list(lambda_results.keys())\n",
    "    final_rewards = [lambda_results[lam]['final_reward'] for lam in lambda_vals]\n",
    "    \n",
    "    bars = axes[1, 0].bar([str(lam) for lam in lambda_vals], final_rewards, \n",
    "                         color=colors[:len(lambda_vals)], alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('λ (trace decay parameter)')\n",
    "    axes[1, 0].set_ylabel('Final Average Reward')\n",
    "    axes[1, 0].set_title('Final Performance vs λ')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, reward in zip(bars, final_rewards):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                       f'{reward:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # TD Error comparison\n",
    "    for i, (lambda_val, results) in enumerate(lambda_results.items()):\n",
    "        agent = results['agent']\n",
    "        if len(agent.td_errors) > 1000:\n",
    "            td_smooth = pd.Series(agent.td_errors[:5000]).rolling(window=100).mean()\n",
    "            axes[1, 1].plot(td_smooth, label=f'λ={lambda_val}', color=colors[i], alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Update Step')\n",
    "    axes[1, 1].set_ylabel('Average TD Error')\n",
    "    axes[1, 1].set_title('TD Error Evolution')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_lambda_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Off-Policy Methods and Importance Sampling\n",
    "\n",
    "**Off-policy** methods learn about a target policy while following a different behavior policy. This is crucial for learning optimal policies while maintaining exploration.\n",
    "\n",
    "### Importance Sampling\n",
    "\n",
    "To correct for the difference between behavior policy $b(a|s)$ and target policy $\\pi(a|s)$, we use **importance sampling**:\n",
    "\n",
    "The **importance sampling ratio** is:\n",
    "$$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$$\n",
    "\n",
    "### Off-Policy Monte Carlo\n",
    "\n",
    "The off-policy MC update is:\n",
    "$$V(S_t) \\leftarrow V(S_t) + \\alpha [\\rho_{t:T-1} G_t - V(S_t)]$$\n",
    "\n",
    "### Ordinary vs Weighted Importance Sampling\n",
    "\n",
    "**Ordinary Importance Sampling**:\n",
    "$$V(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{|\\mathcal{T}(s)|}$$\n",
    "\n",
    "**Weighted Importance Sampling**:\n",
    "$$V(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1}}$$\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Ordinary IS**: Unbiased but higher variance\n",
    "- **Weighted IS**: Biased but lower variance\n",
    "- **Asymptotically**: Both converge to the true value\n",
    "\n",
    "### Off-Policy TD Learning\n",
    "\n",
    "Q-learning is naturally off-policy, but we can also create off-policy versions of SARSA using importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffPolicyMonteCarloAgent:\n",
    "    \"\"\"\n",
    "    Off-policy Monte Carlo agent using importance sampling.\n",
    "    \n",
    "    Learns about a target policy while following a behavior policy,\n",
    "    using importance sampling to correct for the distribution mismatch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma: float = 0.9, epsilon: float = 0.1, \n",
    "                 weighted_is: bool = True):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon  # For behavior policy\n",
    "        self.weighted_is = weighted_is  # Use weighted importance sampling\n",
    "        \n",
    "        self.states = env.get_all_states()\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        # Q-function for target policy (will be greedy)\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        \n",
    "        # For weighted importance sampling\n",
    "        self.C = defaultdict(lambda: np.zeros(self.num_actions))  # Cumulative weights\n",
    "        \n",
    "        # Learning statistics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.importance_ratios = []  # Track importance sampling ratios\n",
    "    \n",
    "    def behavior_policy(self, state: Tuple[int, int]) -> int:\n",
    "        \"\"\"ε-greedy behavior policy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def target_policy_prob(self, state: Tuple[int, int], action: int) -> float:\n",
    "        \"\"\"Target policy probability (greedy with respect to Q).\"\"\"\n",
    "        best_action = np.argmax(self.Q[state])\n",
    "        return 1.0 if action == best_action else 0.0\n",
    "    \n",
    "    def behavior_policy_prob(self, state: Tuple[int, int], action: int) -> float:\n",
    "        \"\"\"Behavior policy probability (ε-greedy).\"\"\"\n",
    "        best_action = np.argmax(self.Q[state])\n",
    "        if action == best_action:\n",
    "            return 1 - self.epsilon + self.epsilon / self.num_actions\n",
    "        else:\n",
    "            return self.epsilon / self.num_actions\n",
    "    \n",
    "    def generate_episode(self, max_steps: int = 1000) -> List:\n",
    "        \"\"\"Generate episode using behavior policy.\"\"\"\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = self.behavior_policy(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            \n",
    "            episode.append((state, action, reward))\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        return episode\n",
    "    \n",
    "    def calculate_returns(self, episode: List) -> List[float]:\n",
    "        \"\"\"Calculate returns for each time step.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            _, _, reward = episode[t]\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def train(self, num_episodes: int = 20000, verbose: bool = True) -> None:\n",
    "        \"\"\"Train using off-policy Monte Carlo with importance sampling.\"\"\"\n",
    "        \n",
    "        for episode_num in range(num_episodes):\n",
    "            # Generate episode using behavior policy\n",
    "            episode = self.generate_episode(max_steps=500)\n",
    "            returns = self.calculate_returns(episode)\n",
    "            \n",
    "            # Track episode statistics\n",
    "            episode_reward = sum(step[2] for step in episode)\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_lengths.append(len(episode))\n",
    "            \n",
    "            # Process episode backwards (off-policy MC)\n",
    "            G = 0\n",
    "            W = 1  # Importance sampling weight\n",
    "            \n",
    "            for t in range(len(episode) - 1, -1, -1):\n",
    "                state, action, reward = episode[t]\n",
    "                G = reward + self.gamma * G\n",
    "                \n",
    "                if self.weighted_is:\n",
    "                    # Weighted importance sampling\n",
    "                    self.C[state][action] += W\n",
    "                    if self.C[state][action] > 0:\n",
    "                        self.Q[state][action] += (W / self.C[state][action]) * (G - self.Q[state][action])\n",
    "                else:\n",
    "                    # Ordinary importance sampling\n",
    "                    self.Q[state][action] += W * (G - self.Q[state][action]) / max(1, episode_num + 1)\n",
    "                \n",
    "                # Update importance sampling weight\n",
    "                target_prob = self.target_policy_prob(state, action)\n",
    "                behavior_prob = self.behavior_policy_prob(state, action)\n",
    "                \n",
    "                if behavior_prob == 0:\n",
    "                    break  # Cannot continue if behavior policy has 0 probability\n",
    "                \n",
    "                W = W * target_prob / behavior_prob\n",
    "                \n",
    "                if W == 0:\n",
    "                    break  # If target policy has 0 probability, truncate\n",
    "            \n",
    "            # Track importance ratio for analysis\n",
    "            if len(episode) > 0:\n",
    "                avg_ratio = W / len(episode) if len(episode) > 0 else 0\n",
    "                self.importance_ratios.append(avg_ratio)\n",
    "            \n",
    "            if episode_num % 2000 == 0 and verbose:\n",
    "                avg_reward = np.mean(self.episode_rewards[-200:]) if len(self.episode_rewards) >= 200 else np.mean(self.episode_rewards)\n",
    "                avg_length = np.mean(self.episode_lengths[-200:]) if len(self.episode_lengths) >= 200 else np.mean(self.episode_lengths)\n",
    "                avg_ratio = np.mean(self.importance_ratios[-200:]) if len(self.importance_ratios) >= 200 else 0\n",
    "                print(f\"Episode {episode_num}: Avg Reward = {avg_reward:.3f}, Avg Length = {avg_length:.1f}, Avg IS Ratio = {avg_ratio:.6f}\")\n",
    "    \n",
    "    def get_policy(self) -> Dict:\n",
    "        \"\"\"Extract target policy (greedy with respect to Q).\"\"\"\n",
    "        policy = {}\n",
    "        for state in self.states:\n",
    "            if not self.env.is_terminal(state):\n",
    "                policy[state] = np.argmax(self.Q[state])\n",
    "        return policy\n",
    "    \n",
    "    def get_value_function(self) -> Dict:\n",
    "        \"\"\"Extract value function from Q-values.\"\"\"\n",
    "        return {state: np.max(self.Q[state]) for state in self.states}\n",
    "\n",
    "print(\"Off-policy Monte Carlo agent implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train off-policy Monte Carlo agent\n",
    "print(\"Training Off-Policy Monte Carlo agent...\")\n",
    "\n",
    "off_policy_agent = OffPolicyMonteCarloAgent(cliff_env, gamma=0.9, epsilon=0.3, weighted_is=True)\n",
    "off_policy_agent.train(num_episodes=25000, verbose=True)\n",
    "\n",
    "off_policy_policy = off_policy_agent.get_policy()\n",
    "off_policy_values = off_policy_agent.get_value_function()\n",
    "\n",
    "print(f\"\\nOff-Policy Monte Carlo completed!\")\n",
    "print(f\"Final avg reward: {np.mean(off_policy_agent.episode_rewards[-200:]):.3f}\")\n",
    "print(f\"Final avg length: {np.mean(off_policy_agent.episode_lengths[-200:]):.1f}\")\n",
    "\n",
    "# Plot learning progress\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Episode rewards\n",
    "window = 300\n",
    "if len(off_policy_agent.episode_rewards) >= window:\n",
    "    smooth_rewards = pd.Series(off_policy_agent.episode_rewards).rolling(window=window).mean()\n",
    "    axes[0].plot(smooth_rewards, 'b-', linewidth=2, label='Off-policy MC')\n",
    "\n",
    "axes[0].plot(off_policy_agent.episode_rewards, alpha=0.2, color='blue')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Episode Reward')\n",
    "axes[0].set_title('Off-Policy MC: Learning Progress')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Importance sampling ratios\n",
    "if len(off_policy_agent.importance_ratios) >= window:\n",
    "    smooth_ratios = pd.Series(off_policy_agent.importance_ratios).rolling(window=window).mean()\n",
    "    axes[1].plot(smooth_ratios, 'r-', linewidth=2)\n",
    "\n",
    "axes[1].plot(off_policy_agent.importance_ratios, alpha=0.2, color='red')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Average Importance Ratio')\n",
    "axes[1].set_title('Importance Sampling Ratios')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "if len(off_policy_agent.episode_lengths) >= window:\n",
    "    smooth_lengths = pd.Series(off_policy_agent.episode_lengths).rolling(window=window).mean()\n",
    "    axes[2].plot(smooth_lengths, 'g-', linewidth=2)\n",
    "\n",
    "axes[2].plot(off_policy_agent.episode_lengths, alpha=0.2, color='green')\n",
    "axes[2].set_xlabel('Episode')\n",
    "axes[2].set_ylabel('Episode Length')\n",
    "axes[2].set_title('Episode Length Progress')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Algorithm Comparison\n",
    "\n",
    "Let's compare all the methods we've implemented to understand their relative strengths and convergence properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of all methods\n",
    "def comprehensive_algorithm_comparison():\n",
    "    \"\"\"Compare all implemented algorithms.\"\"\"\n",
    "    \n",
    "    algorithms = {\n",
    "        'Monte Carlo Control': {\n",
    "            'final_reward': np.mean(mc_agent.episode_rewards[-100:]),\n",
    "            'final_length': np.mean(mc_agent.episode_lengths[-100:]),\n",
    "            'type': 'On-policy MC',\n",
    "            'episodes': len(mc_agent.episode_rewards)\n",
    "        },\n",
    "        '1-Step SARSA': {\n",
    "            'final_reward': n_step_results[1]['final_reward'],\n",
    "            'final_length': n_step_results[1]['final_length'],\n",
    "            'type': 'n-step TD (n=1)',\n",
    "            'episodes': len(n_step_results[1]['agent'].episode_rewards)\n",
    "        },\n",
    "        '3-Step SARSA': {\n",
    "            'final_reward': n_step_results[3]['final_reward'],\n",
    "            'final_length': n_step_results[3]['final_length'],\n",
    "            'type': 'n-step TD (n=3)',\n",
    "            'episodes': len(n_step_results[3]['agent'].episode_rewards)\n",
    "        },\n",
    "        'SARSA(λ=0.7)': {\n",
    "            'final_reward': lambda_results[0.7]['final_reward'],\n",
    "            'final_length': lambda_results[0.7]['final_length'],\n",
    "            'type': 'Eligibility Traces',\n",
    "            'episodes': len(lambda_results[0.7]['agent'].episode_rewards)\n",
    "        },\n",
    "        'Off-Policy MC': {\n",
    "            'final_reward': np.mean(off_policy_agent.episode_rewards[-200:]),\n",
    "            'final_length': np.mean(off_policy_agent.episode_lengths[-200:]),\n",
    "            'type': 'Off-policy MC + IS',\n",
    "            'episodes': len(off_policy_agent.episode_rewards)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=== Comprehensive Algorithm Comparison ===\")\n",
    "    print(f\"{'Algorithm':<18} {'Type':<20} {'Final Reward':<15} {'Final Length':<15} {'Episodes':<10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for name, info in algorithms.items():\n",
    "        print(f\"{name:<18} {info['type']:<20} {info['final_reward']:<15.3f} {info['final_length']:<15.1f} {info['episodes']:<10}\")\n",
    "    \n",
    "    # Find best performing algorithm\n",
    "    best_algorithm = max(algorithms.keys(), key=lambda x: algorithms[x]['final_reward'])\n",
    "    print(f\"\\nBest performing: {best_algorithm} with {algorithms[best_algorithm]['final_reward']:.3f} reward\")\n",
    "    \n",
    "    return algorithms\n",
    "\n",
    "comparison_results = comprehensive_algorithm_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize algorithm comparison\n",
    "def plot_algorithm_comparison():\n",
    "    \"\"\"Plot comprehensive comparison of all algorithms.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # Final performance comparison\n",
    "    names = list(comparison_results.keys())\n",
    "    rewards = [comparison_results[name]['final_reward'] for name in names]\n",
    "    \n",
    "    bars = axes[0, 0].bar(range(len(names)), rewards, \n",
    "                         color=['skyblue', 'lightcoral', 'lightgreen', 'orange', 'plum'],\n",
    "                         alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Algorithm')\n",
    "    axes[0, 0].set_ylabel('Final Average Reward')\n",
    "    axes[0, 0].set_title('Final Performance Comparison')\n",
    "    axes[0, 0].set_xticks(range(len(names)))\n",
    "    axes[0, 0].set_xticklabels(names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, reward in zip(bars, rewards):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                       f'{reward:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Episode lengths comparison\n",
    "    lengths = [comparison_results[name]['final_length'] for name in names]\n",
    "    \n",
    "    bars2 = axes[0, 1].bar(range(len(names)), lengths,\n",
    "                          color=['skyblue', 'lightcoral', 'lightgreen', 'orange', 'plum'],\n",
    "                          alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Algorithm')\n",
    "    axes[0, 1].set_ylabel('Final Average Episode Length')\n",
    "    axes[0, 1].set_title('Episode Length Comparison')\n",
    "    axes[0, 1].set_xticks(range(len(names)))\n",
    "    axes[0, 1].set_xticklabels(names, rotation=45, ha='right')\n",
    "    \n",
    "    # Learning curves comparison (sample)\n",
    "    window = 200\n",
    "    \n",
    "    # Monte Carlo\n",
    "    if len(mc_agent.episode_rewards) >= window:\n",
    "        mc_smooth = pd.Series(mc_agent.episode_rewards).rolling(window=window).mean()\n",
    "        axes[1, 0].plot(mc_smooth, label='MC Control', alpha=0.8)\n",
    "    \n",
    "    # n-step SARSA (n=3)\n",
    "    agent_3step = n_step_results[3]['agent']\n",
    "    if len(agent_3step.episode_rewards) >= window:\n",
    "        sarsa3_smooth = pd.Series(agent_3step.episode_rewards).rolling(window=window).mean()\n",
    "        axes[1, 0].plot(sarsa3_smooth, label='3-Step SARSA', alpha=0.8)\n",
    "    \n",
    "    # SARSA(λ)\n",
    "    agent_lambda = lambda_results[0.7]['agent']\n",
    "    if len(agent_lambda.episode_rewards) >= window:\n",
    "        lambda_smooth = pd.Series(agent_lambda.episode_rewards).rolling(window=window).mean()\n",
    "        axes[1, 0].plot(lambda_smooth, label='SARSA(λ=0.7)', alpha=0.8)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('Average Episode Reward')\n",
    "    axes[1, 0].set_title('Learning Curves Comparison')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sample efficiency comparison\n",
    "    episodes_needed = []\n",
    "    \n",
    "    for name in names:\n",
    "        if name == 'Monte Carlo Control':\n",
    "            agent = mc_agent\n",
    "        elif name == '3-Step SARSA':\n",
    "            agent = n_step_results[3]['agent']\n",
    "        elif name == 'SARSA(λ=0.7)':\n",
    "            agent = lambda_results[0.7]['agent']\n",
    "        elif name == 'Off-Policy MC':\n",
    "            agent = off_policy_agent\n",
    "        else:\n",
    "            agent = n_step_results[1]['agent']\n",
    "        \n",
    "        # Find episodes to reach 90% of final performance\n",
    "        final_perf = comparison_results[name]['final_reward']\n",
    "        target = final_perf * 0.9\n",
    "        episodes_to_target = len(agent.episode_rewards)\n",
    "        \n",
    "        window_small = min(100, len(agent.episode_rewards) // 10)\n",
    "        if window_small > 1:\n",
    "            for i in range(window_small, len(agent.episode_rewards)):\n",
    "                if np.mean(agent.episode_rewards[i-window_small:i]) >= target:\n",
    "                    episodes_to_target = i\n",
    "                    break\n",
    "        \n",
    "        episodes_needed.append(episodes_to_target)\n",
    "    \n",
    "    bars3 = axes[1, 1].bar(range(len(names)), episodes_needed,\n",
    "                          color=['skyblue', 'lightcoral', 'lightgreen', 'orange', 'plum'],\n",
    "                          alpha=0.8)\n",
    "    axes[1, 1].set_xlabel('Algorithm')\n",
    "    axes[1, 1].set_ylabel('Episodes to 90% Performance')\n",
    "    axes[1, 1].set_title('Sample Efficiency Comparison')\n",
    "    axes[1, 1].set_xticks(range(len(names)))\n",
    "    axes[1, 1].set_xticklabels(names, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_algorithm_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Theoretical Analysis and Key Insights\n",
    "\n",
    "From our comprehensive implementation and comparison, we can extract several key theoretical and practical insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze convergence properties and insights\n",
    "def analyze_convergence_properties():\n",
    "    \"\"\"Analyze and summarize convergence properties of different methods.\"\"\"\n",
    "    \n",
    "    print(\"=== Theoretical Analysis and Key Insights ===\")\n",
    "    print()\n",
    "    \n",
    "    print(\"1. BIAS-VARIANCE TRADE-OFF:\")\n",
    "    print(\"   • Monte Carlo: Low bias, high variance (uses full returns)\")\n",
    "    print(\"   • TD(0): Higher bias, lower variance (1-step bootstrap)\")\n",
    "    print(\"   • n-step: Intermediate bias-variance (n-step bootstrap)\")\n",
    "    print(\"   • TD(λ): Weighted combination of all n-step returns\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. SAMPLE EFFICIENCY:\")\n",
    "    # Analyze which method learns fastest\n",
    "    fastest_learner = min(comparison_results.keys(), \n",
    "                         key=lambda x: comparison_results[x]['episodes'])\n",
    "    print(f\"   • Fastest convergence: {fastest_learner}\")\n",
    "    print(\"   • TD methods generally more sample efficient than MC\")\n",
    "    print(\"   • Eligibility traces accelerate learning via backward credit assignment\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. ON-POLICY vs OFF-POLICY:\")\n",
    "    on_policy_reward = comparison_results['Monte Carlo Control']['final_reward']\n",
    "    off_policy_reward = comparison_results['Off-Policy MC']['final_reward']\n",
    "    print(f\"   • On-policy MC final reward: {on_policy_reward:.3f}\")\n",
    "    print(f\"   • Off-policy MC final reward: {off_policy_reward:.3f}\")\n",
    "    if off_policy_reward < on_policy_reward:\n",
    "        print(\"   • Off-policy learning shows higher variance due to importance sampling\")\n",
    "    print(\"   • Off-policy enables learning optimal policy while exploring\")\n",
    "    print()\n",
    "    \n",
    "    print(\"4. PARAMETER SENSITIVITY:\")\n",
    "    print(\"   • n-step performance:\")\n",
    "    for n in [1, 3, 5, 10]:\n",
    "        if n in n_step_results:\n",
    "            print(f\"     - n={n}: {n_step_results[n]['final_reward']:.3f} reward\")\n",
    "    print(\"   • λ parameter performance:\")\n",
    "    for lam in [0.0, 0.3, 0.7, 0.9]:\n",
    "        if lam in lambda_results:\n",
    "            print(f\"     - λ={lam}: {lambda_results[lam]['final_reward']:.3f} reward\")\n",
    "    print()\n",
    "    \n",
    "    print(\"5. COMPUTATIONAL CONSIDERATIONS:\")\n",
    "    print(\"   • MC: Simple updates, requires episode completion\")\n",
    "    print(\"   • TD: Online updates, immediate feedback\")\n",
    "    print(\"   • Eligibility traces: More memory, backward credit assignment\")\n",
    "    print(\"   • Off-policy: Additional importance sampling computation\")\n",
    "    print()\n",
    "    \n",
    "    print(\"6. PRACTICAL RECOMMENDATIONS:\")\n",
    "    best_method = max(comparison_results.keys(), \n",
    "                     key=lambda x: comparison_results[x]['final_reward'])\n",
    "    print(f\"   • Best overall performance: {best_method}\")\n",
    "    print(\"   • For online learning: Use TD methods\")\n",
    "    print(\"   • For sample efficiency: Consider n-step or eligibility traces\")\n",
    "    print(\"   • For exploration: Use off-policy methods with ε-greedy\")\n",
    "    print(\"   • For stability: Start with λ ∈ [0.7, 0.9] for eligibility traces\")\n",
    "\n",
    "analyze_convergence_properties()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive notebook, we've explored the mathematical foundations and practical implementations of Monte Carlo and advanced Temporal Difference learning methods:\n",
    "\n",
    "### **Monte Carlo Methods**\n",
    "- **First-visit vs Every-visit**: Different approaches to handling multiple state visits\n",
    "- **MC Prediction**: Unbiased value function estimation using actual returns\n",
    "- **MC Control**: Policy optimization through generalized policy iteration\n",
    "- **Exploring Starts**: Ensuring sufficient exploration for convergence\n",
    "\n",
    "### **n-Step Temporal Difference Learning**\n",
    "- **n-Step Returns**: Balancing bias and variance through multi-step bootstrapping\n",
    "- **Parameter Selection**: Understanding the trade-offs for different n values\n",
    "- **Unified Framework**: Bridging TD(0) and Monte Carlo methods\n",
    "\n",
    "### **Eligibility Traces**\n",
    "- **TD(λ)**: Efficient implementation of n-step methods\n",
    "- **Forward vs Backward View**: Two equivalent perspectives on eligibility traces\n",
    "- **SARSA(λ)**: Action-value function learning with traces\n",
    "- **Trace Types**: Accumulating vs replacing traces\n",
    "\n",
    "### **Off-Policy Learning**\n",
    "- **Importance Sampling**: Correcting for policy mismatch\n",
    "- **Ordinary vs Weighted IS**: Bias-variance trade-offs in importance sampling\n",
    "- **Off-Policy Monte Carlo**: Learning target policy while following behavior policy\n",
    "\n",
    "### **Key Theoretical Insights**\n",
    "- **Bias-Variance Trade-off**: Fundamental concept affecting all RL algorithms\n",
    "- **Sample Efficiency**: TD methods generally more efficient than MC\n",
    "- **Credit Assignment**: Eligibility traces enable efficient backward credit assignment\n",
    "- **Exploration vs Exploitation**: Off-policy methods enable optimal policy learning with exploration\n",
    "\n",
    "### **Practical Guidelines**\n",
    "- Choose **n-step methods** when you can afford to wait n steps for updates\n",
    "- Use **eligibility traces** for efficient credit assignment and faster learning\n",
    "- Apply **off-policy methods** when you need to learn optimal policies while exploring\n",
    "- Set **λ ∈ [0.7, 0.9]** as a good starting point for eligibility trace parameters\n",
    "\n",
    "These methods form the foundation for understanding function approximation and deep reinforcement learning, which we'll explore in the next notebook as we transition from tabular to neural network-based approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}