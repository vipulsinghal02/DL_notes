{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Plan B - Part 5: Policy Gradient Methods\n",
    "\n",
    "This notebook explores policy gradient methods, moving from value-based learning (DQN) to direct policy optimization. We'll implement REINFORCE, Actor-Critic methods, and Proximal Policy Optimization (PPO).\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- Policy gradient theorem and REINFORCE algorithm\n",
    "- Actor-Critic methods and advantage estimation\n",
    "- Proximal Policy Optimization (PPO) theory and implementation\n",
    "- Continuous vs discrete action spaces\n",
    "- Variance reduction techniques\n",
    "- When to use policy methods vs value methods\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "\n",
    "The fundamental insight of policy gradient methods is to directly optimize the policy parameters $\\theta$ to maximize expected return:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\gamma^t r_t \\right]$$\n",
    "\n",
    "The policy gradient theorem states:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t \\right]$$\n",
    "\n",
    "Where $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$ is the return from time $t$.\n",
    "\n",
    "### REINFORCE Algorithm\n",
    "\n",
    "The REINFORCE algorithm uses Monte Carlo sampling to estimate the policy gradient:\n",
    "\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$$\n",
    "\n",
    "### Baseline Reduction\n",
    "\n",
    "To reduce variance, we subtract a baseline $b(s_t)$ that doesn't depend on the action:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (G_t - b(s_t)) \\right]$$\n",
    "\n",
    "### Actor-Critic Methods\n",
    "\n",
    "Actor-Critic methods use a value function $V^\\pi(s)$ as the baseline and replace returns with TD targets:\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t)$$\n",
    "\n",
    "The advantage estimate is: $A_t = \\delta_t$ (1-step) or $A_t = \\sum_{k=0}^{n-1} \\gamma^k \\delta_{t+k}$ (n-step).\n",
    "\n",
    "### Proximal Policy Optimization (PPO)\n",
    "\n",
    "PPO addresses the challenge of step size in policy optimization by constraining policy updates:\n",
    "\n",
    "$$L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min\\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right]$$\n",
    "\n",
    "Where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$ is the probability ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "import random\n",
    "import math\n",
    "from collections import deque, namedtuple\n",
    "from typing import List, Tuple, Optional, Dict, Any, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try different gym versions\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    gym_version = 'gymnasium'\n",
    "except ImportError:\n",
    "    import gym\n",
    "    gym_version = 'gym'\n",
    "\n",
    "print(f\"Using {gym_version} for environments\")\n",
    "\n",
    "# Set device - optimized for MacBook Air M2\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) for acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for acceleration\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network Architectures\n",
    "\n",
    "We'll implement policy networks for both discrete and continuous action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicy(nn.Module):\n",
    "    \"\"\"Policy network for discrete action spaces.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 128]):\n",
    "        super(DiscretePolicy, self).__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.Tanh(),  # Tanh often works better than ReLU for policy networks\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no activation - we'll apply softmax later)\n",
    "        layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize network weights for policy networks.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Smaller initialization for policy networks\n",
    "            nn.init.orthogonal_(module.weight, gain=0.1)\n",
    "            nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> Categorical:\n",
    "        \"\"\"Forward pass returning action distribution.\"\"\"\n",
    "        logits = self.network(state)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "\n",
    "class ContinuousPolicy(nn.Module):\n",
    "    \"\"\"Policy network for continuous action spaces.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [128, 128],\n",
    "                 log_std_init: float = -0.5):\n",
    "        super(ContinuousPolicy, self).__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Mean network\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output mean\n",
    "        layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        layers.append(nn.Tanh())  # For environments with bounded actions\n",
    "        \n",
    "        self.mean_network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Log standard deviation (learnable parameter)\n",
    "        self.log_std = nn.Parameter(torch.ones(action_dim) * log_std_init)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=0.1)\n",
    "            nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> Normal:\n",
    "        \"\"\"Forward pass returning action distribution.\"\"\"\n",
    "        mean = self.mean_network(state)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mean, std)\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Value function network for Actor-Critic methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, hidden_dims: List[int] = [128, 128]):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),  # ReLU works well for value networks\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (single value)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=1.0)\n",
    "            nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returning state values.\"\"\"\n",
    "        return self.network(state).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Algorithm Implementation\n",
    "\n",
    "Let's implement the classic REINFORCE algorithm with baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE algorithm with optional baseline.\n",
    "    \n",
    "    Implements the policy gradient theorem with Monte Carlo returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr_policy: float = 3e-4,\n",
    "        lr_value: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        use_baseline: bool = True,\n",
    "        hidden_dims: List[int] = [128, 128],\n",
    "        action_type: str = 'discrete'  # 'discrete' or 'continuous'\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.use_baseline = use_baseline\n",
    "        self.action_type = action_type\n",
    "        \n",
    "        # Create policy network\n",
    "        if action_type == 'discrete':\n",
    "            self.policy = DiscretePolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        else:\n",
    "            self.policy = ContinuousPolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        \n",
    "        # Create value network for baseline\n",
    "        if use_baseline:\n",
    "            self.value_net = ValueNetwork(state_dim, hidden_dims).to(device)\n",
    "            self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)\n",
    "        \n",
    "        # Policy optimizer\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
    "        \n",
    "        # Storage for episode data\n",
    "        self.reset_episode_data()\n",
    "        \n",
    "        # Training statistics\n",
    "        self.episode_rewards = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "    \n",
    "    def reset_episode_data(self):\n",
    "        \"\"\"Reset episode data storage.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> Union[int, np.ndarray]:\n",
    "        \"\"\"Select action using current policy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_dist = self.policy(state_tensor)\n",
    "            \n",
    "            if training:\n",
    "                action = action_dist.sample()\n",
    "            else:\n",
    "                # Use mean action for evaluation\n",
    "                if self.action_type == 'discrete':\n",
    "                    action = action_dist.probs.argmax(dim=-1)\n",
    "                else:\n",
    "                    action = action_dist.mean\n",
    "        \n",
    "        if training:\n",
    "            # Store data for training\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action.cpu().numpy())\n",
    "            self.log_probs.append(log_prob.item())\n",
    "        \n",
    "        if self.action_type == 'discrete':\n",
    "            return action.item()\n",
    "        else:\n",
    "            return action.cpu().numpy().squeeze()\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        \"\"\"Store reward for current step.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def compute_returns(self) -> List[float]:\n",
    "        \"\"\"Compute Monte Carlo returns.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        # Compute returns backward\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update_policy(self) -> Tuple[float, Optional[float]]:\n",
    "        \"\"\"Update policy using REINFORCE.\"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return 0.0, None\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = self.compute_returns()\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Normalize returns for stability\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Convert stored data to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states)).to(device)\n",
    "        log_probs = torch.FloatTensor(self.log_probs).to(device)\n",
    "        \n",
    "        # Compute baseline if using\n",
    "        baseline = None\n",
    "        value_loss = None\n",
    "        \n",
    "        if self.use_baseline:\n",
    "            # Update value function\n",
    "            values = self.value_net(states)\n",
    "            value_loss = F.mse_loss(values, returns)\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), max_norm=0.5)\n",
    "            self.value_optimizer.step()\n",
    "            \n",
    "            # Use value function as baseline\n",
    "            with torch.no_grad():\n",
    "                baseline = self.value_net(states)\n",
    "            \n",
    "            advantages = returns - baseline\n",
    "            value_loss = value_loss.item()\n",
    "        else:\n",
    "            advantages = returns\n",
    "        \n",
    "        # Policy gradient update\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # Reset episode data\n",
    "        self.reset_episode_data()\n",
    "        \n",
    "        return policy_loss.item(), value_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Implementation\n",
    "\n",
    "Actor-Critic methods reduce variance by using bootstrapping instead of full Monte Carlo returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    \"\"\"Actor-Critic algorithm with TD(0) updates.\n",
    "    \n",
    "    Combines policy gradients (actor) with value function learning (critic).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr_policy: float = 3e-4,\n",
    "        lr_value: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        hidden_dims: List[int] = [128, 128],\n",
    "        action_type: str = 'discrete',\n",
    "        entropy_coef: float = 0.01  # Entropy regularization\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.action_type = action_type\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # Create networks\n",
    "        if action_type == 'discrete':\n",
    "            self.policy = DiscretePolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        else:\n",
    "            self.policy = ContinuousPolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        \n",
    "        self.value_net = ValueNetwork(state_dim, hidden_dims).to(device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr_policy)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr_value)\n",
    "        \n",
    "        # Training statistics\n",
    "        self.episode_rewards = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> Union[int, np.ndarray]:\n",
    "        \"\"\"Select action using current policy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_dist = self.policy(state_tensor)\n",
    "            \n",
    "            if training:\n",
    "                action = action_dist.sample()\n",
    "            else:\n",
    "                # Use mean action for evaluation\n",
    "                if self.action_type == 'discrete':\n",
    "                    action = action_dist.probs.argmax(dim=-1)\n",
    "                else:\n",
    "                    action = action_dist.mean\n",
    "        \n",
    "        if self.action_type == 'discrete':\n",
    "            return action.item()\n",
    "        else:\n",
    "            return action.cpu().numpy().squeeze()\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: Union[int, np.ndarray],\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        done: bool\n",
    "    ) -> Tuple[float, float, float]:\n",
    "        \"\"\"Update actor and critic networks.\"\"\"\n",
    "        # Convert to tensors\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        reward_tensor = torch.FloatTensor([reward]).to(device)\n",
    "        \n",
    "        if self.action_type == 'discrete':\n",
    "            action_tensor = torch.LongTensor([action]).to(device)\n",
    "        else:\n",
    "            action_tensor = torch.FloatTensor(action).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Compute current and next state values\n",
    "        current_value = self.value_net(state_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if done:\n",
    "                next_value = torch.zeros(1).to(device)\n",
    "            else:\n",
    "                next_value = self.value_net(next_state_tensor)\n",
    "            \n",
    "            # Compute TD target and advantage\n",
    "            td_target = reward_tensor + self.gamma * next_value\n",
    "            advantage = td_target - current_value\n",
    "        \n",
    "        # Update critic (value function)\n",
    "        value_loss = F.mse_loss(current_value, td_target)\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), max_norm=0.5)\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        # Update actor (policy)\n",
    "        action_dist = self.policy(state_tensor)\n",
    "        log_prob = action_dist.log_prob(action_tensor.squeeze())\n",
    "        entropy = action_dist.entropy()\n",
    "        \n",
    "        # Policy loss with entropy regularization\n",
    "        policy_loss = -(log_prob * advantage.detach()).mean() - self.entropy_coef * entropy.mean()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item(), entropy.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO) Implementation\n",
    "\n",
    "PPO is currently one of the most popular policy gradient methods due to its stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"Proximal Policy Optimization (PPO) agent.\n",
    "    \n",
    "    Implements clipped surrogate objective for stable policy updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,  # GAE parameter\n",
    "        clip_epsilon: float = 0.2,  # PPO clipping parameter\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        max_grad_norm: float = 0.5,\n",
    "        ppo_epochs: int = 4,  # Number of PPO epochs per update\n",
    "        mini_batch_size: int = 64,\n",
    "        hidden_dims: List[int] = [128, 128],\n",
    "        action_type: str = 'discrete'\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.action_type = action_type\n",
    "        \n",
    "        # Create networks\n",
    "        if action_type == 'discrete':\n",
    "            self.policy = DiscretePolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        else:\n",
    "            self.policy = ContinuousPolicy(state_dim, action_dim, hidden_dims).to(device)\n",
    "        \n",
    "        self.value_net = ValueNetwork(state_dim, hidden_dims).to(device)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.policy.parameters()) + list(self.value_net.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "        \n",
    "        # Storage for trajectory data\n",
    "        self.reset_storage()\n",
    "        \n",
    "        # Training statistics\n",
    "        self.episode_rewards = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def reset_storage(self):\n",
    "        \"\"\"Reset trajectory storage.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> Union[int, np.ndarray]:\n",
    "        \"\"\"Select action and store data for training.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_dist = self.policy(state_tensor)\n",
    "            value = self.value_net(state_tensor)\n",
    "            \n",
    "            if training:\n",
    "                action = action_dist.sample()\n",
    "                log_prob = action_dist.log_prob(action)\n",
    "                \n",
    "                # Store for training\n",
    "                self.states.append(state)\n",
    "                self.actions.append(action.cpu().numpy())\n",
    "                self.values.append(value.item())\n",
    "                self.log_probs.append(log_prob.item())\n",
    "            else:\n",
    "                # Use mean action for evaluation\n",
    "                if self.action_type == 'discrete':\n",
    "                    action = action_dist.probs.argmax(dim=-1)\n",
    "                else:\n",
    "                    action = action_dist.mean\n",
    "        \n",
    "        if self.action_type == 'discrete':\n",
    "            return action.item()\n",
    "        else:\n",
    "            return action.cpu().numpy().squeeze()\n",
    "    \n",
    "    def store_transition(self, reward: float, done: bool):\n",
    "        \"\"\"Store reward and done flag.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_gae(self, next_value: float = 0.0) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"Compute Generalized Advantage Estimation (GAE).\"\"\"\n",
    "        values = self.values + [next_value]\n",
    "        advantages = []\n",
    "        returns = []\n",
    "        \n",
    "        gae = 0\n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            delta = self.rewards[i] + self.gamma * values[i + 1] * (1 - self.dones[i]) - values[i]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - self.dones[i]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "            returns.insert(0, gae + values[i])\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def update(self, next_state: Optional[np.ndarray] = None) -> Dict[str, float]:\n",
    "        \"\"\"Update policy using PPO.\"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return {'policy_loss': 0.0, 'value_loss': 0.0, 'entropy': 0.0}\n",
    "        \n",
    "        # Compute next state value for GAE\n",
    "        next_value = 0.0\n",
    "        if next_state is not None:\n",
    "            with torch.no_grad():\n",
    "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "                next_value = self.value_net(next_state_tensor).item()\n",
    "        \n",
    "        # Compute advantages and returns\n",
    "        advantages, returns = self.compute_gae(next_value)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states)).to(device)\n",
    "        if self.action_type == 'discrete':\n",
    "            actions = torch.LongTensor(np.array(self.actions).squeeze()).to(device)\n",
    "        else:\n",
    "            actions = torch.FloatTensor(np.array(self.actions)).to(device)\n",
    "        \n",
    "        old_log_probs = torch.FloatTensor(self.log_probs).to(device)\n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update\n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        \n",
    "        for _ in range(self.ppo_epochs):\n",
    "            # Create mini-batches\n",
    "            batch_size = states.size(0)\n",
    "            indices = torch.randperm(batch_size)\n",
    "            \n",
    "            for start_idx in range(0, batch_size, self.mini_batch_size):\n",
    "                end_idx = min(start_idx + self.mini_batch_size, batch_size)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                # Get batch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                # Forward pass\n",
    "                action_dist = self.policy(batch_states)\n",
    "                values = self.value_net(batch_states)\n",
    "                \n",
    "                # Compute probability ratio\n",
    "                log_probs = action_dist.log_prob(batch_actions)\n",
    "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Clipped surrogate objective\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values, batch_returns)\n",
    "                \n",
    "                # Entropy loss\n",
    "                entropy = action_dist.entropy().mean()\n",
    "                \n",
    "                # Total loss\n",
    "                total_loss = policy_loss + self.value_coef * value_loss - self.entropy_coef * entropy\n",
    "                \n",
    "                # Update\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(self.policy.parameters()) + list(self.value_net.parameters()),\n",
    "                    self.max_grad_norm\n",
    "                )\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Accumulate losses\n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "                total_entropy += entropy.item()\n",
    "        \n",
    "        # Reset storage\n",
    "        self.reset_storage()\n",
    "        \n",
    "        # Return average losses\n",
    "        num_updates = self.ppo_epochs * math.ceil(batch_size / self.mini_batch_size)\n",
    "        return {\n",
    "            'policy_loss': total_policy_loss / num_updates,\n",
    "            'value_loss': total_value_loss / num_updates,\n",
    "            'entropy': total_entropy / num_updates\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Wrappers\n",
    "\n",
    "Let's create wrappers for both discrete and continuous control environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleWrapper:\n",
    "    \"\"\"CartPole environment wrapper for discrete action space.\"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        try:\n",
    "            self.env = gym.make('CartPole-v1', render_mode=render_mode)\n",
    "        except:\n",
    "            self.env = gym.make('CartPole-v1')\n",
    "        \n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.action_type = 'discrete'\n",
    "        \n",
    "        # Normalization parameters\n",
    "        self.obs_mean = np.zeros(self.state_dim)\n",
    "        self.obs_std = np.ones(self.state_dim)\n",
    "        self.obs_count = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        if gym_version == 'gymnasium':\n",
    "            obs, _ = self.env.reset()\n",
    "        else:\n",
    "            obs = self.env.reset()\n",
    "        return self._normalize_obs(obs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        if gym_version == 'gym':\n",
    "            done = done or truncated\n",
    "        return self._normalize_obs(obs), reward, done, info\n",
    "    \n",
    "    def _normalize_obs(self, obs):\n",
    "        \"\"\"Simple online normalization.\"\"\"\n",
    "        self.obs_count += 1\n",
    "        delta = obs - self.obs_mean\n",
    "        self.obs_mean += delta / self.obs_count\n",
    "        self.obs_std = np.sqrt(((self.obs_count - 1) * self.obs_std**2 + delta * (obs - self.obs_mean)) / self.obs_count)\n",
    "        self.obs_std = np.maximum(self.obs_std, 1e-8)\n",
    "        return (obs - self.obs_mean) / self.obs_std\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "class PendulumWrapper:\n",
    "    \"\"\"Pendulum environment wrapper for continuous action space.\"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        try:\n",
    "            self.env = gym.make('Pendulum-v1', render_mode=render_mode)\n",
    "        except:\n",
    "            try:\n",
    "                self.env = gym.make('Pendulum-v1')\n",
    "            except:\n",
    "                # Fallback to a simple continuous environment\n",
    "                print(\"Pendulum not available, using simulated environment\")\n",
    "                self.env = None\n",
    "        \n",
    "        if self.env is not None:\n",
    "            self.state_dim = self.env.observation_space.shape[0]\n",
    "            self.action_dim = self.env.action_space.shape[0]\n",
    "            self.action_type = 'continuous'\n",
    "            \n",
    "            # Action scaling\n",
    "            self.action_scale = (self.env.action_space.high - self.env.action_space.low) / 2.0\n",
    "            self.action_bias = (self.env.action_space.high + self.env.action_space.low) / 2.0\n",
    "        else:\n",
    "            # Simulated environment\n",
    "            self.state_dim = 3\n",
    "            self.action_dim = 1\n",
    "            self.action_type = 'continuous'\n",
    "            self.action_scale = np.array([2.0])\n",
    "            self.action_bias = np.array([0.0])\n",
    "            \n",
    "            # Simple pendulum simulation state\n",
    "            self.angle = 0.0\n",
    "            self.angular_velocity = 0.0\n",
    "            self.max_steps = 200\n",
    "            self.current_step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        if self.env is not None:\n",
    "            if gym_version == 'gymnasium':\n",
    "                obs, _ = self.env.reset()\n",
    "            else:\n",
    "                obs = self.env.reset()\n",
    "            return obs\n",
    "        else:\n",
    "            # Reset simulated environment\n",
    "            self.angle = np.random.uniform(-np.pi, np.pi)\n",
    "            self.angular_velocity = np.random.uniform(-1, 1)\n",
    "            self.current_step = 0\n",
    "            return np.array([np.cos(self.angle), np.sin(self.angle), self.angular_velocity])\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.env is not None:\n",
    "            # Scale action to environment range\n",
    "            scaled_action = action * self.action_scale + self.action_bias\n",
    "            obs, reward, done, truncated, info = self.env.step(scaled_action)\n",
    "            if gym_version == 'gym':\n",
    "                done = done or truncated\n",
    "            return obs, reward, done, info\n",
    "        else:\n",
    "            # Simple pendulum simulation\n",
    "            dt = 0.05\n",
    "            g = 10.0\n",
    "            m = 1.0\n",
    "            l = 1.0\n",
    "            \n",
    "            # Clip action\n",
    "            action = np.clip(action, -2.0, 2.0)\n",
    "            \n",
    "            # Simple physics update\n",
    "            self.angular_velocity += dt * (-3 * g / (2 * l) * np.sin(self.angle + np.pi) + 3.0 / (m * l**2) * action)\n",
    "            self.angle += dt * self.angular_velocity\n",
    "            self.angular_velocity = np.clip(self.angular_velocity, -8, 8)\n",
    "            \n",
    "            # Normalize angle\n",
    "            self.angle = ((self.angle + np.pi) % (2 * np.pi)) - np.pi\n",
    "            \n",
    "            # Compute reward\n",
    "            reward = -(self.angle**2 + 0.1 * self.angular_velocity**2 + 0.001 * action**2)\n",
    "            \n",
    "            # Check termination\n",
    "            self.current_step += 1\n",
    "            done = self.current_step >= self.max_steps\n",
    "            \n",
    "            obs = np.array([np.cos(self.angle), np.sin(self.angle), self.angular_velocity])\n",
    "            return obs, reward, done, {}\n",
    "    \n",
    "    def close(self):\n",
    "        if self.env is not None:\n",
    "            self.env.close()\n",
    "\n",
    "\n",
    "# Test environments\n",
    "print(\"Testing discrete environment (CartPole):\")\n",
    "env_discrete = CartPoleWrapper()\n",
    "state = env_discrete.reset()\n",
    "print(f\"State dim: {env_discrete.state_dim}, Action dim: {env_discrete.action_dim}\")\n",
    "print(f\"Sample state: {state}\")\n",
    "env_discrete.close()\n",
    "\n",
    "print(\"\\nTesting continuous environment (Pendulum):\")\n",
    "env_continuous = PendulumWrapper()\n",
    "state = env_continuous.reset()\n",
    "print(f\"State dim: {env_continuous.state_dim}, Action dim: {env_continuous.action_dim}\")\n",
    "print(f\"Sample state: {state}\")\n",
    "action = np.random.uniform(-1, 1, env_continuous.action_dim)\n",
    "next_state, reward, done, _ = env_continuous.step(action)\n",
    "print(f\"Sample action: {action}, Reward: {reward:.3f}\")\n",
    "env_continuous.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_agent(\n",
    "    agent,\n",
    "    env,\n",
    "    num_episodes: int = 500,\n",
    "    max_steps: int = 500,\n",
    "    verbose: bool = True,\n",
    "    agent_type: str = 'reinforce'\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"Train policy gradient agent.\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    \n",
    "    best_reward = -float('inf')\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_policy_loss = []\n",
    "        episode_value_loss = []\n",
    "        episode_entropy = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action = agent.select_action(state, training=True)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if agent_type == 'reinforce':\n",
    "                # Store reward for REINFORCE\n",
    "                agent.store_reward(reward)\n",
    "            elif agent_type == 'actor_critic':\n",
    "                # Update online for Actor-Critic\n",
    "                policy_loss, value_loss, entropy = agent.update(state, action, reward, next_state, done)\n",
    "                episode_policy_loss.append(policy_loss)\n",
    "                episode_value_loss.append(value_loss)\n",
    "                episode_entropy.append(entropy)\n",
    "            elif agent_type == 'ppo':\n",
    "                # Store transition for PPO\n",
    "                agent.store_transition(reward, done)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Update policy (for REINFORCE and PPO)\n",
    "        if agent_type == 'reinforce':\n",
    "            policy_loss, value_loss = agent.update_policy()\n",
    "            episode_policy_loss.append(policy_loss)\n",
    "            if value_loss is not None:\n",
    "                episode_value_loss.append(value_loss)\n",
    "        elif agent_type == 'ppo':\n",
    "            if not done:\n",
    "                # Pass next state for GAE computation\n",
    "                losses = agent.update(next_state)\n",
    "            else:\n",
    "                losses = agent.update()\n",
    "            \n",
    "            episode_policy_loss.append(losses['policy_loss'])\n",
    "            episode_value_loss.append(losses['value_loss'])\n",
    "            episode_entropy.append(losses['entropy'])\n",
    "        \n",
    "        # Record statistics\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if episode_policy_loss:\n",
    "            policy_losses.append(np.mean(episode_policy_loss))\n",
    "        else:\n",
    "            policy_losses.append(0.0)\n",
    "        \n",
    "        if episode_value_loss:\n",
    "            value_losses.append(np.mean(episode_value_loss))\n",
    "        else:\n",
    "            value_losses.append(0.0)\n",
    "        \n",
    "        if episode_entropy:\n",
    "            entropies.append(np.mean(episode_entropy))\n",
    "        else:\n",
    "            entropies.append(0.0)\n",
    "        \n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and episode % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, \"\n",
    "                  f\"Policy Loss: {policy_losses[-1]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'policy_losses': policy_losses,\n",
    "        'value_losses': value_losses,\n",
    "        'entropies': entropies,\n",
    "        'best_reward': best_reward\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_policy_agent(agent, env, num_episodes: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate trained policy agent.\"\"\"\n",
    "    rewards = []\n",
    "    lengths = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state, training=False)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done or steps >= 500:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        lengths.append(steps)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards),\n",
    "        'mean_length': np.mean(lengths),\n",
    "        'std_length': np.std(lengths)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: REINFORCE vs REINFORCE with Baseline\n",
    "\n",
    "Compare vanilla REINFORCE with baseline reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments\n",
    "env_vanilla = CartPoleWrapper()\n",
    "env_baseline = CartPoleWrapper()\n",
    "\n",
    "# Create agents\n",
    "agent_vanilla = REINFORCEAgent(\n",
    "    state_dim=env_vanilla.state_dim,\n",
    "    action_dim=env_vanilla.action_dim,\n",
    "    lr_policy=3e-4,\n",
    "    gamma=0.99,\n",
    "    use_baseline=False,  # Vanilla REINFORCE\n",
    "    hidden_dims=[128, 128],\n",
    "    action_type='discrete'\n",
    ")\n",
    "\n",
    "agent_baseline = REINFORCEAgent(\n",
    "    state_dim=env_baseline.state_dim,\n",
    "    action_dim=env_baseline.action_dim,\n",
    "    lr_policy=3e-4,\n",
    "    gamma=0.99,\n",
    "    use_baseline=True,  # REINFORCE with baseline\n",
    "    hidden_dims=[128, 128],\n",
    "    action_type='discrete'\n",
    ")\n",
    "\n",
    "print(\"Training Vanilla REINFORCE...\")\n",
    "results_vanilla = train_policy_agent(agent_vanilla, env_vanilla, num_episodes=300, \n",
    "                                   verbose=False, agent_type='reinforce')\n",
    "\n",
    "print(\"Training REINFORCE with Baseline...\")\n",
    "results_baseline = train_policy_agent(agent_baseline, env_baseline, num_episodes=300, \n",
    "                                    verbose=False, agent_type='reinforce')\n",
    "\n",
    "# Evaluate both agents\n",
    "print(\"\\nEvaluating Vanilla REINFORCE...\")\n",
    "eval_vanilla = evaluate_policy_agent(agent_vanilla, env_vanilla, num_episodes=100)\n",
    "\n",
    "print(\"Evaluating REINFORCE with Baseline...\")\n",
    "eval_baseline = evaluate_policy_agent(agent_baseline, env_baseline, num_episodes=100)\n",
    "\n",
    "print(f\"\\nVanilla REINFORCE Results:\")\n",
    "print(f\"  Mean Reward: {eval_vanilla['mean_reward']:.2f} ± {eval_vanilla['std_reward']:.2f}\")\n",
    "print(f\"  Mean Episode Length: {eval_vanilla['mean_length']:.1f}\")\n",
    "\n",
    "print(f\"\\nREINFORCE with Baseline Results:\")\n",
    "print(f\"  Mean Reward: {eval_baseline['mean_reward']:.2f} ± {eval_baseline['std_reward']:.2f}\")\n",
    "print(f\"  Mean Episode Length: {eval_baseline['mean_length']:.1f}\")\n",
    "\n",
    "# Clean up\n",
    "env_vanilla.close()\n",
    "env_baseline.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: REINFORCE vs Actor-Critic vs PPO\n",
    "\n",
    "Compare different policy gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments\n",
    "env_reinforce = CartPoleWrapper()\n",
    "env_ac = CartPoleWrapper()\n",
    "env_ppo = CartPoleWrapper()\n",
    "\n",
    "# Create agents\n",
    "agent_reinforce = REINFORCEAgent(\n",
    "    state_dim=env_reinforce.state_dim,\n",
    "    action_dim=env_reinforce.action_dim,\n",
    "    lr_policy=3e-4,\n",
    "    gamma=0.99,\n",
    "    use_baseline=True,\n",
    "    hidden_dims=[128, 128],\n",
    "    action_type='discrete'\n",
    ")\n",
    "\n",
    "agent_ac = ActorCriticAgent(\n",
    "    state_dim=env_ac.state_dim,\n",
    "    action_dim=env_ac.action_dim,\n",
    "    lr_policy=3e-4,\n",
    "    lr_value=1e-3,\n",
    "    gamma=0.99,\n",
    "    hidden_dims=[128, 128],\n",
    "    action_type='discrete',\n",
    "    entropy_coef=0.01\n",
    ")\n",
    "\n",
    "agent_ppo = PPOAgent(\n",
    "    state_dim=env_ppo.state_dim,\n",
    "    action_dim=env_ppo.action_dim,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    entropy_coef=0.01,\n",
    "    hidden_dims=[128, 128],\n",
    "    action_type='discrete'\n",
    ")\n",
    "\n",
    "print(\"Training REINFORCE with Baseline...\")\n",
    "results_reinforce = train_policy_agent(agent_reinforce, env_reinforce, num_episodes=300, \n",
    "                                     verbose=False, agent_type='reinforce')\n",
    "\n",
    "print(\"Training Actor-Critic...\")\n",
    "results_ac = train_policy_agent(agent_ac, env_ac, num_episodes=300, \n",
    "                              verbose=False, agent_type='actor_critic')\n",
    "\n",
    "print(\"Training PPO...\")\n",
    "results_ppo = train_policy_agent(agent_ppo, env_ppo, num_episodes=300, \n",
    "                               verbose=False, agent_type='ppo')\n",
    "\n",
    "# Evaluate all agents\n",
    "print(\"\\nEvaluating REINFORCE...\")\n",
    "eval_reinforce = evaluate_policy_agent(agent_reinforce, env_reinforce, num_episodes=100)\n",
    "\n",
    "print(\"Evaluating Actor-Critic...\")\n",
    "eval_ac = evaluate_policy_agent(agent_ac, env_ac, num_episodes=100)\n",
    "\n",
    "print(\"Evaluating PPO...\")\n",
    "eval_ppo = evaluate_policy_agent(agent_ppo, env_ppo, num_episodes=100)\n",
    "\n",
    "print(f\"\\nREINFORCE Results:\")\n",
    "print(f\"  Mean Reward: {eval_reinforce['mean_reward']:.2f} ± {eval_reinforce['std_reward']:.2f}\")\n",
    "\n",
    "print(f\"\\nActor-Critic Results:\")\n",
    "print(f\"  Mean Reward: {eval_ac['mean_reward']:.2f} ± {eval_ac['std_reward']:.2f}\")\n",
    "\n",
    "print(f\"\\nPPO Results:\")\n",
    "print(f\"  Mean Reward: {eval_ppo['mean_reward']:.2f} ± {eval_ppo['std_reward']:.2f}\")\n",
    "\n",
    "# Clean up\n",
    "env_reinforce.close()\n",
    "env_ac.close()\n",
    "env_ppo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Continuous Control with PPO\n",
    "\n",
    "Test PPO on continuous action space (Pendulum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create continuous environment\n",
    "env_continuous = PendulumWrapper()\n",
    "\n",
    "# Create PPO agent for continuous control\n",
    "agent_continuous = PPOAgent(\n",
    "    state_dim=env_continuous.state_dim,\n",
    "    action_dim=env_continuous.action_dim,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    entropy_coef=0.01,\n",
    "    hidden_dims=[128, 128],\n",
    "    action_type='continuous'  # Continuous action space\n",
    ")\n",
    "\n",
    "print(\"Training PPO on Continuous Control (Pendulum)...\")\n",
    "results_continuous = train_policy_agent(agent_continuous, env_continuous, num_episodes=400, \n",
    "                                      max_steps=200, verbose=True, agent_type='ppo')\n",
    "\n",
    "# Evaluate continuous agent\n",
    "print(\"\\nEvaluating Continuous PPO...\")\n",
    "eval_continuous = evaluate_policy_agent(agent_continuous, env_continuous, num_episodes=50)\n",
    "\n",
    "print(f\"\\nContinuous PPO Results:\")\n",
    "print(f\"  Mean Reward: {eval_continuous['mean_reward']:.2f} ± {eval_continuous['std_reward']:.2f}\")\n",
    "print(f\"  Mean Episode Length: {eval_continuous['mean_length']:.1f}\")\n",
    "\n",
    "# Clean up\n",
    "env_continuous.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_training_results(results_dict: Dict[str, Dict], title: str = \"Policy Gradient Comparison\"):\n",
    "    \"\"\"Plot training results for policy gradient methods.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    # Episode rewards\n",
    "    ax = axes[0, 0]\n",
    "    for i, (name, results) in enumerate(results_dict.items()):\n",
    "        rewards = results['episode_rewards']\n",
    "        # Smooth with moving average\n",
    "        window_size = min(30, len(rewards) // 10)\n",
    "        if window_size > 1:\n",
    "            smoothed = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "            ax.plot(range(window_size-1, len(rewards)), smoothed, \n",
    "                   color=colors[i % len(colors)], label=f'{name} (smoothed)', linewidth=2)\n",
    "        ax.plot(rewards, color=colors[i % len(colors)], alpha=0.3, linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Episode Reward')\n",
    "    ax.set_title('Episode Rewards')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Policy loss\n",
    "    ax = axes[0, 1]\n",
    "    for i, (name, results) in enumerate(results_dict.items()):\n",
    "        losses = results['policy_losses']\n",
    "        # Smooth losses\n",
    "        window_size = min(30, len(losses) // 10)\n",
    "        if window_size > 1 and len(losses) > window_size:\n",
    "            smoothed_loss = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "            ax.plot(range(window_size-1, len(losses)), smoothed_loss, \n",
    "                   color=colors[i % len(colors)], label=name, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Policy Loss')\n",
    "    ax.set_title('Policy Training Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Value loss\n",
    "    ax = axes[1, 0]\n",
    "    for i, (name, results) in enumerate(results_dict.items()):\n",
    "        if 'value_losses' in results and any(v > 0 for v in results['value_losses']):\n",
    "            losses = results['value_losses']\n",
    "            # Smooth losses\n",
    "            window_size = min(30, len(losses) // 10)\n",
    "            if window_size > 1 and len(losses) > window_size:\n",
    "                smoothed_loss = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "                ax.plot(range(window_size-1, len(losses)), smoothed_loss, \n",
    "                       color=colors[i % len(colors)], label=name, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Value Loss')\n",
    "    ax.set_title('Value Function Training Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Entropy (if available)\n",
    "    ax = axes[1, 1]\n",
    "    for i, (name, results) in enumerate(results_dict.items()):\n",
    "        if 'entropies' in results and any(e > 0 for e in results['entropies']):\n",
    "            entropies = results['entropies']\n",
    "            # Smooth entropies\n",
    "            window_size = min(30, len(entropies) // 10)\n",
    "            if window_size > 1 and len(entropies) > window_size:\n",
    "                smoothed_entropy = np.convolve(entropies, np.ones(window_size)/window_size, mode='valid')\n",
    "                ax.plot(range(window_size-1, len(entropies)), smoothed_entropy, \n",
    "                       color=colors[i % len(colors)], label=name, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Entropy')\n",
    "    ax.set_title('Policy Entropy (Exploration)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot all comparisons\n",
    "print(\"Vanilla REINFORCE vs REINFORCE with Baseline:\")\n",
    "plot_policy_training_results({\n",
    "    'Vanilla REINFORCE': results_vanilla,\n",
    "    'REINFORCE + Baseline': results_baseline\n",
    "}, \"REINFORCE: Vanilla vs Baseline\")\n",
    "\n",
    "print(\"\\nPolicy Gradient Method Comparison:\")\n",
    "plot_policy_training_results({\n",
    "    'REINFORCE': results_reinforce,\n",
    "    'Actor-Critic': results_ac,\n",
    "    'PPO': results_ppo\n",
    "}, \"Policy Gradient Methods Comparison\")\n",
    "\n",
    "print(\"\\nContinuous Control (PPO on Pendulum):\")\n",
    "plot_policy_training_results({\n",
    "    'PPO Continuous': results_continuous\n",
    "}, \"PPO on Continuous Control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_policy_distributions(agent, env, num_states: int = 1000):\n",
    "    \"\"\"Analyze policy distributions learned by the agent.\"\"\"\n",
    "    \n",
    "    if env.action_type != 'discrete':\n",
    "        print(\"Analysis currently supports discrete action spaces only.\")\n",
    "        return\n",
    "    \n",
    "    # Collect states and action probabilities\n",
    "    states = []\n",
    "    action_probs = []\n",
    "    \n",
    "    for _ in range(num_states):\n",
    "        # Reset and take random steps to get diverse states\n",
    "        state = env.reset()\n",
    "        steps = np.random.randint(0, 50)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            action = env.env.action_space.sample()\n",
    "            state, _, done, _ = env.step(action)\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                break\n",
    "        \n",
    "        # Get action probabilities for this state\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action_dist = agent.policy(state_tensor)\n",
    "            probs = action_dist.probs.cpu().numpy()[0]\n",
    "        \n",
    "        states.append(state)\n",
    "        action_probs.append(probs)\n",
    "    \n",
    "    states = np.array(states)\n",
    "    action_probs = np.array(action_probs)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Policy Analysis: Action Probabilities vs State Features', fontsize=14)\n",
    "    \n",
    "    feature_names = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity']\n",
    "    \n",
    "    for i in range(4):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        # Plot action probabilities vs state feature\n",
    "        ax.scatter(states[:, i], action_probs[:, 0], \n",
    "                  alpha=0.5, s=10, label='Action 0 (Left)', color='blue')\n",
    "        ax.scatter(states[:, i], action_probs[:, 1], \n",
    "                  alpha=0.5, s=10, label='Action 1 (Right)', color='red')\n",
    "        \n",
    "        ax.set_xlabel(feature_names[i])\n",
    "        ax.set_ylabel('Action Probability')\n",
    "        ax.set_title(f'Policy vs {feature_names[i]}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nPolicy Statistics:\")\n",
    "    print(f\"Mean Action 0 Probability: {np.mean(action_probs[:, 0]):.3f}\")\n",
    "    print(f\"Mean Action 1 Probability: {np.mean(action_probs[:, 1]):.3f}\")\n",
    "    \n",
    "    # Analyze policy determinism\n",
    "    max_probs = np.max(action_probs, axis=1)\n",
    "    determinism = np.mean(max_probs)\n",
    "    print(f\"Policy Determinism (mean max prob): {determinism:.3f}\")\n",
    "    \n",
    "    # Entropy analysis\n",
    "    entropies = -np.sum(action_probs * np.log(action_probs + 1e-8), axis=1)\n",
    "    print(f\"Mean Policy Entropy: {np.mean(entropies):.3f} (max: {np.log(env.action_dim):.3f})\")\n",
    "\n",
    "\n",
    "# Analyze the trained PPO agent\n",
    "env_analysis = CartPoleWrapper()\n",
    "print(\"Analyzing PPO Policy:\")\n",
    "analyze_policy_distributions(agent_ppo, env_analysis, 500)\n",
    "env_analysis.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_gradient_summary():\n",
    "    \"\"\"Create comprehensive summary of policy gradient methods.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"POLICY GRADIENT METHODS PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Method comparison table\n",
    "    methods = {\n",
    "        'Vanilla REINFORCE': eval_vanilla,\n",
    "        'REINFORCE + Baseline': eval_baseline,\n",
    "        'Actor-Critic': eval_ac,\n",
    "        'PPO': eval_ppo\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Method':<20} {'Mean Reward':<12} {'Std Reward':<12} {'Variance':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for method, results in methods.items():\n",
    "        mean_reward = results['mean_reward']\n",
    "        std_reward = results['std_reward']\n",
    "        variance = \"Low\" if std_reward < 30 else \"Medium\" if std_reward < 60 else \"High\"\n",
    "        \n",
    "        print(f\"{method:<20} {mean_reward:<12.1f} {std_reward:<12.1f} {variance:<10}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KEY INSIGHTS AND THEORETICAL UNDERSTANDING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    insights = [\n",
    "        \"1. VARIANCE REDUCTION:\",\n",
    "        \"   • REINFORCE baseline reduces variance significantly\",\n",
    "        \"   • Actor-Critic further reduces variance with bootstrapping\",\n",
    "        \"   • PPO provides most stable learning with clipping\",\n",
    "        \"\",\n",
    "        \"2. SAMPLE EFFICIENCY:\",\n",
    "        \"   • REINFORCE: Low (Monte Carlo, high variance)\",\n",
    "        \"   • Actor-Critic: Medium (bootstrapping, online updates)\", \n",
    "        \"   • PPO: High (GAE, multiple epochs, stable updates)\",\n",
    "        \"\",\n",
    "        \"3. IMPLEMENTATION COMPLEXITY:\",\n",
    "        \"   • REINFORCE: Simple (just policy gradients)\",\n",
    "        \"   • Actor-Critic: Medium (policy + value networks)\",\n",
    "        \"   • PPO: Complex (clipping, GAE, mini-batches)\",\n",
    "        \"\",\n",
    "        \"4. THEORETICAL GUARANTEES:\",\n",
    "        \"   • All methods converge to local optima under assumptions\",\n",
    "        \"   • PPO has additional stability guarantees from clipping\",\n",
    "        \"   • Actor-Critic can be more unstable due to bootstrap bias\"\n",
    "    ]\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(insight)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"WHEN TO USE EACH METHOD\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    recommendations = [\n",
    "        \"REINFORCE:\",\n",
    "        \"  • Simple environments with low-dimensional action spaces\",\n",
    "        \"  • When you need theoretical understanding first\",\n",
    "        \"  • Research/educational purposes\",\n",
    "        \"\",\n",
    "        \"ACTOR-CRITIC:\",\n",
    "        \"  • Online learning scenarios\",\n",
    "        \"  • When sample efficiency matters more than stability\",\n",
    "        \"  • Continuous learning environments\",\n",
    "        \"\",\n",
    "        \"PPO:\",\n",
    "        \"  • Production systems requiring stability\",\n",
    "        \"  • Complex environments with high-dimensional action spaces\",\n",
    "        \"  • When you have computational resources for multiple epochs\",\n",
    "        \"  • Continuous control tasks\",\n",
    "        \"\",\n",
    "        \"GENERAL RECOMMENDATION:\",\n",
    "        \"  Start with PPO for most practical applications - it's currently\",\n",
    "        \"  the most robust and widely-used policy gradient method.\"\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PRACTICAL IMPLEMENTATION TIPS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    tips = [\n",
    "        \"• NETWORK ARCHITECTURE: Use Tanh activations for policy networks\",\n",
    "        \"• LEARNING RATES: Start with 3e-4 for policy, 1e-3 for value\",\n",
    "        \"• NORMALIZATION: Always normalize advantages for stability\",\n",
    "        \"• GRADIENT CLIPPING: Essential for stable policy learning (0.5 max norm)\",\n",
    "        \"• ENTROPY REGULARIZATION: Use 0.01 coefficient to maintain exploration\",\n",
    "        \"• GAE PARAMETER: λ=0.95 works well for most environments\",\n",
    "        \"• PPO CLIPPING: ε=0.2 is a good default, tune based on performance\",\n",
    "        \"• BATCH SIZE: Larger batches (64-256) work better for PPO\",\n",
    "        \"• CONTINUOUS ACTIONS: Use log-std as learnable parameter\",\n",
    "        \"• DEBUGGING: Monitor policy entropy to ensure exploration\"\n",
    "    ]\n",
    "    \n",
    "    for tip in tips:\n",
    "        print(tip)\n",
    "\n",
    "\n",
    "# Create the comprehensive summary\n",
    "create_policy_gradient_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've implemented and analyzed the major policy gradient methods:\n",
    "\n",
    "### Key Achievements\n",
    "\n",
    "1. **REINFORCE Implementation**: Classic policy gradient with Monte Carlo returns\n",
    "2. **Baseline Reduction**: Demonstrated variance reduction with value function baseline\n",
    "3. **Actor-Critic Methods**: Implemented bootstrapping for improved sample efficiency\n",
    "4. **PPO Implementation**: State-of-the-art policy optimization with stability guarantees\n",
    "5. **Continuous Control**: Extended to continuous action spaces with normal distributions\n",
    "6. **Comprehensive Analysis**: Compared methods with statistical rigor and visualization\n",
    "\n",
    "### Mathematical Insights\n",
    "\n",
    "- **Policy Gradient Theorem**: Direct optimization of expected return through log-likelihood gradients\n",
    "- **Variance-Bias Tradeoff**: REINFORCE has high variance but no bias; Actor-Critic reduces variance but introduces bias\n",
    "- **GAE (Generalized Advantage Estimation)**: Provides tunable variance-bias tradeoff\n",
    "- **PPO Clipping**: Prevents destructively large policy updates while maintaining progress\n",
    "\n",
    "### Performance Results\n",
    "\n",
    "- Baseline reduction significantly improves REINFORCE performance\n",
    "- Actor-Critic methods provide better sample efficiency than REINFORCE\n",
    "- PPO offers the best combination of stability and performance\n",
    "- Continuous control requires careful action distribution design\n",
    "\n",
    "### When to Use Policy vs Value Methods\n",
    "\n",
    "**Use Policy Gradient Methods When:**\n",
    "- High-dimensional or continuous action spaces\n",
    "- Stochastic policies are beneficial\n",
    "- Need direct policy optimization\n",
    "- Handling partially observable environments\n",
    "\n",
    "**Use Value-Based Methods (DQN) When:**\n",
    "- Discrete action spaces with moderate dimensionality\n",
    "- Sample efficiency is critical\n",
    "- Deterministic policies are sufficient\n",
    "- Environment is fully observable\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "The final notebook (Part 6) will cover **Advanced Methods & Applications**, exploring:\n",
    "- Soft Actor-Critic (SAC) for continuous control\n",
    "- Model-based RL fundamentals\n",
    "- Transfer learning and domain adaptation\n",
    "- Real-world deployment considerations\n",
    "- Multi-agent RL basics\n",
    "- Current research directions\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "Policy gradient methods are essential for:\n",
    "- Robotics and continuous control\n",
    "- Game playing with complex action spaces\n",
    "- Natural language generation\n",
    "- Recommendation systems\n",
    "- Any domain requiring stochastic policies\n",
    "\n",
    "The understanding gained here provides a solid foundation for tackling complex real-world reinforcement learning problems where direct policy optimization is necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}