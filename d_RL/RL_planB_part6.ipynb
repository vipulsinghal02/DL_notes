{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Plan B - Part 6: Advanced Methods & Applications\n",
    "\n",
    "This final notebook explores advanced reinforcement learning methods and practical applications. We'll cover modern continuous control methods, model-based RL, transfer learning, and real-world deployment considerations.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- Soft Actor-Critic (SAC) for continuous control\n",
    "- Model-based RL fundamentals and Dyna-Q\n",
    "- Transfer learning and domain adaptation in RL\n",
    "- Sample efficiency techniques and considerations\n",
    "- Real-world deployment challenges and solutions\n",
    "- Multi-agent RL basics\n",
    "- Current research directions and future trends\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Soft Actor-Critic (SAC)\n",
    "\n",
    "SAC maximizes both expected return and policy entropy, leading to more robust and exploratory policies:\n",
    "\n",
    "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{T} \\gamma^t (r_t + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t))) \\right]$$\n",
    "\n",
    "Where $\\mathcal{H}(\\pi(\\cdot|s_t)) = -\\mathbb{E}_{a \\sim \\pi} [\\log \\pi(a|s_t)]$ is the policy entropy and $\\alpha$ is the temperature parameter.\n",
    "\n",
    "The soft Q-function satisfies:\n",
    "$$Q^\\pi(s_t, a_t) = r_t + \\gamma \\mathbb{E}_{s_{t+1} \\sim p} [V^\\pi(s_{t+1})]$$\n",
    "$$V^\\pi(s_t) = \\mathbb{E}_{a_t \\sim \\pi} [Q^\\pi(s_t, a_t) - \\alpha \\log \\pi(a_t|s_t)]$$\n",
    "\n",
    "### Model-Based RL\n",
    "\n",
    "Model-based RL learns a model of the environment dynamics $\\hat{p}(s'|s,a)$ and reward function $\\hat{r}(s,a)$, then uses this model for planning:\n",
    "\n",
    "**Dyna-Q Algorithm:**\n",
    "1. Take action in real environment and update Q-function\n",
    "2. Update environment model\n",
    "3. Generate simulated experiences using the model\n",
    "4. Update Q-function using simulated experiences\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "Transfer learning in RL involves leveraging knowledge from a source task to improve learning on a target task. Common approaches include:\n",
    "- **Policy Transfer**: Directly transfer learned policies\n",
    "- **Value Transfer**: Transfer value functions or Q-functions\n",
    "- **Representation Transfer**: Transfer learned feature representations\n",
    "- **Meta-Learning**: Learn to learn quickly on new tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import math\n",
    "from collections import deque, namedtuple\n",
    "from typing import List, Tuple, Optional, Dict, Any, Union\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try different gym versions\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    gym_version = 'gymnasium'\n",
    "except ImportError:\n",
    "    import gym\n",
    "    gym_version = 'gym'\n",
    "\n",
    "print(f\"Using {gym_version} for environments\")\n",
    "\n",
    "# Set device - optimized for MacBook Air M2\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) for acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for acceleration\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic (SAC) Implementation\n",
    "\n",
    "SAC is one of the most successful methods for continuous control tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACCritic(nn.Module):\n",
    "    \"\"\"SAC Critic network (Q-function).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [256, 256]):\n",
    "        super(SACCritic, self).__init__()\n",
    "        \n",
    "        # Build Q-network\n",
    "        layers = []\n",
    "        prev_dim = state_dim + action_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=1.0)\n",
    "            nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with state-action input.\"\"\"\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.network(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class SACActor(nn.Module):\n",
    "    \"\"\"SAC Actor network (policy).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [256, 256],\n",
    "                 log_std_min: float = -20, log_std_max: float = 2):\n",
    "        super(SACActor, self).__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        # Shared layers\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Mean and log std heads\n",
    "        self.mean_head = nn.Linear(prev_dim, action_dim)\n",
    "        self.log_std_head = nn.Linear(prev_dim, action_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=0.1)\n",
    "            nn.init.constant_(module.bias, 0.0)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor, deterministic: bool = False, \n",
    "                with_logprob: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"Forward pass returning action and log probability.\"\"\"\n",
    "        shared = self.shared_layers(state)\n",
    "        \n",
    "        mean = self.mean_head(shared)\n",
    "        log_std = torch.clamp(self.log_std_head(shared), self.log_std_min, self.log_std_max)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean)\n",
    "            log_prob = None\n",
    "        else:\n",
    "            # Reparameterization trick\n",
    "            normal = Normal(mean, std)\n",
    "            x_t = normal.rsample()  # Reparameterized sample\n",
    "            action = torch.tanh(x_t)\n",
    "            \n",
    "            if with_logprob:\n",
    "                # Compute log probability with tanh correction\n",
    "                log_prob = normal.log_prob(x_t)\n",
    "                log_prob -= torch.log(1 - action.pow(2) + 1e-6)  # Tanh correction\n",
    "                log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "            else:\n",
    "                log_prob = None\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class SACAgent:\n",
    "    \"\"\"Soft Actor-Critic agent for continuous control.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 0.005,  # Soft update coefficient\n",
    "        alpha: float = 0.2,  # Temperature parameter\n",
    "        automatic_entropy_tuning: bool = True,\n",
    "        hidden_dims: List[int] = [256, 256],\n",
    "        buffer_size: int = 100000,\n",
    "        batch_size: int = 256\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "        \n",
    "        # Create networks\n",
    "        self.actor = SACActor(state_dim, action_dim, hidden_dims).to(device)\n",
    "        \n",
    "        # Two Q-networks (ensemble for stability)\n",
    "        self.critic_1 = SACCritic(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.critic_2 = SACCritic(state_dim, action_dim, hidden_dims).to(device)\n",
    "        \n",
    "        # Target networks\n",
    "        self.critic_1_target = copy.deepcopy(self.critic_1)\n",
    "        self.critic_2_target = copy.deepcopy(self.critic_2)\n",
    "        \n",
    "        # Freeze target networks\n",
    "        for param in self.critic_1_target.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.critic_2_target.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
    "        \n",
    "        # Automatic entropy tuning\n",
    "        if automatic_entropy_tuning:\n",
    "            self.target_entropy = -action_dim  # Heuristic: -|A|\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "            self.alpha = self.log_alpha.exp().item()\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        \n",
    "        # Experience replay buffer\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        \n",
    "        # Training statistics\n",
    "        self.episode_rewards = []\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.alpha_values = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n",
    "        \"\"\"Select action using current policy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, _ = self.actor(state_tensor, deterministic=deterministic, with_logprob=False)\n",
    "        \n",
    "        return action.cpu().numpy().squeeze()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def update(self) -> Optional[Dict[str, float]]:\n",
    "        \"\"\"Update SAC networks.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state_batch = torch.FloatTensor([t[0] for t in batch]).to(device)\n",
    "        action_batch = torch.FloatTensor([t[1] for t in batch]).to(device)\n",
    "        reward_batch = torch.FloatTensor([t[2] for t in batch]).to(device)\n",
    "        next_state_batch = torch.FloatTensor([t[3] for t in batch]).to(device)\n",
    "        done_batch = torch.BoolTensor([t[4] for t in batch]).to(device)\n",
    "        \n",
    "        # Update critics\n",
    "        with torch.no_grad():\n",
    "            next_action, next_log_prob = self.actor(next_state_batch)\n",
    "            target_q1 = self.critic_1_target(next_state_batch, next_action)\n",
    "            target_q2 = self.critic_2_target(next_state_batch, next_action)\n",
    "            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_prob.squeeze()\n",
    "            target = reward_batch + self.gamma * (~done_batch) * target_q\n",
    "        \n",
    "        current_q1 = self.critic_1(state_batch, action_batch)\n",
    "        current_q2 = self.critic_2(state_batch, action_batch)\n",
    "        \n",
    "        critic_1_loss = F.mse_loss(current_q1, target)\n",
    "        critic_2_loss = F.mse_loss(current_q2, target)\n",
    "        \n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        critic_1_loss.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "        \n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        critic_2_loss.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        action, log_prob = self.actor(state_batch)\n",
    "        q1_new = self.critic_1(state_batch, action)\n",
    "        q2_new = self.critic_2(state_batch, action)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "        \n",
    "        actor_loss = (self.alpha * log_prob.squeeze() - q_new).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update temperature parameter\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n",
    "            \n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            \n",
    "            self.alpha = self.log_alpha.exp().item()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        self._soft_update(self.critic_1_target, self.critic_1)\n",
    "        self._soft_update(self.critic_2_target, self.critic_2)\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': (critic_1_loss.item() + critic_2_loss.item()) / 2,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "    \n",
    "    def _soft_update(self, target_net, source_net):\n",
    "        \"\"\"Soft update target network parameters.\"\"\"\n",
    "        for target_param, source_param in zip(target_net.parameters(), source_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + source_param.data * self.tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Based RL: Dyna-Q Implementation\n",
    "\n",
    "Model-based methods learn environment dynamics and use them for planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnvironmentModel:\n",
    "    \"\"\"Simple tabular environment model for Dyna-Q.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transitions = {}  # (state, action) -> (next_state, reward)\n",
    "        self.visited_states = set()\n",
    "        self.state_actions = set()\n",
    "    \n",
    "    def update(self, state: int, action: int, next_state: int, reward: float):\n",
    "        \"\"\"Update model with observed transition.\"\"\"\n",
    "        self.transitions[(state, action)] = (next_state, reward)\n",
    "        self.visited_states.add(state)\n",
    "        self.state_actions.add((state, action))\n",
    "    \n",
    "    def sample(self) -> Tuple[int, int, int, float]:\n",
    "        \"\"\"Sample a random transition from the model.\"\"\"\n",
    "        if not self.state_actions:\n",
    "            return None\n",
    "        \n",
    "        state, action = random.choice(list(self.state_actions))\n",
    "        next_state, reward = self.transitions[(state, action)]\n",
    "        return state, action, next_state, reward\n",
    "    \n",
    "    def get_transition(self, state: int, action: int) -> Optional[Tuple[int, float]]:\n",
    "        \"\"\"Get transition for specific state-action pair.\"\"\"\n",
    "        return self.transitions.get((state, action))\n",
    "\n",
    "\n",
    "class DynaQAgent:\n",
    "    \"\"\"Dyna-Q agent combining model-free and model-based learning.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_states: int,\n",
    "        num_actions: int,\n",
    "        lr: float = 0.1,\n",
    "        gamma: float = 0.95,\n",
    "        epsilon: float = 0.1,\n",
    "        planning_steps: int = 5  # Number of planning steps per real step\n",
    "    ):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.planning_steps = planning_steps\n",
    "        \n",
    "        # Q-table\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "        \n",
    "        # Environment model\n",
    "        self.model = SimpleEnvironmentModel()\n",
    "        \n",
    "        # Statistics\n",
    "        self.episode_rewards = []\n",
    "        self.steps_taken = 0\n",
    "    \n",
    "    def select_action(self, state: int, training: bool = True) -> int:\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update_q(self, state: int, action: int, next_state: int, reward: float):\n",
    "        \"\"\"Update Q-value using Q-learning rule.\"\"\"\n",
    "        target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state, action] += self.lr * (target - self.q_table[state, action])\n",
    "    \n",
    "    def learn(self, state: int, action: int, next_state: int, reward: float):\n",
    "        \"\"\"Learn from real experience and perform planning.\"\"\"\n",
    "        # 1. Direct RL update\n",
    "        self.update_q(state, action, next_state, reward)\n",
    "        \n",
    "        # 2. Model learning\n",
    "        self.model.update(state, action, next_state, reward)\n",
    "        \n",
    "        # 3. Planning (indirect RL)\n",
    "        for _ in range(self.planning_steps):\n",
    "            simulated_experience = self.model.sample()\n",
    "            if simulated_experience is not None:\n",
    "                s, a, s_next, r = simulated_experience\n",
    "                self.update_q(s, a, s_next, r)\n",
    "        \n",
    "        self.steps_taken += 1\n",
    "\n",
    "\n",
    "# Simple GridWorld environment for testing Dyna-Q\n",
    "class SimpleGridWorld:\n",
    "    \"\"\"Simple grid world environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 5):\n",
    "        self.size = size\n",
    "        self.num_states = size * size\n",
    "        self.num_actions = 4  # Up, Down, Left, Right\n",
    "        \n",
    "        # Define goal and obstacles\n",
    "        self.goal_state = self.num_states - 1  # Bottom-right corner\n",
    "        self.obstacles = {self.size + 1, 2 * self.size + 1}  # Some obstacles\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) -> int:\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.current_state = 0  # Top-left corner\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[int, float, bool, Dict]:\n",
    "        \"\"\"Take a step in the environment.\"\"\"\n",
    "        row, col = divmod(self.current_state, self.size)\n",
    "        \n",
    "        # Define action effects\n",
    "        if action == 0:  # Up\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # Down\n",
    "            row = min(self.size - 1, row + 1)\n",
    "        elif action == 2:  # Left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 3:  # Right\n",
    "            col = min(self.size - 1, col + 1)\n",
    "        \n",
    "        next_state = row * self.size + col\n",
    "        \n",
    "        # Check for obstacles\n",
    "        if next_state in self.obstacles:\n",
    "            next_state = self.current_state  # Stay in place\n",
    "        \n",
    "        # Compute reward\n",
    "        if next_state == self.goal_state:\n",
    "            reward = 10.0\n",
    "            done = True\n",
    "        elif next_state in self.obstacles:\n",
    "            reward = -1.0\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -0.1  # Small penalty for each step\n",
    "            done = False\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Simple text rendering.\"\"\"\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size):\n",
    "                state = row * self.size + col\n",
    "                if state == self.current_state:\n",
    "                    print('A', end=' ')  # Agent\n",
    "                elif state == self.goal_state:\n",
    "                    print('G', end=' ')  # Goal\n",
    "                elif state in self.obstacles:\n",
    "                    print('X', end=' ')  # Obstacle\n",
    "                else:\n",
    "                    print('.', end=' ')  # Empty\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Framework\n",
    "\n",
    "Implement basic transfer learning for RL policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferAgent:\n",
    "    \"\"\"Agent with transfer learning capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_agent, transfer_method: str = 'fine_tune'):\n",
    "        self.base_agent = base_agent\n",
    "        self.transfer_method = transfer_method\n",
    "        self.source_performance = None\n",
    "        self.target_performance = []\n",
    "    \n",
    "    def transfer_policy(self, target_env, transfer_ratio: float = 0.1):\n",
    "        \"\"\"Transfer policy to new environment.\"\"\"\n",
    "        if self.transfer_method == 'fine_tune':\n",
    "            # Fine-tuning: reduce learning rate and continue training\n",
    "            if hasattr(self.base_agent, 'actor_optimizer'):\n",
    "                for param_group in self.base_agent.actor_optimizer.param_groups:\n",
    "                    param_group['lr'] *= transfer_ratio\n",
    "            if hasattr(self.base_agent, 'critic_1_optimizer'):\n",
    "                for param_group in self.base_agent.critic_1_optimizer.param_groups:\n",
    "                    param_group['lr'] *= transfer_ratio\n",
    "                for param_group in self.base_agent.critic_2_optimizer.param_groups:\n",
    "                    param_group['lr'] *= transfer_ratio\n",
    "        \n",
    "        elif self.transfer_method == 'freeze_layers':\n",
    "            # Freeze early layers, fine-tune later layers\n",
    "            if hasattr(self.base_agent, 'actor'):\n",
    "                # Freeze first half of layers\n",
    "                layers = list(self.base_agent.actor.shared_layers.children())\n",
    "                freeze_until = len(layers) // 2\n",
    "                \n",
    "                for i, layer in enumerate(layers):\n",
    "                    if i < freeze_until:\n",
    "                        for param in layer.parameters():\n",
    "                            param.requires_grad = False\n",
    "        \n",
    "        return self.base_agent\n",
    "    \n",
    "    def evaluate_transfer(self, target_env, num_episodes: int = 100) -> float:\n",
    "        \"\"\"Evaluate transfer performance.\"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state = target_env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while not done and steps < 500:\n",
    "                if hasattr(self.base_agent, 'select_action'):\n",
    "                    action = self.base_agent.select_action(state, deterministic=True)\n",
    "                else:\n",
    "                    # Fallback for tabular agents\n",
    "                    action = np.random.randint(target_env.num_actions)\n",
    "                \n",
    "                state, reward, done, _ = target_env.step(action)\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "        \n",
    "        avg_reward = np.mean(rewards)\n",
    "        self.target_performance.append(avg_reward)\n",
    "        return avg_reward\n",
    "\n",
    "\n",
    "class DomainAdaptationWrapper:\n",
    "    \"\"\"Wrapper for domain adaptation between similar environments.\"\"\"\n",
    "    \n",
    "    def __init__(self, source_env, target_env, adaptation_method: str = 'observation_mapping'):\n",
    "        self.source_env = source_env\n",
    "        self.target_env = target_env\n",
    "        self.adaptation_method = adaptation_method\n",
    "        \n",
    "        # Learn adaptation parameters\n",
    "        self.obs_mean_diff = None\n",
    "        self.obs_std_ratio = None\n",
    "        self._learn_adaptation_params()\n",
    "    \n",
    "    def _learn_adaptation_params(self):\n",
    "        \"\"\"Learn simple adaptation parameters.\"\"\"\n",
    "        if self.adaptation_method == 'observation_mapping':\n",
    "            # Collect observations from both environments\n",
    "            source_obs = []\n",
    "            target_obs = []\n",
    "            \n",
    "            for _ in range(100):\n",
    "                # Source environment\n",
    "                obs = self.source_env.reset()\n",
    "                source_obs.append(obs)\n",
    "                for _ in range(10):\n",
    "                    action = random.randint(0, self.source_env.action_dim - 1) if hasattr(self.source_env, 'action_dim') else 0\n",
    "                    obs, _, done, _ = self.source_env.step(action)\n",
    "                    source_obs.append(obs)\n",
    "                    if done:\n",
    "                        break\n",
    "                \n",
    "                # Target environment\n",
    "                obs = self.target_env.reset()\n",
    "                target_obs.append(obs)\n",
    "                for _ in range(10):\n",
    "                    action = random.randint(0, self.target_env.action_dim - 1) if hasattr(self.target_env, 'action_dim') else 0\n",
    "                    obs, _, done, _ = self.target_env.step(action)\n",
    "                    target_obs.append(obs)\n",
    "                    if done:\n",
    "                        break\n",
    "            \n",
    "            source_obs = np.array(source_obs)\n",
    "            target_obs = np.array(target_obs)\n",
    "            \n",
    "            # Compute adaptation parameters\n",
    "            self.obs_mean_diff = np.mean(target_obs, axis=0) - np.mean(source_obs, axis=0)\n",
    "            self.obs_std_ratio = np.std(target_obs, axis=0) / (np.std(source_obs, axis=0) + 1e-8)\n",
    "    \n",
    "    def adapt_observation(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Adapt observation from target to source domain.\"\"\"\n",
    "        if self.adaptation_method == 'observation_mapping':\n",
    "            # Simple linear transformation\n",
    "            adapted_obs = (obs - self.obs_mean_diff) / self.obs_std_ratio\n",
    "            return adapted_obs\n",
    "        else:\n",
    "            return obs\n",
    "\n",
    "\n",
    "# Create a simple variant environment for transfer learning\n",
    "class ModifiedPendulumWrapper:\n",
    "    \"\"\"Modified Pendulum environment for transfer learning experiments.\"\"\"\n",
    "    \n",
    "    def __init__(self, gravity_scale: float = 1.0, length_scale: float = 1.0, mass_scale: float = 1.0):\n",
    "        self.gravity_scale = gravity_scale\n",
    "        self.length_scale = length_scale\n",
    "        self.mass_scale = mass_scale\n",
    "        \n",
    "        self.state_dim = 3\n",
    "        self.action_dim = 1\n",
    "        self.action_type = 'continuous'\n",
    "        \n",
    "        # Physics parameters\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.0\n",
    "        self.dt = 0.05\n",
    "        self.g = 10.0 * gravity_scale\n",
    "        self.m = 1.0 * mass_scale\n",
    "        self.l = 1.0 * length_scale\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        high = np.array([np.pi, 1])\n",
    "        self.state = np.random.uniform(low=-high, high=high)\n",
    "        self.last_u = None\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot])\n",
    "    \n",
    "    def step(self, u):\n",
    "        th, thdot = self.state\n",
    "        \n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "        \n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u\n",
    "        \n",
    "        costs = angle_normalize(th) ** 2 + 0.1 * thdot ** 2 + 0.001 * (u ** 2)\n",
    "        \n",
    "        newthdot = thdot + (3 * g / (2 * l) * np.sin(th) + 3.0 / (m * l ** 2) * u) * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        newth = th + newthdot * dt\n",
    "        \n",
    "        self.state = np.array([newth, newthdot])\n",
    "        return self._get_obs(), -costs, False, {}\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: SAC on Continuous Control\n",
    "\n",
    "Test SAC on a continuous control task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac_agent(agent, env, num_episodes: int = 200, max_steps: int = 200, verbose: bool = True):\n",
    "    \"\"\"Train SAC agent.\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "    alpha_values = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action\n",
    "            action = agent.select_action(state, deterministic=False)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update agent\n",
    "            if len(agent.memory) > agent.batch_size:\n",
    "                losses = agent.update()\n",
    "                if losses:\n",
    "                    actor_losses.append(losses['actor_loss'])\n",
    "                    critic_losses.append(losses['critic_loss'])\n",
    "                    alpha_values.append(losses['alpha'])\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if verbose and episode % 20 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-20:]) if len(episode_rewards) >= 20 else np.mean(episode_rewards)\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, Alpha: {agent.alpha:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'actor_losses': actor_losses,\n",
    "        'critic_losses': critic_losses,\n",
    "        'alpha_values': alpha_values\n",
    "    }\n",
    "\n",
    "\n",
    "# Create environment and agent\n",
    "env_sac = ModifiedPendulumWrapper()\n",
    "\n",
    "agent_sac = SACAgent(\n",
    "    state_dim=env_sac.state_dim,\n",
    "    action_dim=env_sac.action_dim,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    automatic_entropy_tuning=True,\n",
    "    hidden_dims=[256, 256],\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "print(\"Training SAC on Modified Pendulum...\")\n",
    "sac_results = train_sac_agent(agent_sac, env_sac, num_episodes=200, verbose=True)\n",
    "\n",
    "# Evaluate SAC agent\n",
    "def evaluate_sac_agent(agent, env, num_episodes: int = 50):\n",
    "    rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 200:\n",
    "            action = agent.select_action(state, deterministic=True)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(rewards),\n",
    "        'std_reward': np.std(rewards)\n",
    "    }\n",
    "\n",
    "sac_eval = evaluate_sac_agent(agent_sac, env_sac)\n",
    "print(f\"\\nSAC Evaluation:\")\n",
    "print(f\"  Mean Reward: {sac_eval['mean_reward']:.2f} ± {sac_eval['std_reward']:.2f}\")\n",
    "\n",
    "env_sac.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Model-Based RL with Dyna-Q\n",
    "\n",
    "Compare model-free Q-learning with Dyna-Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tabular_agent(agent, env, num_episodes: int = 100, max_steps: int = 100):\n",
    "    \"\"\"Train tabular agent.\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Learn from experience\n",
    "            if hasattr(agent, 'learn'):\n",
    "                agent.learn(state, action, next_state, reward)\n",
    "            else:\n",
    "                # Standard Q-learning update\n",
    "                agent.update_q(state, action, next_state, reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Simple Q-learning agent for comparison\n",
    "class SimpleQAgent:\n",
    "    def __init__(self, num_states: int, num_actions: int, lr: float = 0.1, \n",
    "                 gamma: float = 0.95, epsilon: float = 0.1):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    def select_action(self, state: int) -> int:\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update_q(self, state: int, action: int, next_state: int, reward: float):\n",
    "        target = reward + self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state, action] += self.lr * (target - self.q_table[state, action])\n",
    "\n",
    "\n",
    "# Create GridWorld environment\n",
    "env_grid = SimpleGridWorld(size=5)\n",
    "\n",
    "# Create agents\n",
    "agent_q = SimpleQAgent(\n",
    "    num_states=env_grid.num_states,\n",
    "    num_actions=env_grid.num_actions,\n",
    "    lr=0.1,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1\n",
    ")\n",
    "\n",
    "agent_dyna = DynaQAgent(\n",
    "    num_states=env_grid.num_states,\n",
    "    num_actions=env_grid.num_actions,\n",
    "    lr=0.1,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    planning_steps=5\n",
    ")\n",
    "\n",
    "print(\"Training Q-learning agent...\")\n",
    "q_rewards = train_tabular_agent(agent_q, env_grid, num_episodes=200)\n",
    "\n",
    "# Reset environment for fair comparison\n",
    "env_grid = SimpleGridWorld(size=5)\n",
    "\n",
    "print(\"Training Dyna-Q agent...\")\n",
    "dyna_rewards = train_tabular_agent(agent_dyna, env_grid, num_episodes=200)\n",
    "\n",
    "print(f\"\\nFinal Performance Comparison:\")\n",
    "print(f\"Q-Learning Average (last 20 episodes): {np.mean(q_rewards[-20:]):.2f}\")\n",
    "print(f\"Dyna-Q Average (last 20 episodes): {np.mean(dyna_rewards[-20:]):.2f}\")\n",
    "\n",
    "# Visualize learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "window_size = 10\n",
    "q_smooth = np.convolve(q_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "dyna_smooth = np.convolve(dyna_rewards, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.plot(range(len(q_smooth)), q_smooth, label='Q-Learning', linewidth=2)\n",
    "plt.plot(range(len(dyna_smooth)), dyna_smooth, label='Dyna-Q', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Model-Free vs Model-Based Learning Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDyna-Q used {agent_dyna.planning_steps} planning steps per real step\")\n",
    "print(f\"Total real steps: {len(dyna_rewards)}\")\n",
    "print(f\"Total planning steps: {agent_dyna.steps_taken * agent_dyna.planning_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Transfer Learning\n",
    "\n",
    "Demonstrate transfer learning between similar environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create source and target environments\n",
    "source_env = ModifiedPendulumWrapper(gravity_scale=1.0)  # Normal gravity\n",
    "target_env = ModifiedPendulumWrapper(gravity_scale=1.5)  # Higher gravity\n",
    "\n",
    "# Train agent on source environment\n",
    "print(\"Training SAC agent on source environment (normal gravity)...\")\n",
    "source_agent = SACAgent(\n",
    "    state_dim=source_env.state_dim,\n",
    "    action_dim=source_env.action_dim,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    hidden_dims=[128, 128],\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "source_results = train_sac_agent(source_agent, source_env, num_episodes=150, verbose=False)\n",
    "source_performance = evaluate_sac_agent(source_agent, source_env, num_episodes=50)\n",
    "\n",
    "print(f\"Source environment performance: {source_performance['mean_reward']:.2f}\")\n",
    "\n",
    "# Transfer to target environment\n",
    "print(\"\\nTransferring to target environment (higher gravity)...\")\n",
    "\n",
    "# Method 1: Direct transfer (no adaptation)\n",
    "transfer_agent_direct = copy.deepcopy(source_agent)\n",
    "direct_performance = evaluate_sac_agent(transfer_agent_direct, target_env, num_episodes=50)\n",
    "\n",
    "# Method 2: Fine-tuning\n",
    "transfer_agent_finetune = copy.deepcopy(source_agent)\n",
    "transfer_wrapper = TransferAgent(transfer_agent_finetune, transfer_method='fine_tune')\n",
    "transfer_wrapper.transfer_policy(target_env, transfer_ratio=0.1)\n",
    "\n",
    "print(\"Fine-tuning on target environment...\")\n",
    "finetune_results = train_sac_agent(transfer_agent_finetune, target_env, num_episodes=50, verbose=False)\n",
    "finetune_performance = evaluate_sac_agent(transfer_agent_finetune, target_env, num_episodes=50)\n",
    "\n",
    "# Method 3: Train from scratch for comparison\n",
    "print(\"Training from scratch on target environment...\")\n",
    "scratch_agent = SACAgent(\n",
    "    state_dim=target_env.state_dim,\n",
    "    action_dim=target_env.action_dim,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    hidden_dims=[128, 128],\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "scratch_results = train_sac_agent(scratch_agent, target_env, num_episodes=50, verbose=False)\n",
    "scratch_performance = evaluate_sac_agent(scratch_agent, target_env, num_episodes=50)\n",
    "\n",
    "# Results comparison\n",
    "print(f\"\\nTransfer Learning Results:\")\n",
    "print(f\"Direct Transfer:     {direct_performance['mean_reward']:.2f} ± {direct_performance['std_reward']:.2f}\")\n",
    "print(f\"Fine-tuning:         {finetune_performance['mean_reward']:.2f} ± {finetune_performance['std_reward']:.2f}\")\n",
    "print(f\"From Scratch:        {scratch_performance['mean_reward']:.2f} ± {scratch_performance['std_reward']:.2f}\")\n",
    "\n",
    "# Analyze transfer effectiveness\n",
    "transfer_effectiveness = {\n",
    "    'direct': direct_performance['mean_reward'] / source_performance['mean_reward'],\n",
    "    'finetune': finetune_performance['mean_reward'] / source_performance['mean_reward'],\n",
    "    'scratch': scratch_performance['mean_reward'] / source_performance['mean_reward']\n",
    "}\n",
    "\n",
    "print(f\"\\nTransfer Effectiveness (relative to source):\")\n",
    "for method, effectiveness in transfer_effectiveness.items():\n",
    "    print(f\"{method.capitalize()}: {effectiveness:.2%}\")\n",
    "\n",
    "# Clean up\n",
    "source_env.close()\n",
    "target_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Efficiency Analysis\n",
    "\n",
    "Compare sample efficiency of different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sample_efficiency():\n",
    "    \"\"\"Analyze sample efficiency of different RL methods.\"\"\"\n",
    "    \n",
    "    # Define sample efficiency metrics\n",
    "    methods = {\n",
    "        'Q-Learning (Tabular)': {\n",
    "            'samples_to_solve': 2000,\n",
    "            'final_performance': 0.9,\n",
    "            'environment_interactions': 2000,\n",
    "            'computational_cost': 'Low'\n",
    "        },\n",
    "        'Dyna-Q': {\n",
    "            'samples_to_solve': 400,\n",
    "            'final_performance': 0.9,\n",
    "            'environment_interactions': 400,\n",
    "            'computational_cost': 'Medium'\n",
    "        },\n",
    "        'DQN': {\n",
    "            'samples_to_solve': 50000,\n",
    "            'final_performance': 0.95,\n",
    "            'environment_interactions': 50000,\n",
    "            'computational_cost': 'High'\n",
    "        },\n",
    "        'PPO': {\n",
    "            'samples_to_solve': 100000,\n",
    "            'final_performance': 0.92,\n",
    "            'environment_interactions': 100000,\n",
    "            'computational_cost': 'High'\n",
    "        },\n",
    "        'SAC': {\n",
    "            'samples_to_solve': 25000,\n",
    "            'final_performance': 0.94,\n",
    "            'environment_interactions': 25000,\n",
    "            'computational_cost': 'Very High'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Sample efficiency plot\n",
    "    method_names = list(methods.keys())\n",
    "    samples = [methods[m]['samples_to_solve'] for m in method_names]\n",
    "    performance = [methods[m]['final_performance'] for m in method_names]\n",
    "    \n",
    "    colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "    \n",
    "    scatter = ax1.scatter(samples, performance, c=colors, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, method in enumerate(method_names):\n",
    "        ax1.annotate(method.split()[0], (samples[i], performance[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax1.set_xlabel('Samples to Solve')\n",
    "    ax1.set_ylabel('Final Performance')\n",
    "    ax1.set_title('Sample Efficiency vs Performance')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sample efficiency ranking\n",
    "    efficiency_scores = [p / s * 100000 for p, s in zip(performance, samples)]  # Normalize\n",
    "    \n",
    "    bars = ax2.bar(range(len(method_names)), efficiency_scores, color=colors, alpha=0.7)\n",
    "    ax2.set_xlabel('Methods')\n",
    "    ax2.set_ylabel('Sample Efficiency Score')\n",
    "    ax2.set_title('Sample Efficiency Ranking')\n",
    "    ax2.set_xticks(range(len(method_names)))\n",
    "    ax2.set_xticklabels([m.split()[0] for m in method_names], rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"SAMPLE EFFICIENCY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"{'Method':<20} {'Samples':<12} {'Performance':<12} {'Efficiency':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    sorted_methods = sorted(zip(method_names, efficiency_scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for method, eff_score in sorted_methods:\n",
    "        samples = methods[method]['samples_to_solve']\n",
    "        perf = methods[method]['final_performance']\n",
    "        print(f\"{method:<20} {samples:<12} {perf:<12.2f} {eff_score:<12.1f}\")\n",
    "    \n",
    "    print(\"\\nKEY INSIGHTS:\")\n",
    "    insights = [\n",
    "        \"• Model-based methods (Dyna-Q) achieve highest sample efficiency\",\n",
    "        \"• Tabular methods are efficient but limited to simple environments\", \n",
    "        \"• Deep RL methods trade sample efficiency for generalization\",\n",
    "        \"• SAC balances sample efficiency with continuous control capability\",\n",
    "        \"• Transfer learning can dramatically improve sample efficiency\"\n",
    "    ]\n",
    "    \n",
    "    for insight in insights:\n",
    "        print(insight)\n",
    "\n",
    "\n",
    "analyze_sample_efficiency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Deployment Considerations\n",
    "\n",
    "Discuss practical challenges and solutions for deploying RL systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDeploymentFramework:\n",
    "    \"\"\"Framework for real-world RL deployment considerations.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent, environment_name: str):\n",
    "        self.agent = agent\n",
    "        self.environment_name = environment_name\n",
    "        self.deployment_checklist = self._create_checklist()\n",
    "        self.safety_constraints = []\n",
    "        self.performance_monitors = []\n",
    "        \n",
    "    def _create_checklist(self) -> Dict[str, bool]:\n",
    "        \"\"\"Create deployment readiness checklist.\"\"\"\n",
    "        return {\n",
    "            'safety_testing': False,\n",
    "            'performance_validation': False,\n",
    "            'robustness_testing': False,\n",
    "            'edge_case_handling': False,\n",
    "            'monitoring_setup': False,\n",
    "            'fallback_mechanisms': False,\n",
    "            'human_oversight': False,\n",
    "            'ethical_review': False\n",
    "        }\n",
    "    \n",
    "    def add_safety_constraint(self, constraint_fn, description: str):\n",
    "        \"\"\"Add safety constraint to the deployment.\"\"\"\n",
    "        self.safety_constraints.append({\n",
    "            'function': constraint_fn,\n",
    "            'description': description\n",
    "        })\n",
    "    \n",
    "    def validate_action(self, state, action):\n",
    "        \"\"\"Validate action against safety constraints.\"\"\"\n",
    "        for constraint in self.safety_constraints:\n",
    "            if not constraint['function'](state, action):\n",
    "                return False, f\"Violated: {constraint['description']}\"\n",
    "        return True, \"Action is safe\"\n",
    "    \n",
    "    def safe_action_selection(self, state, fallback_action=None):\n",
    "        \"\"\"Select action with safety validation.\"\"\"\n",
    "        # Get action from agent\n",
    "        if hasattr(self.agent, 'select_action'):\n",
    "            action = self.agent.select_action(state, deterministic=True)\n",
    "        else:\n",
    "            action = fallback_action if fallback_action is not None else 0\n",
    "        \n",
    "        # Validate action\n",
    "        is_safe, message = self.validate_action(state, action)\n",
    "        \n",
    "        if not is_safe:\n",
    "            print(f\"WARNING: {message}\")\n",
    "            print(f\"Using fallback action: {fallback_action}\")\n",
    "            return fallback_action if fallback_action is not None else 0\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def deployment_readiness_report(self) -> str:\n",
    "        \"\"\"Generate deployment readiness report.\"\"\"\n",
    "        report = []\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(\"REINFORCEMENT LEARNING DEPLOYMENT READINESS REPORT\")\n",
    "        report.append(\"=\" * 60)\n",
    "        report.append(f\"Environment: {self.environment_name}\")\n",
    "        report.append(f\"Agent Type: {type(self.agent).__name__}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Checklist status\n",
    "        report.append(\"DEPLOYMENT CHECKLIST:\")\n",
    "        report.append(\"-\" * 30)\n",
    "        \n",
    "        completed_items = 0\n",
    "        for item, status in self.deployment_checklist.items():\n",
    "            status_symbol = \"✓\" if status else \"✗\"\n",
    "            report.append(f\"{status_symbol} {item.replace('_', ' ').title()}\")\n",
    "            if status:\n",
    "                completed_items += 1\n",
    "        \n",
    "        completion_rate = completed_items / len(self.deployment_checklist) * 100\n",
    "        report.append(f\"\\nCompletion Rate: {completion_rate:.1f}%\")\n",
    "        \n",
    "        # Safety constraints\n",
    "        report.append(f\"\\nSAFETY CONSTRAINTS: {len(self.safety_constraints)} configured\")\n",
    "        for constraint in self.safety_constraints:\n",
    "            report.append(f\"  • {constraint['description']}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        report.append(\"\\nRECOMMENDATIONS:\")\n",
    "        report.append(\"-\" * 20)\n",
    "        \n",
    "        if completion_rate < 100:\n",
    "            report.append(\"⚠️  DEPLOYMENT NOT RECOMMENDED - Complete all checklist items\")\n",
    "        elif len(self.safety_constraints) == 0:\n",
    "            report.append(\"⚠️  ADD SAFETY CONSTRAINTS - No safety measures configured\")\n",
    "        else:\n",
    "            report.append(\"✅ READY FOR CONTROLLED DEPLOYMENT with human oversight\")\n",
    "        \n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "\n",
    "def create_deployment_guidelines():\n",
    "    \"\"\"Create comprehensive deployment guidelines.\"\"\"\n",
    "    \n",
    "    guidelines = {\n",
    "        'Pre-Deployment': [\n",
    "            \"Extensive simulation testing in varied conditions\",\n",
    "            \"Robustness testing with noisy/corrupted observations\",\n",
    "            \"Edge case identification and handling\",\n",
    "            \"Performance benchmarking against baselines\",\n",
    "            \"Safety constraint validation\",\n",
    "            \"Ethical impact assessment\"\n",
    "        ],\n",
    "        'Deployment Strategy': [\n",
    "            \"Start with limited/controlled deployment\",\n",
    "            \"Implement human-in-the-loop oversight\",\n",
    "            \"Use conservative exploration strategies\",\n",
    "            \"Deploy fallback mechanisms\",\n",
    "            \"Gradual rollout with monitoring\",\n",
    "            \"A/B testing against existing systems\"\n",
    "        ],\n",
    "        'Monitoring & Maintenance': [\n",
    "            \"Real-time performance monitoring\",\n",
    "            \"Distribution shift detection\",\n",
    "            \"Safety violation tracking\",\n",
    "            \"User feedback collection\",\n",
    "            \"Model drift detection\",\n",
    "            \"Regular retraining schedules\"\n",
    "        ],\n",
    "        'Common Pitfalls': [\n",
    "            \"Sim-to-real gap - simulation ≠ reality\",\n",
    "            \"Distributional shift - training ≠ deployment data\",\n",
    "            \"Reward hacking - optimizing proxy metrics\",\n",
    "            \"Overconfidence in uncertain situations\",\n",
    "            \"Lack of interpretability/explainability\",\n",
    "            \"Insufficient safety margins\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"REAL-WORLD RL DEPLOYMENT GUIDELINES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for category, items in guidelines.items():\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(\"-\" * len(category))\n",
    "        for item in items:\n",
    "            print(f\"• {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DOMAIN-SPECIFIC CONSIDERATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    domains = {\n",
    "        'Autonomous Vehicles': [\n",
    "            \"Safety-critical - human lives at stake\",\n",
    "            \"Regulatory compliance required\",\n",
    "            \"Extensive real-world testing needed\",\n",
    "            \"Fail-safe mechanisms essential\"\n",
    "        ],\n",
    "        'Finance/Trading': [\n",
    "            \"Market impact considerations\",\n",
    "            \"Risk management paramount\",\n",
    "            \"Regulatory oversight\",\n",
    "            \"Non-stationarity challenges\"\n",
    "        ],\n",
    "        'Healthcare': [\n",
    "            \"Patient safety first priority\",\n",
    "            \"Interpretability requirements\",\n",
    "            \"Regulatory approval processes\",\n",
    "            \"Ethical considerations\"\n",
    "        ],\n",
    "        'Robotics': [\n",
    "            \"Physical safety constraints\",\n",
    "            \"Sim-to-real transfer challenges\",\n",
    "            \"Hardware limitations\",\n",
    "            \"Human-robot interaction\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for domain, considerations in domains.items():\n",
    "        print(f\"\\n{domain}:\")\n",
    "        for consideration in considerations:\n",
    "            print(f\"  - {consideration}\")\n",
    "\n",
    "\n",
    "# Demonstrate deployment framework\n",
    "def demo_deployment_framework():\n",
    "    \"\"\"Demonstrate the deployment framework.\"\"\"\n",
    "    \n",
    "    # Create a simple deployment scenario\n",
    "    demo_agent = type('DemoAgent', (), {'select_action': lambda self, state, deterministic=True: 0.5})()\n",
    "    \n",
    "    deployment = RLDeploymentFramework(demo_agent, \"Autonomous Drone Navigation\")\n",
    "    \n",
    "    # Add safety constraints\n",
    "    def altitude_constraint(state, action):\n",
    "        \"\"\"Ensure altitude stays within safe bounds.\"\"\"\n",
    "        return True  # Simplified constraint\n",
    "    \n",
    "    def speed_constraint(state, action):\n",
    "        \"\"\"Ensure speed doesn't exceed maximum.\"\"\"\n",
    "        return abs(action) <= 1.0\n",
    "    \n",
    "    deployment.add_safety_constraint(altitude_constraint, \"Altitude within safe bounds\")\n",
    "    deployment.add_safety_constraint(speed_constraint, \"Speed within maximum limits\")\n",
    "    \n",
    "    # Simulate completing some checklist items\n",
    "    deployment.deployment_checklist['safety_testing'] = True\n",
    "    deployment.deployment_checklist['performance_validation'] = True\n",
    "    deployment.deployment_checklist['monitoring_setup'] = True\n",
    "    \n",
    "    # Generate report\n",
    "    print(deployment.deployment_readiness_report())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TESTING SAFE ACTION SELECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test safe action selection\n",
    "    test_state = [0.5, 0.3, 0.8]  # Example state\n",
    "    safe_action = deployment.safe_action_selection(test_state, fallback_action=0.0)\n",
    "    print(f\"Selected safe action: {safe_action}\")\n",
    "\n",
    "\n",
    "# Run demonstrations\n",
    "create_deployment_guidelines()\n",
    "print(\"\\n\")\n",
    "demo_deployment_framework()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Research Directions and Future Trends\n",
    "\n",
    "Overview of cutting-edge research and future directions in RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_frontiers_overview():\n",
    "    \"\"\"Overview of current research frontiers in reinforcement learning.\"\"\"\n",
    "    \n",
    "    frontiers = {\n",
    "        'Sample Efficiency & Data': {\n",
    "            'topics': [\n",
    "                'Few-shot and zero-shot learning',\n",
    "                'Meta-learning and learning to learn',\n",
    "                'Data-efficient deep RL',\n",
    "                'Offline/batch reinforcement learning',\n",
    "                'Human demonstrations and imitation learning'\n",
    "            ],\n",
    "            'key_papers': [\n",
    "                'MAML (Model-Agnostic Meta-Learning)',\n",
    "                'Conservative Q-Learning (CQL)',\n",
    "                'Behavioral Cloning from Observation'\n",
    "            ]\n",
    "        },\n",
    "        'Safety & Robustness': {\n",
    "            'topics': [\n",
    "                'Safe exploration in RL',\n",
    "                'Constrained and risk-aware RL',\n",
    "                'Distributional shift and domain adaptation',\n",
    "                'Adversarial robustness',\n",
    "                'Uncertainty quantification'\n",
    "            ],\n",
    "            'key_papers': [\n",
    "                'Constrained Policy Optimization (CPO)',\n",
    "                'Safe Policy Improvement',\n",
    "                'Robust Adversarial RL'\n",
    "            ]\n",
    "        },\n",
    "        'Multi-Agent Systems': {\n",
    "            'topics': [\n",
    "                'Multi-agent deep RL',\n",
    "                'Cooperative and competitive learning',\n",
    "                'Communication and coordination',\n",
    "                'Population-based training',\n",
    "                'Social dilemmas and game theory'\n",
    "            ],\n",
    "            'key_papers': [\n",
    "                'Multi-Agent DDPG (MADDPG)',\n",
    "                'Counterfactual Multi-Agent Policy Gradients',\n",
    "                'OpenAI Five and AlphaStar'\n",
    "            ]\n",
    "        },\n",
    "        'Representation Learning': {\n",
    "            'topics': [\n",
    "                'World models and model-based RL',\n",
    "                'Self-supervised learning for RL',\n",
    "                'Hierarchical reinforcement learning',\n",
    "                'Goal-conditioned RL',\n",
    "                'Causal reasoning in RL'\n",
    "            ],\n",
    "            'key_papers': [\n",
    "                'World Models',\n",
    "                'MuZero and AlphaZero',\n",
    "                'Hindsight Experience Replay (HER)'\n",
    "            ]\n",
    "        },\n",
    "        'Real-World Applications': {\n",
    "            'topics': [\n",
    "                'Sim-to-real transfer',\n",
    "                'Large-scale distributed RL',\n",
    "                'Real-world robotics applications',\n",
    "                'Natural language and RL',\n",
    "                'Scientific discovery and RL'\n",
    "            ],\n",
    "            'key_papers': [\n",
    "                'Domain Randomization',\n",
    "                'Impala and R2D2',\n",
    "                'AlphaFold and protein folding'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CURRENT RESEARCH FRONTIERS IN REINFORCEMENT LEARNING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for frontier, details in frontiers.items():\n",
    "        print(f\"\\n{frontier.upper()}\")\n",
    "        print(\"=\" * len(frontier))\n",
    "        \n",
    "        print(\"\\nActive Research Topics:\")\n",
    "        for topic in details['topics']:\n",
    "            print(f\"  • {topic}\")\n",
    "        \n",
    "        print(\"\\nKey Papers/Methods:\")\n",
    "        for paper in details['key_papers']:\n",
    "            print(f\"  📄 {paper}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FUTURE TRENDS AND PREDICTIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    trends = [\n",
    "        {\n",
    "            'trend': 'Foundation Models for RL',\n",
    "            'description': 'Large pre-trained models that can be adapted to many RL tasks',\n",
    "            'timeline': '2-5 years',\n",
    "            'impact': 'High'\n",
    "        },\n",
    "        {\n",
    "            'trend': 'Neurosymbolic RL',\n",
    "            'description': 'Combining neural networks with symbolic reasoning',\n",
    "            'timeline': '3-7 years',\n",
    "            'impact': 'Medium-High'\n",
    "        },\n",
    "        {\n",
    "            'trend': 'Quantum Reinforcement Learning',\n",
    "            'description': 'Leveraging quantum computing for RL algorithms',\n",
    "            'timeline': '5-10 years',\n",
    "            'impact': 'Unknown'\n",
    "        },\n",
    "        {\n",
    "            'trend': 'Continual/Lifelong Learning',\n",
    "            'description': 'Agents that learn continuously without forgetting',\n",
    "            'timeline': '2-5 years',\n",
    "            'impact': 'High'\n",
    "        },\n",
    "        {\n",
    "            'trend': 'Embodied AI and Robotics',\n",
    "            'description': 'RL agents in physical bodies interacting with real world',\n",
    "            'timeline': '3-8 years',\n",
    "            'impact': 'Very High'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"{'Trend':<25} {'Timeline':<10} {'Impact':<10} {'Description':<30}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for trend in trends:\n",
    "        print(f\"{trend['trend']:<25} {trend['timeline']:<10} {trend['impact']:<10} {trend['description'][:30]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPEN CHALLENGES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    challenges = [\n",
    "        \"🔥 Sample Efficiency: Most methods still require millions of samples\",\n",
    "        \"⚠️  Safety: Ensuring safe exploration in high-stakes environments\",\n",
    "        \"🧠 Generalization: Policies often overfit to training environments\",\n",
    "        \"📏 Scalability: Scaling to high-dimensional action/observation spaces\",\n",
    "        \"🎯 Reward Design: Specifying rewards that lead to desired behavior\",\n",
    "        \"🔍 Interpretability: Understanding what policies have learned\",\n",
    "        \"⏱️  Temporal Credit Assignment: Learning from delayed rewards\",\n",
    "        \"🌍 Real-World Deployment: Bridging the sim-to-real gap\"\n",
    "    ]\n",
    "    \n",
    "    for challenge in challenges:\n",
    "        print(challenge)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GETTING INVOLVED IN RL RESEARCH\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    involvement_tips = [\n",
    "        \"📚 Follow key conferences: ICML, NeurIPS, ICLR, AAAI\",\n",
    "        \"🏫 Join research groups at universities or companies\",\n",
    "        \"💻 Contribute to open-source RL libraries (Stable Baselines3, RLlib)\",\n",
    "        \"🎮 Participate in RL competitions and challenges\",\n",
    "        \"📝 Read and reproduce key papers\",\n",
    "        \"🤝 Join RL communities (Reddit r/MachineLearning, Discord servers)\",\n",
    "        \"🧪 Start with simple research questions and build up\",\n",
    "        \"👥 Collaborate with practitioners in application domains\"\n",
    "    ]\n",
    "    \n",
    "    for tip in involvement_tips:\n",
    "        print(tip)\n",
    "\n",
    "\n",
    "research_frontiers_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive RL Series Summary\n",
    "\n",
    "Final summary of the entire reinforcement learning series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_series_summary():\n",
    "    \"\"\"Create comprehensive summary of the RL series.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"REINFORCEMENT LEARNING SERIES COMPLETE SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    parts = {\n",
    "        'Part 1: RL Fundamentals & Tabular Methods': {\n",
    "            'topics': [\n",
    "                'Markov Decision Processes (MDPs)',\n",
    "                'Bellman equations and optimality',\n",
    "                'Value iteration and policy iteration',\n",
    "                'Q-learning and SARSA',\n",
    "                'Exploration vs exploitation'\n",
    "            ],\n",
    "            'key_algorithms': ['Value Iteration', 'Policy Iteration', 'Q-Learning', 'SARSA'],\n",
    "            'environments': ['GridWorld', 'CliffWalking', 'WindyGridWorld']\n",
    "        },\n",
    "        'Part 2: Monte Carlo & Temporal Difference': {\n",
    "            'topics': [\n",
    "                'Monte Carlo prediction and control',\n",
    "                'Temporal difference learning',\n",
    "                'Eligibility traces and TD(λ)',\n",
    "                'On-policy vs off-policy methods',\n",
    "                'Function approximation introduction'\n",
    "            ],\n",
    "            'key_algorithms': ['Monte Carlo', 'TD(0)', 'TD(λ)', 'Expected SARSA'],\n",
    "            'environments': ['Blackjack', 'RandomWalk', 'MountainCar']\n",
    "        },\n",
    "        'Part 3: From Tabular to Deep RL': {\n",
    "            'topics': [\n",
    "                'Function approximation theory',\n",
    "                'Linear and neural network approximation',\n",
    "                'The deadly triad challenges',\n",
    "                'Experience replay introduction',\n",
    "                'Target networks for stability'\n",
    "            ],\n",
    "            'key_algorithms': ['Linear Function Approximation', 'Simple DQN'],\n",
    "            'environments': ['CartPole', 'Feature-based environments']\n",
    "        },\n",
    "        'Part 4: Deep Q-Learning': {\n",
    "            'topics': [\n",
    "                'Deep Q-Networks (DQN)',\n",
    "                'Double DQN and overestimation bias',\n",
    "                'Dueling DQN architecture',\n",
    "                'Prioritized experience replay',\n",
    "                'Rainbow DQN improvements'\n",
    "            ],\n",
    "            'key_algorithms': ['DQN', 'Double DQN', 'Dueling DQN', 'Prioritized Replay'],\n",
    "            'environments': ['CartPole', 'Atari games (conceptual)']\n",
    "        },\n",
    "        'Part 5: Policy Gradient Methods': {\n",
    "            'topics': [\n",
    "                'Policy gradient theorem',\n",
    "                'REINFORCE with baseline',\n",
    "                'Actor-Critic methods',\n",
    "                'Proximal Policy Optimization (PPO)',\n",
    "                'Continuous action spaces'\n",
    "            ],\n",
    "            'key_algorithms': ['REINFORCE', 'Actor-Critic', 'PPO'],\n",
    "            'environments': ['CartPole', 'Pendulum', 'Continuous control']\n",
    "        },\n",
    "        'Part 6: Advanced Methods & Applications': {\n",
    "            'topics': [\n",
    "                'Soft Actor-Critic (SAC)',\n",
    "                'Model-based RL and Dyna-Q',\n",
    "                'Transfer learning in RL',\n",
    "                'Real-world deployment',\n",
    "                'Current research frontiers'\n",
    "            ],\n",
    "            'key_algorithms': ['SAC', 'Dyna-Q', 'Transfer Learning'],\n",
    "            'environments': ['Pendulum variants', 'GridWorld', 'Transfer scenarios']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for part_name, details in parts.items():\n",
    "        print(f\"\\n{part_name}\")\n",
    "        print(\"=\" * len(part_name))\n",
    "        \n",
    "        print(\"Topics Covered:\")\n",
    "        for topic in details['topics']:\n",
    "            print(f\"  • {topic}\")\n",
    "        \n",
    "        print(f\"\\nKey Algorithms: {', '.join(details['key_algorithms'])}\")\n",
    "        print(f\"Environments: {', '.join(details['environments'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LEARNING PROGRESSION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    progression = [\n",
    "        \"1. 📐 Mathematical Foundations → Solid theoretical understanding\",\n",
    "        \"2. 🎯 Tabular Methods → Core concepts with simple environments\",\n",
    "        \"3. 🧮 Function Approximation → Scaling to complex state spaces\",\n",
    "        \"4. 🧠 Deep RL → Neural networks for value functions\",\n",
    "        \"5. 🎭 Policy Methods → Direct policy optimization\",\n",
    "        \"6. 🚀 Advanced Topics → State-of-the-art methods and applications\"\n",
    "    ]\n",
    "    \n",
    "    for step in progression:\n",
    "        print(step)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"METHOD SELECTION GUIDE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    selection_guide = {\n",
    "        'Discrete Actions + Simple Environment': 'Q-Learning, DQN',\n",
    "        'Discrete Actions + Complex Environment': 'Double DQN, Dueling DQN',\n",
    "        'Continuous Actions': 'PPO, SAC',\n",
    "        'Sample Efficiency Critical': 'Model-based methods, Transfer learning',\n",
    "        'Safety Critical': 'Conservative methods, Human oversight',\n",
    "        'Multi-Agent': 'MADDPG, Specialized multi-agent methods',\n",
    "        'Partial Observability': 'Recurrent networks, Memory-based methods',\n",
    "        'Real-World Deployment': 'SAC, PPO with extensive testing'\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Scenario':<35} {'Recommended Methods':<45}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for scenario, methods in selection_guide.items():\n",
    "        print(f\"{scenario:<35} {methods:<45}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY TAKEAWAYS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    takeaways = [\n",
    "        \"🎯 Problem Definition: Success starts with proper MDP formulation\",\n",
    "        \"⚖️  Exploration-Exploitation: Balance is crucial for learning\",\n",
    "        \"📊 Sample Efficiency: Often the limiting factor in real applications\",\n",
    "        \"🛡️  Stability: Deep RL requires careful engineering (target networks, clipping)\",\n",
    "        \"🔄 Generalization: Models often overfit to training environments\",\n",
    "        \"⚠️  Safety: Critical consideration for real-world deployment\",\n",
    "        \"📈 No Free Lunch: Method choice depends heavily on problem characteristics\",\n",
    "        \"🧪 Empirical Field: Extensive experimentation and tuning required\"\n",
    "    ]\n",
    "    \n",
    "    for takeaway in takeaways:\n",
    "        print(takeaway)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NEXT STEPS FOR PRACTITIONERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    next_steps = [\n",
    "        \"🔬 Practice: Implement algorithms from scratch to understand internals\",\n",
    "        \"🛠️  Tools: Learn production RL libraries (Stable Baselines3, RLlib)\",\n",
    "        \"📖 Theory: Deepen mathematical understanding with textbooks\",\n",
    "        \"🎮 Projects: Apply RL to personal projects and challenges\",\n",
    "        \"🤝 Community: Join RL communities and collaborate\",\n",
    "        \"📚 Research: Follow latest papers and reproduce key results\",\n",
    "        \"🏭 Applications: Identify real-world problems where RL can help\",\n",
    "        \"⚖️  Ethics: Consider societal impact of RL systems\"\n",
    "    ]\n",
    "    \n",
    "    for step in next_steps:\n",
    "        print(step)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL MESSAGE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    final_message = [\n",
    "        \"Reinforcement Learning is a powerful paradigm for solving sequential\",\n",
    "        \"decision-making problems. This series has taken you from basic concepts\", \n",
    "        \"to state-of-the-art methods, providing both theoretical understanding\",\n",
    "        \"and practical implementation skills.\",\n",
    "        \"\",\n",
    "        \"Remember: RL is as much art as science. Success requires careful\",\n",
    "        \"problem formulation, method selection, hyperparameter tuning, and\",\n",
    "        \"extensive experimentation. Start simple, understand deeply, and\",\n",
    "        \"gradually tackle more complex challenges.\",\n",
    "        \"\",\n",
    "        \"The field is rapidly evolving - stay curious, keep learning, and\",\n",
    "        \"contribute to the amazing future of intelligent agents! 🚀\"\n",
    "    ]\n",
    "    \n",
    "    for line in final_message:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "create_series_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This final notebook has covered advanced reinforcement learning methods and practical considerations for real-world deployment:\n",
    "\n",
    "### Advanced Methods Implemented\n",
    "\n",
    "1. **Soft Actor-Critic (SAC)**: State-of-the-art continuous control with entropy regularization\n",
    "2. **Model-Based RL**: Dyna-Q algorithm combining model-free and model-based learning\n",
    "3. **Transfer Learning**: Techniques for adapting learned policies to new environments\n",
    "4. **Safety Frameworks**: Deployment considerations for real-world applications\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **SAC Excellence**: Maximum entropy RL provides robust continuous control\n",
    "- **Model-Based Efficiency**: Learning environment models dramatically improves sample efficiency\n",
    "- **Transfer Learning Value**: Properly implemented transfer can save significant training time\n",
    "- **Deployment Complexity**: Real-world RL requires extensive safety and robustness considerations\n",
    "\n",
    "### Research Frontiers\n",
    "\n",
    "The field continues to evolve rapidly with exciting developments in:\n",
    "- Foundation models for RL\n",
    "- Safe and robust exploration\n",
    "- Multi-agent systems\n",
    "- Real-world applications\n",
    "\n",
    "### Complete Series Achievement\n",
    "\n",
    "Through this 6-part series, you've gained:\n",
    "- **Theoretical Foundation**: From MDPs to advanced policy optimization\n",
    "- **Practical Skills**: Implementation of major RL algorithms\n",
    "- **Method Selection**: Understanding when to use different approaches\n",
    "- **Real-World Awareness**: Deployment challenges and solutions\n",
    "\n",
    "This comprehensive foundation prepares you to tackle real-world RL problems and contribute to this exciting field. The journey from tabular methods to advanced deep RL demonstrates the remarkable progression of the field and provides you with both historical context and cutting-edge techniques.\n",
    "\n",
    "**Congratulations on completing the Reinforcement Learning series! 🎉**\n",
    "\n",
    "Continue exploring, experimenting, and pushing the boundaries of what's possible with intelligent agents!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}