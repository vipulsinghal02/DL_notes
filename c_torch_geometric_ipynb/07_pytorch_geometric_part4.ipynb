{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Geometric Part 4: Memory-Enhanced GNNs\n\nThis notebook explores memory-enhanced Graph Neural Networks with comprehensive mathematical exposition, designed for scalability to large graphs. We'll implement GraphSAINT (Graph Sampling and Aggregation) and FastGCN with detailed mathematical foundations for sampling strategies, variance analysis, and scalability theory.\n\n## Mathematical Foundation of Memory-Enhanced GNNs\n\n### The Scalability Challenge: Neighborhood Explosion\n\n**Traditional GNN Complexity:**\nFor an $L$-layer GNN, the receptive field of a node grows exponentially:\n$$|\\text{Receptive Field}| = O(d^L)$$\n\nwhere $d$ is the average node degree.\n\n**Memory Complexity:**\n$$M_{\\text{full-batch}} = O(N \\times d^L \\times D)$$\n\nwhere $N$ is number of nodes, $D$ is feature dimension.\n\n**Computational Complexity:**\n$$\\mathcal{C}_{\\text{full-batch}} = O(|\\mathcal{E}| \\times D^2 \\times L)$$\n\n### Mathematical Solutions\n\n**1. Graph Sampling (GraphSAINT):**\n$$\\mathcal{L}_{\\text{saint}} = \\sum_{s \\in \\mathcal{S}} \\frac{|\\mathcal{V}|}{|\\mathcal{V}_s|} \\mathcal{L}_s(\\theta)$$\n\n**2. Importance Sampling (FastGCN):**\n$$\\mathbb{E}[\\hat{\\mathcal{L}}] = \\mathcal{L} \\text{ with controlled variance}$$\n\n**3. Layer-wise Sampling:**\n$$\\text{Complexity} = O(K \\times D^2 \\times L)$$\n\nwhere $K \\ll |\\mathcal{E}|$ is the sample size.\n\n### Key Mathematical Principles\n\n**Unbiased Estimation:**\n$$\\mathbb{E}[\\hat{\\mathcal{L}}_{\\text{sample}}] = \\mathcal{L}_{\\text{full}}$$\n\n**Variance Control:**\n$$\\text{Var}[\\hat{\\mathcal{L}}_{\\text{sample}}] = O(1/K)$$\n\n**Convergence Guarantees:**\n$$\\|\\hat{\\theta}_T - \\theta^*\\| = O(1/\\sqrt{T})$$\n\nThese mathematical foundations enable training on graphs with millions of nodes while maintaining convergence guarantees and controlling approximation error."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (uncomment if needed)\n",
    "# !pip install torch torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, FastRGCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import Planetoid, Reddit\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.loader import GraphSAINTRandomWalkSampler, GraphSAINTNodeSampler, GraphSAINTEdgeSampler\n",
    "from torch_geometric.utils import degree, to_undirected\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. GraphSAINT Implementation\n\nGraphSAINT (Graph Sampling and Aggregation) uses graph sampling to create mini-batches of subgraphs for training, enabling scalable GNN training on large graphs.\n\n### Mathematical Foundation of GraphSAINT\n\n**Sampling Strategy Mathematical Framework:**\n\n1. **Subgraph Sampling:**\n   For a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, we sample a subgraph $\\mathcal{G}_s = (\\mathcal{V}_s, \\mathcal{E}_s)$ where:\n   $$\\mathcal{V}_s \\subseteq \\mathcal{V}, \\quad \\mathcal{E}_s \\subseteq \\mathcal{E}$$\n\n2. **Sampling Probability Distribution:**\n   Each node $v \\in \\mathcal{V}$ has sampling probability $p_v$:\n   $$\\sum_{v \\in \\mathcal{V}} p_v = 1$$\n\n3. **Importance Sampling Loss:**\n   The unbiased estimator for the full-graph loss is:\n   $$\\mathcal{L}_{\\text{saint}} = \\sum_{s \\in \\mathcal{S}} \\frac{|\\mathcal{V}|}{|\\mathcal{V}_s|} \\mathcal{L}_s(\\theta)$$\n   \n   where $\\mathcal{L}_s(\\theta)$ is the loss on subgraph $s$.\n\n**Sampling Strategies:**\n\n1. **Node Sampling:**\n   $$p_v^{\\text{node}} = \\frac{1}{|\\mathcal{V}|}$$\n   \n2. **Edge Sampling:**\n   $$p_e^{\\text{edge}} = \\frac{1}{|\\mathcal{E}|}$$\n   \n3. **Random Walk Sampling:**\n   $$p_v^{\\text{rw}} = \\frac{\\sum_{w \\in \\mathcal{W}} \\mathbb{I}[v \\in w]}{|\\mathcal{W}| \\cdot L_w}$$\n   \n   where $\\mathcal{W}$ is the set of random walks and $L_w$ is walk length.\n\n**Variance Analysis:**\nThe variance of the sampling estimator is:\n$$\\text{Var}[\\mathcal{L}_{\\text{saint}}] = \\mathbb{E}\\left[\\left(\\frac{|\\mathcal{V}|}{|\\mathcal{V}_s|} \\mathcal{L}_s - \\mathcal{L}\\right)^2\\right]$$\n\n**Convergence Guarantees:**\nUnder proper sampling conditions:\n$$\\mathbb{E}[\\hat{\\theta}_T - \\theta^*] = O(1/\\sqrt{T})$$\n\nwhere $T$ is the number of training iterations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAINTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAINT model with sampling-based training\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Build GCN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            \n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index, edge_weight)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. FastGCN Implementation\n\nFastGCN implements importance sampling to select the most relevant neighbors for each layer, reducing computational complexity from exponential to linear in the number of layers.\n\n### Mathematical Foundation of FastGCN\n\n**Traditional GCN Complexity Problem:**\nStandard GCN for an $L$-layer network requires:\n$$\\mathcal{C}_{\\text{full}} = O(|\\mathcal{E}| \\times L \\times d^{L-1})$$\n\nwhere $d$ is the average degree and $|\\mathcal{E}|$ is the number of edges.\n\n**FastGCN Solution:**\n\n1. **Layer-wise Sampling:**\n   For layer $\\ell$, sample $K^{(\\ell)}$ nodes from the full set:\n   $$\\mathcal{S}^{(\\ell)} \\subset \\mathcal{V}, \\quad |\\mathcal{S}^{(\\ell)}| = K^{(\\ell)} \\ll |\\mathcal{V}|$$\n\n2. **Importance Sampling Probability:**\n   Node $v$ is sampled with probability proportional to its importance:\n   $$q_v^{(\\ell)} = \\frac{w_v^{(\\ell)}}{\\sum_{u \\in \\mathcal{V}} w_u^{(\\ell)}}$$\n   \n   where $w_v^{(\\ell)}$ can be based on degree, feature norm, or learned weights.\n\n3. **Degree-based Importance:**\n   $$q_v^{(\\ell)} = \\frac{d_v}{\\sum_{u \\in \\mathcal{V}} d_u} = \\frac{d_v}{2|\\mathcal{E}|}$$\n\n**Unbiased Estimation:**\nThe FastGCN convolution becomes:\n$$\\mathbf{H}^{(\\ell+1)} = \\sigma\\left(\\frac{1}{K^{(\\ell)}} \\sum_{v \\in \\mathcal{S}^{(\\ell)}} \\frac{\\mathbf{A}_{:,v}}{q_v^{(\\ell)}} \\mathbf{H}_v^{(\\ell)} \\mathbf{W}^{(\\ell)}\\right)$$\n\n**Variance Control:**\nThe variance of the estimator is:\n$$\\text{Var}[\\hat{\\mathbf{H}}^{(\\ell+1)}] = \\frac{1}{K^{(\\ell)}} \\sum_{v \\in \\mathcal{V}} q_v^{(\\ell)} \\left(\\frac{\\mathbf{A}_{:,v}}{q_v^{(\\ell)}} \\mathbf{H}_v^{(\\ell)} \\mathbf{W}^{(\\ell)}\\right)^2$$\n\n**Optimal Sampling Strategy:**\nTo minimize variance:\n$$q_v^{(\\ell)*} = \\frac{\\|\\mathbf{A}_{:,v} \\mathbf{H}_v^{(\\ell)}\\|}{\\sum_{u \\in \\mathcal{V}} \\|\\mathbf{A}_{:,u} \\mathbf{H}_u^{(\\ell)}\\|}$$\n\n**Computational Complexity Reduction:**\n$$\\mathcal{C}_{\\text{FastGCN}} = O\\left(\\sum_{\\ell=1}^L K^{(\\ell)} \\times d \\times D\\right)$$\n\nwhere $D$ is the feature dimension, significantly reducing complexity when $K^{(\\ell)} \\ll |\\mathcal{V}|$."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastGCNConv(nn.Module):\n",
    "    \"\"\"\n",
    "    FastGCN convolution layer with importance sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, sample_size=100):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, x, edge_index, importance_scores=None):\n",
    "        \"\"\"\n",
    "        Forward pass with importance sampling\n",
    "        \"\"\"\n",
    "        num_nodes = x.size(0)\n",
    "        \n",
    "        if importance_scores is None:\n",
    "            # Use degree as importance score\n",
    "            row, col = edge_index\n",
    "            deg = degree(col, num_nodes=num_nodes)\n",
    "            importance_scores = deg / deg.sum()\n",
    "        \n",
    "        # Sample nodes based on importance\n",
    "        sample_size = min(self.sample_size, num_nodes)\n",
    "        \n",
    "        if sample_size < num_nodes:\n",
    "            # Importance sampling\n",
    "            sampled_indices = torch.multinomial(\n",
    "                importance_scores, \n",
    "                sample_size, \n",
    "                replacement=False\n",
    "            )\n",
    "            \n",
    "            # Create subgraph\n",
    "            sampled_x = x[sampled_indices]\n",
    "            \n",
    "            # Transform features\n",
    "            out = torch.matmul(sampled_x, self.weight) + self.bias\n",
    "            \n",
    "            # Map back to original size\n",
    "            result = torch.zeros(num_nodes, self.out_channels, device=x.device)\n",
    "            result[sampled_indices] = out * (num_nodes / sample_size)  # Importance weighting\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            # Regular convolution if sample size is large enough\n",
    "            return torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "class FastGCNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete FastGCN model with importance sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3, \n",
    "                 sample_sizes=None, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        if sample_sizes is None:\n",
    "            sample_sizes = [200, 100, 50][:num_layers]  # Decreasing sample sizes\n",
    "        \n",
    "        # Build FastGCN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        # First layer\n",
    "        self.convs.append(FastGCNConv(input_dim, hidden_dim, sample_sizes[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, num_layers - 1):\n",
    "            self.convs.append(FastGCNConv(hidden_dim, hidden_dim, sample_sizes[min(i, len(sample_sizes)-1)]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(FastGCNConv(hidden_dim, output_dim, sample_sizes[-1]))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Memory Monitoring Utilities\n\nTo understand the memory efficiency of our approaches, we implement utilities to monitor memory usage during training and inference with mathematical analysis of memory complexity.\n\n### Mathematical Memory Analysis\n\n**Full-batch GCN Memory Requirements:**\n\n1. **Node Embeddings Storage:**\n   $$M_{\\text{embeddings}} = L \\times |\\mathcal{V}| \\times D \\times \\text{sizeof}(\\text{float})$$\n\n2. **Adjacency Matrix Storage:**\n   $$M_{\\text{adj}} = |\\mathcal{E}| \\times 2 \\times \\text{sizeof}(\\text{int}) \\quad \\text{(sparse format)}$$\n\n3. **Gradient Storage:**\n   $$M_{\\text{gradients}} = \\sum_{\\ell=1}^L (D^{(\\ell)} \\times D^{(\\ell+1)} + D^{(\\ell+1)}) \\times \\text{sizeof}(\\text{float})$$\n\n4. **Total Memory Complexity:**\n   $$M_{\\text{total}} = O(L \\times |\\mathcal{V}| \\times D + |\\mathcal{E}| + \\sum_{\\ell} D^{(\\ell)} \\times D^{(\\ell+1)})$$\n\n**Memory-Enhanced Approaches:**\n\n**GraphSAINT Memory Reduction:**\n$$M_{\\text{saint}} = O(L \\times |\\mathcal{V}_s| \\times D + |\\mathcal{E}_s|) \\ll M_{\\text{total}}$$\n\nwhere $|\\mathcal{V}_s| \\ll |\\mathcal{V}|$ and $|\\mathcal{E}_s| \\ll |\\mathcal{E}|$.\n\n**FastGCN Memory Reduction:**\n$$M_{\\text{fastgcn}} = O\\left(L \\times \\max_{\\ell} K^{(\\ell)} \\times D + |\\mathcal{E}|\\right)$$\n\n**Memory Efficiency Ratio:**\n$$\\eta_{\\text{memory}} = \\frac{M_{\\text{enhanced}}}{M_{\\text{full}}} = \\frac{\\max(|\\mathcal{V}_s|, \\max_{\\ell} K^{(\\ell)})}{|\\mathcal{V}|}$$\n\n**Peak Memory Analysis:**\nDuring backpropagation, peak memory includes:\n- Forward pass activations: $M_{\\text{forward}}$\n- Gradient computations: $M_{\\text{backward}}$\n- Optimizer states: $M_{\\text{optimizer}}$\n\n$$M_{\\text{peak}} = M_{\\text{forward}} + M_{\\text{backward}} + M_{\\text{optimizer}}$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryMonitor:\n",
    "    \"\"\"\n",
    "    Monitor memory usage during training\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.memory_log = []\n",
    "        self.time_log = []\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def log_memory(self, tag=\"\"):\n",
    "        \"\"\"\n",
    "        Log current memory usage\n",
    "        \"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        current_time = time.time() - self.start_time\n",
    "        \n",
    "        self.memory_log.append((tag, memory_mb))\n",
    "        self.time_log.append(current_time)\n",
    "        \n",
    "        print(f\"[{current_time:.2f}s] {tag}: {memory_mb:.1f} MB\")\n",
    "        return memory_mb\n",
    "    \n",
    "    def get_peak_memory(self):\n",
    "        \"\"\"\n",
    "        Get peak memory usage\n",
    "        \"\"\"\n",
    "        if not self.memory_log:\n",
    "            return 0\n",
    "        return max([mem for _, mem in self.memory_log])\n",
    "    \n",
    "    def plot_memory_usage(self):\n",
    "        \"\"\"\n",
    "        Plot memory usage over time\n",
    "        \"\"\"\n",
    "        if len(self.memory_log) < 2:\n",
    "            print(\"Not enough data points to plot\")\n",
    "            return\n",
    "            \n",
    "        memory_values = [mem for _, mem in self.memory_log]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.time_log, memory_values, 'o-', alpha=0.7)\n",
    "        plt.xlabel('Time (seconds)')\n",
    "        plt.ylabel('Memory Usage (MB)')\n",
    "        plt.title('Memory Usage Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Annotate key points\n",
    "        for i, (tag, mem) in enumerate(self.memory_log[::max(1, len(self.memory_log)//10)]):\n",
    "            if tag:\n",
    "                idx = i * max(1, len(self.memory_log)//10)\n",
    "                plt.annotate(tag, (self.time_log[idx], mem), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def benchmark_model(model, data, num_runs=5, batch_size=None):\n",
    "    \"\"\"\n",
    "    Benchmark model inference time and memory usage\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Warm-up runs\n",
    "    with torch.no_grad():\n",
    "        for _ in range(2):\n",
    "            _ = model(data.x, data.edge_index)\n",
    "    \n",
    "    # Benchmark runs\n",
    "    times = []\n",
    "    monitor = MemoryMonitor()\n",
    "    \n",
    "    monitor.log_memory(\"Before benchmark\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            _ = model(data.x, data.edge_index)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            if i == 0:  # Log memory after first run\n",
    "                monitor.log_memory(\"After forward pass\")\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    peak_memory = monitor.get_peak_memory()\n",
    "    \n",
    "    return {\n",
    "        'avg_time': avg_time,\n",
    "        'std_time': std_time,\n",
    "        'peak_memory': peak_memory,\n",
    "        'times': times\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Dataset Preparation\n\nWe'll use the Cora dataset for our experiments, and create a synthetic larger graph to demonstrate the scalability benefits of these memory-enhanced approaches.\n\n### Mathematical Graph Properties\n\n**Graph Characterization:**\nFor a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$:\n\n1. **Degree Distribution:**\n   $$d_v = |\\{u \\in \\mathcal{V} : (v,u) \\in \\mathcal{E}\\}|$$\n   \n   Average degree: $\\bar{d} = \\frac{2|\\mathcal{E}|}{|\\mathcal{V}|}$\n\n2. **Graph Density:**\n   $$\\rho = \\frac{|\\mathcal{E}|}{|\\mathcal{V}|(|\\mathcal{V}|-1)/2}$$\n\n3. **Memory Scaling with Graph Size:**\n   For dense graphs: $M = O(|\\mathcal{V}|^2)$\n   For sparse graphs: $M = O(|\\mathcal{V}| + |\\mathcal{E}|)$\n\n**Synthetic Graph Generation (Erdős–Rényi Model):**\n\n1. **Edge Probability:**\n   $$P((i,j) \\in \\mathcal{E}) = p = \\frac{\\bar{d}}{|\\mathcal{V}|-1}$$\n\n2. **Expected Properties:**\n   - Expected edges: $\\mathbb{E}[|\\mathcal{E}|] = p \\binom{|\\mathcal{V}|}{2}$\n   - Degree distribution: $d_v \\sim \\text{Binomial}(|\\mathcal{V}|-1, p)$\n\n3. **Clustering Coefficient:**\n   $$C = \\frac{\\text{Number of triangles}}{\\text{Number of connected triples}} \\approx p$$\n\n**Memory Requirements for Different Graph Sizes:**\n\n| Graph Size | Dense Memory | Sparse Memory | Sampling Benefit |\n|------------|-------------|---------------|------------------|\n| 1K nodes   | $O(10^6)$   | $O(10^4)$     | 10x              |\n| 10K nodes  | $O(10^8)$   | $O(10^5)$     | 100x             |\n| 100K nodes | $O(10^{10})$| $O(10^6)$     | 1000x            |\n\n**Scalability Mathematical Framework:**\nMemory efficiency ratio as function of graph size:\n$$\\eta(|\\mathcal{V}|) = \\frac{M_{\\text{sample}}}{M_{\\text{full}}} = \\frac{K}{|\\mathcal{V}|} \\to 0 \\text{ as } |\\mathcal{V}| \\to \\infty$$\n\nwhere $K$ is the fixed sample size."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset\n",
    "print(\"Loading datasets...\")\n",
    "cora_dataset = Planetoid('/tmp/Cora', 'Cora', transform=NormalizeFeatures())\n",
    "cora_data = cora_dataset[0]\n",
    "\n",
    "print(f\"Cora Dataset:\")\n",
    "print(f\"  Nodes: {cora_data.num_nodes}\")\n",
    "print(f\"  Edges: {cora_data.num_edges}\")\n",
    "print(f\"  Features: {cora_data.num_features}\")\n",
    "print(f\"  Classes: {cora_dataset.num_classes}\")\n",
    "\n",
    "def create_synthetic_large_graph(num_nodes=5000, num_features=100, num_classes=10, \n",
    "                                avg_degree=10, seed=42):\n",
    "    \"\"\"\n",
    "    Create a synthetic large graph for scalability testing\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate random features\n",
    "    x = torch.randn(num_nodes, num_features)\n",
    "    \n",
    "    # Generate random labels\n",
    "    y = torch.randint(0, num_classes, (num_nodes,))\n",
    "    \n",
    "    # Generate edges using Erdős–Rényi model\n",
    "    prob = avg_degree / (num_nodes - 1)\n",
    "    adj_matrix = torch.rand(num_nodes, num_nodes) < prob\n",
    "    \n",
    "    # Make symmetric and remove self-loops\n",
    "    adj_matrix = adj_matrix | adj_matrix.T\n",
    "    adj_matrix.fill_diagonal_(False)\n",
    "    \n",
    "    # Convert to edge_index format\n",
    "    edge_index = adj_matrix.nonzero().t()\n",
    "    \n",
    "    # Create train/val/test masks\n",
    "    num_train = int(0.6 * num_nodes)\n",
    "    num_val = int(0.2 * num_nodes)\n",
    "    \n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[:num_train] = True\n",
    "    val_mask[num_train:num_train + num_val] = True\n",
    "    test_mask[num_train + num_val:] = True\n",
    "    \n",
    "    # Shuffle masks\n",
    "    perm = torch.randperm(num_nodes)\n",
    "    train_mask = train_mask[perm]\n",
    "    val_mask = val_mask[perm]\n",
    "    test_mask = test_mask[perm]\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y, \n",
    "                train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
    "\n",
    "# Create synthetic large graph for testing scalability\n",
    "print(\"\\nCreating synthetic large graph...\")\n",
    "large_graph = create_synthetic_large_graph(num_nodes=2000, num_features=50)  # Smaller for CPU\n",
    "\n",
    "print(f\"Synthetic Graph:\")\n",
    "print(f\"  Nodes: {large_graph.num_nodes}\")\n",
    "print(f\"  Edges: {large_graph.num_edges}\")\n",
    "print(f\"  Features: {large_graph.x.size(1)}\")\n",
    "print(f\"  Average degree: {large_graph.num_edges / large_graph.num_nodes:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. GraphSAINT Training Setup\n\nLet's set up GraphSAINT with different sampling strategies. We'll compare random walk sampling, node sampling, and edge sampling to see their effects on performance and memory usage.\n\n### Mathematical Analysis of Sampling Strategies\n\n**Random Walk Sampling Mathematics:**\n\n1. **Transition Probability:**\n   $$P_{uv} = \\frac{A_{uv}}{d_u}$$\n   \n   where $A_{uv}$ is the adjacency matrix element and $d_u$ is node degree.\n\n2. **Stationary Distribution:**\n   $$\\pi_v = \\frac{d_v}{2|E|}$$\n\n3. **Walk Coverage Probability:**\n   For a random walk of length $L$, node $v$ is included with probability:\n   $$P(\\text{visit } v) = 1 - (1 - \\pi_v)^L$$\n\n**Node Sampling Mathematics:**\n\n1. **Uniform Sampling:**\n   $$P(\\text{select } v) = \\frac{1}{|V|}$$\n\n2. **Expected Subgraph Size:**\n   $$E[|V_s|] = |V| \\times P(\\text{selection})$$\n\n3. **Edge Preservation:**\n   $$E[|E_s|] = |E| \\times P(\\text{both endpoints selected})$$\n\n**Edge Sampling Mathematics:**\n\n1. **Edge Selection Probability:**\n   $$P(\\text{select edge } (u,v)) = \\frac{1}{|E|}$$\n\n2. **Induced Subgraph:**\n   $$V_s = \\{u : \\exists v \\text{ s.t. } (u,v) \\in E_s\\}$$\n\n**Variance Analysis for Different Strategies:**\n\n1. **Random Walk Sampling Variance:**\n   $$\\text{Var}[L_{\\text{rw}}] = O\\left(\\frac{1}{L \\times |\\text{walks}|}\\right)$$\n\n2. **Node Sampling Variance:**\n   $$\\text{Var}[L_{\\text{node}}] = O\\left(\\frac{1}{|V_s|}\\right)$$\n\n3. **Edge Sampling Variance:**\n   $$\\text{Var}[L_{\\text{edge}}] = O\\left(\\frac{1}{|E_s|}\\right)$$\n\n**Sample Coverage Parameter:**\nThe coverage parameter $c$ ensures each node is sampled approximately $c$ times:\n$$E[\\text{times node } v \\text{ is sampled}] \\approx c$$\n\n**Batch Size vs. Coverage Trade-off:**\n$$\\text{Total Training Cost} = \\frac{|\\text{epochs}| \\times |V|}{|\\text{batch\\_size}| \\times c}$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_graphsaint_loaders(data, batch_size=512, walk_length=3, num_steps=30, \n",
    "                            sample_coverage=20, cpu_only=True):\n",
    "    \"\"\"\n",
    "    Setup GraphSAINT data loaders with different sampling strategies\n",
    "    \"\"\"\n",
    "    # Make sure edge_index is undirected\n",
    "    data.edge_index = to_undirected(data.edge_index)\n",
    "    \n",
    "    loaders = {}\n",
    "    \n",
    "    # Random Walk Sampler\n",
    "    try:\n",
    "        rw_loader = GraphSAINTRandomWalkSampler(\n",
    "            data,\n",
    "            batch_size=batch_size,\n",
    "            walk_length=walk_length,\n",
    "            num_steps=num_steps,\n",
    "            sample_coverage=sample_coverage,\n",
    "            shuffle=True\n",
    "        )\n",
    "        loaders['random_walk'] = rw_loader\n",
    "        print(f\"Random Walk Sampler: batch_size={batch_size}, walk_length={walk_length}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create Random Walk Sampler: {e}\")\n",
    "    \n",
    "    # Node Sampler\n",
    "    try:\n",
    "        node_loader = GraphSAINTNodeSampler(\n",
    "            data,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        loaders['node'] = node_loader\n",
    "        print(f\"Node Sampler: batch_size={batch_size}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create Node Sampler: {e}\")\n",
    "    \n",
    "    # Edge Sampler\n",
    "    try:\n",
    "        edge_loader = GraphSAINTEdgeSampler(\n",
    "            data,\n",
    "            batch_size=batch_size // 4,  # Smaller batch for edge sampling\n",
    "            shuffle=True\n",
    "        )\n",
    "        loaders['edge'] = edge_loader\n",
    "        print(f\"Edge Sampler: batch_size={batch_size // 4}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create Edge Sampler: {e}\")\n",
    "    \n",
    "    return loaders\n",
    "\n",
    "# Setup GraphSAINT loaders for Cora\n",
    "print(\"Setting up GraphSAINT loaders...\")\n",
    "saint_loaders = setup_graphsaint_loaders(cora_data, batch_size=256, sample_coverage=10)\n",
    "\n",
    "print(f\"\\nAvailable samplers: {list(saint_loaders.keys())}\")\n",
    "\n",
    "# Show example batch\n",
    "if 'node' in saint_loaders:\n",
    "    sample_batch = next(iter(saint_loaders['node']))\n",
    "    print(f\"\\nExample batch from node sampler:\")\n",
    "    print(f\"  Batch nodes: {sample_batch.num_nodes}\")\n",
    "    print(f\"  Batch edges: {sample_batch.num_edges}\")\n",
    "    print(f\"  Has node_norm: {hasattr(sample_batch, 'node_norm')}\")\n",
    "    print(f\"  Has edge_norm: {hasattr(sample_batch, 'edge_norm')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Training Functions for Memory-Enhanced Models\n\nWe create specialized training functions that can handle both GraphSAINT and FastGCN, with built-in memory monitoring to track efficiency gains.\n\n### Mathematical Training Dynamics\n\n**Stochastic Gradient Descent with Sampling:**\n\n1. **Full-batch Gradient:**\n   $$\\nabla \\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{V}_{\\text{train}}|} \\sum_{v \\in \\mathcal{V}_{\\text{train}}} \\nabla \\ell(f_\\theta(\\mathcal{G}, v), y_v)$$\n\n2. **Sampled Gradient (GraphSAINT):**\n   $$\\nabla \\hat{\\mathcal{L}}_{\\text{saint}}(\\theta) = \\frac{|\\mathcal{V}|}{|\\mathcal{V}_s|} \\frac{1}{|\\mathcal{V}_{s,\\text{train}}|} \\sum_{v \\in \\mathcal{V}_{s,\\text{train}}} \\nabla \\ell(f_\\theta(\\mathcal{G}_s, v), y_v)$$\n\n3. **Sampled Gradient (FastGCN):**\n   $$\\nabla \\hat{\\mathcal{L}}_{\\text{fast}}(\\theta) = \\frac{1}{|\\mathcal{V}_{\\text{train}}|} \\sum_{v \\in \\mathcal{V}_{\\text{train}}} \\nabla \\ell(\\hat{f}_\\theta(\\mathcal{G}, v), y_v)$$\n   \n   where $\\hat{f}_\\theta$ uses sampled neighborhoods.\n\n**Unbiased Estimation Property:**\nFor both approaches, we maintain:\n$$\\mathbb{E}[\\nabla \\hat{\\mathcal{L}}(\\theta)] = \\nabla \\mathcal{L}(\\theta)$$\n\n**Convergence Analysis:**\n\n1. **Learning Rate Schedule:**\n   $$\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t}}$$ or $$\\alpha_t = \\alpha_0 \\gamma^{t/T}$$\n\n2. **Convergence Rate:**\n   Under standard assumptions:\n   $$\\mathbb{E}[\\|\\nabla \\mathcal{L}(\\theta_T)\\|^2] = O\\left(\\frac{1}{\\sqrt{T}}\\right)$$\n\n3. **Variance Effect:**\n   Higher sampling variance requires smaller learning rates:\n   $$\\alpha_{\\text{optimal}} \\propto \\frac{1}{\\sqrt{\\text{Var}[\\nabla \\hat{\\mathcal{L}}]}}$$\n\n**Normalization in GraphSAINT:**\n\n1. **Node Normalization:**\n   $$w_v^{\\text{node}} = \\frac{|\\mathcal{V}|}{|\\mathcal{V}_s| \\times P(\\text{node } v \\text{ sampled})}$$\n\n2. **Edge Normalization:**\n   $$w_{uv}^{\\text{edge}} = \\frac{|\\mathcal{E}|}{|\\mathcal{E}_s| \\times P(\\text{edge } (u,v) \\text{ sampled})}$$\n\n**Memory-Time Trade-off:**\n$$\\text{Training Time} \\propto \\frac{\\text{Memory Saved}}{\\text{Convergence Rate}}$$\n\nThe optimal balance depends on:\n- Available memory constraints\n- Desired accuracy requirements  \n- Time budget for training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graphsaint(model, loader, optimizer, criterion, device, monitor=None):\n",
    "    \"\"\"\n",
    "    Train GraphSAINT for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    if monitor:\n",
    "        monitor.log_memory(\"Start training epoch\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with normalization if available\n",
    "        if hasattr(batch, 'edge_norm'):\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_norm)\n",
    "        else:\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "        \n",
    "        # Only use training nodes in the batch\n",
    "        if hasattr(batch, 'train_mask') and batch.train_mask.any():\n",
    "            mask = batch.train_mask\n",
    "        else:\n",
    "            # If no train_mask in batch, assume all nodes are training nodes\n",
    "            mask = torch.ones(batch.num_nodes, dtype=torch.bool, device=device)\n",
    "        \n",
    "        loss = criterion(out[mask], batch.y[mask])\n",
    "        \n",
    "        # Apply node normalization if available\n",
    "        if hasattr(batch, 'node_norm'):\n",
    "            loss = loss * batch.node_norm[mask].sum() / mask.sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = out[mask].argmax(dim=1)\n",
    "        total_correct += (pred == batch.y[mask]).sum().item()\n",
    "        total_nodes += mask.sum().item()\n",
    "        \n",
    "        if monitor and batch_idx == 0:\n",
    "            monitor.log_memory(f\"After batch {batch_idx}\")\n",
    "    \n",
    "    if monitor:\n",
    "        monitor.log_memory(\"End training epoch\")\n",
    "    \n",
    "    return total_loss / len(loader), total_correct / total_nodes\n",
    "\n",
    "def train_fastgcn(model, data, optimizer, criterion, device, monitor=None):\n",
    "    \"\"\"\n",
    "    Train FastGCN for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    data = data.to(device)\n",
    "    \n",
    "    if monitor:\n",
    "        monitor.log_memory(\"Start FastGCN training\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if monitor:\n",
    "        monitor.log_memory(\"End FastGCN training\")\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    pred = out[data.train_mask].argmax(dim=1)\n",
    "    acc = (pred == data.y[data.train_mask]).float().mean()\n",
    "    \n",
    "    return loss.item(), acc.item()\n",
    "\n",
    "def evaluate_model(model, data, device, mask_type='test'):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation or test set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        \n",
    "        if mask_type == 'val':\n",
    "            mask = data.val_mask\n",
    "        else:\n",
    "            mask = data.test_mask\n",
    "        \n",
    "        pred = out[mask].argmax(dim=1)\n",
    "        acc = accuracy_score(data.y[mask].cpu(), pred.cpu())\n",
    "        f1 = f1_score(data.y[mask].cpu(), pred.cpu(), average='weighted')\n",
    "    \n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training GraphSAINT Models\n",
    "\n",
    "Let's train GraphSAINT models using different sampling strategies and compare their performance and memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device and optimization\n",
    "device = torch.device('cpu')\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 50  # Reduced for CPU\n",
    "lr = 0.01\n",
    "hidden_dim = 64\n",
    "\n",
    "print(\"Training GraphSAINT models...\")\n",
    "\n",
    "saint_results = {}\n",
    "\n",
    "# Train with different sampling strategies\n",
    "for sampler_name, loader in saint_loaders.items():\n",
    "    print(f\"\\n=== Training with {sampler_name} sampler ===\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GraphSAINTModel(\n",
    "        input_dim=cora_dataset.num_features,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=cora_dataset.num_classes,\n",
    "        num_layers=2,  # Reduced for CPU\n",
    "        dropout=0.5\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Memory monitor\n",
    "    monitor = MemoryMonitor()\n",
    "    monitor.log_memory(f\"Model initialized ({sampler_name})\")\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        loss, acc = train_graphsaint(model, loader, optimizer, criterion, device, \n",
    "                                   monitor if epoch == 0 else None)\n",
    "        train_losses.append(loss)\n",
    "        train_accs.append(acc)\n",
    "        \n",
    "        # Validation every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            val_acc, val_f1 = evaluate_model(model, cora_data, device, 'val')\n",
    "            val_accs.append(val_acc)\n",
    "            print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Train Acc: {acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Final test evaluation\n",
    "    test_acc, test_f1 = evaluate_model(model, cora_data, device, 'test')\n",
    "    peak_memory = monitor.get_peak_memory()\n",
    "    \n",
    "    saint_results[sampler_name] = {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'peak_memory': peak_memory,\n",
    "        'num_params': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }\n",
    "    \n",
    "    print(f'Final Test Acc: {test_acc:.4f}, Test F1: {test_f1:.4f}')\n",
    "    print(f'Peak Memory: {peak_memory:.1f} MB')\n",
    "\n",
    "print(f\"\\n=== GraphSAINT Results Summary ===\")\n",
    "for name, results in saint_results.items():\n",
    "    print(f\"{name:<15} Test Acc: {results['test_acc']:.4f}, Peak Memory: {results['peak_memory']:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training FastGCN Model\n",
    "\n",
    "Now let's train the FastGCN model and compare its performance with GraphSAINT approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Training FastGCN ===\")\n",
    "\n",
    "# Initialize FastGCN model\n",
    "fastgcn_model = FastGCNModel(\n",
    "    input_dim=cora_dataset.num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=cora_dataset.num_classes,\n",
    "    num_layers=2,\n",
    "    sample_sizes=[100, 50],  # Smaller samples for CPU\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(f\"FastGCN parameters: {sum(p.numel() for p in fastgcn_model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(fastgcn_model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Memory monitor\n",
    "fastgcn_monitor = MemoryMonitor()\n",
    "fastgcn_monitor.log_memory(\"FastGCN initialized\")\n",
    "\n",
    "# Training loop\n",
    "fastgcn_losses = []\n",
    "fastgcn_train_accs = []\n",
    "fastgcn_val_accs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    loss, acc = train_fastgcn(fastgcn_model, cora_data, optimizer, criterion, device, \n",
    "                             fastgcn_monitor if epoch == 0 else None)\n",
    "    fastgcn_losses.append(loss)\n",
    "    fastgcn_train_accs.append(acc)\n",
    "    \n",
    "    # Validation every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        val_acc, val_f1 = evaluate_model(fastgcn_model, cora_data, device, 'val')\n",
    "        fastgcn_val_accs.append(val_acc)\n",
    "        print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Train Acc: {acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Final test evaluation\n",
    "fastgcn_test_acc, fastgcn_test_f1 = evaluate_model(fastgcn_model, cora_data, device, 'test')\n",
    "fastgcn_peak_memory = fastgcn_monitor.get_peak_memory()\n",
    "\n",
    "print(f'\\nFastGCN Final Test Acc: {fastgcn_test_acc:.4f}, Test F1: {fastgcn_test_f1:.4f}')\n",
    "print(f'FastGCN Peak Memory: {fastgcn_peak_memory:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Baseline GCN Comparison\n\nLet's train a standard GCN to compare against our memory-enhanced approaches and understand the trade-offs between efficiency and performance.\n\n### Mathematical Baseline Analysis\n\n**Standard Full-batch GCN:**\n\n1. **Forward Propagation:**\n   $$\\mathbf{H}^{(\\ell+1)} = \\sigma\\left(\\tilde{\\mathbf{A}} \\mathbf{H}^{(\\ell)} \\mathbf{W}^{(\\ell)}\\right)$$\n   \n   where $\\tilde{\\mathbf{A}} = \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ is the normalized adjacency matrix.\n\n2. **Complete Neighborhood Aggregation:**\n   Every node uses its complete neighborhood:\n   $$\\mathbf{h}_v^{(\\ell+1)} = \\sigma\\left(\\mathbf{W}^{(\\ell)} \\sum_{u \\in \\mathcal{N}(v) \\cup \\{v\\}} \\frac{\\mathbf{h}_u^{(\\ell)}}{\\sqrt{d_u d_v}}\\right)$$\n\n3. **Computational Complexity:**\n   $$\\mathcal{C}_{\\text{baseline}} = O(L \\times |\\mathcal{E}| \\times D \\times D')$$\n   \n   where $D$ and $D'$ are input and output dimensions.\n\n**Memory Requirements:**\n\n1. **Forward Pass Memory:**\n   $$M_{\\text{forward}} = \\sum_{\\ell=0}^L |\\mathcal{V}| \\times D^{(\\ell)}$$\n\n2. **Backward Pass Memory:**\n   $$M_{\\text{backward}} = \\sum_{\\ell=0}^{L-1} |\\mathcal{V}| \\times D^{(\\ell)}$$\n\n3. **Parameter Memory:**\n   $$M_{\\text{params}} = \\sum_{\\ell=0}^{L-1} D^{(\\ell)} \\times D^{(\\ell+1)}$$\n\n**Theoretical Performance Bounds:**\n\nFor the baseline GCN with optimal parameters:\n$$\\mathcal{L}_{\\text{baseline}} \\leq \\mathcal{L}_{\\text{Bayes}} + \\epsilon_{\\text{approx}}$$\n\nwhere $\\epsilon_{\\text{approx}}$ is the approximation error due to finite depth $L$.\n\n**Comparison Metrics:**\n\n1. **Accuracy Gap:**\n   $$\\Delta_{\\text{acc}} = \\text{Acc}_{\\text{baseline}} - \\text{Acc}_{\\text{enhanced}}$$\n\n2. **Memory Efficiency:**\n   $$\\eta_{\\text{memory}} = \\frac{M_{\\text{enhanced}}}{M_{\\text{baseline}}}$$\n\n3. **Time Efficiency:**\n   $$\\eta_{\\text{time}} = \\frac{T_{\\text{enhanced}}}{T_{\\text{baseline}}}$$\n\n4. **Overall Efficiency Score:**\n   $$S_{\\text{eff}} = \\frac{\\text{Acc}_{\\text{enhanced}}}{\\eta_{\\text{memory}} \\times \\eta_{\\text{time}}}$$\n\n**Trade-off Analysis:**\nThe fundamental trade-off is captured by:\n$$\\text{Performance} \\propto \\frac{\\text{Information Used}}{\\text{Resources Consumed}}$$\n\nWhere:\n- Information Used $\\propto$ (fraction of graph sampled)\n- Resources Consumed $\\propto$ (memory + computation time)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Training Baseline GCN ===\")\n",
    "\n",
    "# Standard GCN implementation\n",
    "class BaselineGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Initialize baseline GCN\n",
    "baseline_model = BaselineGCN(\n",
    "    input_dim=cora_dataset.num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=cora_dataset.num_classes,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(f\"Baseline GCN parameters: {sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Memory monitor\n",
    "baseline_monitor = MemoryMonitor()\n",
    "baseline_monitor.log_memory(\"Baseline GCN initialized\")\n",
    "\n",
    "# Training loop\n",
    "baseline_losses = []\n",
    "baseline_train_accs = []\n",
    "baseline_val_accs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    baseline_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch == 0:\n",
    "        baseline_monitor.log_memory(\"Start baseline training\")\n",
    "    \n",
    "    out = baseline_model(cora_data.x.to(device), cora_data.edge_index.to(device))\n",
    "    loss = criterion(out[cora_data.train_mask], cora_data.y[cora_data.train_mask].to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch == 0:\n",
    "        baseline_monitor.log_memory(\"After baseline forward/backward\")\n",
    "    \n",
    "    baseline_losses.append(loss.item())\n",
    "    \n",
    "    # Training accuracy\n",
    "    pred = out[cora_data.train_mask].argmax(dim=1)\n",
    "    acc = (pred == cora_data.y[cora_data.train_mask].to(device)).float().mean().item()\n",
    "    baseline_train_accs.append(acc)\n",
    "    \n",
    "    # Validation every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        val_acc, val_f1 = evaluate_model(baseline_model, cora_data, device, 'val')\n",
    "        baseline_val_accs.append(val_acc)\n",
    "        print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Train Acc: {acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Final test evaluation\n",
    "baseline_test_acc, baseline_test_f1 = evaluate_model(baseline_model, cora_data, device, 'test')\n",
    "baseline_peak_memory = baseline_monitor.get_peak_memory()\n",
    "\n",
    "print(f'\\nBaseline GCN Final Test Acc: {baseline_test_acc:.4f}, Test F1: {baseline_test_f1:.4f}')\n",
    "print(f'Baseline GCN Peak Memory: {baseline_peak_memory:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Comprehensive Performance Comparison\n\nLet's create detailed visualizations comparing all approaches across multiple metrics: accuracy, memory usage, and training dynamics.\n\n### Mathematical Performance Analysis Framework\n\n**Multi-Objective Optimization:**\nWe seek to optimize the vector objective:\n$$\\mathbf{f}(\\theta, S) = \\begin{bmatrix} \n\\text{Accuracy}(\\theta) \\\\\n-\\text{Memory}(S) \\\\\n-\\text{Time}(S)\n\\end{bmatrix}$$\n\nwhere $\\theta$ are model parameters and $S$ is the sampling strategy.\n\n**Pareto Efficiency:**\nA configuration $(\\theta^*, S^*)$ is Pareto efficient if there exists no other configuration $(\\theta, S)$ such that:\n$$\\mathbf{f}(\\theta, S) \\geq \\mathbf{f}(\\theta^*, S^*)$$\nwith at least one strict inequality.\n\n**Performance Metrics:**\n\n1. **Relative Accuracy:**\n   $$R_{\\text{acc}} = \\frac{\\text{Acc}_{\\text{method}}}{\\text{Acc}_{\\text{baseline}}}$$\n\n2. **Memory Efficiency Ratio:**\n   $$R_{\\text{mem}} = \\frac{M_{\\text{baseline}}}{M_{\\text{method}}}$$\n\n3. **Speed-up Factor:**\n   $$R_{\\text{speed}} = \\frac{T_{\\text{baseline}}}{T_{\\text{method}}}$$\n\n4. **Efficiency Score:**\n   $$E = R_{\\text{acc}} \\times \\sqrt{R_{\\text{mem}} \\times R_{\\text{speed}}}$$\n\n**Statistical Analysis:**\n\n1. **Confidence Intervals:**\n   For accuracy measurements with $n$ runs:\n   $$\\text{CI}_{95\\%} = \\bar{x} \\pm 1.96 \\frac{s}{\\sqrt{n}}$$\n\n2. **Significance Testing:**\n   H₀: $\\mu_{\\text{method}} = \\mu_{\\text{baseline}}$\n   \n   Test statistic: $t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}}$\n\n**Scalability Analysis:**\n\n1. **Memory Scaling:**\n   $$M(n) = a \\cdot n^b + c$$\n   \n   where $n$ is graph size, $b$ is the scaling exponent.\n\n2. **Time Complexity:**\n   $$T(n, e) = \\alpha \\cdot n^{\\beta_1} \\cdot e^{\\beta_2}$$\n   \n   where $e$ is the number of edges.\n\n**Trade-off Curves:**\nThe accuracy-efficiency trade-off follows:\n$$\\text{Accuracy} = f(\\text{Resource Budget})$$\n\ntypically showing diminishing returns:\n$$\\frac{d(\\text{Accuracy})}{d(\\text{Resources})} \\downarrow \\text{ as Resources} \\uparrow$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results for comparison\n",
    "all_results = {\n",
    "    'Baseline GCN': {\n",
    "        'test_acc': baseline_test_acc,\n",
    "        'test_f1': baseline_test_f1,\n",
    "        'peak_memory': baseline_peak_memory,\n",
    "        'train_losses': baseline_losses,\n",
    "        'val_accs': baseline_val_accs,\n",
    "        'num_params': sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)\n",
    "    },\n",
    "    'FastGCN': {\n",
    "        'test_acc': fastgcn_test_acc,\n",
    "        'test_f1': fastgcn_test_f1,\n",
    "        'peak_memory': fastgcn_peak_memory,\n",
    "        'train_losses': fastgcn_losses,\n",
    "        'val_accs': fastgcn_val_accs,\n",
    "        'num_params': sum(p.numel() for p in fastgcn_model.parameters() if p.requires_grad)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add GraphSAINT results\n",
    "for name, results in saint_results.items():\n",
    "    all_results[f'SAINT-{name}'] = results\n",
    "\n",
    "# Create comprehensive comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Test Accuracy Comparison\n",
    "models = list(all_results.keys())\n",
    "test_accs = [all_results[model]['test_acc'] for model in models]\n",
    "\n",
    "bars1 = axes[0, 0].bar(range(len(models)), test_accs, alpha=0.8, \n",
    "                      color=['red', 'blue', 'green', 'orange', 'purple'][:len(models)])\n",
    "axes[0, 0].set_ylabel('Test Accuracy')\n",
    "axes[0, 0].set_title('Test Accuracy Comparison')\n",
    "axes[0, 0].set_xticks(range(len(models)))\n",
    "axes[0, 0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars1, test_accs):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Memory Usage Comparison\n",
    "peak_memories = [all_results[model]['peak_memory'] for model in models]\n",
    "\n",
    "bars2 = axes[0, 1].bar(range(len(models)), peak_memories, alpha=0.8,\n",
    "                      color=['red', 'blue', 'green', 'orange', 'purple'][:len(models)])\n",
    "axes[0, 1].set_ylabel('Peak Memory (MB)')\n",
    "axes[0, 1].set_title('Memory Usage Comparison')\n",
    "axes[0, 1].set_xticks(range(len(models)))\n",
    "axes[0, 1].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for bar, mem in zip(bars2, peak_memories):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(peak_memories)*0.01, \n",
    "                   f'{mem:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Model Parameters Comparison\n",
    "num_params = [all_results[model]['num_params'] for model in models]\n",
    "\n",
    "bars3 = axes[0, 2].bar(range(len(models)), num_params, alpha=0.8,\n",
    "                      color=['red', 'blue', 'green', 'orange', 'purple'][:len(models)])\n",
    "axes[0, 2].set_ylabel('Number of Parameters')\n",
    "axes[0, 2].set_title('Model Complexity Comparison')\n",
    "axes[0, 2].set_xticks(range(len(models)))\n",
    "axes[0, 2].set_xticklabels(models, rotation=45, ha='right')\n",
    "\n",
    "# 4. Training Loss Curves\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for i, (model, results) in enumerate(all_results.items()):\n",
    "    if 'train_losses' in results:\n",
    "        axes[1, 0].plot(results['train_losses'], label=model, alpha=0.8, color=colors[i])\n",
    "        \n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Training Loss')\n",
    "axes[1, 0].set_title('Training Loss Curves')\n",
    "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Validation Accuracy Curves\n",
    "eval_epochs = range(0, epochs, 10)\n",
    "for i, (model, results) in enumerate(all_results.items()):\n",
    "    if 'val_accs' in results and len(results['val_accs']) > 0:\n",
    "        axes[1, 1].plot(eval_epochs[:len(results['val_accs'])], results['val_accs'], \n",
    "                       'o-', label=model, alpha=0.8, color=colors[i])\n",
    "        \n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "axes[1, 1].set_title('Validation Accuracy Curves')\n",
    "axes[1, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Efficiency Analysis (Memory vs Accuracy)\n",
    "axes[1, 2].scatter(peak_memories, test_accs, s=100, alpha=0.7, \n",
    "                  c=range(len(models)), cmap='viridis')\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    axes[1, 2].annotate(model, (peak_memories[i], test_accs[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "axes[1, 2].set_xlabel('Peak Memory (MB)')\n",
    "axes[1, 2].set_ylabel('Test Accuracy')\n",
    "axes[1, 2].set_title('Efficiency Analysis: Memory vs Accuracy')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison table\n",
    "print(\"\\n=== Detailed Performance Comparison ===\")\n",
    "print(f\"{'Model':<20} {'Test Acc':<10} {'Test F1':<10} {'Peak Mem':<12} {'Params':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model, results in all_results.items():\n",
    "    print(f\"{model:<20} {results['test_acc']:<10.4f} {results['test_f1']:<10.4f} \"\n",
    "          f\"{results['peak_memory']:<12.1f} {results['num_params']:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Scalability Testing\n\nLet's test the scalability of our memory-enhanced approaches on the synthetic large graph to demonstrate their benefits for larger datasets.\n\n### Mathematical Scalability Analysis\n\n**Asymptotic Complexity Analysis:**\n\n1. **Full-batch GCN Scaling:**\n   - Memory: $M_{\\text{full}}(n) = O(L \\cdot n \\cdot d + n^2)$ (dense case)\n   - Memory: $M_{\\text{full}}(n) = O(L \\cdot n \\cdot d + e)$ (sparse case)\n   - Time: $T_{\\text{full}}(n) = O(L \\cdot e \\cdot d)$\n\n2. **GraphSAINT Scaling:**\n   - Memory: $M_{\\text{saint}}(n) = O(L \\cdot k \\cdot d)$ where $k \\ll n$\n   - Time: $T_{\\text{saint}}(n) = O(L \\cdot k \\cdot \\bar{d}_s \\cdot d)$\n   - Scalability Factor: $\\frac{M_{\\text{full}}}{M_{\\text{saint}}} = O(n/k)$\n\n3. **FastGCN Scaling:**\n   - Memory: $M_{\\text{fast}}(n) = O(L \\cdot \\max_\\ell K^{(\\ell)} \\cdot d + e)$\n   - Time: $T_{\\text{fast}}(n) = O(L \\cdot \\sum_\\ell K^{(\\ell)} \\cdot d^2)$\n   - Scalability Factor: $\\frac{M_{\\text{full}}}{M_{\\text{fast}}} = O(n/K_{\\max})$\n\n**Scalability Metrics:**\n\n1. **Memory Scalability Coefficient:**\n   $$\\alpha_{\\text{mem}} = \\frac{\\log(M_2/M_1)}{\\log(n_2/n_1)}$$\n   \n   where $(n_1, M_1)$ and $(n_2, M_2)$ are size-memory pairs.\n\n2. **Time Scalability Coefficient:**\n   $$\\alpha_{\\text{time}} = \\frac{\\log(T_2/T_1)}{\\log(n_2/n_1)}$$\n\n3. **Efficiency Degradation:**\n   $$\\beta = \\frac{d(\\text{Accuracy})}{d(\\log n)}$$\n\n**Break-even Analysis:**\nMemory-enhanced methods become beneficial when:\n$$n > n_{\\text{break}} = \\frac{C_{\\text{overhead}}}{S_{\\text{reduction}}}$$\n\nwhere:\n- $C_{\\text{overhead}}$ is the constant overhead of sampling\n- $S_{\\text{reduction}}$ is the per-node memory/time savings\n\n**Theoretical Limits:**\n\n1. **Information-Theoretic Bound:**\n   Minimum information needed for $\\epsilon$-accurate learning:\n   $$I_{\\min}(\\epsilon) = O\\left(\\frac{d \\log n}{\\epsilon^2}\\right)$$\n\n2. **Sampling Lower Bound:**\n   Any sampling strategy must sample at least:\n   $$K_{\\min} = \\Omega\\left(\\frac{\\log n}{\\epsilon^2}\\right)$$\n   \n   nodes per layer for $\\epsilon$-approximation.\n\n**Practical Scalability Guidelines:**\n\n1. **Memory-Constrained Regime:** $M_{\\text{available}} < M_{\\text{full}}$\n   - Use GraphSAINT with $k = \\lfloor M_{\\text{available}} / (L \\cdot d) \\rfloor$\n\n2. **Time-Constrained Regime:** $T_{\\text{available}} < T_{\\text{full}}$\n   - Use FastGCN with adaptive $K^{(\\ell)}$\n\n3. **Accuracy-Constrained Regime:** $\\text{Acc}_{\\text{required}} > \\text{Acc}_{\\text{sampled}}$\n   - Increase sampling budget or use hybrid approaches"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Scalability Testing on Large Graph ===\")\n",
    "print(f\"Large graph: {large_graph.num_nodes} nodes, {large_graph.num_edges} edges\")\n",
    "\n",
    "# Test baseline GCN on large graph\n",
    "print(\"\\n--- Testing Baseline GCN ---\")\n",
    "baseline_large = BaselineGCN(\n",
    "    input_dim=large_graph.x.size(1),\n",
    "    hidden_dim=32,  # Smaller for scalability\n",
    "    output_dim=10,  # 10 classes in synthetic graph\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "baseline_large_monitor = MemoryMonitor()\n",
    "baseline_large_monitor.log_memory(\"Baseline large model initialized\")\n",
    "\n",
    "try:\n",
    "    # Test forward pass\n",
    "    baseline_large.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        out = baseline_large(large_graph.x.to(device), large_graph.edge_index.to(device))\n",
    "        baseline_time = time.time() - start_time\n",
    "        baseline_large_monitor.log_memory(\"After baseline large forward pass\")\n",
    "    \n",
    "    print(f\"Baseline GCN: {baseline_time:.3f}s, Peak Memory: {baseline_large_monitor.get_peak_memory():.1f} MB\")\n",
    "    baseline_large_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Baseline GCN failed: {e}\")\n",
    "    baseline_large_success = False\n",
    "\n",
    "# Test FastGCN on large graph\n",
    "print(\"\\n--- Testing FastGCN ---\")\n",
    "fastgcn_large = FastGCNModel(\n",
    "    input_dim=large_graph.x.size(1),\n",
    "    hidden_dim=32,\n",
    "    output_dim=10,\n",
    "    num_layers=2,\n",
    "    sample_sizes=[100, 50]  # Small samples for efficiency\n",
    ").to(device)\n",
    "\n",
    "fastgcn_large_monitor = MemoryMonitor()\n",
    "fastgcn_large_monitor.log_memory(\"FastGCN large model initialized\")\n",
    "\n",
    "try:\n",
    "    # Test forward pass\n",
    "    fastgcn_large.eval()\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        out = fastgcn_large(large_graph.x.to(device), large_graph.edge_index.to(device))\n",
    "        fastgcn_time = time.time() - start_time\n",
    "        fastgcn_large_monitor.log_memory(\"After FastGCN large forward pass\")\n",
    "    \n",
    "    print(f\"FastGCN: {fastgcn_time:.3f}s, Peak Memory: {fastgcn_large_monitor.get_peak_memory():.1f} MB\")\n",
    "    fastgcn_large_success = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FastGCN failed: {e}\")\n",
    "    fastgcn_large_success = False\n",
    "\n",
    "# Test GraphSAINT on large graph (if we have a working sampler)\n",
    "if saint_loaders:\n",
    "    print(\"\\n--- Testing GraphSAINT ---\")\n",
    "    \n",
    "    # Setup GraphSAINT for large graph\n",
    "    try:\n",
    "        large_saint_loaders = setup_graphsaint_loaders(\n",
    "            large_graph, \n",
    "            batch_size=128,  # Smaller batch for large graph\n",
    "            sample_coverage=5\n",
    "        )\n",
    "        \n",
    "        if large_saint_loaders:\n",
    "            sampler_name = list(large_saint_loaders.keys())[0]\n",
    "            loader = large_saint_loaders[sampler_name]\n",
    "            \n",
    "            saint_large = GraphSAINTModel(\n",
    "                input_dim=large_graph.x.size(1),\n",
    "                hidden_dim=32,\n",
    "                output_dim=10,\n",
    "                num_layers=2\n",
    "            ).to(device)\n",
    "            \n",
    "            saint_large_monitor = MemoryMonitor()\n",
    "            saint_large_monitor.log_memory(\"GraphSAINT large model initialized\")\n",
    "            \n",
    "            # Test one batch\n",
    "            saint_large.eval()\n",
    "            batch = next(iter(loader))\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                out = saint_large(batch.x, batch.edge_index)\n",
    "                saint_time = time.time() - start_time\n",
    "                saint_large_monitor.log_memory(\"After GraphSAINT large forward pass\")\n",
    "            \n",
    "            print(f\"GraphSAINT ({sampler_name}): {saint_time:.3f}s per batch, \"\n",
    "                  f\"Peak Memory: {saint_large_monitor.get_peak_memory():.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"GraphSAINT setup failed: {e}\")\n",
    "\n",
    "# Performance summary for scalability\n",
    "print(\"\\n=== Scalability Summary ===\")\n",
    "print(\"Memory-enhanced approaches show benefits on larger graphs:\")\n",
    "print(\"• FastGCN: Reduces memory through importance sampling\")\n",
    "print(\"• GraphSAINT: Enables mini-batch training on subgraphs\")\n",
    "print(\"• Both approaches trade some accuracy for significant memory savings\")\n",
    "print(\"• Essential for graphs with millions of nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. CPU Optimization Guidelines for Memory-Enhanced GNNs\n\nLet's provide comprehensive optimization guidelines specifically for running memory-enhanced GNNs on CPU architectures like the MacBook Air M2.\n\n### Mathematical Framework for CPU Optimization\n\n**CPU-Specific Performance Model:**\n\n1. **Memory Bandwidth Limitation:**\n   $$T_{\\text{memory}} = \\frac{\\text{Data Size}}{\\text{Bandwidth}} = \\frac{n \\cdot d \\cdot \\text{sizeof}(\\text{float})}{B_{\\text{mem}}}$$\n\n2. **Cache Efficiency:**\n   $$\\text{Cache Hit Ratio} = \\frac{\\text{Data in Cache}}{\\text{Total Data Access}}$$\n   \n   Optimal performance when working set $< $ L3 cache size.\n\n3. **Vectorization Benefits:**\n   $$\\text{Speedup}_{\\text{SIMD}} = \\min\\left(\\frac{\\text{Vector Width}}{\\text{Element Size}}, \\text{Data Parallelism}\\right)$$\n\n**Memory-Compute Trade-offs:**\n\n1. **Batch Size Optimization:**\n   $$B_{\\text{optimal}} = \\arg\\min_B \\left(T_{\\text{compute}}(B) + T_{\\text{memory}}(B)\\right)$$\n   \n   where:\n   - $T_{\\text{compute}}(B) = \\frac{W}{B \\cdot P}$ (work per core)\n   - $T_{\\text{memory}}(B) = \\frac{B \\cdot S}{B_{\\text{mem}}}$ (memory transfer time)\n\n2. **Sample Size vs. Accuracy:**\n   $$\\text{Accuracy}(K) = A_{\\max} \\left(1 - e^{-\\alpha K}\\right)$$\n   \n   Diminishing returns beyond optimal $K^*$.\n\n**CPU Architecture Considerations:**\n\n1. **Unified Memory Architecture (M2):**\n   - Memory sharing between CPU and GPU\n   - No data transfer overhead\n   - Total memory constraint: $M_{\\text{CPU}} + M_{\\text{GPU}} \\leq M_{\\text{total}}$\n\n2. **Thermal Throttling Model:**\n   $$f_{\\text{CPU}}(t) = f_{\\max} \\cdot \\min\\left(1, \\frac{T_{\\max} - T(t)}{T_{\\max} - T_{\\text{ambient}}}\\right)$$\n   \n   where $T(t)$ is temperature as function of time.\n\n**Optimization Strategies:**\n\n1. **Memory Access Patterns:**\n   - Sequential access: $O(n)$ cache misses\n   - Random access: $O(n \\log n)$ cache misses\n   - Optimize for spatial locality\n\n2. **Computational Intensity:**\n   $$I = \\frac{\\text{Operations}}{\\text{Bytes Transferred}}$$\n   \n   Higher intensity → better CPU utilization\n\n3. **Work Distribution:**\n   $$T_{\\text{parallel}} = \\frac{W}{P} + T_{\\text{synchronization}}$$\n   \n   where $P$ is number of cores, $W$ is total work.\n\n**Platform-Specific Optimizations:**\n\n**For M2 MacBook Air:**\n- 8 cores (4 performance + 4 efficiency)\n- 24GB unified memory\n- 100GB/s memory bandwidth\n- No active cooling\n\n**Optimization Guidelines:**\n1. **Thread Allocation:** Use 4-6 threads for optimal performance\n2. **Memory Usage:** Keep < 16GB for system stability\n3. **Thermal Management:** Use smaller batches for sustained performance\n4. **Vectorization:** Leverage ARM NEON instructions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Optimization Guidelines for Memory-Enhanced GNNs\n",
    "print(\"=== CPU Optimization Guidelines for Memory-Enhanced GNNs ===\")\n",
    "\n",
    "print(\"\\n1. GraphSAINT Optimizations:\")\n",
    "print(\"   • Use smaller batch sizes (128-512) to fit in CPU memory\")\n",
    "print(\"   • Reduce sample_coverage (5-20) for faster sampling\")\n",
    "print(\"   • Choose node sampling over edge sampling for better CPU performance\")\n",
    "print(\"   • Limit walk_length (2-4) in random walk sampling\")\n",
    "print(\"   • Use fewer GCN layers (2-3) to reduce computation\")\n",
    "\n",
    "print(\"\\n2. FastGCN Optimizations:\")\n",
    "print(\"   • Use small sample sizes (50-200) per layer\")\n",
    "print(\"   • Implement decreasing sample sizes by layer\")\n",
    "print(\"   • Cache importance scores when possible\")\n",
    "print(\"   • Use sparse tensors for large graphs\")\n",
    "print(\"   • Consider adaptive sampling based on node degree\")\n",
    "\n",
    "print(\"\\n3. Memory Management:\")\n",
    "print(\"   • Monitor peak memory usage during training\")\n",
    "print(\"   • Use gradient accumulation for effective larger batch sizes\")\n",
    "print(\"   • Clear intermediate tensors explicitly\")\n",
    "print(\"   • Use torch.no_grad() for validation/testing\")\n",
    "print(\"   • Consider mixed precision training if supported\")\n",
    "\n",
    "print(\"\\n4. Training Strategies:\")\n",
    "print(\"   • Use learning rate scheduling for better convergence\")\n",
    "print(\"   • Implement early stopping to prevent overfitting\")\n",
    "print(\"   • Use smaller learning rates (0.001-0.01) for stability\")\n",
    "print(\"   • Apply gradient clipping for numerical stability\")\n",
    "print(\"   • Cache embeddings for repeated evaluations\")\n",
    "\n",
    "print(\"\\n5. Hardware-Specific Tips for M2 MacBook Air:\")\n",
    "print(f\"   • Set torch.set_num_threads({torch.get_num_threads()}) for optimal CPU usage\")\n",
    "print(\"   • Use unified memory architecture efficiently\")\n",
    "print(\"   • Monitor thermal throttling during long training\")\n",
    "print(\"   • Consider training in smaller epochs with checkpoints\")\n",
    "print(\"   • Use vectorized operations over loops when possible\")\n",
    "\n",
    "# Demonstrate memory-efficient evaluation\n",
    "def memory_efficient_evaluation(model, data, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Evaluate large graphs in batches to manage memory\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    if data.num_nodes <= batch_size:\n",
    "        # Small graph - evaluate normally\n",
    "        with torch.no_grad():\n",
    "            return model(data.x.to(device), data.edge_index.to(device))\n",
    "    \n",
    "    # Large graph - batch processing (simplified version)\n",
    "    print(f\"Using batched evaluation for {data.num_nodes} nodes\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # For demonstration - in practice, need more sophisticated batching\n",
    "        # that considers graph connectivity\n",
    "        return model(data.x.to(device), data.edge_index.to(device))\n",
    "\n",
    "print(\"\\n=== Performance Recommendations ===\")\n",
    "print(\"\\nFor small graphs (< 10K nodes):\")\n",
    "print(\"• Use standard GCN for simplicity\")\n",
    "print(\"• Focus on model architecture optimization\")\n",
    "\n",
    "print(\"\\nFor medium graphs (10K - 100K nodes):\")\n",
    "print(\"• Consider FastGCN with moderate sampling\")\n",
    "print(\"• Use GraphSAINT with node sampling\")\n",
    "print(\"• Implement gradient accumulation\")\n",
    "\n",
    "print(\"\\nFor large graphs (> 100K nodes):\")\n",
    "print(\"• GraphSAINT is essential for feasible training\")\n",
    "print(\"• Use aggressive sampling strategies\")\n",
    "print(\"• Consider distributed training if available\")\n",
    "\n",
    "print(f\"\\n=== Current System Status ===\")\n",
    "process = psutil.Process(os.getpid())\n",
    "print(f\"Current memory usage: {process.memory_info().rss / 1024 / 1024:.1f} MB\")\n",
    "print(f\"PyTorch threads: {torch.get_num_threads()}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we explored memory-enhanced Graph Neural Networks designed for scalability with comprehensive mathematical foundations:\n\n### **GraphSAINT (Graph Sampling and Aggregation)**:\n**Mathematical Foundation:**\n- **Sampling Framework:** $\\mathcal{L}_{\\text{saint}} = \\sum_{s \\in \\mathcal{S}} \\frac{|\\mathcal{V}|}{|\\mathcal{V}_s|} \\mathcal{L}_s(\\theta)$\n- **Unbiased Estimation:** $\\mathbb{E}[\\hat{\\mathcal{L}}_{\\text{saint}}] = \\mathcal{L}_{\\text{full}}$\n- **Memory Reduction:** $M_{\\text{saint}} = O(L \\times |\\mathcal{V}_s| \\times D) \\ll M_{\\text{full}}$\n- **Convergence Rate:** $\\mathbb{E}[\\hat{\\theta}_T - \\theta^*] = O(1/\\sqrt{T})$\n\n**Key Benefits:**\n- Enables training on large graphs through subgraph sampling\n- Maintains convergence guarantees with proper normalization\n- Flexible sampling strategies (node, edge, random walk)\n\n### **FastGCN (Fast Graph Convolutional Networks)**:\n**Mathematical Foundation:**\n- **Layer-wise Sampling:** $\\mathbf{H}^{(\\ell+1)} = \\sigma\\left(\\frac{1}{K^{(\\ell)}} \\sum_{v \\in \\mathcal{S}^{(\\ell)}} \\frac{\\mathbf{A}_{:,v}}{q_v^{(\\ell)}} \\mathbf{H}_v^{(\\ell)} \\mathbf{W}^{(\\ell)}\\right)$\n- **Importance Sampling:** $q_v^{(\\ell)} = \\frac{d_v}{2|\\mathcal{E}|}$ (degree-based)\n- **Complexity Reduction:** $\\mathcal{C}_{\\text{FastGCN}} = O(\\sum_{\\ell} K^{(\\ell)} \\times d \\times D)$\n- **Variance Control:** $\\text{Var}[\\hat{\\mathbf{H}}^{(\\ell+1)}] = O(1/K^{(\\ell)})$\n\n**Key Benefits:**\n- Reduces complexity from exponential to linear in layers\n- Maintains performance with controlled variance\n- Optimal for medium-scale graphs with uniform structure\n\n### **Performance Trade-offs**:\n**Mathematical Analysis:**\n- **Accuracy-Memory Trade-off:** $\\eta_{\\text{memory}} = \\frac{M_{\\text{enhanced}}}{M_{\\text{full}}} = \\frac{K}{|\\mathcal{V}|} \\to 0$\n- **Efficiency Score:** $E = R_{\\text{acc}} \\times \\sqrt{R_{\\text{mem}} \\times R_{\\text{speed}}}$\n- **Scalability Factor:** $\\frac{M_{\\text{full}}}{M_{\\text{enhanced}}} = O(n/k)$ for graph size $n$\n\n### **When to Use Each Approach**:\n**Decision Framework:**\n1. **Standard GCN:** Small graphs $(< 10K \\text{ nodes})$, $M_{\\text{available}} \\gg M_{\\text{required}}$\n2. **FastGCN:** Medium graphs $(10K-100K \\text{ nodes})$, uniform degree distribution\n3. **GraphSAINT:** Large graphs $(> 100K \\text{ nodes})$, severe memory constraints\n4. **Hybrid:** Combine techniques for maximum scalability\n\n### **CPU Optimization Mathematical Framework**:\n**Performance Model:**\n- **Memory Bandwidth:** $T_{\\text{memory}} = \\frac{n \\cdot d \\cdot 4}{B_{\\text{mem}}}$ bytes\n- **Optimal Batch Size:** $B_{\\text{optimal}} = \\arg\\min_B (T_{\\text{compute}}(B) + T_{\\text{memory}}(B))$\n- **Thermal Model:** $f_{\\text{CPU}}(t) = f_{\\max} \\cdot \\min(1, \\frac{T_{\\max} - T(t)}{T_{\\max} - T_{\\text{ambient}}})$\n\n### **Key Mathematical Insights**:\n\n1. **Sampling Theory:** Proper importance weighting ensures unbiased gradient estimation\n2. **Variance-Bias Trade-off:** Smaller samples increase variance but reduce computational cost\n3. **Convergence Guarantees:** Both methods maintain $O(1/\\sqrt{T})$ convergence under standard assumptions\n4. **Scalability Laws:** Memory efficiency improves as $O(n/k)$ where $n$ is graph size and $k$ is sample size\n\n### **Practical Guidelines**:\n- **Memory-Constrained:** Use GraphSAINT with adaptive sampling strategies\n- **Time-Constrained:** Use FastGCN with layer-wise sample size optimization  \n- **Accuracy-Critical:** Increase sampling budget or use ensemble methods\n- **CPU-Optimized:** Balance batch size, thread count, and thermal management\n\nThese memory-enhanced techniques make GNNs practical for real-world large-scale applications, enabling deployment on resource-constrained environments while maintaining theoretical guarantees and reasonable performance. The mathematical foundations provide principled approaches to navigate the fundamental trade-offs between accuracy, memory efficiency, and computational speed."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}