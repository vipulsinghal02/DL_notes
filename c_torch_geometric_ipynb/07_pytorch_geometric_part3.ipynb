{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# PyTorch Geometric Part 3: Graph Transformers\n\nThis notebook explores Graph Transformers with comprehensive mathematical exposition, adapting the powerful Transformer architecture to work with graph-structured data. We'll implement GraphiT (Graph Transformer) with detailed mathematical foundations for attention mechanisms, positional encodings, and graph-specific adaptations.\n\n## Mathematical Foundation of Graph Transformers\n\n### Core Concept: Self-Attention on Graphs\n\nGraph Transformers extend the transformer paradigm to irregular graph structures by enabling **global attention** between all node pairs while incorporating **structural inductive biases**:\n\n$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n\n### Key Mathematical Challenges and Solutions\n\n**1. Lack of Natural Order:**\nUnlike sequences, graphs have no inherent ordering. Solution: **Positional Encodings**\n$$\\mathbf{h}_v^{(0)} = \\mathbf{x}_v + \\mathbf{PE}(v)$$\n\n**2. Structural Awareness:**\nPure attention ignores graph topology. Solution: **Structure-aware Attention**\n$$\\text{Attention}_{ij} = \\text{softmax}\\left(\\frac{\\mathbf{q}_i^T \\mathbf{k}_j + \\mathbf{b}_{ij}}{\\sqrt{d_k}}\\right)$$\n\nwhere $\\mathbf{b}_{ij}$ encodes structural relationship between nodes $i$ and $j$.\n\n**3. Computational Complexity:**\nFull attention is $O(N^2)$ in graph size. Solution: **Sparse Attention** patterns.\n\n### Mathematical Advantages\n\n**Global Receptive Field:**\n$$\\text{Receptive Field} = \\text{All nodes at layer 1}$$\n\n**Long-range Dependencies:**\n$$\\text{Path Length} = 1 \\text{ hop for any node pair}$$\n\n**Permutation Equivariance:**\n$$f(\\pi(\\mathbf{X}), \\pi(\\mathbf{A})\\pi^T) = \\pi(f(\\mathbf{X}, \\mathbf{A}))$$\n\nGraph Transformers combine the representational power of attention mechanisms with graph-specific structural awareness, enabling powerful modeling of complex node relationships and global graph patterns."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (uncomment if needed)\n",
    "# !pip install torch torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.utils import to_dense_adj, degree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Positional Encoding for Graphs\n\n### Mathematical Foundation of Graph Positional Encodings\n\nUnlike sequences with natural positional order, graphs require **structural positional encodings** that capture topological information:\n\n**Challenge:** No canonical node ordering in graphs  \n**Solution:** Encode structural properties as positional information\n\n### Laplacian Positional Encoding\n\n**Graph Laplacian Matrix:**\n$$\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$$\n\n**Normalized Laplacian:**\n$$\\mathcal{L} = \\mathbf{D}^{-1/2}\\mathbf{L}\\mathbf{D}^{-1/2} = \\mathbf{I} - \\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}$$\n\n**Eigendecomposition:**\n$$\\mathcal{L} = \\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{U}^T$$\n\nwhere $\\mathbf{U} = [\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_N]$ are eigenvectors and $\\boldsymbol{\\Lambda} = \\text{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_N)$ are eigenvalues.\n\n**Positional Encoding:**\n$$\\mathbf{PE}_{\\text{Laplacian}} = \\mathbf{U}[:, 1:k+1] \\in \\mathbb{R}^{N \\times k}$$\n\n**Mathematical Properties:**\n- $\\lambda_1 = 0$ (connected graphs): Skip first eigenvector (constant)\n- $\\lambda_i \\leq 2$ for normalized Laplacian\n- Eigenvectors capture graph harmonics and structural patterns\n- Lower eigenvalues → global structure, higher → local structure\n\n### Degree-based Positional Encoding\n\n**Node Degree:** $d_v = \\sum_{u \\in \\mathcal{V}} A_{vu}$\n\n**Sinusoidal Encoding:**\n$$\\mathbf{PE}_{\\text{degree}}[v, 2i] = \\sin\\left(\\frac{d_v}{10000^{2i/d_{\\text{model}}}}\\right)$$\n$$\\mathbf{PE}_{\\text{degree}}[v, 2i+1] = \\cos\\left(\\frac{d_v}{10000^{2i/d_{\\text{model}}}}\\right)$$\n\n**Properties:**\n- Simple and computationally efficient: $O(|\\mathcal{E}|)$\n- Captures local connectivity information\n- Differentiable and learnable\n\n### Learned Positional Encoding\n\n**Embedding Matrix:**\n$$\\mathbf{PE}_{\\text{learned}} = \\mathbf{E}_{\\text{pos}} \\in \\mathbb{R}^{N_{\\max} \\times d_{\\text{model}}}$$\n\n**Advantages:** Adaptive to task-specific structural patterns  \n**Disadvantages:** Requires fixed maximum graph size, less interpretable\n\n### Theoretical Justification\n\n**Spectral Graph Theory Connection:**\nLaplacian eigenvectors provide optimal low-dimensional representation preserving graph structure:\n\n$$\\min_{\\mathbf{Y}} \\text{tr}(\\mathbf{Y}^T\\mathcal{L}\\mathbf{Y}) \\text{ s.t. } \\mathbf{Y}^T\\mathbf{Y} = \\mathbf{I}$$\n\nSolution: $\\mathbf{Y} = \\mathbf{U}[:, 1:k]$ (first $k$ non-trivial eigenvectors)\n\n**Expressive Power:**\nDifferent positional encodings capture different aspects:\n- **Laplacian**: Global graph structure and communities\n- **Degree**: Local connectivity patterns  \n- **Random Walk**: Diffusion and reachability information"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Various positional encoding methods for graphs\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=1000, pe_type='laplacian'):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pe_type = pe_type\n",
    "        \n",
    "        if pe_type == 'learned':\n",
    "            self.pe = nn.Embedding(max_len, d_model)\n",
    "    \n",
    "    def laplacian_pe(self, edge_index, num_nodes, k=8):\n",
    "        \"\"\"\n",
    "        Laplacian positional encoding using eigenvectors\n",
    "        \"\"\"\n",
    "        # Convert to dense adjacency matrix\n",
    "        adj = to_dense_adj(edge_index, max_num_nodes=num_nodes)[0]\n",
    "        \n",
    "        # Compute degree matrix\n",
    "        deg = torch.sum(adj, dim=1)\n",
    "        deg_inv_sqrt = torch.diag(torch.pow(deg + 1e-8, -0.5))\n",
    "        \n",
    "        # Normalized Laplacian\n",
    "        laplacian = torch.eye(num_nodes) - deg_inv_sqrt @ adj @ deg_inv_sqrt\n",
    "        \n",
    "        # Compute eigenvectors (use only first k)\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(laplacian)\n",
    "        pe = eigenvectors[:, :min(k, self.d_model)]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if pe.size(1) < self.d_model:\n",
    "            pe = F.pad(pe, (0, self.d_model - pe.size(1)))\n",
    "        \n",
    "        return pe\n",
    "    \n",
    "    def degree_pe(self, edge_index, num_nodes):\n",
    "        \"\"\"\n",
    "        Simple degree-based positional encoding\n",
    "        \"\"\"\n",
    "        deg = degree(edge_index[0], num_nodes=num_nodes)\n",
    "        \n",
    "        # Create sinusoidal encoding based on degree\n",
    "        pe = torch.zeros(num_nodes, self.d_model)\n",
    "        \n",
    "        for i in range(self.d_model):\n",
    "            if i % 2 == 0:\n",
    "                pe[:, i] = torch.sin(deg / (10000 ** (i / self.d_model)))\n",
    "            else:\n",
    "                pe[:, i] = torch.cos(deg / (10000 ** (i / self.d_model)))\n",
    "        \n",
    "        return pe\n",
    "    \n",
    "    def forward(self, edge_index, num_nodes, batch_idx=None):\n",
    "        if self.pe_type == 'laplacian':\n",
    "            return self.laplacian_pe(edge_index, num_nodes)\n",
    "        elif self.pe_type == 'degree':\n",
    "            return self.degree_pe(edge_index, num_nodes)\n",
    "        elif self.pe_type == 'learned':\n",
    "            if batch_idx is None:\n",
    "                batch_idx = torch.arange(num_nodes)\n",
    "            return self.pe(batch_idx)\n",
    "        else:\n",
    "            return torch.zeros(num_nodes, self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Graph Transformer Layer\n\n### Mathematical Formulation of Graph-Aware Attention\n\nThe Graph Transformer layer adapts multi-head self-attention to incorporate graph structure while maintaining global connectivity:\n\n**Standard Multi-Head Attention:**\n$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O$$\n\nwhere each attention head is:\n$$\\text{head}_i = \\text{Attention}(\\mathbf{Q}\\mathbf{W}_i^Q, \\mathbf{K}\\mathbf{W}_i^K, \\mathbf{V}\\mathbf{W}_i^V)$$\n\n**Scaled Dot-Product Attention:**\n$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n\n### Graph-Specific Attention Mechanisms\n\n**1. Attention Masking:**\n$$\\text{Attention}_{ij} = \\begin{cases}\n\\frac{\\exp(\\mathbf{q}_i^T \\mathbf{k}_j / \\sqrt{d_k})}{\\sum_{k \\in \\mathcal{N}(i) \\cup \\{i\\}} \\exp(\\mathbf{q}_i^T \\mathbf{k}_k / \\sqrt{d_k})} & \\text{if } (i,j) \\in \\mathcal{E} \\text{ or } i = j \\\\\n0 & \\text{otherwise}\n\\end{cases}$$\n\n**2. Structural Bias:**\n$$e_{ij} = \\frac{\\mathbf{q}_i^T \\mathbf{k}_j}{\\sqrt{d_k}} + \\mathbf{b}_{ij}$$\n\nwhere $\\mathbf{b}_{ij}$ can be:\n- Edge features: $\\mathbf{b}_{ij} = \\mathbf{W}_b \\mathbf{e}_{ij}$\n- Distance encoding: $\\mathbf{b}_{ij} = f(d_{\\mathcal{G}}(i,j))$\n- Learned relative positions: $\\mathbf{b}_{ij} = \\mathbf{W}_r(\\mathbf{PE}_i - \\mathbf{PE}_j)$\n\n**3. Complete Graph Transformer Update:**\n$$\\mathbf{h}_i^{(l+1)} = \\text{LayerNorm}\\left(\\mathbf{h}_i^{(l)} + \\text{MultiHead}(\\mathbf{H}^{(l)})\\right)$$\n$$\\mathbf{h}_i^{(l+1)} = \\text{LayerNorm}\\left(\\mathbf{h}_i^{(l+1)} + \\text{FFN}(\\mathbf{h}_i^{(l+1)})\\right)$$\n\n**Feed-Forward Network:**\n$$\\text{FFN}(\\mathbf{x}) = \\text{max}(0, \\mathbf{x}\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2$$\n\n### Mathematical Properties\n\n**1. Computational Complexity:**\n- **Standard Attention**: $O(N^2 d)$ for $N$ nodes\n- **Sparse Attention**: $O(|\\mathcal{E}| d)$ with masking\n- **Memory**: $O(N^2)$ for attention matrix\n\n**2. Receptive Field:**\n- **Layer 1**: Global (all nodes can attend to each other)\n- **Expressive Power**: Can capture arbitrary long-range dependencies\n\n**3. Permutation Equivariance:**\n$$\\text{GraphTransformer}(\\pi(\\mathbf{X}), \\pi(\\mathbf{A})\\pi^T) = \\pi(\\text{GraphTransformer}(\\mathbf{X}, \\mathbf{A}))$$\n\nfor any permutation matrix $\\pi$.\n\n**4. Universal Approximation (Theoretical):**\nGraph Transformers with sufficient depth and width can approximate any permutation-equivariant function on graphs.\n\n### Advantages over Traditional GNNs\n\n**1. No Over-smoothing:**\nTraditional GNNs suffer from over-smoothing: $\\mathbf{h}_v^{(l)} \\rightarrow \\mathbf{c}$ as $l \\rightarrow \\infty$  \nGraph Transformers maintain distinct representations through self-attention.\n\n**2. Global Information Flow:**\nDirect connections between all node pairs enable efficient global information propagation.\n\n**3. Interpretability:**\nAttention weights $\\alpha_{ij}$ provide interpretable measure of node importance for each prediction.\n\nThe mathematical framework ensures that Graph Transformers can capture both local graph structure through positional encodings and global patterns through unrestricted attention mechanisms."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Transformer layer with multi-head attention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass with optional attention masking\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection\n",
    "        attn_out, attention_weights = self.attention(\n",
    "            x, x, x, \n",
    "            attn_mask=attention_mask,\n",
    "            need_weights=True\n",
    "        )\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Complete Graph Transformer Architecture\n\n### Mathematical Framework for Full Graph Transformer (GraphiT)\n\nThe complete Graph Transformer integrates multiple components into a unified architecture for graph learning:\n\n**Overall Architecture:**\n$$\\mathbf{H}^{(0)} = \\mathbf{W}_{\\text{input}}\\mathbf{X} + \\mathbf{PE}$$\n$$\\mathbf{H}^{(l+1)} = \\text{GraphTransformerLayer}^{(l)}(\\mathbf{H}^{(l)})$$\n$$\\mathbf{y} = \\text{Classifier}(\\text{Pooling}(\\mathbf{H}^{(L)}))$$\n\n### Component-wise Mathematical Analysis\n\n**1. Input Embedding:**\n$$\\mathbf{h}_v^{(0)} = \\mathbf{W}_{\\text{input}} \\mathbf{x}_v + \\mathbf{PE}(v)$$\n\nwhere:\n- $\\mathbf{W}_{\\text{input}} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{features}}}$: Input projection matrix\n- $\\mathbf{PE}(v) \\in \\mathbb{R}^{d_{\\text{model}}}$: Positional encoding for node $v$\n\n**2. Transformer Stack:**\n$$\\mathbf{H}^{(l+1)} = \\text{TransformerLayer}(\\mathbf{H}^{(l)}, \\mathbf{A})$$\n\nEach layer applies:\n$$\\mathbf{H}^{(l+1)} = \\text{LayerNorm}(\\mathbf{H}^{(l)} + \\text{MultiHeadAttention}(\\mathbf{H}^{(l)}))$$\n$$\\mathbf{H}^{(l+1)} = \\text{LayerNorm}(\\mathbf{H}^{(l+1)} + \\text{FFN}(\\mathbf{H}^{(l+1)}))$$\n\n**3. Graph-Level Representation (for graph classification):**\n\n**Mean Pooling:**\n$$\\mathbf{h}_{\\mathcal{G}} = \\frac{1}{N} \\sum_{v=1}^N \\mathbf{h}_v^{(L)}$$\n\n**Max Pooling:**\n$$\\mathbf{h}_{\\mathcal{G}} = \\max_{v=1}^N \\mathbf{h}_v^{(L)}$$\n\n**Attention Pooling:**\n$$\\alpha_v = \\text{softmax}(\\mathbf{w}_{\\text{att}}^T \\mathbf{h}_v^{(L)})$$\n$$\\mathbf{h}_{\\mathcal{G}} = \\sum_{v=1}^N \\alpha_v \\mathbf{h}_v^{(L)}$$\n\n**4. Output Classification:**\n$$\\mathbf{y} = \\text{softmax}(\\mathbf{W}_{\\text{out}} \\mathbf{h}_{\\mathcal{G}} + \\mathbf{b}_{\\text{out}})$$\n\n### Attention Masking Strategies\n\n**1. No Masking (Full Attention):**\n$$\\text{Mask}_{ij} = 0 \\quad \\forall i,j$$\n\nAllows all nodes to attend to each other - captures global patterns but computationally expensive.\n\n**2. Adjacency-based Masking:**\n$$\\text{Mask}_{ij} = \\begin{cases}\n0 & \\text{if } A_{ij} = 1 \\text{ or } i = j \\\\\n-\\infty & \\text{otherwise}\n\\end{cases}$$\n\nRestricts attention to graph neighbors - more efficient, preserves local structure.\n\n**3. Distance-based Masking:**\n$$\\text{Mask}_{ij} = \\begin{cases}\n0 & \\text{if } d_{\\mathcal{G}}(i,j) \\leq k \\\\\n-\\infty & \\text{otherwise}\n\\end{cases}$$\n\nAllows attention within $k$-hop neighborhoods.\n\n### Mathematical Properties and Analysis\n\n**1. Parameter Count:**\n$$|\\theta| = d_{\\text{input}} \\times d_{\\text{model}} + L \\times (4d_{\\text{model}}^2 + 2d_{\\text{model}} \\times d_{\\text{ff}}) + d_{\\text{model}} \\times C$$\n\nwhere $L$ is number of layers, $d_{\\text{ff}}$ is feed-forward dimension, $C$ is number of classes.\n\n**2. Computational Complexity per Forward Pass:**\n- **Input Projection**: $O(N \\times d_{\\text{input}} \\times d_{\\text{model}})$\n- **Positional Encoding**: $O(N \\times d_{\\text{model}})$ or $O(N^3)$ for Laplacian\n- **Attention Layers**: $O(L \\times N^2 \\times d_{\\text{model}})$ (full) or $O(L \\times |\\mathcal{E}| \\times d_{\\text{model}})$ (sparse)\n- **Output**: $O(d_{\\text{model}} \\times C)$\n\n**3. Memory Complexity:**\n- **Node Features**: $O(N \\times d_{\\text{model}} \\times L)$\n- **Attention Matrices**: $O(h \\times N^2)$ where $h$ is number of heads\n- **Gradients**: $O(|\\theta|)$\n\n**4. Expressiveness:**\nGraph Transformers can express functions that traditional MPNNs cannot:\n- **Global Graph Properties**: Graph diameter, connectivity\n- **Long-range Correlations**: Node similarities across distant parts\n- **Higher-order Structures**: Triangles, motifs, community structures\n\n### Task-Specific Adaptations\n\n**Node Classification:**\n$$p(y_v | \\mathcal{G}) = \\text{softmax}(\\mathbf{W}_{\\text{node}} \\mathbf{h}_v^{(L)} + \\mathbf{b}_{\\text{node}})$$\n\n**Graph Classification:**\n$$p(y_{\\mathcal{G}} | \\mathcal{G}) = \\text{softmax}(\\mathbf{W}_{\\text{graph}} \\text{Pool}(\\mathbf{H}^{(L)}) + \\mathbf{b}_{\\text{graph}})$$\n\n**Link Prediction:**\n$$p(e_{ij} | \\mathcal{G}) = \\sigma(\\mathbf{h}_i^{(L)T} \\mathbf{W}_{\\text{link}} \\mathbf{h}_j^{(L)})$$\n\nThis architectural framework provides the mathematical foundation for powerful and flexible graph learning while maintaining interpretability through attention mechanisms."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Graph Transformer (GraphiT) implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model, n_heads, n_layers, \n",
    "                 num_classes, dim_feedforward=512, dropout=0.1, \n",
    "                 pe_type='degree', max_nodes=1000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.max_nodes = max_nodes\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pe = GraphPositionalEncoding(d_model, max_nodes, pe_type)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            GraphTransformerLayer(d_model, n_heads, dim_feedforward, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output head for node classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Graph-level pooling for graph classification\n",
    "        self.graph_pooling = 'mean'  # 'mean', 'max', or 'attention'\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def create_attention_mask(self, edge_index, num_nodes, mask_type='none'):\n",
    "        \"\"\"\n",
    "        Create attention mask based on graph structure\n",
    "        \"\"\"\n",
    "        if mask_type == 'none':\n",
    "            return None\n",
    "        elif mask_type == 'adjacency':\n",
    "            # Only allow attention between connected nodes\n",
    "            adj = to_dense_adj(edge_index, max_num_nodes=num_nodes)[0]\n",
    "            # Add self-loops\n",
    "            adj = adj + torch.eye(num_nodes)\n",
    "            # Convert to attention mask (0 = attend, -inf = don't attend)\n",
    "            mask = torch.where(adj == 0, float('-inf'), 0.0)\n",
    "            return mask\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def forward(self, x, edge_index, batch=None, return_attention=False):\n",
    "        num_nodes = x.size(0)\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pe = self.pe(edge_index, num_nodes)\n",
    "        x = x + pe\n",
    "        \n",
    "        # Reshape for transformer (batch_size=1, seq_len=num_nodes, d_model)\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # Create attention mask (optional)\n",
    "        attention_mask = self.create_attention_mask(edge_index, num_nodes, 'none')\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        attention_weights_list = []\n",
    "        for layer in self.transformer_layers:\n",
    "            x, attention_weights = layer(x, attention_mask)\n",
    "            if return_attention:\n",
    "                attention_weights_list.append(attention_weights)\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        # For graph classification, pool node features\n",
    "        if batch is not None:\n",
    "            if self.graph_pooling == 'mean':\n",
    "                x = global_mean_pool(x, batch)\n",
    "            elif self.graph_pooling == 'max':\n",
    "                x = global_max_pool(x, batch)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.classifier(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            return out, attention_weights_list\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Lightweight Graph Transformer for CPU\n\n### Mathematical Optimization for Resource-Constrained Environments\n\nThe lightweight Graph Transformer addresses computational constraints while preserving essential transformer capabilities:\n\n**Design Principles:**\n1. **Reduced Model Dimensions**: $d_{\\text{model}} = 64$ vs. typical $d_{\\text{model}} = 512$\n2. **Fewer Attention Heads**: $h = 4$ vs. typical $h = 8-16$  \n3. **Head Averaging**: Use $\\text{concat} = \\text{False}$ to average instead of concatenating heads\n4. **PyG Integration**: Leverage optimized `TransformerConv` operations\n\n**Mathematical Complexity Reduction:**\n\n**Standard Transformer:**\n$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O$$\n\nOutput dimension: $h \\times d_k$ then projected to $d_{\\text{model}}$\n\n**Lightweight Version:**\n$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\frac{1}{h}\\sum_{i=1}^h \\text{head}_i$$\n\nOutput dimension: $d_k = d_{\\text{model}} / h$ directly\n\n**Parameter Reduction:**\n- **Standard**: $4d_{\\text{model}}^2 + d_{\\text{model}} \\times (h \\times d_k)$ per layer\n- **Lightweight**: $4d_{\\text{model}}^2$ per layer (no projection matrix)\n- **Savings**: $\\approx 25\\%$ reduction in transformer parameters\n\n**Memory Complexity:**\n- **Attention Matrix**: $O(h \\times N^2)$ → Shared across heads in PyG implementation\n- **Intermediate Activations**: $O(N \\times d_{\\text{model}})$ vs. $O(N \\times h \\times d_k)$\n\n**Computational Complexity per Layer:**\n- **Attention**: $O(N^2 d_{\\text{model}})$ (same complexity, but smaller constants)\n- **FFN**: $O(N d_{\\text{model}}^2)$ (quadratically reduced with smaller $d_{\\text{model}}$)\n\n**TransformerConv Integration:**\n\nPyTorch Geometric's `TransformerConv` implements efficient sparse attention:\n$$\\mathbf{h}_i^{(l+1)} = \\mathbf{W}_1 \\mathbf{h}_i^{(l)} + \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} \\mathbf{W}_2 \\mathbf{h}_j^{(l)}$$\n\nwhere attention weights are computed as:\n$$\\alpha_{ij} = \\text{softmax}_j\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^T [\\mathbf{W}_1 \\mathbf{h}_i^{(l)} \\| \\mathbf{W}_2 \\mathbf{h}_j^{(l)}]\\right)\\right)$$\n\n**Efficiency Benefits:**\n- **Sparsity**: Only computes attention for graph edges, not all node pairs\n- **Complexity**: $O(|\\mathcal{E}| d_{\\text{model}})$ instead of $O(N^2 d_{\\text{model}})$\n- **Memory**: Linear in graph size rather than quadratic\n\n**Mathematical Approximation Quality:**\nThe lightweight design maintains core transformer properties:\n- **Expressiveness**: Can still capture global patterns through multi-hop attention paths\n- **Interpretability**: Attention weights remain meaningful  \n- **Convergence**: Training dynamics similar to full transformers with proper learning rate tuning\n\nThis mathematical framework enables efficient graph transformer training on CPU while preserving essential modeling capabilities."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightGraphTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    CPU-optimized Graph Transformer with reduced complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, d_model=64, n_heads=4, n_layers=2, \n",
    "                 num_classes=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Smaller input projection\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Simplified positional encoding (degree-based)\n",
    "        self.pe = GraphPositionalEncoding(d_model, pe_type='degree')\n",
    "        \n",
    "        # Lightweight transformer layers using PyG's TransformerConv\n",
    "        self.transformer_convs = nn.ModuleList([\n",
    "            TransformerConv(\n",
    "                in_channels=d_model,\n",
    "                out_channels=d_model,\n",
    "                heads=n_heads,\n",
    "                concat=False,  # Average heads instead of concatenating\n",
    "                dropout=dropout\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer norms\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Input projection\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        pe = self.pe(edge_index, x.size(0))\n",
    "        x = x + pe\n",
    "        \n",
    "        # Apply transformer convolution layers\n",
    "        for i, (conv, norm) in enumerate(zip(self.transformer_convs, self.layer_norms)):\n",
    "            # Transformer convolution\n",
    "            x_new = conv(x, edge_index)\n",
    "            \n",
    "            # Residual connection and layer norm\n",
    "            x = norm(x + self.dropout(x_new))\n",
    "            \n",
    "            # Apply activation except for last layer\n",
    "            if i < len(self.transformer_convs) - 1:\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        # For graph classification, pool features\n",
    "        if batch is not None:\n",
    "            x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Preparation\n",
    "\n",
    "We'll test our Graph Transformers on both node classification (Cora) and graph classification (MUTAG) tasks to demonstrate their versatility across different types of graph learning problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for both node and graph classification\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Node classification dataset (Cora)\n",
    "cora_dataset = Planetoid('/tmp/Cora', 'Cora', transform=NormalizeFeatures())\n",
    "cora_data = cora_dataset[0]\n",
    "\n",
    "print(f\"Cora Dataset:\")\n",
    "print(f\"  Nodes: {cora_data.num_nodes}\")\n",
    "print(f\"  Edges: {cora_data.num_edges}\")\n",
    "print(f\"  Features: {cora_data.num_features}\")\n",
    "print(f\"  Classes: {cora_dataset.num_classes}\")\n",
    "\n",
    "# Graph classification dataset (MUTAG - small for CPU)\n",
    "try:\n",
    "    mutag_dataset = TUDataset('/tmp/MUTAG', 'MUTAG')\n",
    "    print(f\"\\nMUTAG Dataset:\")\n",
    "    print(f\"  Graphs: {len(mutag_dataset)}\")\n",
    "    print(f\"  Classes: {mutag_dataset.num_classes}\")\n",
    "    print(f\"  Features: {mutag_dataset.num_features}\")\n",
    "    \n",
    "    # Sample graph info\n",
    "    sample_graph = mutag_dataset[0]\n",
    "    print(f\"  Avg nodes per graph: ~{sample_graph.num_nodes} (sample)\")\n",
    "    print(f\"  Avg edges per graph: ~{sample_graph.num_edges} (sample)\")\n",
    "    \n",
    "    # Split dataset\n",
    "    torch.manual_seed(42)\n",
    "    mutag_dataset = mutag_dataset.shuffle()\n",
    "    \n",
    "    train_size = int(0.7 * len(mutag_dataset))\n",
    "    val_size = int(0.15 * len(mutag_dataset))\n",
    "    \n",
    "    mutag_train = mutag_dataset[:train_size]\n",
    "    mutag_val = mutag_dataset[train_size:train_size + val_size]\n",
    "    mutag_test = mutag_dataset[train_size + val_size:]\n",
    "    \n",
    "    print(f\"  Train/Val/Test: {len(mutag_train)}/{len(mutag_val)}/{len(mutag_test)}\")\n",
    "    \n",
    "except:\n",
    "    print(\"\\nMUTAG dataset not available - will focus on Cora node classification\")\n",
    "    mutag_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions\n",
    "\n",
    "We'll create training functions for both node classification and graph classification tasks, optimized for CPU performance with the MacBook Air M2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node_classification(model, data, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train Graph Transformer for node classification\n",
    "    \"\"\"\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation accuracy every 20 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "                val_acc = accuracy_score(data.y[data.val_mask].cpu(), pred[data.val_mask].cpu())\n",
    "                val_accuracies.append(val_acc)\n",
    "                print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "            model.train()\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "def train_graph_classification(model, train_loader, val_loader, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train Graph Transformer for graph classification\n",
    "    \"\"\"\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Validation every 20 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = batch.to(device)\n",
    "                    pred = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
    "                    correct += (pred == batch.y).sum().item()\n",
    "                    total += batch.y.size(0)\n",
    "            \n",
    "            val_acc = correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "            print(f'Epoch {epoch:03d}, Loss: {avg_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "def test_model(model, data=None, test_loader=None, task='node'):\n",
    "    \"\"\"\n",
    "    Test trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    if task == 'node':\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(data.x, data.edge_index).argmax(dim=1)\n",
    "            test_acc = accuracy_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu())\n",
    "        return test_acc\n",
    "    \n",
    "    elif task == 'graph':\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
    "                correct += (pred == batch.y).sum().item()\n",
    "                total += batch.y.size(0)\n",
    "        \n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Node Classification with Graph Transformer\n",
    "\n",
    "Let's train our lightweight Graph Transformer on the Cora dataset for node classification. This demonstrates how transformers can capture long-range dependencies between nodes in citation networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CPU optimization\n",
    "device = torch.device('cpu')\n",
    "torch.set_num_threads(8)  # Optimize for M2 8-core CPU\n",
    "\n",
    "print(\"Training Lightweight Graph Transformer on Cora...\")\n",
    "\n",
    "# Initialize model\n",
    "gt_model = LightweightGraphTransformer(\n",
    "    input_dim=cora_dataset.num_features,\n",
    "    d_model=64,  # Smaller for CPU efficiency\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    num_classes=cora_dataset.num_classes,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in gt_model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Train the model\n",
    "gt_losses, gt_val_accs = train_node_classification(gt_model, cora_data, epochs=100, lr=0.005)\n",
    "\n",
    "# Test the model\n",
    "gt_test_acc = test_model(gt_model, cora_data, task='node')\n",
    "print(f\"\\nGraph Transformer Test Accuracy: {gt_test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Traditional GNNs\n",
    "\n",
    "Let's compare our Graph Transformer with a traditional GCN to understand the benefits of the transformer architecture for graph tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with GCN baseline\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "print(\"\\nTraining GCN baseline for comparison...\")\n",
    "\n",
    "# Initialize GCN\n",
    "gcn_model = SimpleGCN(\n",
    "    input_dim=cora_dataset.num_features,\n",
    "    hidden_dim=64,\n",
    "    num_classes=cora_dataset.num_classes\n",
    ")\n",
    "\n",
    "print(f\"GCN parameters: {sum(p.numel() for p in gcn_model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Train GCN\n",
    "gcn_losses, gcn_val_accs = train_node_classification(gcn_model, cora_data, epochs=100, lr=0.01)\n",
    "\n",
    "# Test GCN\n",
    "gcn_test_acc = test_model(gcn_model, cora_data, task='node')\n",
    "print(f\"\\nGCN Test Accuracy: {gcn_test_acc:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\n=== Model Comparison ===\")\n",
    "print(f\"{'Model':<20} {'Test Acc':<10} {'Parameters':<12}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Graph Transformer':<20} {gt_test_acc:<10.4f} {sum(p.numel() for p in gt_model.parameters() if p.requires_grad):<12}\")\n",
    "print(f\"{'GCN':<20} {gcn_test_acc:<10.4f} {sum(p.numel() for p in gcn_model.parameters() if p.requires_grad):<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention Analysis and Visualization\n",
    "\n",
    "One of the key advantages of Graph Transformers is interpretability through attention weights. Let's extract and visualize attention patterns to understand what the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention weights for analysis\n",
    "def analyze_attention(model, data, num_nodes=50):\n",
    "    \"\"\"\n",
    "    Analyze attention patterns in Graph Transformer\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device('cpu')\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Get a subset of nodes for visualization\n",
    "    subset_idx = torch.randperm(data.num_nodes)[:num_nodes]\n",
    "    subset_x = data.x[subset_idx]\n",
    "    \n",
    "    # Create subgraph (simplified - just use subset)\n",
    "    with torch.no_grad():\n",
    "        # For simplicity, we'll analyze the transformer conv layers\n",
    "        # which have built-in attention mechanisms\n",
    "        \n",
    "        x = model.input_proj(subset_x)\n",
    "        pe = model.pe(data.edge_index, subset_x.size(0))\n",
    "        x = x + pe\n",
    "        \n",
    "        # Get attention from first transformer layer\n",
    "        attention_weights = []\n",
    "        \n",
    "        # Note: TransformerConv doesn't directly expose attention weights\n",
    "        # This is a limitation, but we can still analyze embeddings\n",
    "        embeddings = x\n",
    "    \n",
    "    return embeddings, subset_idx\n",
    "\n",
    "# Analyze embeddings from Graph Transformer\n",
    "embeddings, node_indices = analyze_attention(gt_model, cora_data, num_nodes=100)\n",
    "\n",
    "print(f\"Extracted embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Analyzing nodes: {node_indices[:10]}...\")\n",
    "\n",
    "# Visualize embeddings using t-SNE\n",
    "print(\"\\nApplying t-SNE to transformer embeddings...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=20)\n",
    "embeddings_2d = tsne.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "# Get true labels for the subset\n",
    "subset_labels = cora_data.y[node_indices].cpu().numpy()\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Graph Transformer embeddings\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                     c=subset_labels, cmap='tab10', alpha=0.7, s=30)\n",
    "plt.title('Graph Transformer Node Embeddings')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.colorbar(scatter, label='Node Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Compare with GCN embeddings\n",
    "gcn_model.eval()\n",
    "with torch.no_grad():\n",
    "    gcn_x = gcn_model.conv1(cora_data.x, cora_data.edge_index)\n",
    "    gcn_embeddings = F.relu(gcn_x)[node_indices]\n",
    "\n",
    "gcn_embeddings_2d = tsne.fit_transform(gcn_embeddings.cpu().numpy())\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter2 = plt.scatter(gcn_embeddings_2d[:, 0], gcn_embeddings_2d[:, 1], \n",
    "                      c=subset_labels, cmap='tab10', alpha=0.7, s=30)\n",
    "plt.title('GCN Node Embeddings')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.colorbar(scatter2, label='Node Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Graph Classification with MUTAG\n",
    "\n",
    "If the MUTAG dataset is available, let's demonstrate graph-level classification with our Graph Transformer. This shows how transformers can aggregate information across an entire graph for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mutag_dataset is not None:\n",
    "    print(\"Training Graph Transformer on MUTAG for graph classification...\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 32  # Small batch size for CPU\n",
    "    \n",
    "    train_loader = DataLoader(mutag_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(mutag_val, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(mutag_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model for graph classification\n",
    "    mutag_gt_model = LightweightGraphTransformer(\n",
    "        input_dim=mutag_dataset.num_features,\n",
    "        d_model=32,  # Even smaller for graph classification\n",
    "        n_heads=2,\n",
    "        n_layers=2,\n",
    "        num_classes=mutag_dataset.num_classes,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"MUTAG GT parameters: {sum(p.numel() for p in mutag_gt_model.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    # Train model\n",
    "    mutag_losses, mutag_val_accs = train_graph_classification(\n",
    "        mutag_gt_model, train_loader, val_loader, epochs=100, lr=0.01\n",
    "    )\n",
    "    \n",
    "    # Test model\n",
    "    mutag_test_acc = test_model(mutag_gt_model, test_loader=test_loader, task='graph')\n",
    "    print(f\"\\nMUTAG Graph Transformer Test Accuracy: {mutag_test_acc:.4f}\")\n",
    "    \n",
    "    # Train GCN for comparison\n",
    "    mutag_gcn_model = SimpleGCN(\n",
    "        input_dim=mutag_dataset.num_features,\n",
    "        hidden_dim=32,\n",
    "        num_classes=mutag_dataset.num_classes\n",
    "    )\n",
    "    \n",
    "    # Modify GCN for graph classification\n",
    "    class GraphGCN(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "            self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "            self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        def forward(self, x, edge_index, batch):\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = global_mean_pool(x, batch)\n",
    "            return self.classifier(x)\n",
    "    \n",
    "    mutag_gcn_model = GraphGCN(\n",
    "        input_dim=mutag_dataset.num_features,\n",
    "        hidden_dim=32,\n",
    "        num_classes=mutag_dataset.num_classes\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining GCN baseline on MUTAG...\")\n",
    "    gcn_mutag_losses, gcn_mutag_val_accs = train_graph_classification(\n",
    "        mutag_gcn_model, train_loader, val_loader, epochs=100, lr=0.01\n",
    "    )\n",
    "    \n",
    "    gcn_mutag_test_acc = test_model(mutag_gcn_model, test_loader=test_loader, task='graph')\n",
    "    print(f\"\\nMUTAG GCN Test Accuracy: {gcn_mutag_test_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\n=== MUTAG Results ===\")\n",
    "    print(f\"Graph Transformer: {mutag_test_acc:.4f}\")\n",
    "    print(f\"GCN: {gcn_mutag_test_acc:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"MUTAG dataset not available - skipping graph classification demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Analysis and Visualization\n",
    "\n",
    "Let's create comprehensive visualizations comparing the training dynamics and final performance of Graph Transformers versus traditional GNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training loss comparison (Node classification)\n",
    "axes[0, 0].plot(gt_losses, label='Graph Transformer', alpha=0.8, color='blue')\n",
    "axes[0, 0].plot(gcn_losses, label='GCN', alpha=0.8, color='red')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Training Loss')\n",
    "axes[0, 0].set_title('Node Classification: Training Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy comparison (Node classification)\n",
    "epochs_eval = range(0, 100, 20)\n",
    "axes[0, 1].plot(epochs_eval, gt_val_accs, 'o-', label='Graph Transformer', alpha=0.8, color='blue')\n",
    "axes[0, 1].plot(epochs_eval, gcn_val_accs, 'o-', label='GCN', alpha=0.8, color='red')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "axes[0, 1].set_title('Node Classification: Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy comparison\n",
    "models = ['Graph Transformer', 'GCN']\n",
    "node_accs = [gt_test_acc, gcn_test_acc]\n",
    "\n",
    "bars = axes[1, 0].bar(models, node_accs, alpha=0.8, color=['blue', 'red'])\n",
    "axes[1, 0].set_ylabel('Test Accuracy')\n",
    "axes[1, 0].set_title('Node Classification: Final Test Accuracy')\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, node_accs):\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Model complexity comparison\n",
    "gt_params = sum(p.numel() for p in gt_model.parameters() if p.requires_grad)\n",
    "gcn_params = sum(p.numel() for p in gcn_model.parameters() if p.requires_grad)\n",
    "params = [gt_params, gcn_params]\n",
    "\n",
    "bars2 = axes[1, 1].bar(models, params, alpha=0.8, color=['blue', 'red'])\n",
    "axes[1, 1].set_ylabel('Number of Parameters')\n",
    "axes[1, 1].set_title('Model Complexity')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, param in zip(bars2, params):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + param*0.01, \n",
    "                   f'{param}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(\"\\n=== Detailed Performance Analysis ===\")\n",
    "print(f\"{'Metric':<25} {'Graph Transformer':<18} {'GCN':<10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Node Test Accuracy':<25} {gt_test_acc:<18.4f} {gcn_test_acc:<10.4f}\")\n",
    "print(f\"{'Parameters':<25} {gt_params:<18} {gcn_params:<10}\")\n",
    "print(f\"{'Final Train Loss':<25} {gt_losses[-1]:<18.4f} {gcn_losses[-1]:<10.4f}\")\n",
    "print(f\"{'Best Val Accuracy':<25} {max(gt_val_accs):<18.4f} {max(gcn_val_accs):<10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 12. CPU Optimization Tips for Graph Transformers\n\n### Mathematical Framework for Computational Optimization\n\n**Complexity Analysis for M2 MacBook Air:**\n\n**Hardware Characteristics:**\n- **CPU Cores**: 8 cores (4 performance + 4 efficiency)\n- **Memory Architecture**: Unified memory (shared CPU/GPU)\n- **Vector Units**: Advanced SIMD support for matrix operations\n\n**Mathematical Optimization Strategies:**\n\n**1. Attention Complexity Reduction:**\n\n**Full Attention Complexity:**\n$$\\mathcal{C}_{\\text{full}} = O(N^2 d_{\\text{model}} + N d_{\\text{model}}^2)$$\n\n**Sparse Attention Complexity:**\n$$\\mathcal{C}_{\\text{sparse}} = O(|\\mathcal{E}| d_{\\text{model}} + N d_{\\text{model}}^2)$$\n\n**Sparsity Ratio:**\n$$\\rho = \\frac{|\\mathcal{E}|}{N^2} \\ll 1$$\n\n**Speedup Factor:**\n$$\\text{Speedup} \\approx \\frac{1}{\\rho} \\text{ for attention computation}$$\n\n**2. Memory Optimization Mathematics:**\n\n**Memory Requirements:**\n$$M_{\\text{total}} = M_{\\text{features}} + M_{\\text{attention}} + M_{\\text{gradients}}$$\n\nwhere:\n- $M_{\\text{features}} = L \\times N \\times d_{\\text{model}} \\times 4$ bytes\n- $M_{\\text{attention}} = h \\times N^2 \\times 4$ bytes (dense) or $h \\times |\\mathcal{E}| \\times 4$ bytes (sparse)\n- $M_{\\text{gradients}} = 2 \\times |\\theta| \\times 4$ bytes (Adam optimizer)\n\n**Gradient Checkpointing:**\n$$M_{\\text{checkpoint}} = M_{\\text{base}} + \\sqrt{L} \\times M_{\\text{layer}}$$\n\ninstead of $M_{\\text{base}} + L \\times M_{\\text{layer}}$\n\n**3. Batch Size Optimization:**\n\n**Optimal Batch Size Formula:**\n$$B_{\\text{opt}} = \\arg\\max_B \\frac{B \\times \\text{Utilization}(B)}{\\text{Memory}(B)} \\text{ s.t. } \\text{Memory}(B) \\leq M_{\\text{available}}$$\n\n**Throughput Model:**\n$$\\text{Throughput}(B) = \\frac{B}{\\text{ComputeTime}(B) + \\text{MemoryTime}(B)}$$\n\n**4. Numerical Optimization:**\n\n**Mixed Precision Benefits:**\n- **Memory Reduction**: $\\approx 50\\%$ using FP16 vs FP32\n- **Speed Improvement**: $\\approx 1.5-2\\times$ on modern CPUs with proper vectorization\n\n**Learning Rate Scaling:**\n$$\\alpha_{\\text{mixed}} = \\alpha_{\\text{base}} \\times \\sqrt{\\frac{B_{\\text{mixed}}}{B_{\\text{base}}}}$$\n\n**5. Architectural Optimizations:**\n\n**Dimension Scaling Laws:**\n$$\\text{Performance} \\propto d_{\\text{model}}^{\\alpha} \\text{ where } \\alpha \\approx 0.5-0.7$$\n$$\\text{Compute} \\propto d_{\\text{model}}^2$$\n\n**Optimal Scaling:**\n$$d_{\\text{model}}^* = \\arg\\max_{d} \\frac{\\text{Performance}(d)}{\\text{Compute}(d)^\\beta}$$\n\nwhere $\\beta$ is the computational budget constraint.\n\n**6. Training Efficiency:**\n\n**Learning Rate Warm-up:**\n$$\\alpha(t) = \\begin{cases}\n\\alpha_{\\text{base}} \\times \\frac{t}{T_{\\text{warmup}}} & \\text{if } t \\leq T_{\\text{warmup}} \\\\\n\\alpha_{\\text{base}} \\times \\gamma^{\\lfloor (t-T_{\\text{warmup}})/T_{\\text{step}} \\rfloor} & \\text{otherwise}\n\\end{cases}$$\n\n**Gradient Clipping:**\n$$\\mathbf{g}_{\\text{clipped}} = \\mathbf{g} \\times \\min\\left(1, \\frac{\\tau}{\\|\\mathbf{g}\\|_2}\\right)$$\n\n**Early Stopping Criterion:**\n$$\\text{Stop if } \\text{Validation}(t) - \\max_{i<t} \\text{Validation}(i) < \\epsilon \\text{ for } p \\text{ consecutive epochs}$$\n\n**Performance Monitoring:**\n\n**CPU Utilization Efficiency:**\n$$\\eta_{\\text{CPU}} = \\frac{\\text{Actual FLOPS}}{\\text{Peak FLOPS} \\times \\text{Cores Used}}$$\n\n**Memory Bandwidth Utilization:**\n$$\\eta_{\\text{Memory}} = \\frac{\\text{Data Transferred}}{\\text{Peak Bandwidth} \\times \\text{Time}}$$\n\n**Practical Recommendations for M2 MacBook Air:**\n\n**Optimal Configuration:**\n- $d_{\\text{model}} = 32-64$ (balance performance/memory)\n- $h = 2-4$ attention heads\n- $L = 2-3$ layers maximum\n- Batch size = 16-32 for graph classification\n- Learning rate = 0.001-0.01 with warm-up\n\n**Threading Configuration:**\n```python\ntorch.set_num_threads(6)  # Leave 2 cores for system\ntorch.set_num_interop_threads(2)  # Reduce overhead\n```\n\n**Expected Performance:**\n- **Memory Usage**: 2-4 GB for typical graph sizes (1K-10K nodes)\n- **Training Speed**: 10-50 graphs/second depending on complexity\n- **Convergence**: 50-200 epochs typically sufficient\n\nThese mathematical optimizations ensure efficient Graph Transformer training while maintaining model quality on resource-constrained CPU environments."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Optimization Tips for Graph Transformers\n",
    "print(\"=== CPU Optimization Tips for Graph Transformers ===\")\n",
    "\n",
    "print(\"\\n1. Architecture Optimizations:\")\n",
    "print(\"   - Use smaller model dimensions (d_model=32-64)\")\n",
    "print(\"   - Reduce number of attention heads (2-4 heads)\")\n",
    "print(\"   - Limit number of layers (2-3 layers max)\")\n",
    "print(\"   - Use concat=False in TransformerConv to average heads\")\n",
    "\n",
    "print(\"\\n2. Attention Optimizations:\")\n",
    "print(\"   - Use sparse attention when possible\")\n",
    "print(\"   - Apply attention masks to focus on local neighborhoods\")\n",
    "print(\"   - Consider using efficient attention variants (linear attention)\")\n",
    "print(\"   - Cache attention weights when doing multiple forward passes\")\n",
    "\n",
    "print(\"\\n3. Memory Management:\")\n",
    "print(\"   - Process large graphs in smaller batches\")\n",
    "print(\"   - Use gradient checkpointing for deep models\")\n",
    "print(\"   - Clear intermediate attention matrices explicitly\")\n",
    "print(\"   - Monitor memory usage during training\")\n",
    "\n",
    "print(\"\\n4. Training Optimizations:\")\n",
    "print(\"   - Use smaller learning rates (0.001-0.01)\")\n",
    "print(\"   - Implement learning rate warm-up for stability\")\n",
    "print(\"   - Use gradient clipping to prevent instability\")\n",
    "print(\"   - Early stopping based on validation metrics\")\n",
    "\n",
    "print(\"\\n5. Inference Optimizations:\")\n",
    "print(\"   - Cache node embeddings for repeated queries\")\n",
    "print(\"   - Use model.eval() to disable dropout and batch norm\")\n",
    "print(\"   - Batch multiple graphs together when possible\")\n",
    "print(\"   - Consider quantization for deployment\")\n",
    "\n",
    "# Example of memory monitoring\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "print(f\"\\nCurrent memory usage: {get_memory_usage():.1f} MB\")\n",
    "print(f\"PyTorch threads: {torch.get_num_threads()}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Demonstrate efficient evaluation\n",
    "def efficient_inference(model, data, batch_size=100):\n",
    "    \"\"\"\n",
    "    Memory-efficient inference for large graphs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if data.num_nodes <= batch_size:\n",
    "            return model(data.x, data.edge_index)\n",
    "        \n",
    "        # For very large graphs, you might need more sophisticated batching\n",
    "        # This is a simplified version\n",
    "        return model(data.x, data.edge_index)\n",
    "\n",
    "print(\"\\n=== Performance Recommendations ===\")\n",
    "print(\"For MacBook Air M2:\")\n",
    "print(\"• Optimal model size: d_model=32-64, 2-3 layers\")\n",
    "print(\"• Batch size: 16-32 for graph classification\")\n",
    "print(\"• Learning rate: 0.001-0.01 with warm-up\")\n",
    "print(\"• Use early stopping to prevent overfitting\")\n",
    "print(\"• Monitor memory usage and adjust accordingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### Comprehensive Mathematical Foundation of Graph Transformers\n\nIn this notebook, we have explored the mathematical foundations and practical implementations of Graph Transformers for graph learning tasks:\n\n### **Graph Transformer Architecture**\n- **Mathematical Core**: $\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$ adapted for graphs\n- **Key Innovation**: Self-attention on graphs with structural positional encodings\n- **Global Receptive Field**: All nodes can attend to each other from layer 1\n- **Advantages**: Captures long-range dependencies, interpretable attention, no over-smoothing\n- **Limitations**: Quadratic complexity in graph size, requires structural awareness\n\n### **Positional Encoding Strategies**\n- **Laplacian PE**: $\\mathbf{PE} = \\mathbf{U}[:, 1:k+1]$ using eigenvectors of normalized Laplacian\n- **Degree PE**: Sinusoidal encoding based on node degrees\n- **Learned PE**: Task-adaptive embeddings for structural patterns\n- **Mathematical Foundation**: Spectral graph theory provides optimal structural representations\n\n### **Mathematical Insights**\n\n**Theoretical Comparison with GNNs:**\n$$\\text{Graph Transformer} \\supset \\text{Traditional GNNs}$$\n\n**Complexity Analysis:**\n- **Memory**: $O(N^2)$ for full attention vs. $O(|\\mathcal{E}|)$ for GNNs\n- **Computation**: $O(N^2 d_{\\text{model}})$ vs. $O(|\\mathcal{E}| d_{\\text{model}})$\n- **Expressiveness**: Can capture patterns beyond 1-WL hierarchy\n- **Scalability**: Requires optimization for large graphs\n\n**Key Mathematical Properties:**\n1. **Permutation Equivariance**: $f(\\pi(\\mathbf{X}), \\pi(\\mathbf{A})\\pi^T) = \\pi(f(\\mathbf{X}, \\mathbf{A}))$\n2. **Universal Approximation**: Can approximate any permutation-equivariant function\n3. **Global Information Flow**: Direct paths between all node pairs\n4. **Structural Awareness**: Through positional encodings and attention biases\n\n### **Performance Insights**\n\n**Advantages over Traditional GNNs:**\n- **Long-range Dependencies**: Direct attention between distant nodes\n- **No Over-smoothing**: Attention preserves node distinctions across layers\n- **Interpretability**: Attention weights provide model explanations\n- **Global Reasoning**: Captures graph-level properties effectively\n\n**Computational Trade-offs:**\n- **Higher Memory**: Quadratic in graph size for attention matrices\n- **Better Expressiveness**: Can model complex node relationships\n- **Scalability Challenges**: Requires optimization for large graphs\n- **Training Stability**: Generally more stable than deep GNNs\n\n### **Practical Guidelines**\n\n**Architecture Selection:**\n| Use Case | Recommended Configuration | Rationale |\n|----------|--------------------------|-----------|\n| **Small-Medium Graphs** | Full attention | Maximum expressiveness |\n| **Large Graphs** | Sparse attention + masking | Computational efficiency |\n| **Global Reasoning Tasks** | Graph Transformer | Superior long-range modeling |\n| **Local Pattern Tasks** | Traditional GNNs | More efficient |\n| **Interpretability Required** | Graph Transformer | Attention visualization |\n\n**CPU Optimization for M2 MacBook Air:**\n- **Model Size**: $d_{\\text{model}} = 32-64$, $h = 2-4$ heads, $L = 2-3$ layers\n- **Memory Management**: Gradient checkpointing, mixed precision\n- **Training Efficiency**: Learning rate warm-up, gradient clipping\n- **Expected Performance**: 2-4 GB memory, 10-50 graphs/sec\n\n### **Mathematical Extensions and Future Directions**\n\n**Advanced Attention Mechanisms:**\n- **Linear Attention**: $O(N)$ complexity through kernel approximations\n- **Sparse Attention Patterns**: Local + global attention combinations\n- **Hierarchical Attention**: Multi-scale graph representations\n\n**Structural Enhancements:**\n- **Edge-aware Attention**: $e_{ij} = \\frac{\\mathbf{q}_i^T \\mathbf{k}_j}{\\sqrt{d_k}} + \\mathbf{W}_e \\mathbf{e}_{ij}$\n- **Relative Positional Encoding**: Distance-based attention biases\n- **Graph-specific Normalizations**: Degree-aware layer normalization\n\n**Theoretical Developments:**\n- **Expressive Power Analysis**: Beyond 1-WL limitations\n- **Generalization Theory**: Sample complexity for graph learning\n- **Optimization Landscapes**: Training dynamics analysis\n\n### **When to Use Graph Transformers**\n\n**Ideal Applications:**\n- **Global Graph Properties**: Connectivity, diameter, centrality\n- **Long-range Node Relationships**: Cross-community interactions\n- **Complex Pattern Recognition**: Higher-order structural motifs\n- **Interpretable Predictions**: Attention-based explanations\n\n**Consider Alternatives When:**\n- **Very Large Graphs**: $N > 10^4$ nodes (memory constraints)\n- **Simple Local Patterns**: Traditional GNNs more efficient\n- **Limited Computational Resources**: GCN/GraphSAGE alternatives\n- **Real-time Applications**: Latency requirements favor simpler models\n\n### **Next Notebook Preview**\n\nThe next notebook will explore **Advanced GNN Architectures and Scalability**, covering:\n- **GraphSAINT**: $\\mathcal{L} = \\sum_{s \\in \\mathcal{S}} \\frac{|\\mathcal{V}|}{|\\mathcal{V}_s|} \\mathcal{L}_s$ for scalable training\n- **FastGCN**: Importance sampling for layer-wise node sampling\n- **DropEdge and DropNode**: Regularization techniques for large graphs\n- **Memory-enhanced GNNs**: Persistent memory mechanisms\n\nThis mathematical foundation in Graph Transformers provides the necessary background for understanding how attention mechanisms revolutionize graph neural networks while highlighting the computational trade-offs and optimization strategies required for practical deployment."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}