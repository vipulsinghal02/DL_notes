{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions Part 5: Optimization and Gradient-Based Methods\n",
    "\n",
    "This notebook covers optimization algorithms fundamental to machine learning, from basic gradient descent to advanced second-order methods. Each question includes mathematical derivations, algorithmic implementations, and performance analysis.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Gradient descent variants and convergence analysis\n",
    "- Momentum and adaptive learning rate methods\n",
    "- Second-order optimization methods\n",
    "- Constrained optimization and Lagrangian methods\n",
    "- Optimization challenges in high-dimensional spaces\n",
    "\n",
    "**Format:** Each question includes theory, implementation, and empirical analysis sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize, minimize_scalar\n",
    "from scipy.linalg import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Gradient Descent Variants and Convergence Analysis\n",
    "\n",
    "**Question:** Compare batch, stochastic, and mini-batch gradient descent. Analyze their convergence properties and implement each variant with theoretical guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Batch Gradient Descent (BGD):**\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t)$$\n",
    "- Uses entire dataset for each update\n",
    "- Guaranteed convergence to global minimum for convex functions\n",
    "- Convergence rate: O(1/k) for convex, O(log k/k) for strongly convex\n",
    "\n",
    "**Stochastic Gradient Descent (SGD):**\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla J_i(\\theta_t)$$\n",
    "- Uses single sample for each update\n",
    "- Faster updates but noisy convergence\n",
    "- Requires decreasing learning rate for convergence\n",
    "\n",
    "**Mini-batch Gradient Descent:**\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\frac{1}{|B|} \\sum_{i \\in B} \\nabla J_i(\\theta_t)$$\n",
    "- Compromise between BGD and SGD\n",
    "- Batch size affects convergence speed and stability\n",
    "\n",
    "**Learning Rate Requirements:**\n",
    "For SGD convergence: $\\sum_{t=1}^\\infty \\alpha_t = \\infty$ and $\\sum_{t=1}^\\infty \\alpha_t^2 < \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimizer:\n",
    "    \"\"\"\n",
    "    Implementation of gradient descent variants for linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='batch', learning_rate=0.01, batch_size=32, max_iter=1000, tol=1e-6):\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.cost_history_ = []\n",
    "        self.theta_history_ = []\n",
    "        self.grad_norms_ = []\n",
    "        \n",
    "    def _cost_function(self, X, y, theta):\n",
    "        \"\"\"Mean squared error cost function.\"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X @ theta\n",
    "        cost = np.sum((predictions - y) ** 2) / (2 * m)\n",
    "        return cost\n",
    "    \n",
    "    def _gradient(self, X, y, theta):\n",
    "        \"\"\"Compute gradient of MSE cost function.\"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X @ theta\n",
    "        gradient = X.T @ (predictions - y) / m\n",
    "        return gradient\n",
    "    \n",
    "    def _get_learning_rate(self, iteration):\n",
    "        \"\"\"Get learning rate (can be adaptive for SGD).\"\"\"\n",
    "        if self.method == 'sgd_adaptive':\n",
    "            # Learning rate schedule: α_t = α_0 / (1 + t)\n",
    "            return self.learning_rate / (1 + iteration * 0.01)\n",
    "        return self.learning_rate\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model using specified gradient descent variant.\"\"\"\n",
    "        # Add intercept term\n",
    "        X_with_intercept = np.column_stack([np.ones(len(X)), X])\n",
    "        m, n = X_with_intercept.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        theta = np.random.normal(0, 0.01, n)\n",
    "        self.cost_history_ = []\n",
    "        self.theta_history_ = [theta.copy()]\n",
    "        self.grad_norms_ = []\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            if self.method == 'batch':\n",
    "                # Batch Gradient Descent\n",
    "                gradient = self._gradient(X_with_intercept, y, theta)\n",
    "                lr = self._get_learning_rate(iteration)\n",
    "                theta = theta - lr * gradient\n",
    "                \n",
    "            elif self.method in ['sgd', 'sgd_adaptive']:\n",
    "                # Stochastic Gradient Descent\n",
    "                for i in range(m):\n",
    "                    # Random sample\n",
    "                    idx = np.random.randint(0, m)\n",
    "                    X_sample = X_with_intercept[idx:idx+1]\n",
    "                    y_sample = y[idx:idx+1]\n",
    "                    \n",
    "                    gradient = self._gradient(X_sample, y_sample, theta)\n",
    "                    lr = self._get_learning_rate(iteration * m + i)\n",
    "                    theta = theta - lr * gradient\n",
    "                    \n",
    "            elif self.method == 'mini_batch':\n",
    "                # Mini-batch Gradient Descent\n",
    "                # Shuffle data\n",
    "                indices = np.random.permutation(m)\n",
    "                X_shuffled = X_with_intercept[indices]\n",
    "                y_shuffled = y[indices]\n",
    "                \n",
    "                # Process mini-batches\n",
    "                for start_idx in range(0, m, self.batch_size):\n",
    "                    end_idx = min(start_idx + self.batch_size, m)\n",
    "                    X_batch = X_shuffled[start_idx:end_idx]\n",
    "                    y_batch = y_shuffled[start_idx:end_idx]\n",
    "                    \n",
    "                    gradient = self._gradient(X_batch, y_batch, theta)\n",
    "                    lr = self._get_learning_rate(iteration)\n",
    "                    theta = theta - lr * gradient\n",
    "            \n",
    "            # Record history\n",
    "            cost = self._cost_function(X_with_intercept, y, theta)\n",
    "            gradient_full = self._gradient(X_with_intercept, y, theta)\n",
    "            \n",
    "            self.cost_history_.append(cost)\n",
    "            self.theta_history_.append(theta.copy())\n",
    "            self.grad_norms_.append(np.linalg.norm(gradient_full))\n",
    "            \n",
    "            # Check convergence\n",
    "            if len(self.cost_history_) > 1:\n",
    "                cost_diff = abs(self.cost_history_[-2] - self.cost_history_[-1])\n",
    "                if cost_diff < self.tol:\n",
    "                    break\n",
    "        \n",
    "        self.theta_ = theta\n",
    "        self.intercept_ = theta[0]\n",
    "        self.coef_ = theta[1:]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "\n",
    "# Generate regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=5, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare gradient descent variants\n",
    "methods = {\n",
    "    'Batch GD': GradientDescentOptimizer(method='batch', learning_rate=0.01, max_iter=1000),\n",
    "    'SGD': GradientDescentOptimizer(method='sgd', learning_rate=0.01, max_iter=100),\n",
    "    'SGD Adaptive': GradientDescentOptimizer(method='sgd_adaptive', learning_rate=0.1, max_iter=100),\n",
    "    'Mini-batch GD': GradientDescentOptimizer(method='mini_batch', learning_rate=0.01, \n",
    "                                            batch_size=50, max_iter=1000)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "fitted_models = {}\n",
    "\n",
    "for name, model in methods.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    final_cost = model.cost_history_[-1]\n",
    "    iterations = len(model.cost_history_)\n",
    "    final_grad_norm = model.grad_norms_[-1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'Test MSE': mse,\n",
    "        'Final Cost': final_cost,\n",
    "        'Iterations': iterations,\n",
    "        'Final Grad Norm': final_grad_norm\n",
    "    }\n",
    "    fitted_models[name] = model\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Gradient Descent Variants Comparison:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence behavior\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Cost convergence\n",
    "for name, model in fitted_models.items():\n",
    "    axes[0, 0].plot(model.cost_history_, label=name, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Cost')\n",
    "axes[0, 0].set_title('Cost Function Convergence')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Gradient norm convergence\n",
    "for name, model in fitted_models.items():\n",
    "    axes[0, 1].plot(model.grad_norms_, label=name, alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Gradient Norm')\n",
    "axes[0, 1].set_title('Gradient Norm Convergence')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# Parameter trajectory (first parameter)\n",
    "for name, model in fitted_models.items():\n",
    "    theta_trajectory = [theta[1] for theta in model.theta_history_]  # First feature coefficient\n",
    "    axes[1, 0].plot(theta_trajectory, label=name, alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Parameter Value (θ₁)')\n",
    "axes[1, 0].set_title('Parameter Trajectory')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate sensitivity for SGD\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "sgd_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = GradientDescentOptimizer(method='sgd', learning_rate=lr, max_iter=50)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    final_cost = model.cost_history_[-1]\n",
    "    sgd_results.append(final_cost)\n",
    "\n",
    "axes[1, 1].semilogx(learning_rates, sgd_results, 'o-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Learning Rate')\n",
    "axes[1, 1].set_ylabel('Final Cost')\n",
    "axes[1, 1].set_title('SGD Learning Rate Sensitivity')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Momentum and Adaptive Learning Rate Methods\n",
    "\n",
    "**Question:** Implement and compare momentum-based methods (momentum, Nesterov) and adaptive methods (AdaGrad, RMSprop, Adam). Analyze their convergence properties and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Momentum:**\n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta)\\nabla J(\\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha v_t$$\n",
    "\n",
    "**Nesterov Accelerated Gradient:**\n",
    "$$v_t = \\beta v_{t-1} + \\nabla J(\\theta_t - \\alpha\\beta v_{t-1})$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha v_t$$\n",
    "\n",
    "**AdaGrad:**\n",
    "$$G_t = G_{t-1} + \\nabla J(\\theta_t) \\odot \\nabla J(\\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\odot \\nabla J(\\theta_t)$$\n",
    "\n",
    "**RMSprop:**\n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta)\\nabla J(\\theta_t)^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\nabla J(\\theta_t)$$\n",
    "\n",
    "**Adam:**\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla J(\\theta_t)$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)\\nabla J(\\theta_t)^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedOptimizer:\n",
    "    \"\"\"\n",
    "    Implementation of advanced optimization algorithms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='adam', learning_rate=0.001, beta1=0.9, beta2=0.999, \n",
    "                 epsilon=1e-8, max_iter=1000, tol=1e-6):\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.cost_history_ = []\n",
    "        self.grad_norms_ = []\n",
    "        \n",
    "    def _cost_function(self, X, y, theta):\n",
    "        \"\"\"Mean squared error cost function.\"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X @ theta\n",
    "        cost = np.sum((predictions - y) ** 2) / (2 * m)\n",
    "        return cost\n",
    "    \n",
    "    def _gradient(self, X, y, theta):\n",
    "        \"\"\"Compute gradient of MSE cost function.\"\"\"\n",
    "        m = len(y)\n",
    "        predictions = X @ theta\n",
    "        gradient = X.T @ (predictions - y) / m\n",
    "        return gradient\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model using specified optimization method.\"\"\"\n",
    "        # Add intercept term\n",
    "        X_with_intercept = np.column_stack([np.ones(len(X)), X])\n",
    "        m, n = X_with_intercept.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        theta = np.random.normal(0, 0.01, n)\n",
    "        self.cost_history_ = []\n",
    "        self.grad_norms_ = []\n",
    "        \n",
    "        # Initialize optimizer-specific variables\n",
    "        if self.method in ['momentum', 'nesterov']:\n",
    "            v = np.zeros(n)  # Velocity\n",
    "        elif self.method == 'adagrad':\n",
    "            G = np.zeros(n)  # Accumulated squared gradients\n",
    "        elif self.method == 'rmsprop':\n",
    "            v = np.zeros(n)  # Moving average of squared gradients\n",
    "        elif self.method == 'adam':\n",
    "            m = np.zeros(n)  # First moment estimate\n",
    "            v = np.zeros(n)  # Second moment estimate\n",
    "        \n",
    "        for t in range(1, self.max_iter + 1):\n",
    "            # Compute gradient\n",
    "            if self.method == 'nesterov':\n",
    "                # Look ahead gradient\n",
    "                theta_lookahead = theta - self.learning_rate * self.beta1 * v\n",
    "                gradient = self._gradient(X_with_intercept, y, theta_lookahead)\n",
    "            else:\n",
    "                gradient = self._gradient(X_with_intercept, y, theta)\n",
    "            \n",
    "            # Update parameters based on method\n",
    "            if self.method == 'momentum':\n",
    "                v = self.beta1 * v + (1 - self.beta1) * gradient\n",
    "                theta = theta - self.learning_rate * v\n",
    "                \n",
    "            elif self.method == 'nesterov':\n",
    "                v = self.beta1 * v + gradient\n",
    "                theta = theta - self.learning_rate * v\n",
    "                \n",
    "            elif self.method == 'adagrad':\n",
    "                G = G + gradient ** 2\n",
    "                adapted_lr = self.learning_rate / (np.sqrt(G) + self.epsilon)\n",
    "                theta = theta - adapted_lr * gradient\n",
    "                \n",
    "            elif self.method == 'rmsprop':\n",
    "                v = self.beta2 * v + (1 - self.beta2) * gradient ** 2\n",
    "                adapted_lr = self.learning_rate / (np.sqrt(v) + self.epsilon)\n",
    "                theta = theta - adapted_lr * gradient\n",
    "                \n",
    "            elif self.method == 'adam':\n",
    "                m = self.beta1 * m + (1 - self.beta1) * gradient\n",
    "                v = self.beta2 * v + (1 - self.beta2) * gradient ** 2\n",
    "                \n",
    "                # Bias correction\n",
    "                m_hat = m / (1 - self.beta1 ** t)\n",
    "                v_hat = v / (1 - self.beta2 ** t)\n",
    "                \n",
    "                theta = theta - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            \n",
    "            # Record history\n",
    "            cost = self._cost_function(X_with_intercept, y, theta)\n",
    "            grad_norm = np.linalg.norm(gradient)\n",
    "            \n",
    "            self.cost_history_.append(cost)\n",
    "            self.grad_norms_.append(grad_norm)\n",
    "            \n",
    "            # Check convergence\n",
    "            if grad_norm < self.tol:\n",
    "                break\n",
    "        \n",
    "        self.theta_ = theta\n",
    "        self.intercept_ = theta[0]\n",
    "        self.coef_ = theta[1:]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "\n",
    "# Generate more complex data (non-convex-like landscape simulation)\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(500, 10)\n",
    "# Add some feature interactions to make optimization more challenging\n",
    "X_complex = np.column_stack([X, X[:, 0] * X[:, 1], X[:, 2] ** 2])\n",
    "true_coef = np.random.randn(12) * 2\n",
    "y = X_complex @ true_coef + np.random.randn(500) * 0.5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_complex, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare optimization methods\n",
    "optimizers = {\n",
    "    'SGD': AdvancedOptimizer(method='momentum', learning_rate=0.01, beta1=0.0),  # No momentum = SGD\n",
    "    'Momentum': AdvancedOptimizer(method='momentum', learning_rate=0.01, beta1=0.9),\n",
    "    'Nesterov': AdvancedOptimizer(method='nesterov', learning_rate=0.01, beta1=0.9),\n",
    "    'AdaGrad': AdvancedOptimizer(method='adagrad', learning_rate=0.1),\n",
    "    'RMSprop': AdvancedOptimizer(method='rmsprop', learning_rate=0.01, beta2=0.9),\n",
    "    'Adam': AdvancedOptimizer(method='adam', learning_rate=0.01)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "fitted_optimizers = {}\n",
    "\n",
    "for name, optimizer in optimizers.items():\n",
    "    optimizer.fit(X_train_scaled, y_train)\n",
    "    y_pred = optimizer.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    final_cost = optimizer.cost_history_[-1]\n",
    "    iterations = len(optimizer.cost_history_)\n",
    "    final_grad_norm = optimizer.grad_norms_[-1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'Test MSE': mse,\n",
    "        'Final Cost': final_cost,\n",
    "        'Iterations': iterations,\n",
    "        'Final Grad Norm': final_grad_norm\n",
    "    }\n",
    "    fitted_optimizers[name] = optimizer\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Advanced Optimization Methods Comparison:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization behavior\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Cost convergence\n",
    "for name, optimizer in fitted_optimizers.items():\n",
    "    axes[0, 0].semilogy(optimizer.cost_history_, label=name, alpha=0.8, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Cost (log scale)')\n",
    "axes[0, 0].set_title('Cost Function Convergence')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm convergence\n",
    "for name, optimizer in fitted_optimizers.items():\n",
    "    axes[0, 1].semilogy(optimizer.grad_norms_, label=name, alpha=0.8, linewidth=2)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Gradient Norm (log scale)')\n",
    "axes[0, 1].set_title('Gradient Norm Convergence')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance comparison\n",
    "method_names = list(results.keys())\n",
    "test_mses = [results[name]['Test MSE'] for name in method_names]\n",
    "final_costs = [results[name]['Final Cost'] for name in method_names]\n",
    "\n",
    "x_pos = np.arange(len(method_names))\n",
    "axes[0, 2].bar(x_pos, test_mses, alpha=0.7)\n",
    "axes[0, 2].set_xlabel('Optimization Method')\n",
    "axes[0, 2].set_ylabel('Test MSE')\n",
    "axes[0, 2].set_title('Final Test Performance')\n",
    "axes[0, 2].set_xticks(x_pos)\n",
    "axes[0, 2].set_xticklabels(method_names, rotation=45)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate robustness for different methods\n",
    "learning_rates = np.logspace(-3, 0, 10)\n",
    "methods_to_test = ['momentum', 'rmsprop', 'adam']\n",
    "robustness_results = {method: [] for method in methods_to_test}\n",
    "\n",
    "for method in methods_to_test:\n",
    "    for lr in learning_rates:\n",
    "        try:\n",
    "            opt = AdvancedOptimizer(method=method, learning_rate=lr, max_iter=200)\n",
    "            opt.fit(X_train_scaled, y_train)\n",
    "            final_cost = opt.cost_history_[-1]\n",
    "            robustness_results[method].append(final_cost)\n",
    "        except:\n",
    "            robustness_results[method].append(float('inf'))\n",
    "\n",
    "for method in methods_to_test:\n",
    "    axes[1, 0].loglog(learning_rates, robustness_results[method], 'o-', label=method, alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Learning Rate')\n",
    "axes[1, 0].set_ylabel('Final Cost')\n",
    "axes[1, 0].set_title('Learning Rate Robustness')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Adam hyperparameter sensitivity\n",
    "beta1_values = np.linspace(0.5, 0.99, 10)\n",
    "adam_sensitivity = []\n",
    "\n",
    "for beta1 in beta1_values:\n",
    "    opt = AdvancedOptimizer(method='adam', learning_rate=0.01, beta1=beta1, max_iter=200)\n",
    "    opt.fit(X_train_scaled, y_train)\n",
    "    final_cost = opt.cost_history_[-1]\n",
    "    adam_sensitivity.append(final_cost)\n",
    "\n",
    "axes[1, 1].plot(beta1_values, adam_sensitivity, 'o-', linewidth=2, markersize=6)\n",
    "axes[1, 1].set_xlabel('β₁ (momentum parameter)')\n",
    "axes[1, 1].set_ylabel('Final Cost')\n",
    "axes[1, 1].set_title('Adam β₁ Sensitivity')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence speed comparison (iterations to reach threshold)\n",
    "threshold_cost = min([min(opt.cost_history_) for opt in fitted_optimizers.values()]) * 1.1\n",
    "convergence_speeds = []\n",
    "\n",
    "for name, optimizer in fitted_optimizers.items():\n",
    "    # Find first iteration where cost drops below threshold\n",
    "    try:\n",
    "        conv_iter = next(i for i, cost in enumerate(optimizer.cost_history_) if cost <= threshold_cost)\n",
    "    except StopIteration:\n",
    "        conv_iter = len(optimizer.cost_history_)\n",
    "    convergence_speeds.append(conv_iter)\n",
    "\n",
    "axes[1, 2].bar(method_names, convergence_speeds, alpha=0.7, color='orange')\n",
    "axes[1, 2].set_xlabel('Optimization Method')\n",
    "axes[1, 2].set_ylabel('Iterations to Convergence')\n",
    "axes[1, 2].set_title('Convergence Speed Comparison')\n",
    "axes[1, 2].set_xticks(range(len(method_names)))\n",
    "axes[1, 2].set_xticklabels(method_names, rotation=45)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConvergence Analysis:\")\n",
    "print(f\"Threshold cost for convergence: {threshold_cost:.6f}\")\n",
    "for name, speed in zip(method_names, convergence_speeds):\n",
    "    print(f\"{name}: {speed} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Second-Order Optimization Methods\n",
    "\n",
    "**Question:** Implement Newton's method and quasi-Newton methods (BFGS, L-BFGS). Compare computational complexity and convergence rates with first-order methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Newton's Method:**\n",
    "$$\\theta_{t+1} = \\theta_t - H^{-1}(\\theta_t) \\nabla J(\\theta_t)$$\n",
    "- Quadratic convergence near optimum\n",
    "- Requires Hessian computation and inversion: O(n³) per iteration\n",
    "\n",
    "**BFGS (Broyden-Fletcher-Goldfarb-Shanno):**\n",
    "- Approximates inverse Hessian using gradient information\n",
    "- Update rule: $B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$\n",
    "- Where $s_k = \\theta_{k+1} - \\theta_k$ and $y_k = \\nabla J(\\theta_{k+1}) - \\nabla J(\\theta_k)$\n",
    "\n",
    "**L-BFGS (Limited-memory BFGS):**\n",
    "- Stores only m recent vector pairs instead of full matrix\n",
    "- Memory: O(mn) instead of O(n²)\n",
    "- Two-loop recursion for computing search direction\n",
    "\n",
    "**Convergence Rates:**\n",
    "- Newton: Quadratic (very fast near optimum)\n",
    "- BFGS: Superlinear\n",
    "- L-BFGS: Superlinear (slightly slower than BFGS)\n",
    "- First-order: Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondOrderOptimizer:\n",
    "    \"\"\"\n",
    "    Implementation of second-order optimization methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='newton', memory_size=10, max_iter=100, tol=1e-8, line_search=True):\n",
    "        self.method = method\n",
    "        self.memory_size = memory_size  # For L-BFGS\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.line_search = line_search\n",
    "        self.cost_history_ = []\n",
    "        self.grad_norms_ = []\n",
    "        self.step_sizes_ = []\n",
    "        \n",
    "    def _cost_function(self, X, y, theta):\n",
    "        \"\"\"Logistic regression cost function.\"\"\"\n",
    "        z = X @ theta\n",
    "        # Stable sigmoid computation\n",
    "        z = np.clip(z, -500, 500)\n",
    "        sigmoid = 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        # Binary cross-entropy with regularization\n",
    "        epsilon = 1e-15\n",
    "        sigmoid = np.clip(sigmoid, epsilon, 1 - epsilon)\n",
    "        cost = -np.mean(y * np.log(sigmoid) + (1 - y) * np.log(1 - sigmoid))\n",
    "        return cost\n",
    "    \n",
    "    def _gradient(self, X, y, theta):\n",
    "        \"\"\"Gradient of logistic regression cost.\"\"\"\n",
    "        z = X @ theta\n",
    "        z = np.clip(z, -500, 500)\n",
    "        sigmoid = 1 / (1 + np.exp(-z))\n",
    "        gradient = X.T @ (sigmoid - y) / len(y)\n",
    "        return gradient\n",
    "    \n",
    "    def _hessian(self, X, y, theta):\n",
    "        \"\"\"Hessian of logistic regression cost.\"\"\"\n",
    "        z = X @ theta\n",
    "        z = np.clip(z, -500, 500)\n",
    "        sigmoid = 1 / (1 + np.exp(-z))\n",
    "        weights = sigmoid * (1 - sigmoid) + 1e-8  # Add small regularization\n",
    "        hessian = X.T @ (weights[:, np.newaxis] * X) / len(y)\n",
    "        return hessian\n",
    "    \n",
    "    def _line_search(self, X, y, theta, direction, gradient):\n",
    "        \"\"\"Backtracking line search.\"\"\"\n",
    "        alpha = 1.0\n",
    "        c1 = 1e-4  # Armijo condition parameter\n",
    "        rho = 0.5  # Backtracking parameter\n",
    "        \n",
    "        f0 = self._cost_function(X, y, theta)\n",
    "        grad_dot_dir = np.dot(gradient, direction)\n",
    "        \n",
    "        for _ in range(20):  # Max line search iterations\n",
    "            theta_new = theta + alpha * direction\n",
    "            f_new = self._cost_function(X, y, theta_new)\n",
    "            \n",
    "            # Armijo condition\n",
    "            if f_new <= f0 + c1 * alpha * grad_dot_dir:\n",
    "                return alpha\n",
    "            \n",
    "            alpha *= rho\n",
    "        \n",
    "        return alpha\n",
    "    \n",
    "    def _lbfgs_direction(self, gradient, s_list, y_list, rho_list):\n",
    "        \"\"\"Compute L-BFGS search direction using two-loop recursion.\"\"\"\n",
    "        q = gradient.copy()\n",
    "        alpha_list = []\n",
    "        \n",
    "        # First loop\n",
    "        for i in reversed(range(len(s_list))):\n",
    "            alpha_i = rho_list[i] * np.dot(s_list[i], q)\n",
    "            alpha_list.append(alpha_i)\n",
    "            q = q - alpha_i * y_list[i]\n",
    "        \n",
    "        # Initial Hessian approximation (identity scaled)\n",
    "        if len(s_list) > 0:\n",
    "            gamma = np.dot(s_list[-1], y_list[-1]) / np.dot(y_list[-1], y_list[-1])\n",
    "        else:\n",
    "            gamma = 1.0\n",
    "        \n",
    "        r = gamma * q\n",
    "        \n",
    "        # Second loop\n",
    "        alpha_list.reverse()\n",
    "        for i in range(len(s_list)):\n",
    "            beta = rho_list[i] * np.dot(y_list[i], r)\n",
    "            r = r + s_list[i] * (alpha_list[i] - beta)\n",
    "        \n",
    "        return -r\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model using specified second-order method.\"\"\"\n",
    "        # Add intercept\n",
    "        X_with_intercept = np.column_stack([np.ones(len(X)), X])\n",
    "        n_features = X_with_intercept.shape[1]\n",
    "        \n",
    "        # Initialize\n",
    "        theta = np.random.normal(0, 0.01, n_features)\n",
    "        self.cost_history_ = []\n",
    "        self.grad_norms_ = []\n",
    "        self.step_sizes_ = []\n",
    "        \n",
    "        # For BFGS/L-BFGS\n",
    "        if self.method in ['bfgs', 'lbfgs']:\n",
    "            if self.method == 'bfgs':\n",
    "                B_inv = np.eye(n_features)  # Initial inverse Hessian approximation\n",
    "            else:  # L-BFGS\n",
    "                s_list, y_list, rho_list = [], [], []\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Compute cost and gradient\n",
    "            cost = self._cost_function(X_with_intercept, y, theta)\n",
    "            gradient = self._gradient(X_with_intercept, y, theta)\n",
    "            grad_norm = np.linalg.norm(gradient)\n",
    "            \n",
    "            self.cost_history_.append(cost)\n",
    "            self.grad_norms_.append(grad_norm)\n",
    "            \n",
    "            # Check convergence\n",
    "            if grad_norm < self.tol:\n",
    "                break\n",
    "            \n",
    "            # Compute search direction\n",
    "            if self.method == 'newton':\n",
    "                hessian = self._hessian(X_with_intercept, y, theta)\n",
    "                try:\n",
    "                    direction = -np.linalg.solve(hessian, gradient)\n",
    "                except np.linalg.LinAlgError:\n",
    "                    # Fallback to gradient descent if Hessian is singular\n",
    "                    direction = -gradient\n",
    "                    \n",
    "            elif self.method == 'bfgs':\n",
    "                direction = -B_inv @ gradient\n",
    "                \n",
    "            elif self.method == 'lbfgs':\n",
    "                direction = self._lbfgs_direction(gradient, s_list, y_list, rho_list)\n",
    "            \n",
    "            # Line search\n",
    "            if self.line_search:\n",
    "                step_size = self._line_search(X_with_intercept, y, theta, direction, gradient)\n",
    "            else:\n",
    "                step_size = 1.0\n",
    "            \n",
    "            self.step_sizes_.append(step_size)\n",
    "            \n",
    "            # Update parameters\n",
    "            theta_new = theta + step_size * direction\n",
    "            \n",
    "            # Update inverse Hessian approximation for BFGS methods\n",
    "            if self.method in ['bfgs', 'lbfgs'] and iteration > 0:\n",
    "                s = theta_new - theta\n",
    "                y = self._gradient(X_with_intercept, y, theta_new) - gradient\n",
    "                \n",
    "                if np.dot(s, y) > 1e-10:  # Curvature condition\n",
    "                    if self.method == 'bfgs':\n",
    "                        # BFGS update\n",
    "                        rho = 1.0 / np.dot(y, s)\n",
    "                        A1 = np.eye(n_features) - rho * np.outer(s, y)\n",
    "                        A2 = np.eye(n_features) - rho * np.outer(y, s)\n",
    "                        B_inv = A1 @ B_inv @ A2 + rho * np.outer(s, s)\n",
    "                        \n",
    "                    else:  # L-BFGS\n",
    "                        rho = 1.0 / np.dot(y, s)\n",
    "                        \n",
    "                        if len(s_list) >= self.memory_size:\n",
    "                            s_list.pop(0)\n",
    "                            y_list.pop(0)\n",
    "                            rho_list.pop(0)\n",
    "                        \n",
    "                        s_list.append(s)\n",
    "                        y_list.append(y)\n",
    "                        rho_list.append(rho)\n",
    "            \n",
    "            theta = theta_new\n",
    "        \n",
    "        self.theta_ = theta\n",
    "        self.intercept_ = theta[0]\n",
    "        self.coef_ = theta[1:]\n",
    "        self.n_iter_ = len(self.cost_history_)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        z = X @ self.coef_ + self.intercept_\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make binary predictions.\"\"\"\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "# Generate logistic regression data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_redundant=0, \n",
    "                         n_informative=15, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare second-order methods\n",
    "second_order_methods = {\n",
    "    'Newton': SecondOrderOptimizer(method='newton', max_iter=50),\n",
    "    'BFGS': SecondOrderOptimizer(method='bfgs', max_iter=100),\n",
    "    'L-BFGS': SecondOrderOptimizer(method='lbfgs', memory_size=10, max_iter=100),\n",
    "    'Adam (reference)': AdvancedOptimizer(method='adam', learning_rate=0.01, max_iter=200)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "fitted_second_order = {}\n",
    "\n",
    "import time\n",
    "\n",
    "for name, optimizer in second_order_methods.items():\n",
    "    start_time = time.time()\n",
    "    optimizer.fit(X_train_scaled, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    if hasattr(optimizer, 'predict_proba'):\n",
    "        y_pred_proba = optimizer.predict_proba(X_test_scaled)\n",
    "        if len(y_pred_proba.shape) > 1:\n",
    "            y_pred_proba = y_pred_proba[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = optimizer.predict_proba(X_test_scaled)\n",
    "    \n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    if hasattr(optimizer, 'n_iter_'):\n",
    "        iterations = optimizer.n_iter_\n",
    "    else:\n",
    "        iterations = len(optimizer.cost_history_)\n",
    "    \n",
    "    final_grad_norm = optimizer.grad_norms_[-1] if optimizer.grad_norms_ else 'N/A'\n",
    "    \n",
    "    results[name] = {\n",
    "        'Log Loss': logloss,\n",
    "        'Iterations': iterations,\n",
    "        'Training Time (s)': training_time,\n",
    "        'Final Grad Norm': final_grad_norm,\n",
    "        'Time per Iter': training_time / iterations if iterations > 0 else 0\n",
    "    }\n",
    "    fitted_second_order[name] = optimizer\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Second-Order Optimization Methods Comparison:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize second-order methods behavior\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Cost convergence comparison\n",
    "for name, optimizer in fitted_second_order.items():\n",
    "    if optimizer.cost_history_:\n",
    "        axes[0, 0].semilogy(optimizer.cost_history_, label=name, alpha=0.8, linewidth=2, marker='o', markersize=3)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Cost (log scale)')\n",
    "axes[0, 0].set_title('Convergence Speed Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm convergence\n",
    "for name, optimizer in fitted_second_order.items():\n",
    "    if optimizer.grad_norms_:\n",
    "        axes[0, 1].semilogy(optimizer.grad_norms_, label=name, alpha=0.8, linewidth=2, marker='s', markersize=3)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Gradient Norm (log scale)')\n",
    "axes[0, 1].set_title('Gradient Norm Convergence')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Step sizes (for second-order methods with line search)\n",
    "for name, optimizer in fitted_second_order.items():\n",
    "    if hasattr(optimizer, 'step_sizes_') and optimizer.step_sizes_:\n",
    "        axes[0, 2].plot(optimizer.step_sizes_, label=name, alpha=0.8, linewidth=2, marker='^', markersize=3)\n",
    "axes[0, 2].set_xlabel('Iteration')\n",
    "axes[0, 2].set_ylabel('Step Size')\n",
    "axes[0, 2].set_title('Step Size Evolution')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance vs computational cost\n",
    "method_names = list(results.keys())\n",
    "log_losses = [results[name]['Log Loss'] for name in method_names]\n",
    "training_times = [results[name]['Training Time (s)'] for name in method_names]\n",
    "iterations = [results[name]['Iterations'] for name in method_names]\n",
    "\n",
    "scatter = axes[1, 0].scatter(training_times, log_losses, c=iterations, s=100, alpha=0.7, cmap='viridis')\n",
    "for i, name in enumerate(method_names):\n",
    "    axes[1, 0].annotate(name, (training_times[i], log_losses[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1, 0].set_xlabel('Training Time (seconds)')\n",
    "axes[1, 0].set_ylabel('Test Log Loss')\n",
    "axes[1, 0].set_title('Performance vs Computational Cost')\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label='Iterations')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# L-BFGS memory size sensitivity\n",
    "memory_sizes = [3, 5, 10, 20, 50]\n",
    "lbfgs_performance = []\n",
    "lbfgs_times = []\n",
    "\n",
    "for m in memory_sizes:\n",
    "    start_time = time.time()\n",
    "    lbfgs = SecondOrderOptimizer(method='lbfgs', memory_size=m, max_iter=100)\n",
    "    lbfgs.fit(X_train_scaled, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    y_pred_proba = lbfgs.predict_proba(X_test_scaled)\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    lbfgs_performance.append(logloss)\n",
    "    lbfgs_times.append(training_time)\n",
    "\n",
    "axes[1, 1].plot(memory_sizes, lbfgs_performance, 'o-', linewidth=2, markersize=8, label='Log Loss')\n",
    "ax_twin = axes[1, 1].twinx()\n",
    "ax_twin.plot(memory_sizes, lbfgs_times, 's-', color='orange', linewidth=2, markersize=8, label='Training Time')\n",
    "\n",
    "axes[1, 1].set_xlabel('L-BFGS Memory Size')\n",
    "axes[1, 1].set_ylabel('Test Log Loss', color='blue')\n",
    "ax_twin.set_ylabel('Training Time (s)', color='orange')\n",
    "axes[1, 1].set_title('L-BFGS Memory Size Effect')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence rate analysis (log-log plot)\n",
    "newton_opt = fitted_second_order['Newton']\n",
    "if newton_opt.grad_norms_:\n",
    "    # Show quadratic convergence of Newton's method\n",
    "    grad_norms = np.array(newton_opt.grad_norms_[1:])  # Skip first point\n",
    "    iterations_range = np.arange(1, len(grad_norms) + 1)\n",
    "    \n",
    "    # Fit convergence rate: ||∇f|| ≈ C * r^k\n",
    "    if len(grad_norms) > 3:\n",
    "        log_grad_norms = np.log(grad_norms[grad_norms > 0])\n",
    "        log_iterations = np.log(iterations_range[:len(log_grad_norms)])\n",
    "        \n",
    "        # Linear fit in log space\n",
    "        coeffs = np.polyfit(log_iterations, log_grad_norms, 1)\n",
    "        slope = coeffs[0]\n",
    "        \n",
    "        axes[1, 2].loglog(iterations_range[:len(grad_norms)], grad_norms, 'o-', \n",
    "                         label=f'Newton (slope≈{slope:.2f})', linewidth=2)\n",
    "\n",
    "# Compare with first-order method\n",
    "adam_opt = fitted_second_order['Adam (reference)']\n",
    "if adam_opt.grad_norms_:\n",
    "    adam_grad_norms = np.array(adam_opt.grad_norms_[1:20])  # First 20 iterations\n",
    "    adam_iterations = np.arange(1, len(adam_grad_norms) + 1)\n",
    "    axes[1, 2].loglog(adam_iterations, adam_grad_norms, 's-', \n",
    "                     label='Adam (first-order)', linewidth=2, alpha=0.7)\n",
    "\n",
    "axes[1, 2].set_xlabel('Iteration')\n",
    "axes[1, 2].set_ylabel('Gradient Norm')\n",
    "axes[1, 2].set_title('Convergence Rate Analysis')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nComputational Complexity Analysis:\")\n",
    "print(f\"Newton's method - Average time per iteration: {results['Newton']['Time per Iter']:.4f}s\")\n",
    "print(f\"BFGS - Average time per iteration: {results['BFGS']['Time per Iter']:.4f}s\")\n",
    "print(f\"L-BFGS - Average time per iteration: {results['L-BFGS']['Time per Iter']:.4f}s\")\n",
    "print(f\"Adam - Average time per iteration: {results['Adam (reference)']['Time per Iter']:.4f}s\")\n",
    "\n",
    "print(\"\\nConvergence Efficiency (Log Loss / Time):\")\n",
    "for name in method_names:\n",
    "    efficiency = results[name]['Log Loss'] / results[name]['Training Time (s)']\n",
    "    print(f\"{name}: {efficiency:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Constrained Optimization and Lagrangian Methods\n",
    "\n",
    "**Question:** Implement constrained optimization using Lagrange multipliers for SVM-like problems. Compare penalty methods with barrier methods for handling constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Constrained Optimization Problem:**\n",
    "$$\\min_{x} f(x) \\quad \\text{subject to} \\quad g_i(x) \\leq 0, \\quad h_j(x) = 0$$\n",
    "\n",
    "**Lagrangian Function:**\n",
    "$$L(x, \\lambda, \\mu) = f(x) + \\sum_i \\lambda_i g_i(x) + \\sum_j \\mu_j h_j(x)$$\n",
    "\n",
    "**KKT Conditions:**\n",
    "1. Stationarity: $\\nabla f(x^*) + \\sum_i \\lambda_i^* \\nabla g_i(x^*) + \\sum_j \\mu_j^* \\nabla h_j(x^*) = 0$\n",
    "2. Primal feasibility: $g_i(x^*) \\leq 0, \\quad h_j(x^*) = 0$\n",
    "3. Dual feasibility: $\\lambda_i^* \\geq 0$\n",
    "4. Complementary slackness: $\\lambda_i^* g_i(x^*) = 0$\n",
    "\n",
    "**Penalty Method:**\n",
    "$$\\min_{x} f(x) + \\rho \\sum_i \\max(0, g_i(x))^2 + \\rho \\sum_j h_j(x)^2$$\n",
    "\n",
    "**Barrier Method:**\n",
    "$$\\min_{x} f(x) - \\mu \\sum_i \\log(-g_i(x))$$\n",
    "(for inequality constraints $g_i(x) < 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedOptimizer:\n",
    "    \"\"\"\n",
    "    Constrained optimization using penalty and barrier methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='penalty', penalty_param=1.0, barrier_param=1.0, \n",
    "                 max_iter=100, tol=1e-6):\n",
    "        self.method = method\n",
    "        self.penalty_param = penalty_param\n",
    "        self.barrier_param = barrier_param\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.cost_history_ = []\n",
    "        self.penalty_history_ = []\n",
    "        self.constraint_violations_ = []\n",
    "        \n",
    "    def _svm_objective(self, w, b, X, y):\n",
    "        \"\"\"SVM objective function: ||w||² / 2\"\"\"\n",
    "        return 0.5 * np.dot(w, w)\n",
    "    \n",
    "    def _svm_constraints(self, w, b, X, y):\n",
    "        \"\"\"SVM constraints: y_i(w^T x_i + b) >= 1\"\"\"\n",
    "        # Convert to g_i(x) <= 0 form: 1 - y_i(w^T x_i + b) <= 0\n",
    "        margins = y * (X @ w + b)\n",
    "        return 1 - margins  # <= 0 for feasible points\n",
    "    \n",
    "    def _penalty_function(self, w, b, X, y, rho):\n",
    "        \"\"\"Penalty method objective function.\"\"\"\n",
    "        objective = self._svm_objective(w, b, X, y)\n",
    "        constraints = self._svm_constraints(w, b, X, y)\n",
    "        \n",
    "        # Quadratic penalty for violated constraints\n",
    "        penalty = rho * np.sum(np.maximum(0, constraints) ** 2)\n",
    "        \n",
    "        return objective + penalty, penalty\n",
    "    \n",
    "    def _barrier_function(self, w, b, X, y, mu):\n",
    "        \"\"\"Barrier method objective function.\"\"\"\n",
    "        objective = self._svm_objective(w, b, X, y)\n",
    "        constraints = self._svm_constraints(w, b, X, y)\n",
    "        \n",
    "        # Log barrier for inequality constraints\n",
    "        # Check if any constraint is violated (constraint > 0)\n",
    "        if np.any(constraints >= -1e-8):  # Small tolerance for numerical issues\n",
    "            return float('inf'), float('inf')\n",
    "        \n",
    "        barrier = -mu * np.sum(np.log(-constraints))\n",
    "        \n",
    "        return objective + barrier, barrier\n",
    "    \n",
    "    def _gradient_penalty(self, w, b, X, y, rho):\n",
    "        \"\"\"Gradient of penalty function.\"\"\"\n",
    "        # Objective gradient\n",
    "        grad_w_obj = w\n",
    "        grad_b_obj = 0\n",
    "        \n",
    "        # Constraint gradients\n",
    "        constraints = self._svm_constraints(w, b, X, y)\n",
    "        violated = constraints > 0\n",
    "        \n",
    "        if np.any(violated):\n",
    "            grad_w_penalty = -2 * rho * np.sum(\n",
    "                (constraints[violated, np.newaxis] * y[violated, np.newaxis] * X[violated]), axis=0\n",
    "            )\n",
    "            grad_b_penalty = -2 * rho * np.sum(constraints[violated] * y[violated])\n",
    "        else:\n",
    "            grad_w_penalty = np.zeros_like(w)\n",
    "            grad_b_penalty = 0\n",
    "        \n",
    "        return grad_w_obj + grad_w_penalty, grad_b_obj + grad_b_penalty\n",
    "    \n",
    "    def _gradient_barrier(self, w, b, X, y, mu):\n",
    "        \"\"\"Gradient of barrier function.\"\"\"\n",
    "        # Objective gradient\n",
    "        grad_w_obj = w\n",
    "        grad_b_obj = 0\n",
    "        \n",
    "        # Barrier gradients\n",
    "        constraints = self._svm_constraints(w, b, X, y)\n",
    "        \n",
    "        # Check feasibility\n",
    "        if np.any(constraints >= -1e-8):\n",
    "            return np.full_like(w, float('inf')), float('inf')\n",
    "        \n",
    "        barrier_weights = mu / constraints\n",
    "        grad_w_barrier = np.sum(\n",
    "            (barrier_weights[:, np.newaxis] * y[:, np.newaxis] * X), axis=0\n",
    "        )\n",
    "        grad_b_barrier = np.sum(barrier_weights * y)\n",
    "        \n",
    "        return grad_w_obj + grad_w_barrier, grad_b_obj + grad_b_barrier\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Solve SVM using constrained optimization.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        w = np.random.normal(0, 0.1, n_features)\n",
    "        b = 0.0\n",
    "        \n",
    "        self.cost_history_ = []\n",
    "        self.penalty_history_ = []\n",
    "        self.constraint_violations_ = []\n",
    "        \n",
    "        # Adaptive penalty/barrier parameter\n",
    "        rho = self.penalty_param\n",
    "        mu = self.barrier_param\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            if self.method == 'penalty':\n",
    "                # Penalty method\n",
    "                cost, penalty = self._penalty_function(w, b, X, y, rho)\n",
    "                grad_w, grad_b = self._gradient_penalty(w, b, X, y, rho)\n",
    "                \n",
    "                # Update with gradient descent\n",
    "                learning_rate = 0.01 / (1 + iteration * 0.01)\n",
    "                w = w - learning_rate * grad_w\n",
    "                b = b - learning_rate * grad_b\n",
    "                \n",
    "                # Increase penalty parameter\n",
    "                if iteration % 20 == 19:\n",
    "                    rho *= 2\n",
    "                    \n",
    "            elif self.method == 'barrier':\n",
    "                # Barrier method\n",
    "                cost, barrier = self._barrier_function(w, b, X, y, mu)\n",
    "                \n",
    "                if cost == float('inf'):\n",
    "                    # Project to feasible region\n",
    "                    constraints = self._svm_constraints(w, b, X, y)\n",
    "                    violated_idx = np.where(constraints > 0)[0]\n",
    "                    \n",
    "                    if len(violated_idx) > 0:\n",
    "                        # Simple projection: reduce w magnitude\n",
    "                        w *= 0.9\n",
    "                        b *= 0.9\n",
    "                    continue\n",
    "                \n",
    "                grad_w, grad_b = self._gradient_barrier(w, b, X, y, mu)\n",
    "                \n",
    "                if np.any(np.isinf(grad_w)) or np.isinf(grad_b):\n",
    "                    # Projection step\n",
    "                    w *= 0.95\n",
    "                    b *= 0.95\n",
    "                    continue\n",
    "                \n",
    "                # Update with gradient descent\n",
    "                learning_rate = 0.001 / (1 + iteration * 0.01)\n",
    "                w = w - learning_rate * grad_w\n",
    "                b = b - learning_rate * grad_b\n",
    "                \n",
    "                # Decrease barrier parameter\n",
    "                if iteration % 20 == 19:\n",
    "                    mu *= 0.5\n",
    "                    \n",
    "                penalty = barrier\n",
    "            \n",
    "            # Record history\n",
    "            constraints = self._svm_constraints(w, b, X, y)\n",
    "            max_violation = np.maximum(0, constraints).max()\n",
    "            \n",
    "            self.cost_history_.append(cost)\n",
    "            self.penalty_history_.append(penalty)\n",
    "            self.constraint_violations_.append(max_violation)\n",
    "            \n",
    "            # Check convergence\n",
    "            if len(self.cost_history_) > 1:\n",
    "                cost_change = abs(self.cost_history_[-1] - self.cost_history_[-2])\n",
    "                if cost_change < self.tol and max_violation < self.tol:\n",
    "                    break\n",
    "        \n",
    "        self.w_ = w\n",
    "        self.b_ = b\n",
    "        self.n_iter_ = len(self.cost_history_)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return np.sign(X @ self.w_ + self.b_)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute decision function values.\"\"\"\n",
    "        return X @ self.w_ + self.b_\n",
    "\n",
    "# Generate linearly separable data for SVM\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X = np.random.randn(n_samples, 2)\n",
    "# Create a clear separation\n",
    "y = np.where(X[:, 0] + X[:, 1] > 0, 1, -1)\n",
    "# Add some noise to make it more interesting\n",
    "noise_idx = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "y[noise_idx] *= -1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Compare penalty and barrier methods\n",
    "constrained_optimizers = {\n",
    "    'Penalty Method': ConstrainedOptimizer(method='penalty', penalty_param=1.0, max_iter=200),\n",
    "    'Barrier Method': ConstrainedOptimizer(method='barrier', barrier_param=1.0, max_iter=200)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "fitted_constrained = {}\n",
    "\n",
    "for name, optimizer in constrained_optimizers.items():\n",
    "    try:\n",
    "        optimizer.fit(X_train, y_train)\n",
    "        y_pred = optimizer.predict(X_test)\n",
    "        \n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        final_violation = optimizer.constraint_violations_[-1]\n",
    "        objective_value = 0.5 * np.dot(optimizer.w_, optimizer.w_)\n",
    "        \n",
    "        results[name] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Final Constraint Violation': final_violation,\n",
    "            'Objective Value': objective_value,\n",
    "            'Iterations': optimizer.n_iter_\n",
    "        }\n",
    "        fitted_constrained[name] = optimizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {name}: {e}\")\n",
    "        results[name] = {'Error': str(e)}\n",
    "\n",
    "# Add sklearn SVM for comparison\n",
    "from sklearn.svm import SVC\n",
    "svm_sklearn = SVC(kernel='linear', C=1000)  # High C for hard margin\n",
    "svm_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test)\n",
    "\n",
    "results['sklearn SVM'] = {\n",
    "    'Accuracy': np.mean(y_pred_sklearn == y_test),\n",
    "    'Final Constraint Violation': 0,  # Assumes perfect optimization\n",
    "    'Objective Value': 0.5 * np.sum(svm_sklearn.coef_[0] ** 2),\n",
    "    'Iterations': 'N/A'\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Constrained Optimization Methods Comparison:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize constrained optimization behavior\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Cost evolution\n",
    "for name, optimizer in fitted_constrained.items():\n",
    "    axes[0, 0].semilogy(optimizer.cost_history_, label=name, alpha=0.8, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Cost (log scale)')\n",
    "axes[0, 0].set_title('Cost Function Evolution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Constraint violations\n",
    "for name, optimizer in fitted_constrained.items():\n",
    "    axes[0, 1].semilogy(optimizer.constraint_violations_, label=name, alpha=0.8, linewidth=2)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Max Constraint Violation (log scale)')\n",
    "axes[0, 1].set_title('Constraint Satisfaction')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Penalty/Barrier term evolution\n",
    "for name, optimizer in fitted_constrained.items():\n",
    "    if optimizer.penalty_history_:\n",
    "        penalty_values = [p for p in optimizer.penalty_history_ if p != float('inf')]\n",
    "        if penalty_values:\n",
    "            axes[0, 2].semilogy(penalty_values, label=f'{name} Penalty/Barrier', alpha=0.8, linewidth=2)\n",
    "axes[0, 2].set_xlabel('Iteration')\n",
    "axes[0, 2].set_ylabel('Penalty/Barrier Term (log scale)')\n",
    "axes[0, 2].set_title('Penalty/Barrier Evolution')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundaries visualization\n",
    "if fitted_constrained:\n",
    "    # Create mesh for decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Plot for penalty method\n",
    "    if 'Penalty Method' in fitted_constrained:\n",
    "        penalty_opt = fitted_constrained['Penalty Method']\n",
    "        mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = penalty_opt.decision_function(mesh_points)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axes[1, 0].contour(xx, yy, Z, levels=[0], colors='black', linestyles='--', linewidths=2)\n",
    "        axes[1, 0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', alpha=0.8)\n",
    "        axes[1, 0].set_title('Penalty Method Decision Boundary')\n",
    "        axes[1, 0].set_xlabel('Feature 1')\n",
    "        axes[1, 0].set_ylabel('Feature 2')\n",
    "    \n",
    "    # Plot for barrier method\n",
    "    if 'Barrier Method' in fitted_constrained:\n",
    "        barrier_opt = fitted_constrained['Barrier Method']\n",
    "        Z = barrier_opt.decision_function(mesh_points)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axes[1, 1].contour(xx, yy, Z, levels=[0], colors='black', linestyles='--', linewidths=2)\n",
    "        axes[1, 1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', alpha=0.8)\n",
    "        axes[1, 1].set_title('Barrier Method Decision Boundary')\n",
    "        axes[1, 1].set_xlabel('Feature 1')\n",
    "        axes[1, 1].set_ylabel('Feature 2')\n",
    "\n",
    "# Performance comparison\n",
    "method_names = [name for name in results.keys() if 'Error' not in results[name]]\n",
    "accuracies = [results[name]['Accuracy'] for name in method_names]\n",
    "objective_values = [results[name]['Objective Value'] for name in method_names]\n",
    "\n",
    "x_pos = np.arange(len(method_names))\n",
    "width = 0.35\n",
    "\n",
    "ax1 = axes[1, 2]\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "bars1 = ax1.bar(x_pos - width/2, accuracies, width, label='Accuracy', alpha=0.7, color='blue')\n",
    "bars2 = ax2.bar(x_pos + width/2, objective_values, width, label='Objective Value', alpha=0.7, color='orange')\n",
    "\n",
    "ax1.set_xlabel('Method')\n",
    "ax1.set_ylabel('Accuracy', color='blue')\n",
    "ax2.set_ylabel('Objective Value', color='orange')\n",
    "ax1.set_title('Performance Comparison')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(method_names, rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars1, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for bar, val in zip(bars2, objective_values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.05,\n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of constraint satisfaction\n",
    "print(\"\\nConstraint Satisfaction Analysis:\")\n",
    "for name, optimizer in fitted_constrained.items():\n",
    "    # Check final constraint satisfaction\n",
    "    constraints = optimizer._svm_constraints(optimizer.w_, optimizer.b_, X_train, y_train)\n",
    "    n_violated = np.sum(constraints > 1e-6)\n",
    "    max_violation = np.maximum(0, constraints).max()\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Constraints violated: {n_violated}/{len(constraints)}\")\n",
    "    print(f\"  Maximum violation: {max_violation:.6f}\")\n",
    "    print(f\"  Final objective: {results[name]['Objective Value']:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Optimization Challenges in High-Dimensional Spaces\n",
    "\n",
    "**Question:** Analyze the curse of dimensionality in optimization. Implement coordinate descent and demonstrate its effectiveness on high-dimensional problems compared to full gradient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Curse of Dimensionality in Optimization:**\n",
    "1. **Local minima proliferation**: Number of critical points grows exponentially\n",
    "2. **Gradient vanishing**: Gradients become sparse in high dimensions\n",
    "3. **Saddle point dominance**: Most critical points are saddle points, not minima\n",
    "4. **Poor conditioning**: Hessian eigenvalue spectrum becomes poorly conditioned\n",
    "\n",
    "**Coordinate Descent:**\n",
    "$$\\theta_j^{(t+1)} = \\arg\\min_{\\theta_j} f(\\theta_1^{(t+1)}, \\ldots, \\theta_{j-1}^{(t+1)}, \\theta_j, \\theta_{j+1}^{(t)}, \\ldots, \\theta_d^{(t)})$$\n",
    "\n",
    "**Advantages in High Dimensions:**\n",
    "- Avoids computing full gradient (O(d) → O(1) per coordinate)\n",
    "- Natural for sparsity-inducing problems\n",
    "- Can handle non-differentiable objectives\n",
    "- Memory efficient\n",
    "\n",
    "**Block Coordinate Descent:**\n",
    "Updates blocks of coordinates simultaneously:\n",
    "$$\\theta_{B_k}^{(t+1)} = \\arg\\min_{\\theta_{B_k}} f(\\theta_{B_1}^{(t+1)}, \\ldots, \\theta_{B_{k-1}}^{(t+1)}, \\theta_{B_k}, \\theta_{B_{k+1}}^{(t)}, \\ldots)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighDimensionalOptimizer:\n",
    "    \"\"\"\n",
    "    Optimizer for high-dimensional problems comparing different approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='coordinate_descent', block_size=1, max_iter=1000, tol=1e-6, \n",
    "                 learning_rate=0.01):\n",
    "        self.method = method\n",
    "        self.block_size = block_size\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cost_history_ = []\n",
    "        self.grad_norms_ = []\n",
    "        self.coord_updates_ = []  # Track which coordinates are updated\n",
    "        \n",
    "    def _sparse_quadratic_objective(self, theta, A, b, lambda_reg=0.01):\n",
    "        \"\"\"Sparse quadratic objective: 1/2 * theta^T A theta - b^T theta + lambda * ||theta||_1\"\"\"\n",
    "        quadratic_term = 0.5 * theta.T @ A @ theta\n",
    "        linear_term = -b.T @ theta\n",
    "        l1_term = lambda_reg * np.sum(np.abs(theta))\n",
    "        return quadratic_term + linear_term + l1_term\n",
    "    \n",
    "    def _gradient_sparse_quadratic(self, theta, A, b):\n",
    "        \"\"\"Gradient of quadratic part (without L1 term).\"\"\"\n",
    "        return A @ theta - b\n",
    "    \n",
    "    def _coordinate_descent_update(self, theta, A, b, lambda_reg, j):\n",
    "        \"\"\"Single coordinate update for sparse quadratic objective.\"\"\"\n",
    "        # Compute partial residual\n",
    "        r_j = A[j, :] @ theta - A[j, j] * theta[j] - b[j]\n",
    "        \n",
    "        # Soft thresholding\n",
    "        z_j = -r_j / A[j, j] if A[j, j] != 0 else 0\n",
    "        \n",
    "        # L1 soft thresholding\n",
    "        if A[j, j] != 0:\n",
    "            lambda_scaled = lambda_reg / A[j, j]\n",
    "            theta_new_j = np.sign(z_j) * max(0, abs(z_j) - lambda_scaled)\n",
    "        else:\n",
    "            theta_new_j = 0\n",
    "            \n",
    "        return theta_new_j\n",
    "    \n",
    "    def _create_high_dim_problem(self, n_features, sparsity=0.1, condition_number=100, random_state=42):\n",
    "        \"\"\"Create a high-dimensional sparse optimization problem.\"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Create a well-conditioned positive definite matrix\n",
    "        eigenvals = np.logspace(0, np.log10(condition_number), n_features)\n",
    "        Q = np.random.randn(n_features, n_features)\n",
    "        Q, _ = np.linalg.qr(Q)  # Orthogonal matrix\n",
    "        A = Q @ np.diag(eigenvals) @ Q.T\n",
    "        \n",
    "        # Create sparse true solution\n",
    "        theta_true = np.zeros(n_features)\n",
    "        n_nonzero = int(sparsity * n_features)\n",
    "        nonzero_idx = np.random.choice(n_features, n_nonzero, replace=False)\n",
    "        theta_true[nonzero_idx] = np.random.randn(n_nonzero) * 2\n",
    "        \n",
    "        # Create b such that theta_true is close to optimum\n",
    "        b = A @ theta_true + np.random.randn(n_features) * 0.1\n",
    "        \n",
    "        return A, b, theta_true\n",
    "    \n",
    "    def fit(self, A, b, lambda_reg=0.01, theta_init=None):\n",
    "        \"\"\"Fit the high-dimensional optimization problem.\"\"\"\n",
    "        n_features = len(b)\n",
    "        \n",
    "        if theta_init is None:\n",
    "            theta = np.zeros(n_features)\n",
    "        else:\n",
    "            theta = theta_init.copy()\n",
    "        \n",
    "        self.cost_history_ = []\n",
    "        self.grad_norms_ = []\n",
    "        self.coord_updates_ = []\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            theta_old = theta.copy()\n",
    "            \n",
    "            if self.method == 'coordinate_descent':\n",
    "                # Cyclic coordinate descent\n",
    "                for j in range(n_features):\n",
    "                    theta[j] = self._coordinate_descent_update(theta, A, b, lambda_reg, j)\n",
    "                    \n",
    "            elif self.method == 'random_coordinate_descent':\n",
    "                # Random coordinate descent\n",
    "                j = np.random.randint(n_features)\n",
    "                theta[j] = self._coordinate_descent_update(theta, A, b, lambda_reg, j)\n",
    "                self.coord_updates_.append(j)\n",
    "                \n",
    "            elif self.method == 'block_coordinate_descent':\n",
    "                # Block coordinate descent\n",
    "                n_blocks = n_features // self.block_size\n",
    "                for block in range(n_blocks):\n",
    "                    start_idx = block * self.block_size\n",
    "                    end_idx = min((block + 1) * self.block_size, n_features)\n",
    "                    \n",
    "                    for j in range(start_idx, end_idx):\n",
    "                        theta[j] = self._coordinate_descent_update(theta, A, b, lambda_reg, j)\n",
    "                        \n",
    "            elif self.method == 'gradient_descent':\n",
    "                # Full gradient descent (ignoring L1 term for simplicity)\n",
    "                gradient = self._gradient_sparse_quadratic(theta, A, b)\n",
    "                theta = theta - self.learning_rate * gradient\n",
    "                \n",
    "                # Apply soft thresholding for L1 regularization\n",
    "                theta = np.sign(theta) * np.maximum(np.abs(theta) - self.learning_rate * lambda_reg, 0)\n",
    "            \n",
    "            # Record history\n",
    "            cost = self._sparse_quadratic_objective(theta, A, b, lambda_reg)\n",
    "            gradient = self._gradient_sparse_quadratic(theta, A, b)\n",
    "            grad_norm = np.linalg.norm(gradient)\n",
    "            \n",
    "            self.cost_history_.append(cost)\n",
    "            self.grad_norms_.append(grad_norm)\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(theta - theta_old) < self.tol:\n",
    "                break\n",
    "        \n",
    "        self.theta_ = theta\n",
    "        self.n_iter_ = len(self.cost_history_)\n",
    "        return self\n",
    "\n",
    "# Test high-dimensional optimization\n",
    "dimensions = [100, 500, 1000, 2000]\n",
    "methods = {\n",
    "    'Coordinate Descent': 'coordinate_descent',\n",
    "    'Random CD': 'random_coordinate_descent', \n",
    "    'Block CD (10)': 'block_coordinate_descent',\n",
    "    'Gradient Descent': 'gradient_descent'\n",
    "}\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"\\nTesting dimension: {dim}\")\n",
    "    \n",
    "    # Create problem\n",
    "    optimizer = HighDimensionalOptimizer()\n",
    "    A, b, theta_true = optimizer._create_high_dim_problem(dim, sparsity=0.05, condition_number=50)\n",
    "    \n",
    "    for method_name, method_key in methods.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if method_key == 'block_coordinate_descent':\n",
    "            opt = HighDimensionalOptimizer(method=method_key, block_size=10, max_iter=1000)\n",
    "        elif method_key == 'gradient_descent':\n",
    "            opt = HighDimensionalOptimizer(method=method_key, learning_rate=0.001, max_iter=1000)\n",
    "        else:\n",
    "            opt = HighDimensionalOptimizer(method=method_key, max_iter=1000)\n",
    "        \n",
    "        try:\n",
    "            opt.fit(A, b, lambda_reg=0.01)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            final_cost = opt.cost_history_[-1]\n",
    "            final_grad_norm = opt.grad_norms_[-1]\n",
    "            n_nonzero = np.sum(np.abs(opt.theta_) > 1e-6)\n",
    "            recovery_error = np.linalg.norm(opt.theta_ - theta_true)\n",
    "            \n",
    "            performance_results.append({\n",
    "                'Dimension': dim,\n",
    "                'Method': method_name,\n",
    "                'Final Cost': final_cost,\n",
    "                'Training Time': training_time,\n",
    "                'Iterations': opt.n_iter_,\n",
    "                'Final Grad Norm': final_grad_norm,\n",
    "                'Non-zero Coeffs': n_nonzero,\n",
    "                'Recovery Error': recovery_error\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name} at dim {dim}: {e}\")\n",
    "            performance_results.append({\n",
    "                'Dimension': dim,\n",
    "                'Method': method_name,\n",
    "                'Error': str(e)\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(performance_results)\n",
    "successful_results = results_df[~results_df.get('Error', pd.Series(dtype='object')).notna()]\n",
    "\n",
    "print(\"\\nHigh-Dimensional Optimization Results:\")\n",
    "for dim in dimensions:\n",
    "    print(f\"\\nDimension {dim}:\")\n",
    "    dim_results = successful_results[successful_results['Dimension'] == dim]\n",
    "    if not dim_results.empty:\n",
    "        print(dim_results[['Method', 'Final Cost', 'Training Time', 'Iterations', 'Non-zero Coeffs']].round(4))\n",
    "    else:\n",
    "        print(\"No successful results for this dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis for a specific dimension\n",
    "analysis_dim = 1000\n",
    "print(f\"\\nDetailed Analysis for {analysis_dim} dimensions:\")\n",
    "\n",
    "# Create problem for detailed analysis\n",
    "optimizer = HighDimensionalOptimizer()\n",
    "A, b, theta_true = optimizer._create_high_dim_problem(analysis_dim, sparsity=0.05, condition_number=50)\n",
    "\n",
    "# Run all methods and store detailed results\n",
    "detailed_optimizers = {}\n",
    "detailed_results = {}\n",
    "\n",
    "methods_detailed = {\n",
    "    'Coordinate Descent': HighDimensionalOptimizer(method='coordinate_descent', max_iter=200),\n",
    "    'Random CD': HighDimensionalOptimizer(method='random_coordinate_descent', max_iter=200*analysis_dim),\n",
    "    'Block CD': HighDimensionalOptimizer(method='block_coordinate_descent', block_size=50, max_iter=200),\n",
    "    'Gradient Descent': HighDimensionalOptimizer(method='gradient_descent', learning_rate=0.0001, max_iter=200)\n",
    "}\n",
    "\n",
    "for name, opt in methods_detailed.items():\n",
    "    start_time = time.time()\n",
    "    opt.fit(A, b, lambda_reg=0.01)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    detailed_optimizers[name] = opt\n",
    "    detailed_results[name] = {\n",
    "        'training_time': training_time,\n",
    "        'final_cost': opt.cost_history_[-1],\n",
    "        'recovery_error': np.linalg.norm(opt.theta_ - theta_true),\n",
    "        'sparsity': np.sum(np.abs(opt.theta_) > 1e-6)\n",
    "    }\n",
    "\n",
    "# Visualize detailed results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Cost convergence\n",
    "for name, opt in detailed_optimizers.items():\n",
    "    if opt.cost_history_:\n",
    "        axes[0, 0].semilogy(opt.cost_history_, label=name, alpha=0.8, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Cost (log scale)')\n",
    "axes[0, 0].set_title(f'Convergence Comparison ({analysis_dim}D)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm convergence\n",
    "for name, opt in detailed_optimizers.items():\n",
    "    if opt.grad_norms_:\n",
    "        axes[0, 1].semilogy(opt.grad_norms_, label=name, alpha=0.8, linewidth=2)\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Gradient Norm (log scale)')\n",
    "axes[0, 1].set_title('Gradient Norm Convergence')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Solution sparsity comparison\n",
    "true_sparsity = np.sum(np.abs(theta_true) > 1e-6)\n",
    "method_names = list(detailed_results.keys())\n",
    "sparsities = [detailed_results[name]['sparsity'] for name in method_names]\n",
    "\n",
    "bars = axes[0, 2].bar(method_names, sparsities, alpha=0.7)\n",
    "axes[0, 2].axhline(y=true_sparsity, color='red', linestyle='--', linewidth=2, label=f'True sparsity: {true_sparsity}')\n",
    "axes[0, 2].set_ylabel('Number of Non-zero Coefficients')\n",
    "axes[0, 2].set_title('Solution Sparsity')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].set_xticklabels(method_names, rotation=45)\n",
    "for bar, val in zip(bars, sparsities):\n",
    "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                   f'{val}', ha='center', va='bottom')\n",
    "\n",
    "# Scalability analysis across dimensions\n",
    "if not successful_results.empty:\n",
    "    for method in methods.keys():\n",
    "        method_data = successful_results[successful_results['Method'] == method]\n",
    "        if not method_data.empty:\n",
    "            axes[1, 0].loglog(method_data['Dimension'], method_data['Training Time'], \n",
    "                             'o-', label=method, alpha=0.8, linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Dimension')\n",
    "    axes[1, 0].set_ylabel('Training Time (s)')\n",
    "    axes[1, 0].set_title('Scalability Analysis')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Recovery error comparison\n",
    "recovery_errors = [detailed_results[name]['recovery_error'] for name in method_names]\n",
    "bars = axes[1, 1].bar(method_names, recovery_errors, alpha=0.7, color='orange')\n",
    "axes[1, 1].set_ylabel('Recovery Error ||θ - θ_true||')\n",
    "axes[1, 1].set_title('Solution Quality')\n",
    "axes[1, 1].set_xticklabels(method_names, rotation=45)\n",
    "for bar, val in zip(bars, recovery_errors):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + bar.get_height()*0.05,\n",
    "                   f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Coordinate update pattern for Random CD\n",
    "if 'Random CD' in detailed_optimizers and detailed_optimizers['Random CD'].coord_updates_:\n",
    "    coord_updates = detailed_optimizers['Random CD'].coord_updates_[:1000]  # First 1000 updates\n",
    "    axes[1, 2].hist(coord_updates, bins=50, alpha=0.7, density=True)\n",
    "    axes[1, 2].set_xlabel('Coordinate Index')\n",
    "    axes[1, 2].set_ylabel('Update Frequency (normalized)')\n",
    "    axes[1, 2].set_title('Random CD Coordinate Selection Pattern')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(f\"\\nDetailed Results for {analysis_dim}D problem:\")\n",
    "print(f\"True solution sparsity: {true_sparsity}\")\n",
    "print(\"\\nMethod Comparison:\")\n",
    "for name in method_names:\n",
    "    result = detailed_results[name]\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Training time: {result['training_time']:.4f}s\")\n",
    "    print(f\"  Final cost: {result['final_cost']:.6f}\")\n",
    "    print(f\"  Recovery error: {result['recovery_error']:.6f}\")\n",
    "    print(f\"  Solution sparsity: {result['sparsity']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Optimization Methods in Machine Learning:\n",
    "\n",
    "1. **Gradient Descent Variants**:\n",
    "   - **Batch GD**: Stable convergence but slow for large datasets\n",
    "   - **SGD**: Fast updates but noisy convergence; requires learning rate scheduling\n",
    "   - **Mini-batch GD**: Good compromise between stability and speed\n",
    "\n",
    "2. **Advanced First-Order Methods**:\n",
    "   - **Momentum**: Accelerates convergence and dampens oscillations\n",
    "   - **Nesterov**: Look-ahead gradient provides better convergence\n",
    "   - **AdaGrad**: Adapts learning rate per parameter but can stop too early\n",
    "   - **RMSprop**: Fixes AdaGrad's aggressive learning rate decay\n",
    "   - **Adam**: Combines momentum and adaptive learning rates; robust default choice\n",
    "\n",
    "3. **Second-Order Methods**:\n",
    "   - **Newton's Method**: Fastest convergence but O(n³) complexity\n",
    "   - **BFGS**: Quasi-Newton method with superlinear convergence\n",
    "   - **L-BFGS**: Memory-efficient version for large-scale problems\n",
    "   - Trade-off: Faster convergence vs. computational cost per iteration\n",
    "\n",
    "4. **Constrained Optimization**:\n",
    "   - **Penalty Methods**: Convert constraints to penalty terms; simple but can be ill-conditioned\n",
    "   - **Barrier Methods**: Use logarithmic barriers; maintain feasibility but require good initialization\n",
    "   - **Lagrangian Methods**: Theoretical foundation for understanding optimality conditions\n",
    "\n",
    "5. **High-Dimensional Challenges**:\n",
    "   - **Curse of Dimensionality**: Gradients become sparse, saddle points dominate\n",
    "   - **Coordinate Descent**: Effective for sparse problems and separable objectives\n",
    "   - **Block Coordinate Descent**: Balances efficiency with parallelization\n",
    "   - **Random Coordinate Selection**: Can improve convergence for some problems\n",
    "\n",
    "### Practical Guidelines:\n",
    "\n",
    "- **Default Choice**: Adam optimizer for neural networks\n",
    "- **Large Datasets**: Mini-batch SGD with momentum\n",
    "- **Sparse Problems**: Coordinate descent or L-BFGS\n",
    "- **Small Datasets**: L-BFGS or Newton's method\n",
    "- **Constrained Problems**: Penalty methods for simple constraints, specialized solvers for complex ones\n",
    "\n",
    "### Performance Insights:\n",
    "- Second-order methods converge in fewer iterations but cost more per iteration\n",
    "- Adaptive methods (Adam, RMSprop) are more robust to hyperparameter choices\n",
    "- Coordinate descent scales better with dimension for sparse problems\n",
    "- Proper learning rate scheduling is crucial for SGD variants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}