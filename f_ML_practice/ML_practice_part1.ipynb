{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions - Part 1: ML Fundamentals and Problem Types\n",
    "\n",
    "This notebook covers fundamental machine learning concepts including different learning paradigms, problem types, and basic evaluation concepts. Each question includes detailed explanations, mathematical foundations, and practical implementation examples.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing these questions, you will:\n",
    "- Understand the key differences between supervised, unsupervised, and reinforcement learning\n",
    "- Classify real-world problems into appropriate ML categories\n",
    "- Understand the importance of proper data splitting\n",
    "- Apply basic performance metrics to different problem types\n",
    "- Recognize common pitfalls in ML problem formulation\n",
    "\n",
    "## Difficulty Levels\n",
    "- ★☆☆ **Beginner**: Basic conceptual understanding\n",
    "- ★★☆ **Intermediate**: Applied knowledge and implementation\n",
    "- ★★★ **Advanced**: Deep understanding and complex scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Learning Paradigms ★☆☆\n",
    "\n",
    "**Question:** Explain the key differences between supervised, unsupervised, and reinforcement learning. For each paradigm, provide:\n",
    "1. A clear definition\n",
    "2. The type of data required\n",
    "3. The learning objective\n",
    "4. Two real-world examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1: Learning Paradigms\n",
    "\n",
    "#### **Supervised Learning**\n",
    "**Definition:** Learning from labeled examples to make predictions on new, unseen data.\n",
    "\n",
    "**Data Required:** \n",
    "- Input features (X) paired with target labels/values (y)\n",
    "- Training dataset: {(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}\n",
    "\n",
    "**Learning Objective:** \n",
    "Learn a mapping function f: X → Y that minimizes prediction error on new data\n",
    "\n",
    "**Examples:**\n",
    "1. **Email Spam Detection**: Classify emails as spam/not spam based on content features\n",
    "2. **House Price Prediction**: Predict property values based on location, size, amenities\n",
    "\n",
    "#### **Unsupervised Learning**\n",
    "**Definition:** Finding hidden patterns or structures in data without labeled examples.\n",
    "\n",
    "**Data Required:**\n",
    "- Only input features (X) without target labels\n",
    "- Dataset: {x₁, x₂, ..., xₙ}\n",
    "\n",
    "**Learning Objective:**\n",
    "Discover underlying data structure, relationships, or patterns\n",
    "\n",
    "**Examples:**\n",
    "1. **Customer Segmentation**: Group customers based on purchasing behavior\n",
    "2. **Anomaly Detection**: Identify unusual network traffic patterns\n",
    "\n",
    "#### **Reinforcement Learning**\n",
    "**Definition:** Learning through interaction with an environment via trial and error.\n",
    "\n",
    "**Data Required:**\n",
    "- States, actions, and rewards from environment interactions\n",
    "- Sequential decision-making scenarios\n",
    "\n",
    "**Learning Objective:**\n",
    "Maximize cumulative reward through optimal action selection\n",
    "\n",
    "**Examples:**\n",
    "1. **Game Playing**: AI learning to play chess or video games\n",
    "2. **Autonomous Driving**: Vehicle learning optimal driving strategies\n",
    "\n",
    "#### **Key Distinguishing Factors:**\n",
    "\n",
    "| Aspect | Supervised | Unsupervised | Reinforcement |\n",
    "|--------|------------|--------------|---------------|\n",
    "| **Feedback** | Immediate (labels) | None | Delayed (rewards) |\n",
    "| **Goal** | Prediction accuracy | Pattern discovery | Cumulative reward |\n",
    "| **Data Structure** | (X, y) pairs | X only | (state, action, reward) sequences |\n",
    "| **Evaluation** | Test set performance | Intrinsic metrics | Environment performance |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of the three learning paradigms\n",
    "\n",
    "# 1. Supervised Learning Example: Binary Classification\n",
    "print(\"=== Supervised Learning Example ===\")\n",
    "X_sup, y_sup = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                                   n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Train a simple classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_sup, y_sup)\n",
    "accuracy = clf.score(X_sup, y_sup)\n",
    "print(f\"Supervised model accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# 2. Unsupervised Learning Example: Clustering\n",
    "print(\"\\n=== Unsupervised Learning Example ===\")\n",
    "X_unsup, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Apply clustering (note: no labels used!)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_unsup)\n",
    "print(f\"Discovered {len(np.unique(clusters))} clusters in unlabeled data\")\n",
    "\n",
    "# 3. Reinforcement Learning Concept (simplified)\n",
    "print(\"\\n=== Reinforcement Learning Concept ===\")\n",
    "print(\"RL involves sequential decision making:\")\n",
    "print(\"State → Action → Reward → New State → ...\")\n",
    "print(\"Goal: Learn optimal policy to maximize cumulative reward\")\n",
    "\n",
    "# Simple RL-like example: Multi-armed bandit\n",
    "np.random.seed(42)\n",
    "# Three slot machines with different reward probabilities\n",
    "arm_probabilities = [0.1, 0.5, 0.8]  # Arm 3 is best\n",
    "n_trials = 100\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Simple epsilon-greedy strategy\n",
    "arm_counts = np.zeros(3)\n",
    "arm_rewards = np.zeros(3)\n",
    "total_reward = 0\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Epsilon-greedy action selection\n",
    "    if np.random.random() < epsilon or trial < 3:\n",
    "        action = np.random.randint(3)  # Explore\n",
    "    else:\n",
    "        action = np.argmax(arm_rewards / (arm_counts + 1e-10))  # Exploit\n",
    "    \n",
    "    # Get reward (simulate pulling arm)\n",
    "    reward = 1 if np.random.random() < arm_probabilities[action] else 0\n",
    "    \n",
    "    # Update statistics\n",
    "    arm_counts[action] += 1\n",
    "    arm_rewards[action] += reward\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"RL agent total reward: {total_reward}/{n_trials}\")\n",
    "print(f\"Best arm discovered: Arm {np.argmax(arm_rewards / arm_counts) + 1}\")\n",
    "print(f\"True best arm: Arm {np.argmax(arm_probabilities) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Problem Type Classification ★★☆\n",
    "\n",
    "**Question:** For each of the following scenarios, identify:\n",
    "1. The learning paradigm (supervised/unsupervised/reinforcement)\n",
    "2. The specific problem type (classification/regression/clustering/etc.)\n",
    "3. What would constitute the input features (X) and target (y, if applicable)\n",
    "4. An appropriate evaluation metric\n",
    "\n",
    "**Scenarios:**\n",
    "a) Predicting stock prices for the next week\n",
    "b) Grouping customers by shopping behavior\n",
    "c) Determining if a medical image contains a tumor\n",
    "d) Optimizing ad placement to maximize click-through rates\n",
    "e) Predicting the number of stars (1-5) a user will give to a product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2: Problem Type Classification\n",
    "\n",
    "#### **a) Predicting stock prices for the next week**\n",
    "- **Learning Paradigm:** Supervised Learning\n",
    "- **Problem Type:** Regression (continuous target values)\n",
    "- **Input Features (X):** Historical prices, trading volume, technical indicators, market sentiment, economic indicators\n",
    "- **Target (y):** Future stock prices (continuous values)\n",
    "- **Evaluation Metric:** MAE (Mean Absolute Error) or RMSE (Root Mean Squared Error)\n",
    "- **Rationale:** We have historical data (features) and known outcomes (past prices) to learn from\n",
    "\n",
    "#### **b) Grouping customers by shopping behavior**\n",
    "- **Learning Paradigm:** Unsupervised Learning\n",
    "- **Problem Type:** Clustering\n",
    "- **Input Features (X):** Purchase frequency, average order value, product categories, seasonality patterns\n",
    "- **Target (y):** None (no predefined labels)\n",
    "- **Evaluation Metric:** Silhouette score, inertia, or business-relevant metrics\n",
    "- **Rationale:** No predefined customer segments; we want to discover natural groupings\n",
    "\n",
    "#### **c) Determining if a medical image contains a tumor**\n",
    "- **Learning Paradigm:** Supervised Learning\n",
    "- **Problem Type:** Binary Classification\n",
    "- **Input Features (X):** Image pixels, extracted features, or deep learning representations\n",
    "- **Target (y):** Binary labels (tumor/no tumor)\n",
    "- **Evaluation Metric:** Sensitivity (recall), specificity, AUC-ROC (medical context requires high sensitivity)\n",
    "- **Rationale:** We have labeled medical images from expert radiologists\n",
    "\n",
    "#### **d) Optimizing ad placement to maximize click-through rates**\n",
    "- **Learning Paradigm:** Reinforcement Learning\n",
    "- **Problem Type:** Sequential decision making / Multi-armed bandit\n",
    "- **State/Features:** User demographics, browsing history, time of day, device type\n",
    "- **Actions:** Different ad placements, formats, targeting strategies\n",
    "- **Reward:** Click-through rates, conversion rates\n",
    "- **Evaluation Metric:** Cumulative click-through rate, total conversions\n",
    "- **Rationale:** Need to balance exploration (trying new strategies) vs exploitation (using known good strategies)\n",
    "\n",
    "#### **e) Predicting the number of stars (1-5) a user will give to a product**\n",
    "- **Learning Paradigm:** Supervised Learning\n",
    "- **Problem Type:** Ordinal Regression or Multi-class Classification\n",
    "- **Input Features (X):** User profile, product features, past ratings, review text, price\n",
    "- **Target (y):** Star rating (1, 2, 3, 4, or 5)\n",
    "- **Evaluation Metric:** Mean Absolute Error (preserves ordinal nature) or classification accuracy\n",
    "- **Rationale:** Historical user ratings provide labeled training data; ordinal nature of ratings matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of different problem types with synthetic data\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Different ML Problem Types', fontsize=16)\n",
    "\n",
    "# 1. Regression Example (Stock Price Prediction)\n",
    "np.random.seed(42)\n",
    "time = np.linspace(0, 100, 200)\n",
    "trend = 0.5 * time\n",
    "noise = np.random.normal(0, 5, 200)\n",
    "stock_price = 100 + trend + noise\n",
    "\n",
    "axes[0, 0].plot(time, stock_price, 'b-', alpha=0.7)\n",
    "axes[0, 0].set_title('Regression: Stock Price Prediction')\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Price')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Binary Classification (Medical Diagnosis)\n",
    "X_med, y_med = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                                   n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "colors = ['red' if label == 1 else 'blue' for label in y_med]\n",
    "axes[0, 1].scatter(X_med[:, 0], X_med[:, 1], c=colors, alpha=0.6)\n",
    "axes[0, 1].set_title('Binary Classification: Tumor Detection')\n",
    "axes[0, 1].set_xlabel('Feature 1')\n",
    "axes[0, 1].set_ylabel('Feature 2')\n",
    "axes[0, 1].legend(['No Tumor', 'Tumor'])\n",
    "\n",
    "# 3. Clustering (Customer Segmentation)\n",
    "X_cluster, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.5, random_state=42)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_cluster)\n",
    "axes[0, 2].scatter(X_cluster[:, 0], X_cluster[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "axes[0, 2].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "axes[0, 2].set_title('Clustering: Customer Segmentation')\n",
    "axes[0, 2].set_xlabel('Purchase Frequency')\n",
    "axes[0, 2].set_ylabel('Average Order Value')\n",
    "\n",
    "# 4. Multi-class Classification (Star Ratings)\n",
    "X_rating, y_rating = make_classification(n_samples=300, n_features=2, n_redundant=0, \n",
    "                                         n_informative=2, n_classes=5, random_state=42)\n",
    "scatter = axes[1, 0].scatter(X_rating[:, 0], X_rating[:, 1], c=y_rating, cmap='RdYlBu', alpha=0.6)\n",
    "axes[1, 0].set_title('Multi-class: Star Ratings (1-5)')\n",
    "axes[1, 0].set_xlabel('Product Quality')\n",
    "axes[1, 0].set_ylabel('User Satisfaction')\n",
    "plt.colorbar(scatter, ax=axes[1, 0])\n",
    "\n",
    "# 5. Reinforcement Learning Concept (Multi-armed Bandit)\n",
    "# Show the learning curve of our bandit example\n",
    "trials = range(1, n_trials + 1)\n",
    "cumulative_rewards = []\n",
    "running_reward = 0\n",
    "\n",
    "# Simulate the learning process again for plotting\n",
    "np.random.seed(42)\n",
    "arm_counts = np.zeros(3)\n",
    "arm_rewards = np.zeros(3)\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    if np.random.random() < epsilon or trial < 3:\n",
    "        action = np.random.randint(3)\n",
    "    else:\n",
    "        action = np.argmax(arm_rewards / (arm_counts + 1e-10))\n",
    "    \n",
    "    reward = 1 if np.random.random() < arm_probabilities[action] else 0\n",
    "    arm_counts[action] += 1\n",
    "    arm_rewards[action] += reward\n",
    "    running_reward += reward\n",
    "    cumulative_rewards.append(running_reward / (trial + 1))\n",
    "\n",
    "axes[1, 1].plot(trials, cumulative_rewards, 'g-', linewidth=2)\n",
    "axes[1, 1].axhline(y=max(arm_probabilities), color='r', linestyle='--', \n",
    "                   label='Optimal Performance')\n",
    "axes[1, 1].set_title('RL: Multi-armed Bandit Learning')\n",
    "axes[1, 1].set_xlabel('Trial')\n",
    "axes[1, 1].set_ylabel('Average Reward')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Problem Complexity Comparison\n",
    "problem_types = ['Regression', 'Binary\\nClassif.', 'Multi-class\\nClassif.', 'Clustering', 'RL']\n",
    "complexity_scores = [3, 4, 5, 6, 8]  # Relative complexity\n",
    "colors_complexity = ['lightblue', 'lightgreen', 'orange', 'lightcoral', 'purple']\n",
    "\n",
    "bars = axes[1, 2].bar(problem_types, complexity_scores, color=colors_complexity, alpha=0.7)\n",
    "axes[1, 2].set_title('Problem Complexity Comparison')\n",
    "axes[1, 2].set_ylabel('Relative Complexity Score')\n",
    "axes[1, 2].set_ylim(0, 10)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, complexity_scores):\n",
    "    axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                     str(score), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"• Regression predicts continuous values (stock prices, temperatures)\")\n",
    "print(\"• Classification predicts discrete categories (spam/not spam, tumor/no tumor)\")\n",
    "print(\"• Clustering finds hidden patterns without labels\")\n",
    "print(\"• RL learns through trial and error with delayed feedback\")\n",
    "print(\"• Problem complexity increases with more classes, unlabeled data, and sequential decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Data Splitting Strategy ★★☆\n",
    "\n",
    "**Question:** You're building a machine learning model to predict customer churn (binary classification). You have a dataset with 10,000 customers collected over 2 years, with 15% churn rate.\n",
    "\n",
    "1. Design an appropriate data splitting strategy (train/validation/test)\n",
    "2. Explain why you chose those proportions\n",
    "3. What potential issues should you watch out for with this dataset?\n",
    "4. How would your strategy change if you only had 1,000 customers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3: Data Splitting Strategy\n",
    "\n",
    "#### **1. Recommended Data Splitting Strategy (10,000 customers)**\n",
    "\n",
    "**Split Proportions:**\n",
    "- **Training Set: 70% (7,000 customers)**\n",
    "- **Validation Set: 15% (1,500 customers)**\n",
    "- **Test Set: 15% (1,500 customers)**\n",
    "\n",
    "#### **2. Rationale for These Proportions**\n",
    "\n",
    "**Training Set (70%):**\n",
    "- Need sufficient data to learn complex patterns\n",
    "- With 15% churn rate: ~1,050 positive examples, 5,950 negative examples\n",
    "- Enough samples for reliable model training\n",
    "\n",
    "**Validation Set (15%):**\n",
    "- Used for hyperparameter tuning and model selection\n",
    "- ~225 churners, 1,275 non-churners\n",
    "- Large enough for reliable performance estimates\n",
    "- Prevents overfitting to training data\n",
    "\n",
    "**Test Set (15%):**\n",
    "- Final, unbiased performance evaluation\n",
    "- Never used during model development\n",
    "- Simulates real-world performance\n",
    "\n",
    "#### **3. Potential Issues to Watch For**\n",
    "\n",
    "**Class Imbalance:**\n",
    "- Only 15% positive cases (churn)\n",
    "- May lead to models biased toward majority class\n",
    "- **Solution:** Stratified sampling, balanced metrics (F1, AUC-ROC)\n",
    "\n",
    "**Temporal Dependencies:**\n",
    "- Customer behavior may change over 2 years\n",
    "- Seasonal patterns in churn behavior\n",
    "- **Solution:** Time-based splitting, temporal validation\n",
    "\n",
    "**Data Leakage:**\n",
    "- Features that wouldn't be available at prediction time\n",
    "- Information from the future predicting the past\n",
    "- **Solution:** Careful feature engineering, temporal awareness\n",
    "\n",
    "**Customer-Level Splitting:**\n",
    "- Ensure same customer doesn't appear in multiple sets\n",
    "- Account for potential data from same household/company\n",
    "\n",
    "#### **4. Strategy for Smaller Dataset (1,000 customers)**\n",
    "\n",
    "**Modified Split:**\n",
    "- **Training: 80% (800 customers)**\n",
    "- **Validation: 20% (200 customers)**\n",
    "- **Test: Use k-fold cross-validation instead**\n",
    "\n",
    "**Rationale:**\n",
    "- With only 150 churners total, need to maximize training data\n",
    "- Use k-fold CV (k=5 or k=10) for more robust evaluation\n",
    "- Consider stratified k-fold to maintain class balance\n",
    "- Might need simpler models to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of proper data splitting for churn prediction\n",
    "\n",
    "# Generate synthetic churn dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 10000\n",
    "churn_rate = 0.15\n",
    "\n",
    "# Create synthetic features\n",
    "X_churn = np.random.randn(n_customers, 5)  # 5 features\n",
    "# Add some correlation to make the problem realistic\n",
    "churn_probability = 1 / (1 + np.exp(-(X_churn[:, 0] + 0.5 * X_churn[:, 1] - 0.3)))\n",
    "y_churn = np.random.binomial(1, churn_probability)\n",
    "\n",
    "print(f\"Dataset Overview:\")\n",
    "print(f\"Total customers: {n_customers:,}\")\n",
    "print(f\"Churned customers: {y_churn.sum():,} ({y_churn.mean():.1%})\")\n",
    "print(f\"Non-churned customers: {(1-y_churn).sum():,} ({(1-y_churn).mean():.1%})\")\n",
    "\n",
    "# Proper stratified splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: train+val vs test (85% vs 15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.15, stratify=y_churn, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: train vs val (70% vs 15% of original)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15/0.85, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nData Split Results:\")\n",
    "print(f\"Training set: {len(X_train):,} samples ({len(X_train)/n_customers:.1%})\")\n",
    "print(f\"  - Churned: {y_train.sum():,} ({y_train.mean():.1%})\")\n",
    "print(f\"Validation set: {len(X_val):,} samples ({len(X_val)/n_customers:.1%})\")\n",
    "print(f\"  - Churned: {y_val.sum():,} ({y_val.mean():.1%})\")\n",
    "print(f\"Test set: {len(X_test):,} samples ({len(X_test)/n_customers:.1%})\")\n",
    "print(f\"  - Churned: {y_test.sum():,} ({y_test.mean():.1%})\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\nStratification Check:\")\n",
    "print(f\"Original churn rate: {y_churn.mean():.3f}\")\n",
    "print(f\"Train churn rate: {y_train.mean():.3f}\")\n",
    "print(f\"Val churn rate: {y_val.mean():.3f}\")\n",
    "print(f\"Test churn rate: {y_test.mean():.3f}\")\n",
    "\n",
    "# Visualization of data splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Sample sizes\n",
    "splits = ['Train', 'Validation', 'Test']\n",
    "sizes = [len(X_train), len(X_val), len(X_test)]\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "bars = axes[0].bar(splits, sizes, color=colors, alpha=0.7)\n",
    "axes[0].set_title('Data Split Sizes')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "for bar, size in zip(bars, sizes):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                 f'{size:,}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Churn rates across splits\n",
    "churn_rates = [y_train.mean(), y_val.mean(), y_test.mean()]\n",
    "bars = axes[1].bar(splits, churn_rates, color=colors, alpha=0.7)\n",
    "axes[1].set_title('Churn Rates Across Splits')\n",
    "axes[1].set_ylabel('Churn Rate')\n",
    "axes[1].set_ylim(0, 0.2)\n",
    "for bar, rate in zip(bars, churn_rates):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f'{rate:.1%}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Class distribution\n",
    "split_data = [y_train, y_val, y_test]\n",
    "bottoms = [0, 0, 0]\n",
    "churned = [data.sum() for data in split_data]\n",
    "not_churned = [len(data) - data.sum() for data in split_data]\n",
    "\n",
    "axes[2].bar(splits, not_churned, label='Not Churned', color='lightblue', alpha=0.7)\n",
    "axes[2].bar(splits, churned, bottom=not_churned, label='Churned', color='red', alpha=0.7)\n",
    "axes[2].set_title('Class Distribution Across Splits')\n",
    "axes[2].set_ylabel('Number of Samples')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate what happens with smaller dataset\n",
    "print(f\"\\n\\n=== Smaller Dataset Analysis (1,000 customers) ===\")\n",
    "# Sample 1,000 customers from our dataset\n",
    "small_indices = np.random.choice(n_customers, 1000, replace=False)\n",
    "X_small = X_churn[small_indices]\n",
    "y_small = y_churn[small_indices]\n",
    "\n",
    "print(f\"Small dataset: {len(X_small)} customers, {y_small.sum()} churned ({y_small.mean():.1%})\")\n",
    "\n",
    "# For small dataset, use 80/20 split + cross-validation\n",
    "X_small_train, X_small_test, y_small_train, y_small_test = train_test_split(\n",
    "    X_small, y_small, test_size=0.2, stratify=y_small, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Small dataset split:\")\n",
    "print(f\"  Train: {len(X_small_train)} ({len(X_small_train)/len(X_small):.0%})\")\n",
    "print(f\"  Test: {len(X_small_test)} ({len(X_small_test)/len(X_small):.0%})\")\n",
    "print(f\"  Use cross-validation on training set for validation\")\n",
    "\n",
    "# Demonstrate cross-validation for small dataset\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "clf_small = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "cv_scores = cross_val_score(clf_small, X_small_train, y_small_train, \n",
    "                           cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                           scoring='f1')\n",
    "\n",
    "print(f\"\\n5-Fold CV F1 Scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1 Score: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "print(f\"\\nKey Takeaways:\")\n",
    "print(f\"• Stratified sampling maintains class balance across splits\")\n",
    "print(f\"• Larger datasets allow for dedicated validation sets\")\n",
    "print(f\"• Smaller datasets benefit from cross-validation\")\n",
    "print(f\"• Always consider temporal aspects in time-series data\")\n",
    "print(f\"• Test set should never be used during model development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Performance Metrics Selection ★★★\n",
    "\n",
    "**Question:** You're working on different ML projects. For each scenario below, choose the most appropriate primary evaluation metric and explain your reasoning. Also mention what secondary metrics you'd monitor.\n",
    "\n",
    "**Scenarios:**\n",
    "1. **Fraud Detection**: Detecting credit card fraud (0.1% fraud rate)\n",
    "2. **Medical Screening**: Identifying potential cancer cases for further testing\n",
    "3. **Recommendation System**: Movie recommendations for streaming platform\n",
    "4. **A/B Testing**: Comparing two website designs for conversion rate\n",
    "5. **Demand Forecasting**: Predicting daily sales for inventory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4: Performance Metrics Selection\n",
    "\n",
    "#### **1. Fraud Detection (0.1% fraud rate)**\n",
    "\n",
    "**Primary Metric: Precision-Recall AUC (AP Score)**\n",
    "- **Rationale:** Extreme class imbalance makes accuracy misleading\n",
    "- Precision-Recall curve better handles imbalanced datasets than ROC\n",
    "- Focus on identifying fraud cases without too many false alarms\n",
    "\n",
    "**Secondary Metrics:**\n",
    "- **Precision at fixed recall** (e.g., 80% recall): Business constraint on missing fraud\n",
    "- **F2 Score**: Emphasizes recall (catching fraud) over precision\n",
    "- **Cost-sensitive metrics**: Actual monetary impact of false positives vs false negatives\n",
    "\n",
    "**Key Consideration:** Cost of missing fraud >> Cost of investigating false alarms\n",
    "\n",
    "#### **2. Medical Screening (Cancer Detection)**\n",
    "\n",
    "**Primary Metric: Sensitivity (Recall)**\n",
    "- **Rationale:** Missing a cancer case has severe consequences\n",
    "- High sensitivity ensures we catch most potential cases\n",
    "- False positives lead to additional testing; false negatives can be fatal\n",
    "\n",
    "**Secondary Metrics:**\n",
    "- **Specificity**: To avoid overwhelming the healthcare system with false positives\n",
    "- **NPV (Negative Predictive Value)**: Confidence that negative results are truly negative\n",
    "- **F1 Score**: Balance between precision and recall\n",
    "\n",
    "**Key Consideration:** \"First, do no harm\" - don't miss cancer cases\n",
    "\n",
    "#### **3. Recommendation System (Movies)**\n",
    "\n",
    "**Primary Metric: Mean Average Precision (MAP@k)**\n",
    "- **Rationale:** Ranking quality matters more than binary classification\n",
    "- Measures precision at different cutoff points\n",
    "- Accounts for position bias in recommendations\n",
    "\n",
    "**Secondary Metrics:**\n",
    "- **NDCG (Normalized Discounted Cumulative Gain)**: Considers rating scores, not just binary relevance\n",
    "- **Diversity metrics**: Avoid filter bubbles\n",
    "- **Coverage**: Percentage of catalog being recommended\n",
    "- **Click-through rate**: Real user engagement\n",
    "\n",
    "**Key Consideration:** User satisfaction and engagement drive business value\n",
    "\n",
    "#### **4. A/B Testing (Website Design)**\n",
    "\n",
    "**Primary Metric: Statistical Significance Test (e.g., Chi-square, Fisher's exact)**\n",
    "- **Rationale:** Need to determine if observed difference is statistically significant\n",
    "- Conversion rate difference with confidence intervals\n",
    "- Account for multiple testing if comparing multiple metrics\n",
    "\n",
    "**Secondary Metrics:**\n",
    "- **Effect Size**: Practical significance, not just statistical\n",
    "- **Confidence Intervals**: Range of likely true effect\n",
    "- **Power Analysis**: Ensure sufficient sample size\n",
    "- **Business Impact**: Revenue per visitor, lifetime value\n",
    "\n",
    "**Key Consideration:** Statistical rigor prevents false conclusions\n",
    "\n",
    "#### **5. Demand Forecasting (Sales Prediction)**\n",
    "\n",
    "**Primary Metric: MAPE (Mean Absolute Percentage Error)**\n",
    "- **Rationale:** Scale-independent, easy to interpret across different products\n",
    "- Symmetric treatment of over/under-forecasting\n",
    "- Directly relates to business planning accuracy\n",
    "\n",
    "**Secondary Metrics:**\n",
    "- **WMAPE (Weighted MAPE)**: Accounts for volume differences across products\n",
    "- **Forecast Bias**: Systematic over/under-forecasting\n",
    "- **Safety Stock Impact**: Cost of stockouts vs excess inventory\n",
    "- **MAE**: Absolute error in units for inventory planning\n",
    "\n",
    "**Key Consideration:** Balance between stockouts (lost sales) and excess inventory (holding costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of appropriate metrics for different scenarios\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scenario 1: Fraud Detection (Highly Imbalanced)\n",
    "print(\"=== Scenario 1: Fraud Detection ===\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate imbalanced fraud data (0.1% fraud rate)\n",
    "n_transactions = 10000\n",
    "fraud_rate = 0.001\n",
    "y_true_fraud = np.random.binomial(1, fraud_rate, n_transactions)\n",
    "\n",
    "# Simulate two models with different characteristics\n",
    "# Model A: High precision, lower recall\n",
    "y_scores_A = np.random.beta(0.5, 2, n_transactions)  # Conservative model\n",
    "y_scores_A[y_true_fraud == 1] += 0.3  # Boost fraud scores\n",
    "\n",
    "# Model B: Higher recall, lower precision\n",
    "y_scores_B = np.random.beta(1, 1.5, n_transactions)  # More aggressive model\n",
    "y_scores_B[y_true_fraud == 1] += 0.4  # Boost fraud scores more\n",
    "\n",
    "# Calculate metrics\n",
    "ap_A = average_precision_score(y_true_fraud, y_scores_A)\n",
    "ap_B = average_precision_score(y_true_fraud, y_scores_B)\n",
    "roc_auc_A = roc_auc_score(y_true_fraud, y_scores_A)\n",
    "roc_auc_B = roc_auc_score(y_true_fraud, y_scores_B)\n",
    "\n",
    "print(f\"Fraud cases: {y_true_fraud.sum()} out of {n_transactions} ({y_true_fraud.mean():.3%})\")\n",
    "print(f\"Model A - AP Score: {ap_A:.4f}, ROC-AUC: {roc_auc_A:.4f}\")\n",
    "print(f\"Model B - AP Score: {ap_B:.4f}, ROC-AUC: {roc_auc_B:.4f}\")\n",
    "print(f\"For fraud detection, focus on AP Score due to extreme imbalance\")\n",
    "\n",
    "# Scenario 2: Medical Screening\n",
    "print(\"\\n=== Scenario 2: Medical Screening ===\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate medical screening data (5% positive cases)\n",
    "n_patients = 1000\n",
    "disease_rate = 0.05\n",
    "y_true_medical = np.random.binomial(1, disease_rate, n_patients)\n",
    "\n",
    "# Simulate model predictions (threshold affects sensitivity/specificity trade-off)\n",
    "y_scores_medical = np.random.beta(2, 5, n_patients)\n",
    "y_scores_medical[y_true_medical == 1] += 0.4\n",
    "\n",
    "# Test different thresholds for sensitivity/specificity trade-off\n",
    "thresholds = [0.1, 0.3, 0.5]\n",
    "print(f\"Disease cases: {y_true_medical.sum()} out of {n_patients} ({y_true_medical.mean():.1%})\")\n",
    "print(f\"Threshold | Sensitivity | Specificity | PPV | NPV\")\n",
    "print(f\"-\" * 50)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_medical >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_medical, y_pred).ravel()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)  # Recall\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Precision\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"   {threshold:.1f}    |    {sensitivity:.3f}   |    {specificity:.3f}   | {ppv:.3f} | {npv:.3f}\")\n",
    "\n",
    "# Scenario 3: Recommendation System\n",
    "print(\"\\n=== Scenario 3: Recommendation System ===\")\n",
    "\n",
    "# Simulate recommendation rankings (simplified)\n",
    "def calculate_map_at_k(y_true, y_scores, k=10):\n",
    "    \"\"\"Calculate Mean Average Precision at K\"\"\"\n",
    "    # Sort by scores (descending)\n",
    "    sorted_indices = np.argsort(y_scores)[::-1]\n",
    "    y_true_sorted = y_true[sorted_indices]\n",
    "    \n",
    "    # Calculate AP@K\n",
    "    relevant_items = 0\n",
    "    precision_sum = 0\n",
    "    \n",
    "    for i in range(min(k, len(y_true_sorted))):\n",
    "        if y_true_sorted[i] == 1:\n",
    "            relevant_items += 1\n",
    "            precision_at_i = relevant_items / (i + 1)\n",
    "            precision_sum += precision_at_i\n",
    "    \n",
    "    return precision_sum / min(k, np.sum(y_true)) if np.sum(y_true) > 0 else 0\n",
    "\n",
    "# Generate movie relevance data\n",
    "n_movies = 100\n",
    "y_true_movies = np.random.binomial(1, 0.15, n_movies)  # 15% relevant\n",
    "y_scores_movies = np.random.beta(2, 5, n_movies)\n",
    "y_scores_movies[y_true_movies == 1] += 0.3  # Boost relevant movie scores\n",
    "\n",
    "map_at_5 = calculate_map_at_k(y_true_movies, y_scores_movies, 5)\n",
    "map_at_10 = calculate_map_at_k(y_true_movies, y_scores_movies, 10)\n",
    "\n",
    "print(f\"Relevant movies: {y_true_movies.sum()} out of {n_movies} ({y_true_movies.mean():.1%})\")\n",
    "print(f\"MAP@5: {map_at_5:.4f}\")\n",
    "print(f\"MAP@10: {map_at_10:.4f}\")\n",
    "\n",
    "# Scenario 4: A/B Testing\n",
    "print(\"\\n=== Scenario 4: A/B Testing ===\")\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Simulate A/B test data\n",
    "np.random.seed(42)\n",
    "n_visitors_A = 5000\n",
    "n_visitors_B = 5000\n",
    "conversion_rate_A = 0.05  # 5% baseline\n",
    "conversion_rate_B = 0.055  # 5.5% treatment (10% relative improvement)\n",
    "\n",
    "conversions_A = np.random.binomial(n_visitors_A, conversion_rate_A)\n",
    "conversions_B = np.random.binomial(n_visitors_B, conversion_rate_B)\n",
    "\n",
    "# Contingency table\n",
    "observed = np.array([[conversions_A, n_visitors_A - conversions_A],\n",
    "                     [conversions_B, n_visitors_B - conversions_B]])\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(observed)\n",
    "\n",
    "print(f\"Version A: {conversions_A}/{n_visitors_A} conversions ({conversions_A/n_visitors_A:.3%})\")\n",
    "print(f\"Version B: {conversions_B}/{n_visitors_B} conversions ({conversions_B/n_visitors_B:.3%})\")\n",
    "print(f\"Chi-square statistic: {chi2:.3f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Significant at α=0.05: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "# Effect size (Cohen's h for proportions)\n",
    "p1, p2 = conversions_A/n_visitors_A, conversions_B/n_visitors_B\n",
    "cohens_h = 2 * (np.arcsin(np.sqrt(p2)) - np.arcsin(np.sqrt(p1)))\n",
    "print(f\"Effect size (Cohen's h): {cohens_h:.4f}\")\n",
    "\n",
    "# Scenario 5: Demand Forecasting\n",
    "print(\"\\n=== Scenario 5: Demand Forecasting ===\")\n",
    "\n",
    "# Generate sales forecasting data\n",
    "np.random.seed(42)\n",
    "n_days = 100\n",
    "true_sales = 100 + 10 * np.sin(np.linspace(0, 4*np.pi, n_days)) + np.random.normal(0, 5, n_days)\n",
    "predicted_sales = true_sales + np.random.normal(0, 8, n_days)  # Add prediction error\n",
    "\n",
    "# Calculate forecasting metrics\n",
    "mae = mean_absolute_error(true_sales, predicted_sales)\n",
    "mape = mean_absolute_percentage_error(true_sales, predicted_sales)\n",
    "rmse = np.sqrt(np.mean((true_sales - predicted_sales)**2))\n",
    "\n",
    "# Forecast bias\n",
    "bias = np.mean(predicted_sales - true_sales)\n",
    "\n",
    "print(f\"Forecasting Performance:\")\n",
    "print(f\"MAE: {mae:.2f} units\")\n",
    "print(f\"MAPE: {mape:.2%}\")\n",
    "print(f\"RMSE: {rmse:.2f} units\")\n",
    "print(f\"Forecast Bias: {bias:.2f} units ({'Over' if bias > 0 else 'Under'}forecasting)\")\n",
    "\n",
    "print(f\"\\n=== Key Insights ===\")\n",
    "print(f\"• Fraud Detection: Use AP Score for extreme imbalance\")\n",
    "print(f\"• Medical Screening: Prioritize Sensitivity (don't miss cases)\")\n",
    "print(f\"• Recommendations: Use ranking metrics (MAP@K, NDCG)\")\n",
    "print(f\"• A/B Testing: Focus on statistical significance and effect size\")\n",
    "print(f\"• Forecasting: Use scale-independent metrics (MAPE) for interpretability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Common ML Pitfalls ★★★\n",
    "\n",
    "**Question:** Identify and explain the problems in each of the following ML scenarios. For each problem, provide a solution.\n",
    "\n",
    "1. **Data Leakage**: A model predicting loan defaults uses the applicant's credit score from 6 months after the loan application as a feature.\n",
    "\n",
    "2. **Selection Bias**: A medical AI trained on data from a prestigious hospital is deployed in rural clinics.\n",
    "\n",
    "3. **Target Leakage**: An e-commerce model predicting purchase likelihood includes \"items in cart\" as a feature.\n",
    "\n",
    "4. **Survivorship Bias**: A model predicting startup success is trained only on companies that lasted at least 2 years.\n",
    "\n",
    "5. **Simpson's Paradox**: A hiring algorithm shows better performance for both male and female candidates separately, but worse overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 5: Common ML Pitfalls\n",
    "\n",
    "#### **1. Data Leakage Problem**\n",
    "\n",
    "**Problem Identified:**\n",
    "- Using **future information** (credit score 6 months later) to predict past events (loan default)\n",
    "- This creates artificially high performance that won't generalize to real predictions\n",
    "- The model learns from information that wouldn't be available at prediction time\n",
    "\n",
    "**Why It's Problematic:**\n",
    "- Credit scores often change due to loan default itself\n",
    "- Creates impossibly good validation metrics\n",
    "- Will fail catastrophically in production\n",
    "\n",
    "**Solution:**\n",
    "- **Temporal Cutoff**: Only use features available at or before loan application time\n",
    "- **Feature Engineering**: Use historical credit score trends, not future values\n",
    "- **Time-Aware Validation**: Use temporal splits instead of random splits\n",
    "- **Domain Expertise**: Collaborate with loan officers to identify appropriate features\n",
    "\n",
    "#### **2. Selection Bias Problem**\n",
    "\n",
    "**Problem Identified:**\n",
    "- **Population Mismatch**: Training data from prestigious hospitals doesn't represent rural clinic patients\n",
    "- Different demographics, disease prevalence, imaging equipment, protocols\n",
    "- Model learns patterns specific to the training population\n",
    "\n",
    "**Why It's Problematic:**\n",
    "- Prestigious hospitals: younger, wealthier patients, better equipment, specialist care\n",
    "- Rural clinics: older, diverse socioeconomic backgrounds, basic equipment\n",
    "- Disease presentation and prevalence can vary significantly\n",
    "\n",
    "**Solution:**\n",
    "- **Representative Sampling**: Include data from diverse healthcare settings\n",
    "- **Domain Adaptation**: Techniques to adapt models across populations\n",
    "- **Continuous Monitoring**: Track performance across different demographics\n",
    "- **Local Validation**: Test on target population before deployment\n",
    "- **Federated Learning**: Train on distributed data while preserving privacy\n",
    "\n",
    "#### **3. Target Leakage Problem**\n",
    "\n",
    "**Problem Identified:**\n",
    "- **Feature is a direct result of the target**: Items in cart strongly indicates purchase intent\n",
    "- This feature is essentially the target variable in disguise\n",
    "- Creates circular reasoning in the model\n",
    "\n",
    "**Why It's Problematic:**\n",
    "- High correlation doesn't imply causation\n",
    "- Cart abandonment is common in e-commerce\n",
    "- Model won't help with actual business decisions (driving cart additions)\n",
    "\n",
    "**Solution:**\n",
    "- **Redefine Target**: Predict cart addition likelihood instead of purchase given cart\n",
    "- **Temporal Separation**: Use past behavior to predict future actions\n",
    "- **Causal Features**: Focus on features that influence behavior (browsing patterns, seasonality)\n",
    "- **Business Logic**: Work with stakeholders to define meaningful prediction tasks\n",
    "\n",
    "#### **4. Survivorship Bias Problem**\n",
    "\n",
    "**Problem Identified:**\n",
    "- **Missing Failed Cases**: Only training on successful startups ignores valuable failure signals\n",
    "- Creates biased understanding of success factors\n",
    "- Underestimates actual failure rates\n",
    "\n",
    "**Why It's Problematic:**\n",
    "- Early failures contain crucial information about what doesn't work\n",
    "- Model will be overly optimistic about success probability\n",
    "- Missing the majority of the actual startup population\n",
    "\n",
    "**Solution:**\n",
    "- **Complete Population**: Include all startups, regardless of outcome\n",
    "- **Right-Censored Data**: Use survival analysis techniques for ongoing companies\n",
    "- **Multiple Data Sources**: Government registrations, funding databases, news archives\n",
    "- **Time-Based Analysis**: Track companies from inception with proper follow-up periods\n",
    "\n",
    "#### **5. Simpson's Paradox Problem**\n",
    "\n",
    "**Problem Identified:**\n",
    "- **Confounding Variables**: Hidden factors affecting both gender and performance\n",
    "- Aggregation level changes the apparent relationship\n",
    "- Different base rates or conditions for subgroups\n",
    "\n",
    "**Why It's Problematic:**\n",
    "- May indicate discrimination in hiring pools or evaluation criteria\n",
    "- Overall metric masks important subgroup differences\n",
    "- Can lead to unfair or biased decisions\n",
    "\n",
    "**Solution:**\n",
    "- **Stratified Analysis**: Analyze performance within each subgroup\n",
    "- **Causal Inference**: Identify and control for confounding variables\n",
    "- **Fairness Metrics**: Use appropriate fairness definitions for the context\n",
    "- **Domain Investigation**: Understand why paradox occurs in this specific context\n",
    "- **Policy Review**: Examine hiring practices and evaluation criteria for bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrations of ML pitfalls and their effects\n",
    "\n",
    "# 1. Data Leakage Demonstration\n",
    "print(\"=== 1. Data Leakage Demonstration ===\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate loan data\n",
    "n_loans = 1000\n",
    "# Features available at application time\n",
    "income = np.random.normal(50000, 15000, n_loans)\n",
    "credit_score_initial = np.random.normal(650, 100, n_loans)\n",
    "loan_amount = np.random.normal(25000, 10000, n_loans)\n",
    "\n",
    "# Generate default outcome\n",
    "default_prob = 1 / (1 + np.exp(-(0.05 - 0.00002*income - 0.01*credit_score_initial + 0.00001*loan_amount)))\n",
    "defaults = np.random.binomial(1, default_prob)\n",
    "\n",
    "# Credit score 6 months later (LEAKED feature - affects by default)\n",
    "credit_score_later = credit_score_initial - 100 * defaults + np.random.normal(0, 20, n_loans)\n",
    "\n",
    "# Compare models with and without leakage\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Model WITHOUT leakage (proper features)\n",
    "X_proper = np.column_stack([income, credit_score_initial, loan_amount])\n",
    "model_proper = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "scores_proper = cross_val_score(model_proper, X_proper, defaults, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Model WITH leakage (includes future credit score)\n",
    "X_leaked = np.column_stack([income, credit_score_initial, loan_amount, credit_score_later])\n",
    "model_leaked = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "scores_leaked = cross_val_score(model_leaked, X_leaked, defaults, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(f\"Default rate: {defaults.mean():.1%}\")\n",
    "print(f\"Model without leakage AUC: {scores_proper.mean():.3f} ± {scores_proper.std():.3f}\")\n",
    "print(f\"Model with leakage AUC: {scores_leaked.mean():.3f} ± {scores_leaked.std():.3f}\")\n",
    "print(f\"Leakage creates artificially high performance!\")\n",
    "\n",
    "# 2. Selection Bias Demonstration\n",
    "print(\"\\n=== 2. Selection Bias Demonstration ===\")\n",
    "\n",
    "# Simulate medical data from two different populations\n",
    "np.random.seed(42)\n",
    "n_patients = 500\n",
    "\n",
    "# Prestigious hospital population (younger, healthier baseline)\n",
    "age_prestigious = np.random.normal(45, 15, n_patients)\n",
    "health_score_prestigious = np.random.normal(80, 10, n_patients)  # Better baseline health\n",
    "disease_prob_prestigious = 1 / (1 + np.exp(-(0.05*age_prestigious - 0.02*health_score_prestigious - 2)))\n",
    "disease_prestigious = np.random.binomial(1, disease_prob_prestigious)\n",
    "\n",
    "# Rural clinic population (older, different health patterns)\n",
    "age_rural = np.random.normal(60, 20, n_patients)  # Older population\n",
    "health_score_rural = np.random.normal(70, 15, n_patients)  # Different health baseline\n",
    "disease_prob_rural = 1 / (1 + np.exp(-(0.03*age_rural - 0.015*health_score_rural - 1)))\n",
    "disease_rural = np.random.binomial(1, disease_prob_rural)\n",
    "\n",
    "# Train on prestigious hospital, test on rural clinic\n",
    "X_prestigious = np.column_stack([age_prestigious, health_score_prestigious])\n",
    "X_rural = np.column_stack([age_rural, health_score_rural])\n",
    "\n",
    "model_biased = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_biased.fit(X_prestigious, disease_prestigious)\n",
    "\n",
    "# Performance on same population (overoptimistic)\n",
    "score_same = model_biased.score(X_prestigious, disease_prestigious)\n",
    "# Performance on different population (realistic)\n",
    "score_different = model_biased.score(X_rural, disease_rural)\n",
    "\n",
    "print(f\"Training population disease rate: {disease_prestigious.mean():.1%}\")\n",
    "print(f\"Target population disease rate: {disease_rural.mean():.1%}\")\n",
    "print(f\"Performance on training population: {score_same:.3f}\")\n",
    "print(f\"Performance on target population: {score_different:.3f}\")\n",
    "print(f\"Selection bias causes {(score_same - score_different):.3f} performance drop!\")\n",
    "\n",
    "# 3. Simpson's Paradox Demonstration\n",
    "print(\"\\n=== 3. Simpson's Paradox Demonstration ===\")\n",
    "\n",
    "# Simulate hiring data with confounding variable (department)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Department A: Male-dominated, higher overall performance\n",
    "dept_A_male = {'hired': 80, 'total': 100}  # 80% hire rate\n",
    "dept_A_female = {'hired': 45, 'total': 50}  # 90% hire rate\n",
    "\n",
    "# Department B: Female-dominated, lower overall performance  \n",
    "dept_B_male = {'hired': 10, 'total': 20}   # 50% hire rate\n",
    "dept_B_female = {'hired': 120, 'total': 200}  # 60% hire rate\n",
    "\n",
    "# Calculate rates\n",
    "male_A_rate = dept_A_male['hired'] / dept_A_male['total']\n",
    "female_A_rate = dept_A_female['hired'] / dept_A_female['total']\n",
    "male_B_rate = dept_B_male['hired'] / dept_B_male['total']\n",
    "female_B_rate = dept_B_female['hired'] / dept_B_female['total']\n",
    "\n",
    "# Overall rates\n",
    "male_overall = (dept_A_male['hired'] + dept_B_male['hired']) / (dept_A_male['total'] + dept_B_male['total'])\n",
    "female_overall = (dept_A_female['hired'] + dept_B_female['hired']) / (dept_A_female['total'] + dept_B_female['total'])\n",
    "\n",
    "print(f\"Department A - Male: {male_A_rate:.1%}, Female: {female_A_rate:.1%}\")\n",
    "print(f\"Department B - Male: {male_B_rate:.1%}, Female: {female_B_rate:.1%}\")\n",
    "print(f\"Overall - Male: {male_overall:.1%}, Female: {female_overall:.1%}\")\n",
    "print(f\"\\nParadox: Females outperform in each department, but underperform overall!\")\n",
    "print(f\"This is due to different application distributions across departments.\")\n",
    "\n",
    "# 4. Survivorship Bias Visualization\n",
    "print(\"\\n=== 4. Survivorship Bias Impact ===\")\n",
    "\n",
    "# Simulate startup data\n",
    "np.random.seed(42)\n",
    "n_startups = 1000\n",
    "\n",
    "# Features\n",
    "funding = np.random.exponential(100000, n_startups)  # Funding amount\n",
    "team_size = np.random.poisson(5, n_startups)  # Initial team size\n",
    "market_size = np.random.normal(1000000, 500000, n_startups)  # Target market size\n",
    "\n",
    "# Survival probability (2+ years)\n",
    "survival_prob = 1 / (1 + np.exp(-(0.000001*funding + 0.1*team_size + 0.0000005*market_size - 3)))\n",
    "survived = np.random.binomial(1, survival_prob)\n",
    "\n",
    "print(f\"Total startups: {n_startups}\")\n",
    "print(f\"Survived 2+ years: {survived.sum()} ({survived.mean():.1%})\")\n",
    "print(f\"Training only on survivors ignores {(1-survived.mean()):.1%} of the data!\")\n",
    "print(f\"This creates overly optimistic success predictions.\")\n",
    "\n",
    "# Show bias in feature importance\n",
    "X_startup = np.column_stack([funding, team_size, market_size])\n",
    "\n",
    "# Model with survivorship bias (only successful companies)\n",
    "X_survivors = X_startup[survived == 1]\n",
    "y_survivors = np.ones(X_survivors.shape[0])  # All are \"successful\"\n",
    "\n",
    "print(f\"\\nAverage funding - All startups: ${funding.mean():,.0f}\")\n",
    "print(f\"Average funding - Survivors only: ${funding[survived == 1].mean():,.0f}\")\n",
    "print(f\"Survivorship bias overestimates importance of high funding.\")\n",
    "\n",
    "print(f\"\\n=== Key Takeaways ===\")\n",
    "print(f\"• Always validate features are available at prediction time\")\n",
    "print(f\"• Ensure training data represents the target population\")\n",
    "print(f\"• Be careful of features that are consequences of the target\")\n",
    "print(f\"• Include all relevant cases, not just successful ones\")\n",
    "print(f\"• Analyze subgroups separately to detect Simpson's Paradox\")\n",
    "print(f\"• Domain expertise is crucial for identifying these pitfalls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### **Core Concepts Mastered**\n",
    "\n",
    "1. **Learning Paradigms**: Clear distinction between supervised, unsupervised, and reinforcement learning\n",
    "2. **Problem Classification**: Ability to map real-world scenarios to appropriate ML problem types\n",
    "3. **Data Splitting**: Understanding proper train/validation/test strategies for different dataset sizes\n",
    "4. **Metric Selection**: Choosing appropriate evaluation metrics based on problem context and business needs\n",
    "5. **Pitfall Recognition**: Identifying and avoiding common ML mistakes that lead to poor real-world performance\n",
    "\n",
    "### **Critical Success Factors**\n",
    "\n",
    "- **Domain Understanding**: Always collaborate with subject matter experts\n",
    "- **Data Quality**: Invest time in understanding your data before modeling\n",
    "- **Validation Strategy**: Use appropriate evaluation methods for your specific problem\n",
    "- **Temporal Awareness**: Consider time dependencies in your data and features\n",
    "- **Bias Recognition**: Actively look for sources of bias in data collection and model development\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "Continue to Part 2 to master data preprocessing and feature engineering techniques that build upon these foundational concepts.\n",
    "\n",
    "### **Practice Recommendations**\n",
    "\n",
    "1. Apply these concepts to real datasets in your domain\n",
    "2. Practice identifying problem types in news articles and research papers\n",
    "3. Review ML papers critically for potential pitfalls\n",
    "4. Implement custom evaluation metrics for your specific use cases\n",
    "5. Build a checklist for avoiding common ML mistakes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}