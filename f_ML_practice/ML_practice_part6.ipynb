{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions Part 6: Tree-Based Methods\n",
    "\n",
    "This notebook covers decision trees and tree-based algorithms, including their theoretical foundations, implementation details, and practical considerations. Each question includes mathematical derivations, algorithmic implementations, and empirical analysis.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Decision tree construction and splitting criteria\n",
    "- Pruning techniques and overfitting prevention\n",
    "- Tree ensemble methods fundamentals\n",
    "- Feature importance and interpretation\n",
    "- Handling categorical and missing data\n",
    "\n",
    "**Format:** Each question includes theory, implementation, and analysis sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_regression, load_iris, load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Decision Tree Construction and Splitting Criteria\n",
    "\n",
    "**Question:** Implement a decision tree from scratch with multiple splitting criteria (Gini, Entropy, MSE). Compare their behavior and explain when to use each criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Decision Tree Algorithm:**\n",
    "1. Start with entire dataset at root\n",
    "2. Find best feature and threshold to split\n",
    "3. Recursively apply to child nodes\n",
    "4. Stop when stopping criterion met\n",
    "\n",
    "**Splitting Criteria:**\n",
    "\n",
    "**Gini Impurity (Classification):**\n",
    "$$\\text{Gini}(S) = 1 - \\sum_{i=1}^c p_i^2$$\n",
    "where $p_i$ is proportion of class $i$ in set $S$\n",
    "\n",
    "**Entropy (Classification):**\n",
    "$$\\text{Entropy}(S) = -\\sum_{i=1}^c p_i \\log_2(p_i)$$\n",
    "\n",
    "**Mean Squared Error (Regression):**\n",
    "$$\\text{MSE}(S) = \\frac{1}{|S|} \\sum_{i \\in S} (y_i - \\bar{y})^2$$\n",
    "\n",
    "**Information Gain:**\n",
    "$$\\text{IG}(S, A) = \\text{Impurity}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Impurity}(S_v)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    \"\"\"Node in a decision tree.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None, samples=None):\n",
    "        self.feature = feature      # Feature index to split on\n",
    "        self.threshold = threshold  # Threshold value for split\n",
    "        self.left = left           # Left child node\n",
    "        self.right = right         # Right child node\n",
    "        self.value = value         # Prediction value (for leaf nodes)\n",
    "        self.samples = samples     # Number of samples in this node\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeCustom:\n",
    "    \"\"\"Decision tree implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                 min_samples_leaf=1, max_features=None, task='classification'):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.task = task\n",
    "        self.root = None\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "    def _gini_impurity(self, y):\n",
    "        \"\"\"Calculate Gini impurity.\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calculate entropy.\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        # Avoid log(0)\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "    \n",
    "    def _mse(self, y):\n",
    "        \"\"\"Calculate mean squared error.\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        mean_y = np.mean(y)\n",
    "        return np.mean((y - mean_y) ** 2)\n",
    "    \n",
    "    def _calculate_impurity(self, y):\n",
    "        \"\"\"Calculate impurity based on criterion.\"\"\"\n",
    "        if self.task == 'classification':\n",
    "            if self.criterion == 'gini':\n",
    "                return self._gini_impurity(y)\n",
    "            elif self.criterion == 'entropy':\n",
    "                return self._entropy(y)\n",
    "        else:  # regression\n",
    "            return self._mse(y)\n",
    "    \n",
    "    def _information_gain(self, y, y_left, y_right):\n",
    "        \"\"\"Calculate information gain from a split.\"\"\"\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y_left), len(y_right)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "        \n",
    "        parent_impurity = self._calculate_impurity(y)\n",
    "        left_impurity = self._calculate_impurity(y_left)\n",
    "        right_impurity = self._calculate_impurity(y_right)\n",
    "        \n",
    "        weighted_impurity = (n_left / n) * left_impurity + (n_right / n) * right_impurity\n",
    "        \n",
    "        return parent_impurity - weighted_impurity\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split for the given data.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if n_samples < self.min_samples_split:\n",
    "            return None, None\n",
    "        \n",
    "        # Determine features to consider\n",
    "        if self.max_features is None:\n",
    "            features_to_consider = range(n_features)\n",
    "        else:\n",
    "            n_features_to_consider = min(self.max_features, n_features)\n",
    "            features_to_consider = np.random.choice(n_features, n_features_to_consider, replace=False)\n",
    "        \n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in features_to_consider:\n",
    "            # Get unique values for potential thresholds\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Split data\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                # Check minimum samples constraint\n",
    "                if np.sum(left_mask) < self.min_samples_leaf or np.sum(right_mask) < self.min_samples_leaf:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain\n",
    "                y_left, y_right = y[left_mask], y[right_mask]\n",
    "                gain = self._information_gain(y, y_left, y_right)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build the decision tree.\"\"\"\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Determine leaf value\n",
    "        if self.task == 'classification':\n",
    "            leaf_value = np.bincount(y).argmax()  # Majority class\n",
    "        else:\n",
    "            leaf_value = np.mean(y)  # Mean for regression\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           n_samples < self.min_samples_split or \\\n",
    "           len(np.unique(y)) == 1:  # Pure node\n",
    "            return DecisionTreeNode(value=leaf_value, samples=n_samples)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return DecisionTreeNode(value=leaf_value, samples=n_samples)\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        # Recursively build left and right subtrees\n",
    "        left_child = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_child = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return DecisionTreeNode(\n",
    "            feature=best_feature,\n",
    "            threshold=best_threshold,\n",
    "            left=left_child,\n",
    "            right=right_child,\n",
    "            samples=n_samples\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the decision tree.\"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.root = self._build_tree(X, y)\n",
    "        \n",
    "        # Calculate feature importances\n",
    "        self._calculate_feature_importances(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _predict_sample(self, x, node):\n",
    "        \"\"\"Predict a single sample.\"\"\"\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_sample(x, node.left)\n",
    "        else:\n",
    "            return self._predict_sample(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        X = np.array(X)\n",
    "        return np.array([self._predict_sample(x, self.root) for x in X])\n",
    "    \n",
    "    def _calculate_feature_importances(self, X, y):\n",
    "        \"\"\"Calculate feature importances based on information gain.\"\"\"\n",
    "        importances = np.zeros(self.n_features_)\n",
    "        \n",
    "        def _traverse(node, X_subset, y_subset):\n",
    "            if node.is_leaf():\n",
    "                return\n",
    "            \n",
    "            # Calculate weighted information gain\n",
    "            n_samples = len(y_subset)\n",
    "            left_mask = X_subset[:, node.feature] <= node.threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            y_left, y_right = y_subset[left_mask], y_subset[right_mask]\n",
    "            gain = self._information_gain(y_subset, y_left, y_right)\n",
    "            \n",
    "            # Weight by number of samples\n",
    "            importances[node.feature] += (n_samples / len(y)) * gain\n",
    "            \n",
    "            # Recursively traverse children\n",
    "            if not node.left.is_leaf():\n",
    "                _traverse(node.left, X_subset[left_mask], y_left)\n",
    "            if not node.right.is_leaf():\n",
    "                _traverse(node.right, X_subset[right_mask], y_right)\n",
    "        \n",
    "        _traverse(self.root, X, y)\n",
    "        \n",
    "        # Normalize importances\n",
    "        if np.sum(importances) > 0:\n",
    "            importances = importances / np.sum(importances)\n",
    "        \n",
    "        self.feature_importances_ = importances\n",
    "\n",
    "# Generate classification dataset\n",
    "X_cls, y_cls = make_classification(n_samples=1000, n_features=10, n_informative=5, \n",
    "                                  n_redundant=2, n_clusters_per_class=1, random_state=42)\n",
    "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_cls, y_cls, test_size=0.3, random_state=42)\n",
    "\n",
    "# Generate regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=10, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "# Compare splitting criteria for classification\n",
    "criteria = ['gini', 'entropy']\n",
    "classification_results = {}\n",
    "\n",
    "for criterion in criteria:\n",
    "    # Custom implementation\n",
    "    dt_custom = DecisionTreeCustom(criterion=criterion, max_depth=5, task='classification')\n",
    "    dt_custom.fit(X_train_cls, y_train_cls)\n",
    "    y_pred_custom = dt_custom.predict(X_test_cls)\n",
    "    \n",
    "    # Sklearn implementation\n",
    "    dt_sklearn = DecisionTreeClassifier(criterion=criterion, max_depth=5, random_state=42)\n",
    "    dt_sklearn.fit(X_train_cls, y_train_cls)\n",
    "    y_pred_sklearn = dt_sklearn.predict(X_test_cls)\n",
    "    \n",
    "    classification_results[criterion] = {\n",
    "        'custom_accuracy': accuracy_score(y_test_cls, y_pred_custom),\n",
    "        'sklearn_accuracy': accuracy_score(y_test_cls, y_pred_sklearn),\n",
    "        'custom_importances': dt_custom.feature_importances_,\n",
    "        'sklearn_importances': dt_sklearn.feature_importances_\n",
    "    }\n",
    "\n",
    "# Test regression with MSE\n",
    "dt_reg_custom = DecisionTreeCustom(criterion='mse', max_depth=5, task='regression')\n",
    "dt_reg_custom.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_custom = dt_reg_custom.predict(X_test_reg)\n",
    "\n",
    "dt_reg_sklearn = DecisionTreeRegressor(criterion='squared_error', max_depth=5, random_state=42)\n",
    "dt_reg_sklearn.fit(X_train_reg, y_train_reg)\n",
    "y_pred_reg_sklearn = dt_reg_sklearn.predict(X_test_reg)\n",
    "\n",
    "print(\"Classification Results:\")\n",
    "for criterion in criteria:\n",
    "    result = classification_results[criterion]\n",
    "    print(f\"\\n{criterion.upper()}:\")\n",
    "    print(f\"  Custom accuracy: {result['custom_accuracy']:.4f}\")\n",
    "    print(f\"  Sklearn accuracy: {result['sklearn_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nRegression Results:\")\n",
    "print(f\"Custom MSE: {mean_squared_error(y_test_reg, y_pred_reg_custom):.4f}\")\n",
    "print(f\"Sklearn MSE: {mean_squared_error(y_test_reg, y_pred_reg_sklearn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize splitting criteria comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Compare Gini vs Entropy curves\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "gini_values = [1 - p**2 - (1-p)**2 for p in p_values]\n",
    "entropy_values = [-p*np.log2(p) - (1-p)*np.log2(1-p) for p in p_values]\n",
    "\n",
    "axes[0, 0].plot(p_values, gini_values, label='Gini Impurity', linewidth=2)\n",
    "axes[0, 0].plot(p_values, entropy_values, label='Entropy', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Probability of Class 1')\n",
    "axes[0, 0].set_ylabel('Impurity')\n",
    "axes[0, 0].set_title('Gini vs Entropy Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance comparison\n",
    "feature_names = [f'Feature {i+1}' for i in range(10)]\n",
    "x_pos = np.arange(len(feature_names))\n",
    "width = 0.35\n",
    "\n",
    "gini_importances = classification_results['gini']['custom_importances']\n",
    "entropy_importances = classification_results['entropy']['custom_importances']\n",
    "\n",
    "axes[0, 1].bar(x_pos - width/2, gini_importances, width, label='Gini', alpha=0.8)\n",
    "axes[0, 1].bar(x_pos + width/2, entropy_importances, width, label='Entropy', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Features')\n",
    "axes[0, 1].set_ylabel('Importance')\n",
    "axes[0, 1].set_title('Feature Importance: Gini vs Entropy')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(feature_names, rotation=45)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy vs max_depth\n",
    "depths = range(1, 11)\n",
    "gini_scores = []\n",
    "entropy_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    # Gini\n",
    "    dt_gini = DecisionTreeCustom(criterion='gini', max_depth=depth, task='classification')\n",
    "    dt_gini.fit(X_train_cls, y_train_cls)\n",
    "    gini_scores.append(accuracy_score(y_test_cls, dt_gini.predict(X_test_cls)))\n",
    "    \n",
    "    # Entropy\n",
    "    dt_entropy = DecisionTreeCustom(criterion='entropy', max_depth=depth, task='classification')\n",
    "    dt_entropy.fit(X_train_cls, y_train_cls)\n",
    "    entropy_scores.append(accuracy_score(y_test_cls, dt_entropy.predict(X_test_cls)))\n",
    "\n",
    "axes[0, 2].plot(depths, gini_scores, 'o-', label='Gini', linewidth=2, markersize=6)\n",
    "axes[0, 2].plot(depths, entropy_scores, 's-', label='Entropy', linewidth=2, markersize=6)\n",
    "axes[0, 2].set_xlabel('Max Depth')\n",
    "axes[0, 2].set_ylabel('Test Accuracy')\n",
    "axes[0, 2].set_title('Accuracy vs Tree Depth')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary visualization (2D projection)\n",
    "# Use first 2 features for visualization\n",
    "X_2d = X_train_cls[:, :2]\n",
    "dt_2d = DecisionTreeCustom(criterion='gini', max_depth=3, task='classification')\n",
    "dt_2d.fit(X_2d, y_train_cls)\n",
    "\n",
    "# Create mesh\n",
    "h = 0.02\n",
    "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = dt_2d.predict(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1, 0].contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')\n",
    "scatter = axes[1, 0].scatter(X_2d[:, 0], X_2d[:, 1], c=y_train_cls, cmap='RdYlBu', edgecolors='black')\n",
    "axes[1, 0].set_xlabel('Feature 1')\n",
    "axes[1, 0].set_ylabel('Feature 2')\n",
    "axes[1, 0].set_title('Decision Tree Boundary (Gini, depth=3)')\n",
    "\n",
    "# Impurity reduction at each split simulation\n",
    "# Generate simple 1D data for clear visualization\n",
    "np.random.seed(42)\n",
    "x_simple = np.random.randn(100)\n",
    "y_simple = (x_simple > 0).astype(int)\n",
    "\n",
    "# Calculate impurity before and after split at x=0\n",
    "gini_before = 1 - (np.sum(y_simple == 0)/len(y_simple))**2 - (np.sum(y_simple == 1)/len(y_simple))**2\n",
    "entropy_before = -np.sum([(np.sum(y_simple == i)/len(y_simple)) * np.log2(np.sum(y_simple == i)/len(y_simple) + 1e-10) for i in [0, 1]])\n",
    "\n",
    "left_mask = x_simple <= 0\n",
    "right_mask = x_simple > 0\n",
    "y_left, y_right = y_simple[left_mask], y_simple[right_mask]\n",
    "\n",
    "if len(y_left) > 0 and len(y_right) > 0:\n",
    "    gini_left = 1 - (np.sum(y_left == 0)/len(y_left))**2 - (np.sum(y_left == 1)/len(y_left))**2\n",
    "    gini_right = 1 - (np.sum(y_right == 0)/len(y_right))**2 - (np.sum(y_right == 1)/len(y_right))**2\n",
    "    gini_after = (len(y_left)/len(y_simple)) * gini_left + (len(y_right)/len(y_simple)) * gini_right\n",
    "    \n",
    "    entropy_left = -np.sum([(np.sum(y_left == i)/len(y_left)) * np.log2(np.sum(y_left == i)/len(y_left) + 1e-10) for i in [0, 1]])\n",
    "    entropy_right = -np.sum([(np.sum(y_right == i)/len(y_right)) * np.log2(np.sum(y_right == i)/len(y_right) + 1e-10) for i in [0, 1]])\n",
    "    entropy_after = (len(y_left)/len(y_simple)) * entropy_left + (len(y_right)/len(y_simple)) * entropy_right\n",
    "\n",
    "    categories = ['Before Split', 'After Split']\n",
    "    gini_values_split = [gini_before, gini_after]\n",
    "    entropy_values_split = [entropy_before, entropy_after]\n",
    "    \n",
    "    x_cat = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 1].bar(x_cat - width/2, gini_values_split, width, label='Gini', alpha=0.8)\n",
    "    axes[1, 1].bar(x_cat + width/2, entropy_values_split, width, label='Entropy', alpha=0.8)\n",
    "    axes[1, 1].set_ylabel('Impurity')\n",
    "    axes[1, 1].set_title('Impurity Reduction After Split')\n",
    "    axes[1, 1].set_xticks(x_cat)\n",
    "    axes[1, 1].set_xticklabels(categories)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add gain annotations\n",
    "    gini_gain = gini_before - gini_after\n",
    "    entropy_gain = entropy_before - entropy_after\n",
    "    axes[1, 1].text(0.5, max(max(gini_values_split), max(entropy_values_split)) * 0.8, \n",
    "                   f'Gini Gain: {gini_gain:.3f}\\nEntropy Gain: {entropy_gain:.3f}', \n",
    "                   ha='center', va='center', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# MSE visualization for regression\n",
    "y_mean = np.mean(y_train_reg)\n",
    "mse_values = [(y_val - y_mean)**2 for y_val in y_train_reg[:50]]  # First 50 samples\n",
    "\n",
    "axes[1, 2].hist(mse_values, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1, 2].axvline(x=np.mean(mse_values), color='red', linestyle='--', linewidth=2, label=f'Mean MSE: {np.mean(mse_values):.2f}')\n",
    "axes[1, 2].set_xlabel('Squared Error')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_title('Distribution of Squared Errors (Regression)')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSplitting Criteria Analysis:\")\n",
    "print(f\"Gini impurity range: [0, 0.5] for binary classification\")\n",
    "print(f\"Entropy range: [0, 1] for binary classification\")\n",
    "print(f\"Gini tends to find pure nodes faster (favors larger, purer partitions)\")\n",
    "print(f\"Entropy is more sensitive to impurity changes\")\n",
    "print(f\"MSE is used for regression problems to minimize variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Pruning Techniques and Overfitting Prevention\n",
    "\n",
    "**Question:** Implement pre-pruning and post-pruning techniques. Compare their effectiveness in preventing overfitting and analyze the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Pre-pruning (Early Stopping):**\n",
    "- Stop tree growth based on criteria:\n",
    "  - Maximum depth\n",
    "  - Minimum samples per leaf\n",
    "  - Minimum information gain\n",
    "  - Maximum number of leaves\n",
    "\n",
    "**Post-pruning:**\n",
    "- Build full tree then remove branches\n",
    "- **Cost Complexity Pruning (Minimal Cost-Complexity Pruning):**\n",
    "\n",
    "$$R_{\\alpha}(T) = R(T) + \\alpha|\\tilde{T}|$$\n",
    "\n",
    "Where:\n",
    "- $R(T)$ = misclassification rate of tree $T$\n",
    "- $\\alpha$ = complexity parameter\n",
    "- $|\\tilde{T}|$ = number of terminal nodes\n",
    "\n",
    "**Reduced Error Pruning:**\n",
    "- Use validation set to decide pruning\n",
    "- Remove node if validation error doesn't increase\n",
    "\n",
    "**Bias-Variance Tradeoff:**\n",
    "- Unpruned trees: Low bias, high variance\n",
    "- Pruned trees: Higher bias, lower variance\n",
    "- Optimal pruning minimizes total error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrunedDecisionTree(DecisionTreeCustom):\n",
    "    \"\"\"Decision tree with pruning capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                 min_samples_leaf=1, min_info_gain=0.0, ccp_alpha=0.0, **kwargs):\n",
    "        super().__init__(criterion=criterion, max_depth=max_depth, \n",
    "                        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, **kwargs)\n",
    "        self.min_info_gain = min_info_gain\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        self.tree_size_ = 0\n",
    "        \n",
    "    def _best_split_with_gain_threshold(self, X, y):\n",
    "        \"\"\"Find best split with minimum information gain threshold.\"\"\"\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return None, None\n",
    "        \n",
    "        # Calculate information gain for the best split\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        y_left, y_right = y[left_mask], y[right_mask]\n",
    "        \n",
    "        gain = self._information_gain(y, y_left, y_right)\n",
    "        \n",
    "        if gain < self.min_info_gain:\n",
    "            return None, None\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _build_tree_with_prepruning(self, X, y, depth=0):\n",
    "        \"\"\"Build tree with pre-pruning (early stopping).\"\"\"\n",
    "        n_samples = len(y)\n",
    "        self.tree_size_ += 1\n",
    "        \n",
    "        # Determine leaf value\n",
    "        if self.task == 'classification':\n",
    "            leaf_value = np.bincount(y).argmax()\n",
    "        else:\n",
    "            leaf_value = np.mean(y)\n",
    "        \n",
    "        # Pre-pruning conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           n_samples < self.min_samples_split or \\\n",
    "           len(np.unique(y)) == 1:\n",
    "            return DecisionTreeNode(value=leaf_value, samples=n_samples)\n",
    "        \n",
    "        # Find best split with gain threshold\n",
    "        best_feature, best_threshold = self._best_split_with_gain_threshold(X, y)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return DecisionTreeNode(value=leaf_value, samples=n_samples)\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        # Build children\n",
    "        left_child = self._build_tree_with_prepruning(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_child = self._build_tree_with_prepruning(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return DecisionTreeNode(\n",
    "            feature=best_feature,\n",
    "            threshold=best_threshold,\n",
    "            left=left_child,\n",
    "            right=right_child,\n",
    "            samples=n_samples\n",
    "        )\n",
    "    \n",
    "    def _calculate_tree_error(self, node, X, y):\n",
    "        \"\"\"Calculate misclassification error for a tree/subtree.\"\"\"\n",
    "        if node.is_leaf():\n",
    "            if self.task == 'classification':\n",
    "                predictions = np.full(len(y), node.value)\n",
    "                return np.sum(predictions != y)\n",
    "            else:\n",
    "                return np.sum((y - node.value) ** 2)\n",
    "        \n",
    "        left_mask = X[:, node.feature] <= node.threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        left_error = 0 if np.sum(left_mask) == 0 else self._calculate_tree_error(node.left, X[left_mask], y[left_mask])\n",
    "        right_error = 0 if np.sum(right_mask) == 0 else self._calculate_tree_error(node.right, X[right_mask], y[right_mask])\n",
    "        \n",
    "        return left_error + right_error\n",
    "    \n",
    "    def _count_leaves(self, node):\n",
    "        \"\"\"Count number of leaf nodes in tree.\"\"\"\n",
    "        if node.is_leaf():\n",
    "            return 1\n",
    "        return self._count_leaves(node.left) + self._count_leaves(node.right)\n",
    "    \n",
    "    def _post_prune_ccp(self, X, y):\n",
    "        \"\"\"Cost complexity post-pruning.\"\"\"\n",
    "        if self.ccp_alpha <= 0:\n",
    "            return\n",
    "        \n",
    "        def _prune_recursive(node, X_subset, y_subset):\n",
    "            if node.is_leaf():\n",
    "                return node\n",
    "            \n",
    "            # Get subsets for children\n",
    "            left_mask = X_subset[:, node.feature] <= node.threshold\n",
    "            right_mask = ~left_mask\n",
    "            \n",
    "            X_left, y_left = X_subset[left_mask], y_subset[left_mask]\n",
    "            X_right, y_right = X_subset[right_mask], y_subset[right_mask]\n",
    "            \n",
    "            # Recursively prune children\n",
    "            node.left = _prune_recursive(node.left, X_left, y_left)\n",
    "            node.right = _prune_recursive(node.right, X_right, y_right)\n",
    "            \n",
    "            # Calculate error and complexity for current subtree\n",
    "            subtree_error = self._calculate_tree_error(node, X_subset, y_subset)\n",
    "            subtree_leaves = self._count_leaves(node)\n",
    "            \n",
    "            # Calculate error if we prune this node (make it a leaf)\n",
    "            if self.task == 'classification':\n",
    "                leaf_value = np.bincount(y_subset).argmax()\n",
    "                leaf_error = np.sum(y_subset != leaf_value)\n",
    "            else:\n",
    "                leaf_value = np.mean(y_subset)\n",
    "                leaf_error = np.sum((y_subset - leaf_value) ** 2)\n",
    "            \n",
    "            # Cost complexity criterion\n",
    "            subtree_cost = subtree_error + self.ccp_alpha * subtree_leaves\n",
    "            leaf_cost = leaf_error + self.ccp_alpha * 1\n",
    "            \n",
    "            # Prune if leaf is better\n",
    "            if leaf_cost <= subtree_cost:\n",
    "                return DecisionTreeNode(value=leaf_value, samples=len(y_subset))\n",
    "            \n",
    "            return node\n",
    "        \n",
    "        self.root = _prune_recursive(self.root, X, y)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit tree with pruning.\"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_size_ = 0\n",
    "        \n",
    "        # Build tree with pre-pruning\n",
    "        self.root = self._build_tree_with_prepruning(X, y)\n",
    "        \n",
    "        # Apply post-pruning if specified\n",
    "        self._post_prune_ccp(X, y)\n",
    "        \n",
    "        # Calculate feature importances\n",
    "        self._calculate_feature_importances(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Generate dataset with noise to demonstrate overfitting\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "X_noise = np.random.randn(n_samples, 20)  # 20 features\n",
    "# Only first 5 features are relevant\n",
    "y_noise = (X_noise[:, 0] + X_noise[:, 1] - X_noise[:, 2] + 0.5*X_noise[:, 3] - 0.3*X_noise[:, 4] > 0).astype(int)\n",
    "# Add noise to labels\n",
    "noise_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "y_noise[noise_indices] = 1 - y_noise[noise_indices]\n",
    "\n",
    "X_train_noise, X_test_noise, y_train_noise, y_test_noise = train_test_split(\n",
    "    X_noise, y_noise, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split training data for validation (for reduced error pruning)\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train_noise, y_train_noise, test_size=0.3, random_state=42)\n",
    "\n",
    "# Test different pruning strategies\n",
    "pruning_configs = {\n",
    "    'No Pruning': {'max_depth': None, 'min_samples_leaf': 1, 'min_info_gain': 0.0, 'ccp_alpha': 0.0},\n",
    "    'Max Depth 5': {'max_depth': 5, 'min_samples_leaf': 1, 'min_info_gain': 0.0, 'ccp_alpha': 0.0},\n",
    "    'Min Samples Leaf 10': {'max_depth': None, 'min_samples_leaf': 10, 'min_info_gain': 0.0, 'ccp_alpha': 0.0},\n",
    "    'Min Info Gain 0.01': {'max_depth': None, 'min_samples_leaf': 1, 'min_info_gain': 0.01, 'ccp_alpha': 0.0},\n",
    "    'CCP Alpha 0.01': {'max_depth': None, 'min_samples_leaf': 1, 'min_info_gain': 0.0, 'ccp_alpha': 0.01},\n",
    "    'Combined Pruning': {'max_depth': 8, 'min_samples_leaf': 5, 'min_info_gain': 0.005, 'ccp_alpha': 0.005}\n",
    "}\n",
    "\n",
    "pruning_results = {}\n",
    "\n",
    "for config_name, config in pruning_configs.items():\n",
    "    dt = PrunedDecisionTree(\n",
    "        criterion='gini',\n",
    "        task='classification',\n",
    "        **config\n",
    "    )\n",
    "    \n",
    "    dt.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "    # Evaluate on different sets\n",
    "    train_acc = accuracy_score(y_train_sub, dt.predict(X_train_sub))\n",
    "    val_acc = accuracy_score(y_val, dt.predict(X_val))\n",
    "    test_acc = accuracy_score(y_test_noise, dt.predict(X_test_noise))\n",
    "    \n",
    "    # Count tree complexity\n",
    "    n_leaves = dt._count_leaves(dt.root)\n",
    "    \n",
    "    pruning_results[config_name] = {\n",
    "        'train_accuracy': train_acc,\n",
    "        'val_accuracy': val_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'n_leaves': n_leaves,\n",
    "        'overfitting': train_acc - test_acc  # Measure of overfitting\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "pruning_df = pd.DataFrame(pruning_results).T\n",
    "print(\"Pruning Strategies Comparison:\")\n",
    "print(pruning_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pruning effects and bias-variance tradeoff\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "config_names = list(pruning_results.keys())\n",
    "train_accs = [pruning_results[name]['train_accuracy'] for name in config_names]\n",
    "val_accs = [pruning_results[name]['val_accuracy'] for name in config_names]\n",
    "test_accs = [pruning_results[name]['test_accuracy'] for name in config_names]\n",
    "\n",
    "x_pos = np.arange(len(config_names))\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 0].bar(x_pos - width, train_accs, width, label='Train', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos, val_accs, width, label='Validation', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + width, test_accs, width, label='Test', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Pruning Strategy')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Accuracy Comparison')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(config_names, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tree complexity (number of leaves)\n",
    "n_leaves = [pruning_results[name]['n_leaves'] for name in config_names]\n",
    "bars = axes[0, 1].bar(config_names, n_leaves, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_ylabel('Number of Leaves')\n",
    "axes[0, 1].set_title('Tree Complexity')\n",
    "axes[0, 1].set_xticklabels(config_names, rotation=45, ha='right')\n",
    "for bar, val in zip(bars, n_leaves):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,\n",
    "                   f'{val}', ha='center', va='bottom')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting measure (train - test accuracy)\n",
    "overfitting = [pruning_results[name]['overfitting'] for name in config_names]\n",
    "colors = ['red' if x > 0.05 else 'green' for x in overfitting]\n",
    "bars = axes[0, 2].bar(config_names, overfitting, alpha=0.7, color=colors)\n",
    "axes[0, 2].set_ylabel('Train - Test Accuracy')\n",
    "axes[0, 2].set_title('Overfitting Measure')\n",
    "axes[0, 2].set_xticklabels(config_names, rotation=45, ha='right')\n",
    "axes[0, 2].axhline(y=0.05, color='black', linestyle='--', alpha=0.5, label='Overfitting threshold')\n",
    "axes[0, 2].legend()\n",
    "for bar, val in zip(bars, overfitting):\n",
    "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.005,\n",
    "                   f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Bias-variance tradeoff simulation\n",
    "# Test different tree depths\n",
    "depths = range(1, 16)\n",
    "n_bootstrap = 50\n",
    "bias_variance_results = {'depth': [], 'bias': [], 'variance': [], 'total_error': []}\n",
    "\n",
    "for depth in depths:\n",
    "    predictions = []\n",
    "    \n",
    "    # Bootstrap sampling for variance estimation\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        bootstrap_idx = np.random.choice(len(X_train_sub), len(X_train_sub), replace=True)\n",
    "        X_bootstrap = X_train_sub[bootstrap_idx]\n",
    "        y_bootstrap = y_train_sub[bootstrap_idx]\n",
    "        \n",
    "        # Train tree\n",
    "        dt_bootstrap = PrunedDecisionTree(criterion='gini', max_depth=depth, task='classification')\n",
    "        dt_bootstrap.fit(X_bootstrap, y_bootstrap)\n",
    "        \n",
    "        # Predict on test set\n",
    "        pred = dt_bootstrap.predict(X_test_noise)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate bias and variance\n",
    "    mean_predictions = np.mean(predictions, axis=0)\n",
    "    \n",
    "    # Bias: difference between average prediction and true labels\n",
    "    bias_squared = np.mean((mean_predictions - y_test_noise) ** 2)\n",
    "    \n",
    "    # Variance: average variance of predictions across bootstrap samples\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    # Total error approximation\n",
    "    total_error = bias_squared + variance\n",
    "    \n",
    "    bias_variance_results['depth'].append(depth)\n",
    "    bias_variance_results['bias'].append(bias_squared)\n",
    "    bias_variance_results['variance'].append(variance)\n",
    "    bias_variance_results['total_error'].append(total_error)\n",
    "\n",
    "# Plot bias-variance tradeoff\n",
    "axes[1, 0].plot(bias_variance_results['depth'], bias_variance_results['bias'], 'o-', \n",
    "               label='BiasÂ²', linewidth=2, markersize=6)\n",
    "axes[1, 0].plot(bias_variance_results['depth'], bias_variance_results['variance'], 's-', \n",
    "               label='Variance', linewidth=2, markersize=6)\n",
    "axes[1, 0].plot(bias_variance_results['depth'], bias_variance_results['total_error'], '^-', \n",
    "               label='Total Error', linewidth=2, markersize=6)\n",
    "axes[1, 0].set_xlabel('Max Depth')\n",
    "axes[1, 0].set_ylabel('Error')\n",
    "axes[1, 0].set_title('Bias-Variance Tradeoff')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Complexity vs Performance\n",
    "axes[1, 1].scatter(n_leaves, test_accs, c=overfitting, cmap='RdYlGn_r', \n",
    "                  s=100, alpha=0.8, edgecolors='black')\n",
    "for i, name in enumerate(config_names):\n",
    "    axes[1, 1].annotate(name, (n_leaves[i], test_accs[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[1, 1].set_xlabel('Number of Leaves (Complexity)')\n",
    "axes[1, 1].set_ylabel('Test Accuracy')\n",
    "axes[1, 1].set_title('Complexity vs Performance')\n",
    "cbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
    "cbar.set_label('Overfitting')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning curves for different pruning strategies\n",
    "selected_configs = ['No Pruning', 'Max Depth 5', 'Combined Pruning']\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "for config_name in selected_configs:\n",
    "    config = pruning_configs[config_name]\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for train_size in train_sizes:\n",
    "        n_samples = int(train_size * len(X_train_sub))\n",
    "        X_subset = X_train_sub[:n_samples]\n",
    "        y_subset = y_train_sub[:n_samples]\n",
    "        \n",
    "        dt = PrunedDecisionTree(criterion='gini', task='classification', **config)\n",
    "        dt.fit(X_subset, y_subset)\n",
    "        \n",
    "        train_scores.append(accuracy_score(y_subset, dt.predict(X_subset)))\n",
    "        val_scores.append(accuracy_score(y_val, dt.predict(X_val)))\n",
    "    \n",
    "    axes[1, 2].plot(train_sizes * len(X_train_sub), train_scores, 'o-', \n",
    "                   label=f'{config_name} (Train)', alpha=0.7)\n",
    "    axes[1, 2].plot(train_sizes * len(X_train_sub), val_scores, 's--', \n",
    "                   label=f'{config_name} (Val)', alpha=0.7)\n",
    "\n",
    "axes[1, 2].set_xlabel('Training Set Size')\n",
    "axes[1, 2].set_ylabel('Accuracy')\n",
    "axes[1, 2].set_title('Learning Curves')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal pruning strategy\n",
    "best_config = max(pruning_results.keys(), key=lambda x: pruning_results[x]['test_accuracy'])\n",
    "print(f\"\\nBest pruning strategy: {best_config}\")\n",
    "print(f\"Test accuracy: {pruning_results[best_config]['test_accuracy']:.4f}\")\n",
    "print(f\"Overfitting measure: {pruning_results[best_config]['overfitting']:.4f}\")\n",
    "print(f\"Tree complexity: {pruning_results[best_config]['n_leaves']} leaves\")\n",
    "\n",
    "print(\"\\nPruning Guidelines:\")\n",
    "print(\"- Pre-pruning is computationally efficient but may stop too early\")\n",
    "print(\"- Post-pruning builds full tree then removes branches (more expensive but thorough)\")\n",
    "print(\"- Combined approach often works best in practice\")\n",
    "print(\"- Monitor validation performance to detect optimal pruning level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Feature Importance and Tree Interpretation\n",
    "\n",
    "**Question:** Compare different feature importance measures in decision trees. Implement permutation importance and analyze how tree structure affects interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Feature Importance Measures:**\n",
    "\n",
    "1. **Gini/Entropy Importance (Mean Decrease Impurity):**\n",
    "$$\\text{Importance}_j = \\sum_{t \\in \\text{splits on feature } j} p_t \\cdot \\Delta I_t$$\n",
    "where $p_t$ is proportion of samples at node $t$ and $\\Delta I_t$ is impurity decrease\n",
    "\n",
    "2. **Permutation Importance:**\n",
    "$$\\text{Importance}_j = \\text{Score}(\\text{original}) - \\text{Score}(\\text{permuted}_j)$$\n",
    "- More reliable as it measures actual predictive contribution\n",
    "- Model-agnostic approach\n",
    "\n",
    "3. **Drop-Column Importance:**\n",
    "$$\\text{Importance}_j = \\text{Score}(\\text{all features}) - \\text{Score}(\\text{without feature } j)$$\n",
    "\n",
    "**Interpretation Challenges:**\n",
    "- **Feature interactions**: Trees naturally capture interactions\n",
    "- **Bias toward high-cardinality features**: More split opportunities\n",
    "- **Correlated features**: May substitute for each other\n",
    "- **Tree instability**: Small data changes can create very different trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableDecisionTree(PrunedDecisionTree):\n",
    "    \"\"\"Decision tree with enhanced interpretability features.\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.permutation_importances_ = None\n",
    "        self.drop_column_importances_ = None\n",
    "        \n",
    "    def _calculate_permutation_importance(self, X, y, metric='accuracy', n_repeats=10):\n",
    "        \"\"\"Calculate permutation importance.\"\"\"\n",
    "        X = np.array(X)\n",
    "        baseline_score = self._calculate_score(X, y, metric)\n",
    "        \n",
    "        importances = np.zeros(X.shape[1])\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            scores = []\n",
    "            \n",
    "            for _ in range(n_repeats):\n",
    "                # Permute feature\n",
    "                X_permuted = X.copy()\n",
    "                X_permuted[:, feature_idx] = np.random.permutation(X_permuted[:, feature_idx])\n",
    "                \n",
    "                # Calculate score with permuted feature\n",
    "                permuted_score = self._calculate_score(X_permuted, y, metric)\n",
    "                scores.append(baseline_score - permuted_score)\n",
    "            \n",
    "            importances[feature_idx] = np.mean(scores)\n",
    "        \n",
    "        return importances\n",
    "    \n",
    "    def _calculate_drop_column_importance(self, X, y, metric='accuracy'):\n",
    "        \"\"\"Calculate drop-column importance.\"\"\"\n",
    "        X = np.array(X)\n",
    "        baseline_score = self._calculate_score(X, y, metric)\n",
    "        \n",
    "        importances = np.zeros(X.shape[1])\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            # Create dataset without this feature\n",
    "            X_drop = np.delete(X, feature_idx, axis=1)\n",
    "            \n",
    "            # Train new model without this feature\n",
    "            dt_drop = InterpretableDecisionTree(\n",
    "                criterion=self.criterion,\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                task=self.task\n",
    "            )\n",
    "            dt_drop.fit(X_drop, y)\n",
    "            \n",
    "            # Calculate score without this feature\n",
    "            drop_score = dt_drop._calculate_score(X_drop, y, metric)\n",
    "            importances[feature_idx] = baseline_score - drop_score\n",
    "        \n",
    "        return importances\n",
    "    \n",
    "    def _calculate_score(self, X, y, metric):\n",
    "        \"\"\"Calculate model score.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        \n",
    "        if metric == 'accuracy':\n",
    "            return accuracy_score(y, predictions)\n",
    "        elif metric == 'mse':\n",
    "            return -mean_squared_error(y, predictions)  # Negative for consistency\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric}\")\n",
    "    \n",
    "    def calculate_all_importances(self, X, y, metric='accuracy'):\n",
    "        \"\"\"Calculate all types of feature importance.\"\"\"\n",
    "        # Gini/Entropy importance already calculated in fit\n",
    "        self.permutation_importances_ = self._calculate_permutation_importance(X, y, metric)\n",
    "        self.drop_column_importances_ = self._calculate_drop_column_importance(X, y, metric)\n",
    "        \n",
    "        return {\n",
    "            'gini_entropy': self.feature_importances_,\n",
    "            'permutation': self.permutation_importances_,\n",
    "            'drop_column': self.drop_column_importances_\n",
    "        }\n",
    "    \n",
    "    def get_tree_rules(self, feature_names=None):\n",
    "        \"\"\"Extract decision rules from the tree.\"\"\"\n",
    "        if feature_names is None:\n",
    "            feature_names = [f'feature_{i}' for i in range(self.n_features_)]\n",
    "        \n",
    "        rules = []\n",
    "        \n",
    "        def _extract_rules(node, path=[]):\n",
    "            if node.is_leaf():\n",
    "                rule = ' AND '.join(path) if path else 'True'\n",
    "                rules.append({\n",
    "                    'rule': rule,\n",
    "                    'prediction': node.value,\n",
    "                    'samples': node.samples\n",
    "                })\n",
    "                return\n",
    "            \n",
    "            feature_name = feature_names[node.feature]\n",
    "            \n",
    "            # Left child (<=)\n",
    "            left_condition = f\"{feature_name} <= {node.threshold:.3f}\"\n",
    "            _extract_rules(node.left, path + [left_condition])\n",
    "            \n",
    "            # Right child (>)\n",
    "            right_condition = f\"{feature_name} > {node.threshold:.3f}\"\n",
    "            _extract_rules(node.right, path + [right_condition])\n",
    "        \n",
    "        _extract_rules(self.root)\n",
    "        return rules\n",
    "    \n",
    "    def analyze_feature_interactions(self, X, y, top_features=5):\n",
    "        \"\"\"Analyze feature interactions in the tree.\"\"\"\n",
    "        # Get top features by importance\n",
    "        top_feature_indices = np.argsort(self.feature_importances_)[-top_features:]\n",
    "        \n",
    "        interactions = {}\n",
    "        \n",
    "        def _find_interactions(node, features_in_path=set()):\n",
    "            if node.is_leaf():\n",
    "                if len(features_in_path) > 1:\n",
    "                    # Record interaction\n",
    "                    interaction_key = tuple(sorted(features_in_path))\n",
    "                    if interaction_key not in interactions:\n",
    "                        interactions[interaction_key] = 0\n",
    "                    interactions[interaction_key] += node.samples\n",
    "                return\n",
    "            \n",
    "            # Add current feature to path\n",
    "            new_features_in_path = features_in_path.copy()\n",
    "            if node.feature in top_feature_indices:\n",
    "                new_features_in_path.add(node.feature)\n",
    "            \n",
    "            _find_interactions(node.left, new_features_in_path)\n",
    "            _find_interactions(node.right, new_features_in_path)\n",
    "        \n",
    "        _find_interactions(self.root)\n",
    "        \n",
    "        # Sort by frequency\n",
    "        sorted_interactions = sorted(interactions.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return sorted_interactions\n",
    "\n",
    "# Create dataset with known feature interactions\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "X_interpret = np.random.randn(n_samples, n_features)\n",
    "# Create target with known feature importance and interactions\n",
    "y_interpret = (\n",
    "    2 * X_interpret[:, 0] +          # Strong individual effect\n",
    "    1.5 * X_interpret[:, 1] +        # Medium individual effect\n",
    "    X_interpret[:, 0] * X_interpret[:, 1] +  # Interaction effect\n",
    "    0.5 * X_interpret[:, 2] +        # Weak individual effect\n",
    "    np.random.randn(n_samples) * 0.3  # Noise\n",
    ") > 0\n",
    "y_interpret = y_interpret.astype(int)\n",
    "\n",
    "# Add irrelevant features (should have low importance)\n",
    "X_train_interp, X_test_interp, y_train_interp, y_test_interp = train_test_split(\n",
    "    X_interpret, y_interpret, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train interpretable decision tree\n",
    "dt_interp = InterpretableDecisionTree(\n",
    "    criterion='gini',\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=10,\n",
    "    task='classification'\n",
    ")\n",
    "dt_interp.fit(X_train_interp, y_train_interp)\n",
    "\n",
    "# Calculate all importance measures\n",
    "feature_names = [f'Feature_{i}' for i in range(n_features)]\n",
    "all_importances = dt_interp.calculate_all_importances(X_test_interp, y_test_interp)\n",
    "\n",
    "# Get decision rules\n",
    "rules = dt_interp.get_tree_rules(feature_names)\n",
    "\n",
    "# Analyze feature interactions\n",
    "interactions = dt_interp.analyze_feature_interactions(X_train_interp, y_train_interp)\n",
    "\n",
    "print(\"Feature Importance Comparison:\")\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Gini_Entropy': all_importances['gini_entropy'],\n",
    "    'Permutation': all_importances['permutation'],\n",
    "    'Drop_Column': all_importances['drop_column']\n",
    "})\n",
    "print(importance_df.round(4))\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(y_test_interp, dt_interp.predict(X_test_interp)):.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Decision Rules:\")\n",
    "for i, rule in enumerate(rules[:5]):\n",
    "    print(f\"{i+1}. IF {rule['rule']} THEN class={rule['prediction']} (samples={rule['samples']})\")\n",
    "\n",
    "print(\"\\nTop Feature Interactions:\")\n",
    "for interaction, frequency in interactions[:5]:\n",
    "    feature_names_interaction = [feature_names[i] for i in interaction]\n",
    "    print(f\"{' & '.join(feature_names_interaction)}: {frequency} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance and interpretability\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Feature importance comparison\n",
    "x_pos = np.arange(len(feature_names))\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 0].bar(x_pos - width, all_importances['gini_entropy'], width, \n",
    "              label='Gini/Entropy', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos, all_importances['permutation'], width, \n",
    "              label='Permutation', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + width, all_importances['drop_column'], width, \n",
    "              label='Drop Column', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Features')\n",
    "axes[0, 0].set_ylabel('Importance')\n",
    "axes[0, 0].set_title('Feature Importance Comparison')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(feature_names, rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation between importance measures\n",
    "axes[0, 1].scatter(all_importances['gini_entropy'], all_importances['permutation'], \n",
    "                  alpha=0.7, s=100, edgecolors='black')\n",
    "for i, feature in enumerate(feature_names):\n",
    "    axes[0, 1].annotate(feature, \n",
    "                       (all_importances['gini_entropy'][i], all_importances['permutation'][i]),\n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Add correlation line\n",
    "corr = np.corrcoef(all_importances['gini_entropy'], all_importances['permutation'])[0, 1]\n",
    "axes[0, 1].set_xlabel('Gini/Entropy Importance')\n",
    "axes[0, 1].set_ylabel('Permutation Importance')\n",
    "axes[0, 1].set_title(f'Importance Correlation (r={corr:.3f})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature stability analysis\n",
    "n_bootstrap = 30\n",
    "bootstrap_importances = []\n",
    "\n",
    "for _ in range(n_bootstrap):\n",
    "    # Bootstrap sample\n",
    "    bootstrap_idx = np.random.choice(len(X_train_interp), len(X_train_interp), replace=True)\n",
    "    X_bootstrap = X_train_interp[bootstrap_idx]\n",
    "    y_bootstrap = y_train_interp[bootstrap_idx]\n",
    "    \n",
    "    # Train tree\n",
    "    dt_bootstrap = InterpretableDecisionTree(\n",
    "        criterion='gini', max_depth=6, min_samples_leaf=10, task='classification'\n",
    "    )\n",
    "    dt_bootstrap.fit(X_bootstrap, y_bootstrap)\n",
    "    \n",
    "    bootstrap_importances.append(dt_bootstrap.feature_importances_)\n",
    "\n",
    "bootstrap_importances = np.array(bootstrap_importances)\n",
    "\n",
    "# Box plot of feature importance stability\n",
    "axes[0, 2].boxplot([bootstrap_importances[:, i] for i in range(n_features)], \n",
    "                  labels=feature_names)\n",
    "axes[0, 2].set_ylabel('Feature Importance')\n",
    "axes[0, 2].set_title('Feature Importance Stability')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Tree depth vs interpretability\n",
    "depths = range(1, 11)\n",
    "n_rules = []\n",
    "avg_rule_length = []\n",
    "test_accuracies = []\n",
    "\n",
    "for depth in depths:\n",
    "    dt_depth = InterpretableDecisionTree(\n",
    "        criterion='gini', max_depth=depth, min_samples_leaf=10, task='classification'\n",
    "    )\n",
    "    dt_depth.fit(X_train_interp, y_train_interp)\n",
    "    \n",
    "    rules_depth = dt_depth.get_tree_rules(feature_names)\n",
    "    \n",
    "    n_rules.append(len(rules_depth))\n",
    "    avg_length = np.mean([len(rule['rule'].split(' AND ')) for rule in rules_depth])\n",
    "    avg_rule_length.append(avg_length)\n",
    "    \n",
    "    test_acc = accuracy_score(y_test_interp, dt_depth.predict(X_test_interp))\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "ax1 = axes[1, 0]\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "line1 = ax1.plot(depths, n_rules, 'b-o', label='Number of Rules')\n",
    "line2 = ax2.plot(depths, test_accuracies, 'r-s', label='Test Accuracy')\n",
    "\n",
    "ax1.set_xlabel('Max Depth')\n",
    "ax1.set_ylabel('Number of Rules', color='b')\n",
    "ax2.set_ylabel('Test Accuracy', color='r')\n",
    "ax1.set_title('Complexity vs Performance')\n",
    "\n",
    "# Combine legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='center right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Rule length distribution\n",
    "axes[1, 1].plot(depths, avg_rule_length, 'g-^', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Max Depth')\n",
    "axes[1, 1].set_ylabel('Average Rule Length')\n",
    "axes[1, 1].set_title('Rule Complexity vs Tree Depth')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature usage frequency in tree\n",
    "feature_usage = np.zeros(n_features)\n",
    "\n",
    "def count_feature_usage(node):\n",
    "    if node.is_leaf():\n",
    "        return\n",
    "    feature_usage[node.feature] += 1\n",
    "    count_feature_usage(node.left)\n",
    "    count_feature_usage(node.right)\n",
    "\n",
    "count_feature_usage(dt_interp.root)\n",
    "\n",
    "bars = axes[1, 2].bar(feature_names, feature_usage, alpha=0.7, color='purple')\n",
    "axes[1, 2].set_ylabel('Usage Count in Tree')\n",
    "axes[1, 2].set_title('Feature Usage in Tree Structure')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "for bar, val in zip(bars, feature_usage):\n",
    "    if val > 0:\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "                       f'{int(val)}', ha='center', va='bottom')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate feature importance statistics\n",
    "print(\"\\nFeature Importance Statistics:\")\n",
    "for i, feature in enumerate(feature_names):\n",
    "    std_dev = np.std(bootstrap_importances[:, i])\n",
    "    cv = std_dev / (np.mean(bootstrap_importances[:, i]) + 1e-10)  # Coefficient of variation\n",
    "    print(f\"{feature}: Mean={np.mean(bootstrap_importances[:, i]):.4f}, \"\n",
    "          f\"Std={std_dev:.4f}, CV={cv:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretability Insights:\")\n",
    "print(f\"- Tree has {len(rules)} decision rules\")\n",
    "print(f\"- Average rule length: {np.mean([len(rule['rule'].split(' AND ')) for rule in rules]):.2f} conditions\")\n",
    "print(f\"- Most important features: {', '.join([feature_names[i] for i in np.argsort(all_importances['permutation'])[-3:]])}\")\n",
    "print(f\"- Feature importance correlation (Gini vs Permutation): {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Tree-Based Methods Fundamentals:\n",
    "\n",
    "1. **Splitting Criteria**:\n",
    "   - **Gini Impurity**: Range [0, 0.5] for binary classification; computationally efficient\n",
    "   - **Entropy**: Range [0, 1] for binary classification; more sensitive to changes\n",
    "   - **MSE**: Used for regression; minimizes variance in predictions\n",
    "   - Choice of criterion has minimal impact on final performance in most cases\n",
    "\n",
    "2. **Pruning Strategies**:\n",
    "   - **Pre-pruning**: Prevents overfitting during construction; computationally efficient\n",
    "   - **Post-pruning**: More thorough but expensive; builds full tree then removes branches\n",
    "   - **Cost Complexity Pruning**: Balances tree size and accuracy using Î± parameter\n",
    "   - Combined approaches often work best in practice\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - **Gini/Entropy Importance**: Fast to compute but can be biased\n",
    "   - **Permutation Importance**: More reliable; measures actual predictive contribution\n",
    "   - **Drop-Column Importance**: Most accurate but computationally expensive\n",
    "   - Feature importance can be unstable across different tree structures\n",
    "\n",
    "### Practical Guidelines:\n",
    "\n",
    "**Preventing Overfitting:**\n",
    "- Set reasonable max_depth (typically 3-8 for interpretability)\n",
    "- Use min_samples_leaf (5-20) to ensure statistical significance\n",
    "- Monitor validation performance for optimal pruning\n",
    "- Consider ensemble methods for better generalization\n",
    "\n",
    "**Interpretability:**\n",
    "- Shallow trees (depth â¤ 5) are most interpretable\n",
    "- Decision rules provide clear logical explanations\n",
    "- Feature interactions are naturally captured\n",
    "- Use permutation importance for reliable feature ranking\n",
    "\n",
    "**When to Use Decision Trees:**\n",
    "- Need interpretable models\n",
    "- Mixed data types (numerical and categorical)\n",
    "- Non-linear relationships and interactions\n",
    "- Missing values can be handled naturally\n",
    "\n",
    "**Limitations:**\n",
    "- High variance (small data changes â different trees)\n",
    "- Bias toward features with more levels\n",
    "- Difficulty with linear relationships\n",
    "- Can create overly complex models without pruning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}