{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions Part 8: Neural Networks Fundamentals\n",
    "\n",
    "This notebook covers the mathematical foundations and practical implementation of neural networks, including forward propagation, backpropagation, and optimization techniques. Each question builds understanding from basic perceptrons to multi-layer networks.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Perceptron and multi-layer perceptron architecture\n",
    "- Forward propagation and activation functions\n",
    "- Backpropagation algorithm and gradient computation\n",
    "- Loss functions and optimization techniques\n",
    "- Regularization and generalization in neural networks\n",
    "\n",
    "**Format:** Each question includes theory, implementation, and analysis sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_regression, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Perceptron and Forward Propagation\n",
    "\n",
    "**Question:** Implement a multi-layer perceptron from scratch with different activation functions. Analyze how activation functions affect gradient flow and network expressiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Multi-Layer Perceptron Architecture:**\n",
    "- **Input layer**: $\\mathbf{x} \\in \\mathbb{R}^{n}$\n",
    "- **Hidden layers**: $\\mathbf{h}^{(l)} = f(\\mathbf{W}^{(l)}\\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)})$\n",
    "- **Output layer**: $\\mathbf{y} = g(\\mathbf{W}^{(L)}\\mathbf{h}^{(L-1)} + \\mathbf{b}^{(L)})$\n",
    "\n",
    "**Forward Propagation:**\n",
    "$$\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)}\\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}$$\n",
    "$$\\mathbf{a}^{(l)} = f(\\mathbf{z}^{(l)})$$\n",
    "\n",
    "**Activation Functions:**\n",
    "\n",
    "**Sigmoid**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "**Tanh**: $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$, $\\tanh'(z) = 1 - \\tanh^2(z)$\n",
    "\n",
    "**ReLU**: $\\text{ReLU}(z) = \\max(0, z)$, $\\text{ReLU}'(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "**Leaky ReLU**: $\\text{LeakyReLU}(z) = \\max(\\alpha z, z)$, where $\\alpha = 0.01$\n",
    "\n",
    "**Universal Approximation Theorem:**\n",
    "A feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    \"\"\"Base class for activation functions.\"\"\"\n",
    "    \n",
    "    def forward(self, z):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, z):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def forward(self, z):\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def backward(self, z):\n",
    "        s = self.forward(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def forward(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def backward(self, z):\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "\n",
    "class ReLU(ActivationFunction):\n",
    "    def forward(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def backward(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "class LeakyReLU(ActivationFunction):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return np.where(z > 0, z, self.alpha * z)\n",
    "    \n",
    "    def backward(self, z):\n",
    "        return np.where(z > 0, 1, self.alpha)\n",
    "\n",
    "class MLPCustom:\n",
    "    \"\"\"Multi-Layer Perceptron implementation from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_sizes, activation='relu', output_activation='sigmoid', \n",
    "                 learning_rate=0.01, max_iter=1000, random_state=None):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set activation functions\n",
    "        activations = {\n",
    "            'sigmoid': Sigmoid(),\n",
    "            'tanh': Tanh(),\n",
    "            'relu': ReLU(),\n",
    "            'leaky_relu': LeakyReLU()\n",
    "        }\n",
    "        \n",
    "        self.activation = activations[activation]\n",
    "        self.output_activation = activations[output_activation]\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def _initialize_parameters(self, input_size, output_size):\n",
    "        \"\"\"Initialize weights and biases using Xavier initialization.\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # Build layer sizes\n",
    "        layer_sizes = [input_size] + self.hidden_sizes + [output_size]\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Xavier initialization\n",
    "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
    "            W = np.random.uniform(-limit, limit, (layer_sizes[i + 1], layer_sizes[i]))\n",
    "            b = np.zeros((layer_sizes[i + 1], 1))\n",
    "            \n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation through the network.\"\"\"\n",
    "        activations = [X.T]  # Transpose for column vectors\n",
    "        z_values = []\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            # Linear transformation\n",
    "            z = self.weights[i] @ activations[-1] + self.biases[i]\n",
    "            z_values.append(z)\n",
    "            \n",
    "            # Apply activation function\n",
    "            if i == len(self.weights) - 1:  # Output layer\n",
    "                a = self.output_activation.forward(z)\n",
    "            else:  # Hidden layers\n",
    "                a = self.activation.forward(z)\n",
    "            \n",
    "            activations.append(a)\n",
    "        \n",
    "        return activations, z_values\n",
    "    \n",
    "    def _compute_cost(self, y_true, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "        m = y_true.shape[1]\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        \n",
    "        cost = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "        return cost\n",
    "    \n",
    "    def _backward_propagation(self, X, y, activations, z_values):\n",
    "        \"\"\"Backward propagation to compute gradients.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        n_layers = len(self.weights)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW = [np.zeros_like(w) for w in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dz = activations[-1] - y.T\n",
    "        \n",
    "        # Backpropagate through all layers\n",
    "        for i in reversed(range(n_layers)):\n",
    "            # Gradients for weights and biases\n",
    "            dW[i] = dz @ activations[i].T / m\n",
    "            db[i] = np.sum(dz, axis=1, keepdims=True) / m\n",
    "            \n",
    "            # Gradient for previous layer (if not input layer)\n",
    "            if i > 0:\n",
    "                da_prev = self.weights[i].T @ dz\n",
    "                \n",
    "                # Apply derivative of activation function\n",
    "                if i == 1:  # First hidden layer\n",
    "                    dz = da_prev * self.activation.backward(z_values[i-1])\n",
    "                else:\n",
    "                    dz = da_prev * self.activation.backward(z_values[i-1])\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X.shape[1], 1)\n",
    "        \n",
    "        self.loss_history = []\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            # Forward propagation\n",
    "            activations, z_values = self._forward_propagation(X)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self._compute_cost(y.T, activations[-1])\n",
    "            self.loss_history.append(cost)\n",
    "            \n",
    "            # Backward propagation\n",
    "            dW, db = self._backward_propagation(X, y, activations, z_values)\n",
    "            \n",
    "            # Update parameters\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= self.learning_rate * dW[i]\n",
    "                self.biases[i] -= self.learning_rate * db[i]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        activations, _ = self._forward_propagation(X)\n",
    "        return activations[-1].T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make binary predictions.\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= 0.5).astype(int).flatten()\n",
    "\n",
    "# Generate datasets for testing\n",
    "# Linear separable data\n",
    "X_linear, y_linear = make_classification(n_samples=500, n_features=2, n_redundant=0, \n",
    "                                        n_informative=2, n_clusters_per_class=1, \n",
    "                                        class_sep=2.0, random_state=42)\n",
    "\n",
    "# Non-linear data (moons)\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "# Non-linear data (circles)\n",
    "X_circles, y_circles = make_circles(n_samples=500, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "datasets = {\n",
    "    'Linear': (X_linear, y_linear),\n",
    "    'Moons': (X_moons, y_moons),\n",
    "    'Circles': (X_circles, y_circles)\n",
    "}\n",
    "\n",
    "# Test different activation functions\n",
    "activation_functions = ['sigmoid', 'tanh', 'relu', 'leaky_relu']\n",
    "results = {}\n",
    "\n",
    "for dataset_name, (X, y) in datasets.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    dataset_results = {}\n",
    "    \n",
    "    for activation in activation_functions:\n",
    "        # Custom MLP\n",
    "        mlp_custom = MLPCustom(\n",
    "            hidden_sizes=[10, 5],\n",
    "            activation=activation,\n",
    "            learning_rate=0.1,\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        mlp_custom.fit(X_train_scaled, y_train)\n",
    "        y_pred_custom = mlp_custom.predict(X_test_scaled)\n",
    "        accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "        \n",
    "        # Sklearn MLP for comparison\n",
    "        mlp_sklearn = MLPClassifier(\n",
    "            hidden_layer_sizes=(10, 5),\n",
    "            activation=activation,\n",
    "            learning_rate_init=0.1,\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        mlp_sklearn.fit(X_train_scaled, y_train)\n",
    "        y_pred_sklearn = mlp_sklearn.predict(X_test_scaled)\n",
    "        accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "        \n",
    "        dataset_results[activation] = {\n",
    "            'custom_accuracy': accuracy_custom,\n",
    "            'sklearn_accuracy': accuracy_sklearn,\n",
    "            'final_loss': mlp_custom.loss_history[-1] if mlp_custom.loss_history else np.inf\n",
    "        }\n",
    "    \n",
    "    results[dataset_name] = dataset_results\n",
    "\n",
    "# Print results\n",
    "for dataset_name, dataset_results in results.items():\n",
    "    print(f\"\\n{dataset_name} Dataset Results:\")\n",
    "    df = pd.DataFrame(dataset_results).T\n",
    "    print(df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions and their derivatives\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "z = np.linspace(-5, 5, 1000)\n",
    "activations = {\n",
    "    'Sigmoid': Sigmoid(),\n",
    "    'Tanh': Tanh(),\n",
    "    'ReLU': ReLU(),\n",
    "    'Leaky ReLU': LeakyReLU()\n",
    "}\n",
    "\n",
    "for i, (name, activation) in enumerate(activations.items()):\n",
    "    # Activation function\n",
    "    y = activation.forward(z)\n",
    "    axes[0, i].plot(z, y, 'b-', linewidth=2, label=f'{name}')\n",
    "    axes[0, i].set_title(f'{name} Activation')\n",
    "    axes[0, i].set_xlabel('z')\n",
    "    axes[0, i].set_ylabel('f(z)')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    axes[0, i].legend()\n",
    "    \n",
    "    # Derivative\n",
    "    dy = activation.backward(z)\n",
    "    axes[1, i].plot(z, dy, 'r-', linewidth=2, label=f\"{name}' \")\n",
    "    axes[1, i].set_title(f'{name} Derivative')\n",
    "    axes[1, i].set_xlabel('z')\n",
    "    axes[1, i].set_ylabel(\"f'(z)\")\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "    axes[1, i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze gradient flow issues\n",
    "print(\"\\nGradient Flow Analysis:\")\n",
    "print(\"Sigmoid: Suffers from vanishing gradients (max derivative = 0.25)\")\n",
    "print(\"Tanh: Better than sigmoid but still vanishing gradients (max derivative = 1.0)\")\n",
    "print(\"ReLU: Solves vanishing gradients but has dying ReLU problem\")\n",
    "print(\"Leaky ReLU: Addresses dying ReLU with small negative slope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Backpropagation Algorithm Implementation\n",
    "\n",
    "**Question:** Implement the backpropagation algorithm step by step and verify gradients using numerical differentiation. Analyze how gradient computation scales with network depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Backpropagation Algorithm:**\n",
    "\n",
    "**Chain Rule Application:**\n",
    "$$\\frac{\\partial L}{\\partial W^{(l)}} = \\frac{\\partial L}{\\partial z^{(l)}} \\frac{\\partial z^{(l)}}{\\partial W^{(l)}}$$\n",
    "\n",
    "**Layer-wise Gradient Computation:**\n",
    "1. **Output layer error**: $\\delta^{(L)} = \\frac{\\partial L}{\\partial z^{(L)}} = \\frac{\\partial L}{\\partial a^{(L)}} \\odot f'(z^{(L)})$\n",
    "\n",
    "2. **Hidden layer error**: $\\delta^{(l)} = ((W^{(l+1)})^T \\delta^{(l+1)}) \\odot f'(z^{(l)})$\n",
    "\n",
    "3. **Weight gradients**: $\\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} (a^{(l-1)})^T$\n",
    "\n",
    "4. **Bias gradients**: $\\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)}$\n",
    "\n",
    "**Numerical Gradient Checking:**\n",
    "$$\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta - \\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "**Computational Complexity:**\n",
    "- Forward pass: $O(W)$ where $W$ is total number of weights\n",
    "- Backward pass: $O(W)$ (same as forward pass)\n",
    "- Memory: $O(W + A)$ where $A$ is total activations stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackpropagationAnalyzer:\n",
    "    \"\"\"Detailed backpropagation implementation with gradient checking.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='tanh'):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation = Tanh() if activation == 'tanh' else ReLU()\n",
    "        self.output_activation = Sigmoid()\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.initialize_parameters()\n",
    "        \n",
    "        # For analysis\n",
    "        self.gradient_norms = []\n",
    "        self.activation_stats = []\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize parameters with small random values.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            # He initialization for ReLU, Xavier for tanh\n",
    "            if isinstance(self.activation, ReLU):\n",
    "                std = np.sqrt(2.0 / self.layer_sizes[i])\n",
    "            else:\n",
    "                std = np.sqrt(1.0 / self.layer_sizes[i])\n",
    "            \n",
    "            W = np.random.normal(0, std, (self.layer_sizes[i+1], self.layer_sizes[i]))\n",
    "            b = np.zeros((self.layer_sizes[i+1], 1))\n",
    "            \n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def forward_propagation_detailed(self, X):\n",
    "        \"\"\"Forward propagation with detailed tracking.\"\"\"\n",
    "        activations = [X.T]\n",
    "        z_values = []\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            # Linear transformation\n",
    "            z = self.weights[i] @ activations[-1] + self.biases[i]\n",
    "            z_values.append(z)\n",
    "            \n",
    "            # Activation\n",
    "            if i == len(self.weights) - 1:  # Output layer\n",
    "                a = self.output_activation.forward(z)\n",
    "            else:\n",
    "                a = self.activation.forward(z)\n",
    "            \n",
    "            activations.append(a)\n",
    "        \n",
    "        return activations, z_values\n",
    "    \n",
    "    def backward_propagation_detailed(self, X, y, activations, z_values):\n",
    "        \"\"\"Detailed backward propagation with analysis.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        n_layers = len(self.weights)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW = [np.zeros_like(w) for w in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "        deltas = []\n",
    "        \n",
    "        # Output layer gradient (for binary cross-entropy)\n",
    "        delta = activations[-1] - y.T\n",
    "        deltas.append(delta)\n",
    "        \n",
    "        # Backpropagate through all layers\n",
    "        for i in reversed(range(n_layers)):\n",
    "            # Gradients for current layer\n",
    "            dW[i] = delta @ activations[i].T / m\n",
    "            db[i] = np.sum(delta, axis=1, keepdims=True) / m\n",
    "            \n",
    "            # Propagate error to previous layer\n",
    "            if i > 0:\n",
    "                # Error propagation\n",
    "                delta_prev = self.weights[i].T @ delta\n",
    "                \n",
    "                # Apply activation derivative\n",
    "                activation_derivative = self.activation.backward(z_values[i-1])\n",
    "                delta = delta_prev * activation_derivative\n",
    "                \n",
    "                deltas.append(delta)\n",
    "        \n",
    "        # Store gradient norms for analysis\n",
    "        layer_grad_norms = []\n",
    "        for i, dw in enumerate(dW):\n",
    "            grad_norm = np.linalg.norm(dw)\n",
    "            layer_grad_norms.append(grad_norm)\n",
    "        \n",
    "        self.gradient_norms.append(layer_grad_norms)\n",
    "        \n",
    "        return dW, db, deltas[::-1]  # Reverse to match layer order\n",
    "    \n",
    "    def numerical_gradient(self, X, y, epsilon=1e-7):\n",
    "        \"\"\"Compute numerical gradients for verification.\"\"\"\n",
    "        numerical_dW = []\n",
    "        numerical_db = []\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            # Numerical gradient for weights\n",
    "            dW_num = np.zeros_like(self.weights[i])\n",
    "            \n",
    "            for row in range(self.weights[i].shape[0]):\n",
    "                for col in range(self.weights[i].shape[1]):\n",
    "                    # Forward perturbation\n",
    "                    self.weights[i][row, col] += epsilon\n",
    "                    activations_plus, _ = self.forward_propagation_detailed(X)\n",
    "                    loss_plus = self.compute_loss(y, activations_plus[-1])\n",
    "                    \n",
    "                    # Backward perturbation\n",
    "                    self.weights[i][row, col] -= 2 * epsilon\n",
    "                    activations_minus, _ = self.forward_propagation_detailed(X)\n",
    "                    loss_minus = self.compute_loss(y, activations_minus[-1])\n",
    "                    \n",
    "                    # Numerical gradient\n",
    "                    dW_num[row, col] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                    \n",
    "                    # Restore original value\n",
    "                    self.weights[i][row, col] += epsilon\n",
    "            \n",
    "            numerical_dW.append(dW_num)\n",
    "            \n",
    "            # Numerical gradient for biases (simplified - just first few)\n",
    "            db_num = np.zeros_like(self.biases[i])\n",
    "            for row in range(min(3, self.biases[i].shape[0])):  # Check first 3 biases only\n",
    "                # Forward perturbation\n",
    "                self.biases[i][row, 0] += epsilon\n",
    "                activations_plus, _ = self.forward_propagation_detailed(X)\n",
    "                loss_plus = self.compute_loss(y, activations_plus[-1])\n",
    "                \n",
    "                # Backward perturbation\n",
    "                self.biases[i][row, 0] -= 2 * epsilon\n",
    "                activations_minus, _ = self.forward_propagation_detailed(X)\n",
    "                loss_minus = self.compute_loss(y, activations_minus[-1])\n",
    "                \n",
    "                # Numerical gradient\n",
    "                db_num[row, 0] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                \n",
    "                # Restore original value\n",
    "                self.biases[i][row, 0] += epsilon\n",
    "            \n",
    "            numerical_db.append(db_num)\n",
    "        \n",
    "        return numerical_dW, numerical_db\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        y_pred = np.clip(y_pred.T, 1e-15, 1 - 1e-15)\n",
    "        y_true = y_true.reshape(-1, 1)\n",
    "        \n",
    "        loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def gradient_check(self, X, y, tolerance=1e-5):\n",
    "        \"\"\"Verify gradients using numerical differentiation.\"\"\"\n",
    "        # Compute analytical gradients\n",
    "        activations, z_values = self.forward_propagation_detailed(X)\n",
    "        dW_analytical, db_analytical, _ = self.backward_propagation_detailed(X, y, activations, z_values)\n",
    "        \n",
    "        # Compute numerical gradients (subset for efficiency)\n",
    "        dW_numerical, db_numerical = self.numerical_gradient(X, y)\n",
    "        \n",
    "        # Compare gradients\n",
    "        gradient_errors = []\n",
    "        \n",
    "        for i in range(len(dW_analytical)):\n",
    "            # Weight gradients\n",
    "            if dW_numerical[i].size > 0:\n",
    "                diff_W = np.abs(dW_analytical[i] - dW_numerical[i])\n",
    "                relative_error_W = np.max(diff_W / (np.abs(dW_analytical[i]) + np.abs(dW_numerical[i]) + 1e-10))\n",
    "                gradient_errors.append(('Weight', i, relative_error_W))\n",
    "            \n",
    "            # Bias gradients (first few only)\n",
    "            if db_numerical[i].size > 0:\n",
    "                n_check = min(3, db_analytical[i].shape[0])\n",
    "                diff_b = np.abs(db_analytical[i][:n_check] - db_numerical[i][:n_check])\n",
    "                relative_error_b = np.max(diff_b / (np.abs(db_analytical[i][:n_check]) + np.abs(db_numerical[i][:n_check]) + 1e-10))\n",
    "                gradient_errors.append(('Bias', i, relative_error_b))\n",
    "        \n",
    "        return gradient_errors\n",
    "\n",
    "# Test gradient implementation\n",
    "print(\"Testing Backpropagation Implementation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create small test dataset\n",
    "X_test = np.random.randn(20, 3)\n",
    "y_test = (X_test[:, 0] + X_test[:, 1] > 0).astype(int)\n",
    "\n",
    "# Test different network architectures\n",
    "architectures = {\n",
    "    'Shallow': [3, 5, 1],\n",
    "    'Deep': [3, 10, 8, 5, 1],\n",
    "    'Very Deep': [3, 12, 10, 8, 6, 4, 1]\n",
    "}\n",
    "\n",
    "for arch_name, layer_sizes in architectures.items():\n",
    "    print(f\"\\n{arch_name} Network ({len(layer_sizes)-1} layers):\")\n",
    "    \n",
    "    # Create analyzer\n",
    "    analyzer = BackpropagationAnalyzer(layer_sizes, activation='tanh')\n",
    "    \n",
    "    # Check gradients\n",
    "    gradient_errors = analyzer.gradient_check(X_test, y_test)\n",
    "    \n",
    "    # Print results\n",
    "    max_error = max([error for _, _, error in gradient_errors])\n",
    "    print(f\"Maximum gradient error: {max_error:.2e}\")\n",
    "    \n",
    "    if max_error < 1e-4:\n",
    "        print(\"✓ Gradients are correct\")\n",
    "    else:\n",
    "        print(\"✗ Gradient computation may have errors\")\n",
    "        \n",
    "    # Analyze gradient flow\n",
    "    activations, z_values = analyzer.forward_propagation_detailed(X_test)\n",
    "    dW, db, deltas = analyzer.backward_propagation_detailed(X_test, y_test, activations, z_values)\n",
    "    \n",
    "    print(f\"Gradient norms by layer: {[f'{norm:.2e}' for norm in analyzer.gradient_norms[-1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient flow and vanishing gradient problem\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Test vanishing gradients with deep networks\n",
    "depths = range(2, 11)\n",
    "gradient_ratios_sigmoid = []\n",
    "gradient_ratios_relu = []\n",
    "\n",
    "for depth in depths:\n",
    "    # Create deep network with sigmoid\n",
    "    layer_sizes_sigmoid = [3] + [10] * (depth - 1) + [1]\n",
    "    analyzer_sigmoid = BackpropagationAnalyzer(layer_sizes_sigmoid, activation='tanh')\n",
    "    analyzer_sigmoid.activation = Sigmoid()  # Use sigmoid for vanishing gradient demo\n",
    "    \n",
    "    # Forward and backward pass\n",
    "    activations, z_values = analyzer_sigmoid.forward_propagation_detailed(X_test)\n",
    "    dW, db, deltas = analyzer_sigmoid.backward_propagation_detailed(X_test, y_test, activations, z_values)\n",
    "    \n",
    "    # Calculate gradient ratio (first layer / last layer)\n",
    "    first_layer_grad = np.linalg.norm(dW[0])\n",
    "    last_layer_grad = np.linalg.norm(dW[-1])\n",
    "    ratio_sigmoid = first_layer_grad / (last_layer_grad + 1e-10)\n",
    "    gradient_ratios_sigmoid.append(ratio_sigmoid)\n",
    "    \n",
    "    # Create deep network with ReLU\n",
    "    analyzer_relu = BackpropagationAnalyzer(layer_sizes_sigmoid, activation='relu')\n",
    "    \n",
    "    # Forward and backward pass\n",
    "    activations, z_values = analyzer_relu.forward_propagation_detailed(X_test)\n",
    "    dW, db, deltas = analyzer_relu.backward_propagation_detailed(X_test, y_test, activations, z_values)\n",
    "    \n",
    "    # Calculate gradient ratio\n",
    "    first_layer_grad = np.linalg.norm(dW[0])\n",
    "    last_layer_grad = np.linalg.norm(dW[-1])\n",
    "    ratio_relu = first_layer_grad / (last_layer_grad + 1e-10)\n",
    "    gradient_ratios_relu.append(ratio_relu)\n",
    "\n",
    "# Plot gradient ratios\n",
    "axes[0, 0].semilogy(depths, gradient_ratios_sigmoid, 'o-', label='Sigmoid', linewidth=2, markersize=6)\n",
    "axes[0, 0].semilogy(depths, gradient_ratios_relu, 's-', label='ReLU', linewidth=2, markersize=6)\n",
    "axes[0, 0].set_xlabel('Network Depth')\n",
    "axes[0, 0].set_ylabel('Gradient Ratio (First/Last Layer)')\n",
    "axes[0, 0].set_title('Vanishing Gradient Problem')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient flow visualization for specific network\n",
    "deep_network = BackpropagationAnalyzer([3, 12, 10, 8, 6, 4, 1], activation='tanh')\n",
    "activations, z_values = deep_network.forward_propagation_detailed(X_test)\n",
    "dW, db, deltas = deep_network.backward_propagation_detailed(X_test, y_test, activations, z_values)\n",
    "\n",
    "layer_indices = range(len(dW))\n",
    "gradient_norms = [np.linalg.norm(dw) for dw in dW]\n",
    "\n",
    "axes[0, 1].bar(layer_indices, gradient_norms, alpha=0.7, color='purple')\n",
    "axes[0, 1].set_xlabel('Layer Index')\n",
    "axes[0, 1].set_ylabel('Gradient Norm')\n",
    "axes[0, 1].set_title('Gradient Magnitudes by Layer')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Activation statistics\n",
    "activation_means = [np.mean(np.abs(a)) for a in activations[1:-1]]  # Exclude input and output\n",
    "activation_stds = [np.std(a) for a in activations[1:-1]]\n",
    "\n",
    "hidden_layer_indices = range(len(activation_means))\n",
    "axes[1, 0].bar(hidden_layer_indices, activation_means, alpha=0.7, color='green', label='Mean |activation|')\n",
    "axes[1, 0].set_xlabel('Hidden Layer Index')\n",
    "axes[1, 0].set_ylabel('Mean Absolute Activation')\n",
    "axes[1, 0].set_title('Activation Statistics by Layer')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Computational complexity analysis\n",
    "network_sizes = [(3, 10, 1), (3, 20, 10, 1), (3, 30, 20, 10, 1), (3, 40, 30, 20, 10, 1)]\n",
    "forward_times = []\n",
    "backward_times = []\n",
    "total_parameters = []\n",
    "\n",
    "import time\n",
    "\n",
    "for sizes in network_sizes:\n",
    "    analyzer = BackpropagationAnalyzer(list(sizes))\n",
    "    \n",
    "    # Count parameters\n",
    "    n_params = sum(w.size + b.size for w, b in zip(analyzer.weights, analyzer.biases))\n",
    "    total_parameters.append(n_params)\n",
    "    \n",
    "    # Time forward pass\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        activations, z_values = analyzer.forward_propagation_detailed(X_test)\n",
    "    forward_time = (time.time() - start_time) / 100\n",
    "    forward_times.append(forward_time)\n",
    "    \n",
    "    # Time backward pass\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        dW, db, deltas = analyzer.backward_propagation_detailed(X_test, y_test, activations, z_values)\n",
    "    backward_time = (time.time() - start_time) / 100\n",
    "    backward_times.append(backward_time)\n",
    "\n",
    "axes[1, 1].plot(total_parameters, forward_times, 'o-', label='Forward Pass', linewidth=2, markersize=6)\n",
    "axes[1, 1].plot(total_parameters, backward_times, 's-', label='Backward Pass', linewidth=2, markersize=6)\n",
    "axes[1, 1].set_xlabel('Number of Parameters')\n",
    "axes[1, 1].set_ylabel('Time per Pass (seconds)')\n",
    "axes[1, 1].set_title('Computational Complexity')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient Flow Analysis:\")\n",
    "print(f\"Sigmoid networks show exponential decay in gradient ratios with depth\")\n",
    "print(f\"ReLU networks maintain more stable gradients\")\n",
    "print(f\"Backward pass time is approximately equal to forward pass time\")\n",
    "print(f\"Both scale linearly with number of parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Loss Functions and Optimization Techniques\n",
    "\n",
    "**Question:** Compare different loss functions for neural networks and implement various optimization algorithms. Analyze their convergence properties and suitability for different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Loss Functions:**\n",
    "\n",
    "**Binary Cross-Entropy**: $L = -\\frac{1}{m}\\sum_{i=1}^m [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$\n",
    "\n",
    "**Categorical Cross-Entropy**: $L = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{c=1}^C y_{ic} \\log(\\hat{y}_{ic})$\n",
    "\n",
    "**Mean Squared Error**: $L = \\frac{1}{2m}\\sum_{i=1}^m (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "**Huber Loss**: $L_{\\delta} = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\ \\delta|y - \\hat{y}| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "**Optimization Algorithms:**\n",
    "\n",
    "**SGD with Momentum**: \n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta)\\nabla L(\\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha v_t$$\n",
    "\n",
    "**Adam**:\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)\\nabla L(\\theta_t)$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)(\\nabla L(\\theta_t))^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    \"\"\"Base class for loss functions.\"\"\"\n",
    "    \n",
    "    def forward(self, y_true, y_pred):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BinaryCrossEntropy(LossFunction):\n",
    "    def forward(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred)) / len(y_true)\n",
    "\n",
    "class MeanSquaredError(LossFunction):\n",
    "    def forward(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true) / len(y_true)\n",
    "\n",
    "class HuberLoss(LossFunction):\n",
    "    def __init__(self, delta=1.0):\n",
    "        self.delta = delta\n",
    "    \n",
    "    def forward(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = np.abs(error) <= self.delta\n",
    "        \n",
    "        squared_loss = 0.5 * error ** 2\n",
    "        linear_loss = self.delta * np.abs(error) - 0.5 * self.delta ** 2\n",
    "        \n",
    "        return np.mean(np.where(is_small_error, squared_loss, linear_loss))\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        error = y_pred - y_true\n",
    "        is_small_error = np.abs(error) <= self.delta\n",
    "        \n",
    "        return np.where(is_small_error, error, self.delta * np.sign(error)) / len(y_true)\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"Base class for optimizers.\"\"\"\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        updated_params = []\n",
    "        for i, (param, grad) in enumerate(zip(params, gradients)):\n",
    "            self.velocity[i] = self.momentum * self.velocity[i] + (1 - self.momentum) * grad\n",
    "            updated_param = param - self.learning_rate * self.velocity[i]\n",
    "            updated_params.append(updated_param)\n",
    "        \n",
    "        return updated_params\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.m = None  # First moment estimate\n",
    "        self.v = None  # Second moment estimate\n",
    "        self.t = 0     # Time step\n",
    "    \n",
    "    def update(self, params, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        self.t += 1\n",
    "        updated_params = []\n",
    "        \n",
    "        for i, (param, grad) in enumerate(zip(params, gradients)):\n",
    "            # Update biased first and second moment estimates\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2\n",
    "            \n",
    "            # Compute bias-corrected estimates\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            updated_param = param - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            updated_params.append(updated_param)\n",
    "        \n",
    "        return updated_params\n",
    "\n",
    "class NeuralNetworkTrainer:\n",
    "    \"\"\"Neural network trainer with different loss functions and optimizers.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, loss_function, optimizer, activation='relu'):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Set activation functions\n",
    "        activations = {\n",
    "            'sigmoid': Sigmoid(),\n",
    "            'tanh': Tanh(),\n",
    "            'relu': ReLU()\n",
    "        }\n",
    "        self.activation = activations[activation]\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            # He initialization for ReLU\n",
    "            if isinstance(self.activation, ReLU):\n",
    "                std = np.sqrt(2.0 / self.layer_sizes[i])\n",
    "            else:\n",
    "                std = np.sqrt(1.0 / self.layer_sizes[i])\n",
    "            \n",
    "            W = np.random.normal(0, std, (self.layer_sizes[i+1], self.layer_sizes[i]))\n",
    "            b = np.zeros((self.layer_sizes[i+1], 1))\n",
    "            \n",
    "            self.weights.append(W)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Forward propagation.\"\"\"\n",
    "        activations = [X.T]\n",
    "        z_values = []\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            z = self.weights[i] @ activations[-1] + self.biases[i]\n",
    "            z_values.append(z)\n",
    "            \n",
    "            if i == len(self.weights) - 1:  # Output layer\n",
    "                a = z  # Linear output for regression, or add sigmoid for classification\n",
    "            else:\n",
    "                a = self.activation.forward(z)\n",
    "            \n",
    "            activations.append(a)\n",
    "        \n",
    "        return activations, z_values\n",
    "    \n",
    "    def backward_propagation(self, X, y, activations, z_values):\n",
    "        \"\"\"Backward propagation.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        n_layers = len(self.weights)\n",
    "        \n",
    "        dW = [np.zeros_like(w) for w in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dz = self.loss_function.backward(y.T, activations[-1])\n",
    "        \n",
    "        for i in reversed(range(n_layers)):\n",
    "            dW[i] = dz @ activations[i].T\n",
    "            db[i] = np.sum(dz, axis=1, keepdims=True)\n",
    "            \n",
    "            if i > 0:\n",
    "                da_prev = self.weights[i].T @ dz\n",
    "                dz = da_prev * self.activation.backward(z_values[i-1])\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=False):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        self.initialize_parameters()\n",
    "        self.loss_history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            activations, z_values = self.forward_propagation(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.loss_function.forward(y, activations[-1].T)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            dW, db = self.backward_propagation(X, y, activations, z_values)\n",
    "            \n",
    "            # Flatten gradients for optimizer\n",
    "            all_params = []\n",
    "            all_gradients = []\n",
    "            \n",
    "            for w, b, dw, db_grad in zip(self.weights, self.biases, dW, db):\n",
    "                all_params.extend([w, b])\n",
    "                all_gradients.extend([dw, db_grad])\n",
    "            \n",
    "            # Update parameters\n",
    "            updated_params = self.optimizer.update(all_params, all_gradients)\n",
    "            \n",
    "            # Restore parameter structure\n",
    "            param_idx = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = updated_params[param_idx]\n",
    "                self.biases[i] = updated_params[param_idx + 1]\n",
    "                param_idx += 2\n",
    "            \n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return activations[-1].T\n",
    "\n",
    "# Test different loss functions and optimizers\n",
    "print(\"Comparing Loss Functions and Optimizers:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate regression dataset\n",
    "X_reg, y_reg = make_regression(n_samples=500, n_features=5, noise=10, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_reg_scaled = scaler_X.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_X.transform(X_test_reg)\n",
    "y_train_reg_scaled = scaler_y.fit_transform(y_train_reg.reshape(-1, 1)).flatten()\n",
    "y_test_reg_scaled = scaler_y.transform(y_test_reg.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Test configurations\n",
    "configurations = {\n",
    "    'MSE + SGD': {\n",
    "        'loss': MeanSquaredError(),\n",
    "        'optimizer': SGD(learning_rate=0.01)\n",
    "    },\n",
    "    'MSE + SGD-Momentum': {\n",
    "        'loss': MeanSquaredError(),\n",
    "        'optimizer': SGD(learning_rate=0.01, momentum=0.9)\n",
    "    },\n",
    "    'MSE + Adam': {\n",
    "        'loss': MeanSquaredError(),\n",
    "        'optimizer': Adam(learning_rate=0.001)\n",
    "    },\n",
    "    'Huber + Adam': {\n",
    "        'loss': HuberLoss(delta=1.0),\n",
    "        'optimizer': Adam(learning_rate=0.001)\n",
    "    }\n",
    "}\n",
    "\n",
    "training_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for config_name, config in configurations.items():\n",
    "    print(f\"\\nTraining with {config_name}...\")\n",
    "    \n",
    "    trainer = NeuralNetworkTrainer(\n",
    "        layer_sizes=[5, 20, 10, 1],\n",
    "        loss_function=config['loss'],\n",
    "        optimizer=config['optimizer'],\n",
    "        activation='relu'\n",
    "    )\n",
    "    \n",
    "    trainer.train(X_train_reg_scaled, y_train_reg_scaled, epochs=500)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_train = trainer.predict(X_train_reg_scaled)\n",
    "    y_pred_test = trainer.predict(X_test_reg_scaled)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train_reg_scaled, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test_reg_scaled, y_pred_test)\n",
    "    \n",
    "    training_results[config_name] = {\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'final_loss': trainer.loss_history[-1]\n",
    "    }\n",
    "    \n",
    "    trained_models[config_name] = trainer\n",
    "    print(f\"Final loss: {trainer.loss_history[-1]:.6f}, Test MSE: {test_mse:.6f}\")\n",
    "\n",
    "# Print comparison\n",
    "results_df = pd.DataFrame(training_results).T\n",
    "print(\"\\nComparison Results:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training dynamics and loss landscapes\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Loss curves\n",
    "for config_name, trainer in trained_models.items():\n",
    "    axes[0, 0].plot(trainer.loss_history, label=config_name, alpha=0.8, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss Curves')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Convergence speed comparison\n",
    "convergence_epochs = []\n",
    "config_names = list(trained_models.keys())\n",
    "\n",
    "for config_name, trainer in trained_models.items():\n",
    "    # Find epoch where loss reaches 95% of final value\n",
    "    final_loss = trainer.loss_history[-1]\n",
    "    target_loss = final_loss * 1.05\n",
    "    \n",
    "    convergence_epoch = len(trainer.loss_history)\n",
    "    for i, loss in enumerate(trainer.loss_history):\n",
    "        if loss <= target_loss:\n",
    "            convergence_epoch = i\n",
    "            break\n",
    "    \n",
    "    convergence_epochs.append(convergence_epoch)\n",
    "\n",
    "bars = axes[0, 1].bar(config_names, convergence_epochs, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_ylabel('Epochs to Convergence')\n",
    "axes[0, 1].set_title('Convergence Speed')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "for bar, val in zip(bars, convergence_epochs):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 5,\n",
    "                   f'{val}', ha='center', va='bottom')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss function comparison\n",
    "y_range = np.linspace(-3, 3, 100)\n",
    "y_true_val = 0  # Target value\n",
    "\n",
    "mse_loss = MeanSquaredError()\n",
    "huber_loss = HuberLoss(delta=1.0)\n",
    "\n",
    "mse_values = [mse_loss.forward(np.array([y_true_val]), np.array([y_pred])) for y_pred in y_range]\n",
    "huber_values = [huber_loss.forward(np.array([y_true_val]), np.array([y_pred])) for y_pred in y_range]\n",
    "\n",
    "axes[0, 2].plot(y_range, mse_values, 'b-', label='MSE', linewidth=2)\n",
    "axes[0, 2].plot(y_range, huber_values, 'r-', label='Huber (δ=1)', linewidth=2)\n",
    "axes[0, 2].axvline(x=y_true_val, color='black', linestyle='--', alpha=0.5, label='True value')\n",
    "axes[0, 2].set_xlabel('Predicted Value')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].set_title('Loss Function Comparison')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Optimizer behavior simulation\n",
    "# Simple 2D quadratic function: f(x,y) = x² + 10y²\n",
    "def quadratic_function(x, y):\n",
    "    return x**2 + 10*y**2\n",
    "\n",
    "def quadratic_gradient(x, y):\n",
    "    return np.array([2*x, 20*y])\n",
    "\n",
    "# Test different optimizers\n",
    "optimizers_2d = {\n",
    "    'SGD': SGD(learning_rate=0.1),\n",
    "    'SGD-Momentum': SGD(learning_rate=0.1, momentum=0.9),\n",
    "    'Adam': Adam(learning_rate=0.3)\n",
    "}\n",
    "\n",
    "# Starting point\n",
    "start_point = np.array([2.0, 1.0])\n",
    "n_steps = 50\n",
    "\n",
    "optimizer_paths = {}\n",
    "\n",
    "for opt_name, optimizer in optimizers_2d.items():\n",
    "    path = [start_point.copy()]\n",
    "    current_point = start_point.copy()\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        grad = quadratic_gradient(current_point[0], current_point[1])\n",
    "        updated_params = optimizer.update([current_point], [grad])\n",
    "        current_point = updated_params[0]\n",
    "        path.append(current_point.copy())\n",
    "    \n",
    "    optimizer_paths[opt_name] = np.array(path)\n",
    "\n",
    "# Plot optimization paths\n",
    "x = np.linspace(-2.5, 2.5, 100)\n",
    "y = np.linspace(-1.5, 1.5, 100)\n",
    "X_mesh, Y_mesh = np.meshgrid(x, y)\n",
    "Z = quadratic_function(X_mesh, Y_mesh)\n",
    "\n",
    "contour = axes[1, 0].contour(X_mesh, Y_mesh, Z, levels=20, alpha=0.6)\n",
    "axes[1, 0].clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (opt_name, path) in enumerate(optimizer_paths.items()):\n",
    "    axes[1, 0].plot(path[:, 0], path[:, 1], 'o-', color=colors[i], \n",
    "                   label=opt_name, alpha=0.8, linewidth=2, markersize=4)\n",
    "\n",
    "axes[1, 0].plot(0, 0, 'k*', markersize=15, label='Optimum')\n",
    "axes[1, 0].set_xlabel('x')\n",
    "axes[1, 0].set_ylabel('y')\n",
    "axes[1, 0].set_title('Optimizer Paths on Quadratic Function')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate sensitivity\n",
    "learning_rates = np.logspace(-3, 0, 20)\n",
    "final_losses_sgd = []\n",
    "final_losses_adam = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # SGD\n",
    "    trainer_sgd = NeuralNetworkTrainer(\n",
    "        layer_sizes=[5, 10, 1],\n",
    "        loss_function=MeanSquaredError(),\n",
    "        optimizer=SGD(learning_rate=lr),\n",
    "        activation='relu'\n",
    "    )\n",
    "    trainer_sgd.train(X_train_reg_scaled[:100], y_train_reg_scaled[:100], epochs=200)\n",
    "    final_losses_sgd.append(trainer_sgd.loss_history[-1])\n",
    "    \n",
    "    # Adam\n",
    "    trainer_adam = NeuralNetworkTrainer(\n",
    "        layer_sizes=[5, 10, 1],\n",
    "        loss_function=MeanSquaredError(),\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        activation='relu'\n",
    "    )\n",
    "    trainer_adam.train(X_train_reg_scaled[:100], y_train_reg_scaled[:100], epochs=200)\n",
    "    final_losses_adam.append(trainer_adam.loss_history[-1])\n",
    "\n",
    "axes[1, 1].loglog(learning_rates, final_losses_sgd, 'o-', label='SGD', linewidth=2, markersize=6)\n",
    "axes[1, 1].loglog(learning_rates, final_losses_adam, 's-', label='Adam', linewidth=2, markersize=6)\n",
    "axes[1, 1].set_xlabel('Learning Rate')\n",
    "axes[1, 1].set_ylabel('Final Loss')\n",
    "axes[1, 1].set_title('Learning Rate Sensitivity')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance vs network depth\n",
    "depths = [1, 2, 3, 4, 5]\n",
    "depth_performance = []\n",
    "\n",
    "for depth in depths:\n",
    "    hidden_sizes = [20] * depth\n",
    "    layer_sizes = [5] + hidden_sizes + [1]\n",
    "    \n",
    "    trainer = NeuralNetworkTrainer(\n",
    "        layer_sizes=layer_sizes,\n",
    "        loss_function=MeanSquaredError(),\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        activation='relu'\n",
    "    )\n",
    "    \n",
    "    trainer.train(X_train_reg_scaled, y_train_reg_scaled, epochs=300)\n",
    "    \n",
    "    y_pred = trainer.predict(X_test_reg_scaled)\n",
    "    test_mse = mean_squared_error(y_test_reg_scaled, y_pred)\n",
    "    depth_performance.append(test_mse)\n",
    "\n",
    "axes[1, 2].plot(depths, depth_performance, 'o-', linewidth=2, markersize=8, color='purple')\n",
    "axes[1, 2].set_xlabel('Network Depth (Hidden Layers)')\n",
    "axes[1, 2].set_ylabel('Test MSE')\n",
    "axes[1, 2].set_title('Performance vs Network Depth')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOptimization Analysis:\")\n",
    "print(f\"Adam generally converges faster than SGD\")\n",
    "print(f\"Momentum helps SGD escape local minima and accelerate convergence\")\n",
    "print(f\"Huber loss is more robust to outliers than MSE\")\n",
    "print(f\"Learning rate sensitivity is lower for adaptive optimizers (Adam)\")\n",
    "best_config = min(training_results.keys(), key=lambda x: training_results[x]['test_mse'])\n",
    "print(f\"Best configuration: {best_config} with test MSE: {training_results[best_config]['test_mse']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Neural Networks Fundamentals:\n",
    "\n",
    "1. **Activation Functions**:\n",
    "   - **Sigmoid**: Output range [0,1], suffers from vanishing gradients (max derivative = 0.25)\n",
    "   - **Tanh**: Output range [-1,1], zero-centered, still has vanishing gradient problem\n",
    "   - **ReLU**: Solves vanishing gradients, computationally efficient, but can \"die\" (always output 0)\n",
    "   - **Leaky ReLU**: Addresses dying ReLU problem with small negative slope\n",
    "\n",
    "2. **Forward Propagation**:\n",
    "   - Linear transformation: $\\mathbf{z}^{(l)} = \\mathbf{W}^{(l)}\\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)}$\n",
    "   - Activation: $\\mathbf{a}^{(l)} = f(\\mathbf{z}^{(l)})$\n",
    "   - Universal approximation theorem guarantees expressiveness with sufficient width\n",
    "\n",
    "3. **Backpropagation Algorithm**:\n",
    "   - Efficiently computes gradients using chain rule\n",
    "   - Computational complexity: O(W) for both forward and backward passes\n",
    "   - Gradient checking with numerical differentiation verifies implementation\n",
    "   - Deeper networks face vanishing/exploding gradient problems\n",
    "\n",
    "4. **Loss Functions**:\n",
    "   - **MSE**: Smooth, differentiable, sensitive to outliers\n",
    "   - **Cross-entropy**: Preferred for classification, probabilistic interpretation\n",
    "   - **Huber loss**: Robust to outliers, combines MSE and MAE benefits\n",
    "   - Choice affects convergence speed and robustness\n",
    "\n",
    "5. **Optimization Methods**:\n",
    "   - **SGD**: Simple, requires careful learning rate tuning\n",
    "   - **SGD + Momentum**: Accelerates convergence, helps escape local minima\n",
    "   - **Adam**: Adaptive learning rates, generally robust and fast converging\n",
    "   - **Learning rate**: Critical hyperparameter affecting convergence\n",
    "\n",
    "### Practical Guidelines:\n",
    "\n",
    "**Architecture Design:**\n",
    "- Start with ReLU activation for hidden layers\n",
    "- Use appropriate output activation (sigmoid for binary, softmax for multiclass)\n",
    "- Begin with 2-3 hidden layers, increase if needed\n",
    "- Layer width: start with 2-10x input size\n",
    "\n",
    "**Training Best Practices:**\n",
    "- Initialize weights properly (Xavier/He initialization)\n",
    "- Standardize input features\n",
    "- Use Adam optimizer as default choice\n",
    "- Monitor both training and validation loss\n",
    "- Implement gradient checking for custom implementations\n",
    "\n",
    "**Common Issues:**\n",
    "- **Vanishing gradients**: Use ReLU, proper initialization, batch normalization\n",
    "- **Exploding gradients**: Gradient clipping, lower learning rate\n",
    "- **Overfitting**: Regularization, dropout, early stopping\n",
    "- **Slow convergence**: Learning rate scheduling, momentum, adaptive optimizers\n",
    "\n",
    "### Mathematical Insights:\n",
    "- Backpropagation is automatic differentiation applied to neural networks\n",
    "- Gradient descent finds local minima in non-convex loss landscapes\n",
    "- Activation function choice critically affects gradient flow\n",
    "- Optimization landscape becomes more complex with network depth\n",
    "- Proper initialization and normalization are crucial for trainability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}