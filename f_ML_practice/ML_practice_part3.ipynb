{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions - Part 3: Model Evaluation and Validation\n",
    "\n",
    "This notebook covers essential concepts in model evaluation and validation, including cross-validation strategies, bias-variance tradeoff, overfitting/underfitting, performance metrics selection, and model selection criteria. Each question includes theoretical foundations, practical implementations, and real-world applications.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master cross-validation techniques and their appropriate use cases\n",
    "- Understand bias-variance tradeoff and its implications for model selection\n",
    "- Identify and address overfitting and underfitting in machine learning models\n",
    "- Select appropriate performance metrics for different types of problems\n",
    "- Apply rigorous model selection and comparison methodologies\n",
    "\n",
    "## Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with scikit-learn and numpy\n",
    "- Knowledge of basic statistics and probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, StratifiedKFold, \n",
    "    TimeSeriesSplit, LeaveOneOut, validation_curve, learning_curve\n",
    ")\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    classification_report, confusion_matrix, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Cross-Validation Strategies and Their Applications ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Problem**: You are working on three different machine learning projects:\n",
    "1. A medical diagnosis system with limited, expensive-to-collect samples\n",
    "2. A time series forecasting model for stock prices\n",
    "3. A large-scale image classification system with millions of samples\n",
    "\n",
    "For each scenario, determine the most appropriate cross-validation strategy, implement it, and explain why it's optimal for that specific use case.\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "Cross-validation is a statistical method used to estimate the performance of machine learning models on unseen data. The key principle is to use different subsets of data for training and validation to get a more robust estimate of model performance.\n",
    "\n",
    "**K-Fold Cross-Validation**:\n",
    "- Divide data into k folds\n",
    "- Train on k-1 folds, validate on 1 fold\n",
    "- Repeat k times\n",
    "- Average performance across all folds\n",
    "\n",
    "**Stratified K-Fold**:\n",
    "- Maintains class distribution in each fold\n",
    "- Essential for imbalanced datasets\n",
    "- Ensures each fold is representative of the overall dataset\n",
    "\n",
    "**Time Series Split**:\n",
    "- Respects temporal ordering\n",
    "- No future information leak into past predictions\n",
    "- Simulates real-world deployment scenario\n",
    "\n",
    "**Leave-One-Out (LOO)**:\n",
    "- Special case of k-fold where k = n (number of samples)\n",
    "- Maximizes training data usage\n",
    "- High variance but unbiased estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Medical diagnosis with limited samples\n",
    "print(\"=\" * 60)\n",
    "print(\"SCENARIO 1: Medical Diagnosis (Limited Samples)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate small medical dataset\n",
    "X_medical, y_medical = make_classification(\n",
    "    n_samples=150,  # Small dataset\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create class imbalance (common in medical diagnosis)\n",
    "mask = np.random.choice(len(y_medical), size=int(0.2 * len(y_medical)), replace=False)\n",
    "y_medical[mask] = 1\n",
    "\n",
    "print(f\"Dataset size: {X_medical.shape[0]} samples\")\n",
    "print(f\"Class distribution: {np.bincount(y_medical)}\")\n",
    "print(f\"Class imbalance ratio: {np.bincount(y_medical)[1] / np.bincount(y_medical)[0]:.2f}\")\n",
    "\n",
    "# Compare different CV strategies for small dataset\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Standard K-Fold\n",
    "kfold_scores = cross_val_score(model, X_medical, y_medical, cv=5, scoring='roc_auc')\n",
    "\n",
    "# Stratified K-Fold (recommended for imbalanced data)\n",
    "stratified_scores = cross_val_score(\n",
    "    model, X_medical, y_medical, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "# Leave-One-Out (maximizes training data)\n",
    "loo_scores = cross_val_score(model, X_medical, y_medical, cv=LeaveOneOut(), scoring='roc_auc')\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"K-Fold (5): {kfold_scores.mean():.3f} ¬± {kfold_scores.std():.3f}\")\n",
    "print(f\"Stratified K-Fold (5): {stratified_scores.mean():.3f} ¬± {stratified_scores.std():.3f}\")\n",
    "print(f\"Leave-One-Out: {loo_scores.mean():.3f} ¬± {loo_scores.std():.3f}\")\n",
    "\n",
    "print(\"\\nüìä Analysis for Medical Diagnosis:\")\n",
    "print(\"‚úÖ RECOMMENDED: Stratified K-Fold\")\n",
    "print(\"   - Maintains class distribution in each fold\")\n",
    "print(\"   - Handles class imbalance effectively\")\n",
    "print(\"   - Provides stable performance estimates\")\n",
    "print(\"   - Computationally efficient compared to LOO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Time series forecasting\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCENARIO 2: Time Series Forecasting\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate time series data (stock prices)\n",
    "np.random.seed(42)\n",
    "n_timepoints = 500\n",
    "time_index = np.arange(n_timepoints)\n",
    "\n",
    "# Create features: lagged values, moving averages, etc.\n",
    "price_series = 100 + np.cumsum(np.random.randn(n_timepoints) * 0.5)\n",
    "X_ts = np.column_stack([\n",
    "    price_series[:-1],  # lag-1\n",
    "    np.roll(price_series, 2)[:-1],  # lag-2\n",
    "    np.convolve(price_series, np.ones(5)/5, mode='same')[:-1],  # 5-day MA\n",
    "    np.convolve(price_series, np.ones(10)/10, mode='same')[:-1]  # 10-day MA\n",
    "])[10:]  # Remove first 10 rows due to moving averages\n",
    "\n",
    "y_ts = price_series[11:]  # Target: next day price\n",
    "\n",
    "print(f\"Time series length: {len(y_ts)} time points\")\n",
    "print(f\"Features: {X_ts.shape[1]} (lag-1, lag-2, 5-day MA, 10-day MA)\")\n",
    "\n",
    "# Compare standard CV vs Time Series CV\n",
    "model_ts = Ridge(alpha=1.0)\n",
    "\n",
    "# WRONG: Standard K-Fold (violates temporal order)\n",
    "wrong_cv_scores = cross_val_score(model_ts, X_ts, y_ts, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# CORRECT: Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "correct_cv_scores = cross_val_score(model_ts, X_ts, y_ts, cv=tscv, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"\\nCross-Validation Results (MSE):\")\n",
    "print(f\"‚ùå Standard K-Fold: {-wrong_cv_scores.mean():.3f} ¬± {wrong_cv_scores.std():.3f}\")\n",
    "print(f\"‚úÖ Time Series Split: {-correct_cv_scores.mean():.3f} ¬± {correct_cv_scores.std():.3f}\")\n",
    "\n",
    "# Visualize the time series splits\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot original time series\n",
    "axes[0].plot(y_ts, alpha=0.7, label='Price Series')\n",
    "axes[0].set_title('Original Time Series')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].legend()\n",
    "\n",
    "# Visualize TimeSeriesSplit\n",
    "for i, (train_idx, val_idx) in enumerate(tscv.split(X_ts)):\n",
    "    if i < 3:  # Show first 3 splits\n",
    "        axes[1].scatter(train_idx, [i] * len(train_idx), alpha=0.6, s=1, label=f'Train {i+1}')\n",
    "        axes[1].scatter(val_idx, [i] * len(val_idx), alpha=0.8, s=2, label=f'Val {i+1}')\n",
    "\n",
    "axes[1].set_xlabel('Time Index')\n",
    "axes[1].set_ylabel('CV Split')\n",
    "axes[1].set_title('Time Series Cross-Validation Splits')\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis for Time Series:\")\n",
    "print(\"‚úÖ MANDATORY: Time Series Split\")\n",
    "print(\"   - Respects temporal ordering\")\n",
    "print(\"   - Prevents data leakage from future\")\n",
    "print(\"   - Simulates realistic deployment scenario\")\n",
    "print(\"   - Training set always precedes validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Large-scale image classification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCENARIO 3: Large-Scale Image Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate large dataset\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=10000,  # Large dataset\n",
    "    n_features=2048,  # High-dimensional (like CNN features)\n",
    "    n_informative=1000,\n",
    "    n_redundant=500,\n",
    "    n_classes=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {X_large.shape[0]:,} samples\")\n",
    "print(f\"Feature dimension: {X_large.shape[1]:,} features\")\n",
    "print(f\"Number of classes: {len(np.unique(y_large))}\")\n",
    "print(f\"Class distribution: {np.bincount(y_large)}\")\n",
    "\n",
    "# For large datasets, we can use simpler CV strategies\n",
    "model_large = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Simple train-validation split (sufficient for large datasets)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_large, y_large, test_size=0.2, stratify=y_large, random_state=42\n",
    ")\n",
    "\n",
    "model_large.fit(X_train, y_train)\n",
    "val_accuracy = model_large.score(X_val, y_val)\n",
    "\n",
    "print(f\"\\nSimple Train-Val Split Accuracy: {val_accuracy:.3f}\")\n",
    "\n",
    "# 3-Fold CV (efficient for large datasets)\n",
    "cv_scores_3fold = cross_val_score(\n",
    "    model_large, X_large, y_large, \n",
    "    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42), \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"3-Fold Stratified CV: {cv_scores_3fold.mean():.3f} ¬± {cv_scores_3fold.std():.3f}\")\n",
    "\n",
    "# Compare computational time\n",
    "import time\n",
    "\n",
    "# Time simple split\n",
    "start_time = time.time()\n",
    "model_temp = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "model_temp.fit(X_train, y_train)\n",
    "_ = model_temp.score(X_val, y_val)\n",
    "simple_split_time = time.time() - start_time\n",
    "\n",
    "# Time 3-fold CV\n",
    "start_time = time.time()\n",
    "_ = cross_val_score(\n",
    "    RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1), \n",
    "    X_large, y_large, \n",
    "    cv=3, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "cv_3fold_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nComputational Efficiency:\")\n",
    "print(f\"Simple Split: {simple_split_time:.2f} seconds\")\n",
    "print(f\"3-Fold CV: {cv_3fold_time:.2f} seconds (√ó{cv_3fold_time/simple_split_time:.1f} slower)\")\n",
    "\n",
    "print(\"\\nüìä Analysis for Large-Scale Classification:\")\n",
    "print(\"‚úÖ RECOMMENDED: Simple Train-Val Split or 3-Fold CV\")\n",
    "print(\"   - Large datasets provide stable estimates with fewer folds\")\n",
    "print(\"   - Computational efficiency is crucial\")\n",
    "print(\"   - Simple split often sufficient for performance estimation\")\n",
    "print(\"   - 3-fold CV provides good balance of robustness and speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways for Cross-Validation Strategy Selection\n",
    "\n",
    "1. **Small Datasets (< 1000 samples)**: Use Stratified K-Fold or Leave-One-Out\n",
    "2. **Time Series Data**: Always use TimeSeriesSplit to prevent data leakage\n",
    "3. **Large Datasets (> 10,000 samples)**: Simple train-val split or 3-fold CV\n",
    "4. **Imbalanced Classes**: Always use Stratified K-Fold\n",
    "5. **Computational Constraints**: Reduce number of folds or use simple split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Understanding and Diagnosing Bias-Variance Tradeoff ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Problem**: You are comparing three different models for a regression task:\n",
    "1. Linear regression (high bias, low variance)\n",
    "2. Random forest (medium bias, medium variance)\n",
    "3. K-nearest neighbors with k=1 (low bias, high variance)\n",
    "\n",
    "Implement a bias-variance decomposition analysis to understand how each model's complexity affects its prediction error components.\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "The bias-variance decomposition is fundamental to understanding model performance:\n",
    "\n",
    "**Expected Test Error = Bias¬≤ + Variance + Irreducible Error**\n",
    "\n",
    "Where:\n",
    "- **Bias**: Error from overly simplistic assumptions\n",
    "- **Variance**: Error from sensitivity to small fluctuations in training set\n",
    "- **Irreducible Error**: Noise inherent in the problem\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x)] - f(x)$$\n",
    "$$\\text{Variance}[\\hat{f}(x)] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$\n",
    "\n",
    "**Model Complexity Effects**:\n",
    "- **Underfit** (High Bias, Low Variance): Model too simple\n",
    "- **Overfit** (Low Bias, High Variance): Model too complex\n",
    "- **Sweet Spot** (Balanced): Optimal complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_decomposition(model_class, model_params, X, y, n_trials=100, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Perform bias-variance decomposition for a given model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_class : class\n",
    "        The model class (e.g., LinearRegression)\n",
    "    model_params : dict\n",
    "        Parameters for the model\n",
    "    X, y : array-like\n",
    "        Training data\n",
    "    n_trials : int\n",
    "        Number of bootstrap trials\n",
    "    test_size : float\n",
    "        Fraction of data to use for testing\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Bias, variance, and total error components\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_test = int(n_samples * test_size)\n",
    "    \n",
    "    # Fixed test set for fair comparison\n",
    "    test_indices = np.random.choice(n_samples, n_test, replace=False)\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Bootstrap sample from training data\n",
    "        train_indices = np.random.choice(\n",
    "            [i for i in range(n_samples) if i not in test_indices], \n",
    "            size=n_samples - n_test, \n",
    "            replace=True\n",
    "        )\n",
    "        X_train_boot = X[train_indices]\n",
    "        y_train_boot = y[train_indices]\n",
    "        \n",
    "        # Train model on bootstrap sample\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_boot, y_train_boot)\n",
    "        \n",
    "        # Predict on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions.append(y_pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate bias and variance\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    bias_squared = np.mean((mean_prediction - y_test) ** 2)\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    # Total error (approximation)\n",
    "    total_error = np.mean([(pred - y_test) ** 2 for pred in predictions])\n",
    "    \n",
    "    return {\n",
    "        'bias_squared': bias_squared,\n",
    "        'variance': variance,\n",
    "        'total_error': total_error,\n",
    "        'irreducible_error': total_error - bias_squared - variance\n",
    "    }\n",
    "\n",
    "# Generate synthetic regression data with known noise\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_reg = np.random.randn(n_samples, 1)\n",
    "true_function = lambda x: 1.5 * x + 0.3 * x**2 - 0.1 * x**3\n",
    "noise_std = 0.3\n",
    "y_reg = true_function(X_reg.ravel()) + np.random.normal(0, noise_std, n_samples)\n",
    "\n",
    "print(\"Bias-Variance Decomposition Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {n_samples} samples with noise std = {noise_std}\")\n",
    "print(f\"True function: f(x) = 1.5x + 0.3x¬≤ - 0.1x¬≥\")\n",
    "\n",
    "# Compare different models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "models_to_test = [\n",
    "    (LinearRegression, {}, \"Linear Regression (High Bias)\"),\n",
    "    (RandomForestRegressor, {'n_estimators': 10, 'random_state': 42}, \"Random Forest (Medium)\"),\n",
    "    (KNeighborsRegressor, {'n_neighbors': 1}, \"KNN-1 (High Variance)\"),\n",
    "    (KNeighborsRegressor, {'n_neighbors': 10}, \"KNN-10 (Lower Variance)\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for model_class, params, name in models_to_test:\n",
    "    decomp = bias_variance_decomposition(model_class, params, X_reg, y_reg)\n",
    "    decomp['model'] = name\n",
    "    results.append(decomp)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Bias¬≤: {decomp['bias_squared']:.4f}\")\n",
    "    print(f\"  Variance: {decomp['variance']:.4f}\")\n",
    "    print(f\"  Total Error: {decomp['total_error']:.4f}\")\n",
    "    print(f\"  Irreducible: {decomp['irreducible_error']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias-variance tradeoff\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Bias vs Variance\n",
    "models = [r['model'] for r in results]\n",
    "bias_values = [r['bias_squared'] for r in results]\n",
    "variance_values = [r['variance'] for r in results]\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, bias_values, width, label='Bias¬≤', alpha=0.7, color='red')\n",
    "axes[0].bar(x_pos + width/2, variance_values, width, label='Variance', alpha=0.7, color='blue')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Error Component')\n",
    "axes[0].set_title('Bias-Variance Tradeoff')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels([m.split('(')[0].strip() for m in models], rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Total Error Components\n",
    "total_errors = [r['total_error'] for r in results]\n",
    "irreducible_errors = [r['irreducible_error'] for r in results]\n",
    "\n",
    "axes[1].bar(x_pos, bias_values, label='Bias¬≤', alpha=0.7, color='red')\n",
    "axes[1].bar(x_pos, variance_values, bottom=bias_values, label='Variance', alpha=0.7, color='blue')\n",
    "axes[1].bar(x_pos, irreducible_errors, \n",
    "           bottom=[b+v for b,v in zip(bias_values, variance_values)], \n",
    "           label='Irreducible', alpha=0.7, color='gray')\n",
    "\n",
    "axes[1].scatter(x_pos, total_errors, color='black', s=100, marker='x', linewidth=3, label='Total Error')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Error')\n",
    "axes[1].set_title('Total Error Decomposition')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([m.split('(')[0].strip() for m in models], rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the model with best bias-variance tradeoff\n",
    "best_model_idx = np.argmin(total_errors)\n",
    "best_model = results[best_model_idx]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model['model']}\")\n",
    "print(f\"   Total Error: {best_model['total_error']:.4f}\")\n",
    "print(f\"   Bias-Variance Balance: {best_model['bias_squared']:.4f} vs {best_model['variance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate model complexity effect with polynomial regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"\\nModel Complexity Analysis: Polynomial Regression\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "degrees = range(1, 11)\n",
    "complexity_results = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial pipeline\n",
    "    poly_model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Simplified bias-variance analysis for efficiency\n",
    "    n_trials = 50\n",
    "    test_errors = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=trial)\n",
    "        poly_model.fit(X_train, y_train)\n",
    "        y_pred = poly_model.predict(X_test)\n",
    "        test_errors.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    complexity_results.append({\n",
    "        'degree': degree,\n",
    "        'mean_error': np.mean(test_errors),\n",
    "        'error_std': np.std(test_errors)\n",
    "    })\n",
    "\n",
    "# Plot complexity vs error\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "degrees_list = [r['degree'] for r in complexity_results]\n",
    "mean_errors = [r['mean_error'] for r in complexity_results]\n",
    "error_stds = [r['error_std'] for r in complexity_results]\n",
    "\n",
    "ax.errorbar(degrees_list, mean_errors, yerr=error_stds, marker='o', capsize=5)\n",
    "ax.set_xlabel('Polynomial Degree (Model Complexity)')\n",
    "ax.set_ylabel('Test Error (MSE)')\n",
    "ax.set_title('Model Complexity vs Test Error')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark optimal complexity\n",
    "optimal_idx = np.argmin(mean_errors)\n",
    "optimal_degree = degrees_list[optimal_idx]\n",
    "ax.axvline(x=optimal_degree, color='red', linestyle='--', alpha=0.7, label=f'Optimal Degree = {optimal_degree}')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal polynomial degree: {optimal_degree}\")\n",
    "print(f\"Minimum test error: {min(mean_errors):.4f} ¬± {error_stds[optimal_idx]:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(\"1. Low complexity (degree 1-2): High bias, low variance\")\n",
    "print(f\"2. Optimal complexity (degree {optimal_degree}): Balanced bias-variance\")\n",
    "print(\"3. High complexity (degree 8+): Low bias, high variance\")\n",
    "print(\"4. Error increases beyond optimal point due to overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways for Bias-Variance Tradeoff\n",
    "\n",
    "1. **High Bias Models**: Underfit the data, consistent but inaccurate predictions\n",
    "2. **High Variance Models**: Overfit the data, accurate on training but unstable\n",
    "3. **Optimal Complexity**: Minimizes total error by balancing bias and variance\n",
    "4. **Model Selection**: Use validation curves to find optimal complexity\n",
    "5. **Ensemble Methods**: Can reduce variance while maintaining low bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Performance Metrics Selection for Different Problem Types ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Problem**: You are working on four different machine learning projects with distinct characteristics:\n",
    "\n",
    "1. **Fraud Detection**: Highly imbalanced (0.1% fraud cases), high cost of false negatives\n",
    "2. **Medical Screening**: Imbalanced (5% positive cases), high cost of false negatives\n",
    "3. **Marketing Response**: Balanced classes, equal cost for both error types\n",
    "4. **Regression Task**: Predicting house prices with outliers present\n",
    "\n",
    "For each scenario, determine the most appropriate evaluation metric and explain why it's optimal.\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "**Classification Metrics**:\n",
    "- **Accuracy**: (TP + TN) / (TP + TN + FP + FN)\n",
    "- **Precision**: TP / (TP + FP) - Quality of positive predictions\n",
    "- **Recall (Sensitivity)**: TP / (TP + FN) - Coverage of actual positives\n",
    "- **F1-Score**: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "- **ROC-AUC**: Area under Receiver Operating Characteristic curve\n",
    "- **PR-AUC**: Area under Precision-Recall curve\n",
    "\n",
    "**Regression Metrics**:\n",
    "- **MSE**: Mean Squared Error - Penalizes large errors heavily\n",
    "- **MAE**: Mean Absolute Error - Robust to outliers\n",
    "- **MAPE**: Mean Absolute Percentage Error - Scale-independent\n",
    "- **R¬≤**: Coefficient of determination - Proportion of variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Fraud Detection (Extremely Imbalanced)\n",
    "print(\"=\" * 60)\n",
    "print(\"SCENARIO 1: Fraud Detection (0.1% fraud rate)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create extremely imbalanced dataset\n",
    "n_samples = 10000\n",
    "fraud_rate = 0.001  # 0.1% fraud\n",
    "n_fraud = int(n_samples * fraud_rate)\n",
    "n_normal = n_samples - n_fraud\n",
    "\n",
    "X_fraud, _ = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[1-fraud_rate, fraud_rate],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_fraud = np.zeros(n_samples)\n",
    "y_fraud[:n_fraud] = 1\n",
    "np.random.shuffle(y_fraud)\n",
    "\n",
    "print(f\"Dataset: {n_samples:,} transactions\")\n",
    "print(f\"Fraud cases: {n_fraud} ({fraud_rate*100:.1f}%)\")\n",
    "print(f\"Normal cases: {n_normal} ({(1-fraud_rate)*100:.1f}%)\")\n",
    "\n",
    "# Train models\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_fraud, y_fraud, test_size=0.3, stratify=y_fraud, random_state=42\n",
    ")\n",
    "\n",
    "# Compare different models\n",
    "models_fraud = {\n",
    "    'Naive Classifier': lambda: type('', (), {'predict_proba': lambda self, X: np.column_stack([np.ones(len(X))*0.999, np.ones(len(X))*0.001])}),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Balanced RF': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "}\n",
    "\n",
    "fraud_results = []\n",
    "\n",
    "for name, model in models_fraud.items():\n",
    "    if name == 'Naive Classifier':\n",
    "        # Naive classifier that always predicts majority class\n",
    "        y_pred = np.zeros(len(y_test))\n",
    "        y_pred_proba = np.column_stack([np.ones(len(y_test))*0.999, np.ones(len(y_test))*0.001])\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate various metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    if len(np.unique(y_pred_proba[:, 1])) > 1:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        # PR-AUC (Precision-Recall AUC)\n",
    "        from sklearn.metrics import average_precision_score\n",
    "        pr_auc = average_precision_score(y_test, y_pred_proba[:, 1])\n",
    "    else:\n",
    "        roc_auc = 0.5\n",
    "        pr_auc = fraud_rate\n",
    "    \n",
    "    fraud_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'PR-AUC': pr_auc\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "fraud_df = pd.DataFrame(fraud_results)\n",
    "print(\"\\nMetrics Comparison:\")\n",
    "print(fraud_df.round(4))\n",
    "\n",
    "print(\"\\nüìä Analysis for Fraud Detection:\")\n",
    "print(\"‚ùå Accuracy is misleading (99.9% by always predicting 'no fraud')\")\n",
    "print(\"‚ùå ROC-AUC can be optimistic for extreme imbalance\")\n",
    "print(\"‚úÖ RECOMMENDED: PR-AUC (Precision-Recall AUC)\")\n",
    "print(\"   - Focuses on positive class performance\")\n",
    "print(\"   - Not affected by large number of true negatives\")\n",
    "print(\"   - Baseline = positive class proportion\")\n",
    "print(\"‚úÖ ALSO CONSIDER: Recall (to minimize false negatives)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Medical Screening (Moderately Imbalanced)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCENARIO 2: Medical Screening (5% positive rate)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create medical dataset\n",
    "X_medical, y_medical = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=15,\n",
    "    n_informative=12,\n",
    "    n_classes=2,\n",
    "    weights=[0.95, 0.05],  # 5% positive cases\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Medical dataset: {len(y_medical):,} patients\")\n",
    "print(f\"Positive cases: {sum(y_medical)} ({sum(y_medical)/len(y_medical)*100:.1f}%)\")\n",
    "\n",
    "# Train model\n",
    "X_train_med, X_test_med, y_train_med, y_test_med = train_test_split(\n",
    "    X_medical, y_medical, test_size=0.3, stratify=y_medical, random_state=42\n",
    ")\n",
    "\n",
    "model_med = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_med.fit(X_train_med, y_train_med)\n",
    "y_pred_med = model_med.predict(X_test_med)\n",
    "y_pred_proba_med = model_med.predict_proba(X_test_med)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test_med, y_pred_med, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_med, y_pred_med)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"Actual    Neg    Pos\")\n",
    "print(f\"Neg     {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "print(f\"Pos     {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "\n",
    "# Cost-sensitive analysis\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "cost_fn = 100  # Cost of missing a positive case\n",
    "cost_fp = 10   # Cost of false alarm\n",
    "\n",
    "total_cost = fn * cost_fn + fp * cost_fp\n",
    "print(f\"\\nCost Analysis:\")\n",
    "print(f\"False Negatives: {fn} √ó ${cost_fn} = ${fn * cost_fn}\")\n",
    "print(f\"False Positives: {fp} √ó ${cost_fp} = ${fp * cost_fp}\")\n",
    "print(f\"Total Cost: ${total_cost}\")\n",
    "\n",
    "# ROC and PR curves\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_med, y_pred_proba_med[:, 1])\n",
    "roc_auc_med = roc_auc_score(y_test_med, y_pred_proba_med[:, 1])\n",
    "\n",
    "axes[0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_med:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve - Medical Screening')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test_med, y_pred_proba_med[:, 1])\n",
    "pr_auc_med = average_precision_score(y_test_med, y_pred_proba_med[:, 1])\n",
    "baseline_precision = sum(y_test_med) / len(y_test_med)\n",
    "\n",
    "axes[1].plot(recall, precision, label=f'PR Curve (AUC = {pr_auc_med:.3f})')\n",
    "axes[1].axhline(y=baseline_precision, color='k', linestyle='--', label=f'Baseline ({baseline_precision:.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis for Medical Screening:\")\n",
    "print(\"‚úÖ RECOMMENDED: F1-Score or PR-AUC\")\n",
    "print(\"   - Balances precision and recall\")\n",
    "print(\"   - Appropriate for moderate imbalance\")\n",
    "print(\"‚úÖ ALSO CONSIDER: Recall (high sensitivity needed)\")\n",
    "print(\"‚úÖ COST-SENSITIVE: Custom metric based on misclassification costs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Marketing Response (Balanced)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCENARIO 3: Marketing Response (Balanced Classes)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create balanced dataset\n",
    "X_marketing, y_marketing = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    n_classes=2,\n",
    "    weights=[0.5, 0.5],  # Balanced\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Marketing dataset: {len(y_marketing):,} customers\")\n",
    "print(f\"Response rate: {sum(y_marketing)} ({sum(y_marketing)/len(y_marketing)*100:.1f}%)\")\n",
    "\n",
    "# Train model\n",
    "X_train_mrkt, X_test_mrkt, y_train_mrkt, y_test_mrkt = train_test_split(\n",
    "    X_marketing, y_marketing, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model_mrkt = LogisticRegression(random_state=42)\n",
    "model_mrkt.fit(X_train_mrkt, y_train_mrkt)\n",
    "y_pred_mrkt = model_mrkt.predict(X_test_mrkt)\n",
    "y_pred_proba_mrkt = model_mrkt.predict_proba(X_test_mrkt)\n",
    "\n",
    "# Comprehensive evaluation for balanced case\n",
    "accuracy_mrkt = accuracy_score(y_test_mrkt, y_pred_mrkt)\n",
    "precision_mrkt = precision_score(y_test_mrkt, y_pred_mrkt)\n",
    "recall_mrkt = recall_score(y_test_mrkt, y_pred_mrkt)\n",
    "f1_mrkt = f1_score(y_test_mrkt, y_pred_mrkt)\n",
    "roc_auc_mrkt = roc_auc_score(y_test_mrkt, y_pred_proba_mrkt[:, 1])\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_mrkt:.3f}\")\n",
    "print(f\"Precision: {precision_mrkt:.3f}\")\n",
    "print(f\"Recall: {recall_mrkt:.3f}\")\n",
    "print(f\"F1-Score: {f1_mrkt:.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_mrkt:.3f}\")\n",
    "\n",
    "print(\"\\nüìä Analysis for Marketing Response:\")\n",
    "print(\"‚úÖ RECOMMENDED: Accuracy or F1-Score\")\n",
    "print(\"   - Classes are balanced\")\n",
    "print(\"   - Equal cost for both error types\")\n",
    "print(\"   - Accuracy is interpretable and reliable\")\n",
    "print(\"‚úÖ ALSO GOOD: ROC-AUC for ranking customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4: House Price Regression (With Outliers)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCENARIO 4: House Price Regression (With Outliers)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create regression dataset with outliers\n",
    "np.random.seed(42)\n",
    "n_houses = 500\n",
    "X_houses = np.random.randn(n_houses, 5)  # Features: size, location, age, etc.\n",
    "true_prices = 200000 + 50000 * X_houses[:, 0] + 30000 * X_houses[:, 1] + np.random.randn(n_houses) * 10000\n",
    "\n",
    "# Add outliers (luxury mansions)\n",
    "n_outliers = 20\n",
    "outlier_indices = np.random.choice(n_houses, n_outliers, replace=False)\n",
    "true_prices[outlier_indices] *= 3  # Make some houses 3x more expensive\n",
    "\n",
    "y_houses = true_prices\n",
    "\n",
    "print(f\"House dataset: {n_houses} properties\")\n",
    "print(f\"Price range: ${y_houses.min():,.0f} - ${y_houses.max():,.0f}\")\n",
    "print(f\"Median price: ${np.median(y_houses):,.0f}\")\n",
    "print(f\"Number of outliers: {n_outliers}\")\n",
    "\n",
    "# Split data\n",
    "X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
    "    X_houses, y_houses, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train different models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import HuberRegressor  # Robust to outliers\n",
    "\n",
    "models_regression = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Huber Regressor': HuberRegressor()\n",
    "}\n",
    "\n",
    "regression_results = []\n",
    "\n",
    "for name, model in models_regression.items():\n",
    "    model.fit(X_train_house, y_train_house)\n",
    "    y_pred_house = model.predict(X_test_house)\n",
    "    \n",
    "    # Calculate different regression metrics\n",
    "    mse = mean_squared_error(y_test_house, y_pred_house)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_house, y_pred_house)\n",
    "    r2 = r2_score(y_test_house, y_pred_house)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error)\n",
    "    mape = np.mean(np.abs((y_test_house - y_pred_house) / y_test_house)) * 100\n",
    "    \n",
    "    # Median Absolute Error (robust to outliers)\n",
    "    median_ae = np.median(np.abs(y_test_house - y_pred_house))\n",
    "    \n",
    "    regression_results.append({\n",
    "        'Model': name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'MAPE (%)': mape,\n",
    "        'Median AE': median_ae\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "regression_df = pd.DataFrame(regression_results)\n",
    "print(\"\\nRegression Metrics Comparison:\")\n",
    "for col in ['MSE', 'RMSE', 'MAE', 'Median AE']:\n",
    "    regression_df[col] = regression_df[col].apply(lambda x: f\"${x:,.0f}\")\n",
    "print(regression_df)\n",
    "\n",
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, model) in enumerate(models_regression.items()):\n",
    "    y_pred = model.predict(X_test_house)\n",
    "    \n",
    "    axes[i].scatter(y_test_house, y_pred, alpha=0.6)\n",
    "    axes[i].plot([y_test_house.min(), y_test_house.max()], \n",
    "                [y_test_house.min(), y_test_house.max()], 'r--', lw=2)\n",
    "    axes[i].set_xlabel('Actual Price ($)')\n",
    "    axes[i].set_ylabel('Predicted Price ($)')\n",
    "    axes[i].set_title(f'{name}')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R¬≤ to plot\n",
    "    r2_val = r2_score(y_test_house, y_pred)\n",
    "    axes[i].text(0.05, 0.95, f'R¬≤ = {r2_val:.3f}', \n",
    "                transform=axes[i].transAxes, fontsize=12, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Analysis for House Price Regression:\")\n",
    "print(\"‚ùå MSE heavily penalized by outliers\")\n",
    "print(\"‚úÖ RECOMMENDED: MAE (Mean Absolute Error)\")\n",
    "print(\"   - Robust to outliers\")\n",
    "print(\"   - Interpretable in dollar terms\")\n",
    "print(\"   - Linear penalty for errors\")\n",
    "print(\"‚úÖ ALSO CONSIDER: Median Absolute Error for extreme outliers\")\n",
    "print(\"‚úÖ FOR EXPLANATION: R¬≤ for variance explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Metric Selection Guidelines\n",
    "\n",
    "| Problem Type | Best Metric | Reason |\n",
    "|--------------|-------------|--------|\n",
    "| **Extreme Imbalance** | PR-AUC, Recall | Focus on positive class |\n",
    "| **Moderate Imbalance** | F1-Score, PR-AUC | Balance precision/recall |\n",
    "| **Balanced Classes** | Accuracy, ROC-AUC | Simple and reliable |\n",
    "| **Cost-Sensitive** | Custom Cost Function | Reflect business impact |\n",
    "| **Regression (Normal)** | RMSE, R¬≤ | Standard and interpretable |\n",
    "| **Regression (Outliers)** | MAE, Median AE | Robust to outliers |\n",
    "\n",
    "### Key Principles:\n",
    "1. **Consider class distribution** and business context\n",
    "2. **Match metric to problem objectives**\n",
    "3. **Use multiple metrics** for comprehensive evaluation\n",
    "4. **Validate on realistic test conditions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Model Selection and Comparison Framework ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Problem**: You need to select the best model among several candidates for a classification task. Implement a rigorous model comparison framework that accounts for:\n",
    "1. Statistical significance of performance differences\n",
    "2. Multiple evaluation metrics\n",
    "3. Computational efficiency\n",
    "4. Model interpretability\n",
    "\n",
    "Create a scoring system that combines these factors to make an objective model selection decision.\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "**Statistical Testing for Model Comparison**:\n",
    "- **Paired t-test**: Compare mean performance across CV folds\n",
    "- **McNemar's test**: Compare error patterns on same test set\n",
    "- **Wilcoxon signed-rank test**: Non-parametric alternative\n",
    "\n",
    "**Multi-Criteria Decision Making**:\n",
    "- **Weighted scoring**: Combine normalized metrics\n",
    "- **Pareto analysis**: Trade-off between competing objectives\n",
    "- **Business constraints**: Incorporate practical limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "class ModelComparison:\n",
    "    def __init__(self, X, y, cv_folds=5):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.cv_folds = cv_folds\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, name, model, scoring=['accuracy', 'f1', 'roc_auc']):\n",
    "        \"\"\"Evaluate a single model with cross-validation.\"\"\"\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        \n",
    "        # Measure training time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cross-validation with multiple metrics\n",
    "        cv_results = cross_validate(\n",
    "            model, self.X, self.y, \n",
    "            cv=self.cv_folds, \n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Measure prediction time\n",
    "        model.fit(self.X, self.y)\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(self.X)\n",
    "        prediction_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        self.results[name] = {\n",
    "            'cv_results': cv_results,\n",
    "            'training_time': training_time,\n",
    "            'prediction_time': prediction_time,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        for metric in scoring:\n",
    "            test_scores = cv_results[f'test_{metric}']\n",
    "            self.results[name][f'{metric}_mean'] = np.mean(test_scores)\n",
    "            self.results[name][f'{metric}_std'] = np.std(test_scores)\n",
    "            self.results[name][f'{metric}_scores'] = test_scores\n",
    "    \n",
    "    def statistical_comparison(self, model1, model2, metric='accuracy'):\n",
    "        \"\"\"Perform statistical test to compare two models.\"\"\"\n",
    "        scores1 = self.results[model1][f'{metric}_scores']\n",
    "        scores2 = self.results[model2][f'{metric}_scores']\n",
    "        \n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(scores1, scores2)\n",
    "        \n",
    "        return {\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'better_model': model1 if np.mean(scores1) > np.mean(scores2) else model2\n",
    "        }\n",
    "    \n",
    "    def create_comparison_table(self):\n",
    "        \"\"\"Create comprehensive comparison table.\"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for name, results in self.results.items():\n",
    "            comparison_data.append({\n",
    "                'Model': name,\n",
    "                'Accuracy': f\"{results['accuracy_mean']:.3f} ¬± {results['accuracy_std']:.3f}\",\n",
    "                'F1-Score': f\"{results['f1_mean']:.3f} ¬± {results['f1_std']:.3f}\",\n",
    "                'ROC-AUC': f\"{results['roc_auc_mean']:.3f} ¬± {results['roc_auc_std']:.3f}\",\n",
    "                'Train Time (s)': f\"{results['training_time']:.2f}\",\n",
    "                'Pred Time (s)': f\"{results['prediction_time']:.4f}\"\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(comparison_data)\n",
    "    \n",
    "    def multi_criteria_scoring(self, weights={'accuracy': 0.3, 'f1': 0.3, 'roc_auc': 0.2, \n",
    "                                            'speed': 0.1, 'interpretability': 0.1}):\n",
    "        \"\"\"Calculate multi-criteria scores for model selection.\"\"\"\n",
    "        \n",
    "        # Interpretability scores (subjective, domain-dependent)\n",
    "        interpretability_scores = {\n",
    "            'Logistic Regression': 0.9,\n",
    "            'Naive Bayes': 0.8,\n",
    "            'Random Forest': 0.6,\n",
    "            'SVM': 0.3,\n",
    "            'Gradient Boosting': 0.4,\n",
    "            'Neural Network': 0.2\n",
    "        }\n",
    "        \n",
    "        # Normalize metrics (higher is better)\n",
    "        models = list(self.results.keys())\n",
    "        \n",
    "        # Performance metrics (already 0-1, higher is better)\n",
    "        accuracy_scores = [self.results[m]['accuracy_mean'] for m in models]\n",
    "        f1_scores = [self.results[m]['f1_mean'] for m in models]\n",
    "        roc_auc_scores = [self.results[m]['roc_auc_mean'] for m in models]\n",
    "        \n",
    "        # Speed metrics (lower is better, so invert)\n",
    "        total_times = [self.results[m]['training_time'] + self.results[m]['prediction_time'] \n",
    "                      for m in models]\n",
    "        max_time = max(total_times)\n",
    "        speed_scores = [(max_time - t) / max_time for t in total_times]\n",
    "        \n",
    "        # Calculate weighted scores\n",
    "        final_scores = []\n",
    "        for i, model in enumerate(models):\n",
    "            score = (\n",
    "                weights['accuracy'] * accuracy_scores[i] +\n",
    "                weights['f1'] * f1_scores[i] +\n",
    "                weights['roc_auc'] * roc_auc_scores[i] +\n",
    "                weights['speed'] * speed_scores[i] +\n",
    "                weights['interpretability'] * interpretability_scores.get(model, 0.5)\n",
    "            )\n",
    "            final_scores.append(score)\n",
    "        \n",
    "        # Create ranking\n",
    "        ranking_data = []\n",
    "        for i, model in enumerate(models):\n",
    "            ranking_data.append({\n",
    "                'Model': model,\n",
    "                'Accuracy': accuracy_scores[i],\n",
    "                'F1': f1_scores[i],\n",
    "                'ROC-AUC': roc_auc_scores[i],\n",
    "                'Speed': speed_scores[i],\n",
    "                'Interpretability': interpretability_scores.get(model, 0.5),\n",
    "                'Final Score': final_scores[i]\n",
    "            })\n",
    "        \n",
    "        ranking_df = pd.DataFrame(ranking_data)\n",
    "        ranking_df = ranking_df.sort_values('Final Score', ascending=False)\n",
    "        ranking_df['Rank'] = range(1, len(ranking_df) + 1)\n",
    "        \n",
    "        return ranking_df\n",
    "\n",
    "# Load dataset for comparison\n",
    "data = load_breast_cancer()\n",
    "X_comp, y_comp = data.data, data.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_comp = scaler.fit_transform(X_comp)\n",
    "\n",
    "print(\"Model Comparison Framework\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset: {X_comp.shape[0]} samples, {X_comp.shape[1]} features\")\n",
    "print(f\"Task: Binary classification (breast cancer detection)\")\n",
    "\n",
    "# Initialize comparison framework\n",
    "comparator = ModelComparison(X_comp, y_comp, cv_folds=5)\n",
    "\n",
    "# Define models to compare\n",
    "models_to_compare = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), random_state=42, max_iter=500)\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "for name, model in models_to_compare.items():\n",
    "    comparator.evaluate_model(name, model)\n",
    "\n",
    "print(\"\\nModel evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "comparison_table = comparator.create_comparison_table()\n",
    "print(comparison_table.to_string(index=False))\n",
    "\n",
    "# Statistical significance testing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare top 3 models\n",
    "accuracy_means = {name: results['accuracy_mean'] for name, results in comparator.results.items()}\n",
    "top_3_models = sorted(accuracy_means.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "print(f\"\\nTop 3 models by accuracy:\")\n",
    "for i, (model, acc) in enumerate(top_3_models, 1):\n",
    "    print(f\"{i}. {model}: {acc:.3f}\")\n",
    "\n",
    "print(\"\\nPairwise statistical comparisons (accuracy):\")\n",
    "for i in range(len(top_3_models)):\n",
    "    for j in range(i+1, len(top_3_models)):\n",
    "        model1, model2 = top_3_models[i][0], top_3_models[j][0]\n",
    "        comparison = comparator.statistical_comparison(model1, model2, 'accuracy')\n",
    "        \n",
    "        significance = \"‚úÖ Significant\" if comparison['significant'] else \"‚ùå Not significant\"\n",
    "        print(f\"{model1} vs {model2}:\")\n",
    "        print(f\"   p-value: {comparison['p_value']:.4f} ({significance})\")\n",
    "        print(f\"   Better: {comparison['better_model']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-criteria decision making\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTI-CRITERIA MODEL SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define different weighting scenarios\n",
    "scenarios = {\n",
    "    'Performance Focus': {'accuracy': 0.4, 'f1': 0.3, 'roc_auc': 0.3, 'speed': 0.0, 'interpretability': 0.0},\n",
    "    'Balanced': {'accuracy': 0.3, 'f1': 0.3, 'roc_auc': 0.2, 'speed': 0.1, 'interpretability': 0.1},\n",
    "    'Production Focus': {'accuracy': 0.2, 'f1': 0.2, 'roc_auc': 0.1, 'speed': 0.3, 'interpretability': 0.2},\n",
    "    'Interpretability Focus': {'accuracy': 0.2, 'f1': 0.2, 'roc_auc': 0.1, 'speed': 0.1, 'interpretability': 0.4}\n",
    "}\n",
    "\n",
    "for scenario_name, weights in scenarios.items():\n",
    "    print(f\"\\n{scenario_name} Scenario:\")\n",
    "    ranking = comparator.multi_criteria_scoring(weights)\n",
    "    print(ranking[['Rank', 'Model', 'Final Score']].head(3).to_string(index=False))\n",
    "\n",
    "# Detailed ranking for balanced scenario\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED RANKING (Balanced Scenario)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "balanced_ranking = comparator.multi_criteria_scoring(scenarios['Balanced'])\n",
    "print(balanced_ranking.round(3).to_string(index=False))\n",
    "\n",
    "# Visualization of trade-offs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Performance vs Speed trade-off\n",
    "models = balanced_ranking['Model'].values\n",
    "performance = balanced_ranking['Accuracy'].values\n",
    "speed = balanced_ranking['Speed'].values\n",
    "interpretability = balanced_ranking['Interpretability'].values\n",
    "\n",
    "scatter = axes[0,0].scatter(speed, performance, c=interpretability, s=100, alpha=0.7, cmap='viridis')\n",
    "for i, model in enumerate(models):\n",
    "    axes[0,0].annotate(model.split()[0], (speed[i], performance[i]), \n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0,0].set_xlabel('Speed Score (normalized)')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].set_title('Performance vs Speed Trade-off')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0,0], label='Interpretability')\n",
    "\n",
    "# F1 vs ROC-AUC\n",
    "f1_scores = balanced_ranking['F1'].values\n",
    "roc_scores = balanced_ranking['ROC-AUC'].values\n",
    "axes[0,1].scatter(f1_scores, roc_scores, s=100, alpha=0.7)\n",
    "for i, model in enumerate(models):\n",
    "    axes[0,1].annotate(model.split()[0], (f1_scores[i], roc_scores[i]), \n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0,1].set_xlabel('F1-Score')\n",
    "axes[0,1].set_ylabel('ROC-AUC')\n",
    "axes[0,1].set_title('F1 vs ROC-AUC Performance')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Final scores comparison\n",
    "final_scores = balanced_ranking['Final Score'].values\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(models)))\n",
    "bars = axes[1,0].bar(range(len(models)), final_scores, color=colors)\n",
    "axes[1,0].set_xlabel('Model Rank')\n",
    "axes[1,0].set_ylabel('Final Score')\n",
    "axes[1,0].set_title('Multi-Criteria Final Scores')\n",
    "axes[1,0].set_xticks(range(len(models)))\n",
    "axes[1,0].set_xticklabels([m.split()[0] for m in models], rotation=45)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Component contribution\n",
    "components = ['Accuracy', 'F1', 'ROC-AUC', 'Speed', 'Interpretability']\n",
    "weights_balanced = list(scenarios['Balanced'].values())\n",
    "best_model_idx = 0  # Top ranked model\n",
    "best_model_scores = [\n",
    "    balanced_ranking.iloc[best_model_idx]['Accuracy'],\n",
    "    balanced_ranking.iloc[best_model_idx]['F1'],\n",
    "    balanced_ranking.iloc[best_model_idx]['ROC-AUC'],\n",
    "    balanced_ranking.iloc[best_model_idx]['Speed'],\n",
    "    balanced_ranking.iloc[best_model_idx]['Interpretability']\n",
    "]\n",
    "contributions = [w * s for w, s in zip(weights_balanced, best_model_scores)]\n",
    "\n",
    "axes[1,1].pie(contributions, labels=components, autopct='%1.1f%%', startangle=90)\n",
    "axes[1,1].set_title(f'Score Contribution - {models[0]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final recommendation\n",
    "best_model_name = balanced_ranking.iloc[0]['Model']\n",
    "best_score = balanced_ranking.iloc[0]['Final Score']\n",
    "\n",
    "print(f\"\\nüèÜ FINAL RECOMMENDATION: {best_model_name}\")\n",
    "print(f\"   Overall Score: {best_score:.3f}\")\n",
    "print(f\"   Rationale: Best balance of performance, speed, and interpretability\")\n",
    "print(f\"   Statistical significance: Validated through cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways for Model Selection Framework\n",
    "\n",
    "1. **Statistical Validation**: Always test for significance of performance differences\n",
    "2. **Multi-Criteria Evaluation**: Consider performance, speed, interpretability, and business constraints\n",
    "3. **Scenario-Based Selection**: Adapt weighting based on deployment requirements\n",
    "4. **Cross-Validation**: Use robust evaluation methods for reliable estimates\n",
    "5. **Documentation**: Clearly document selection criteria and trade-offs made\n",
    "\n",
    "### Model Selection Checklist:\n",
    "- ‚úÖ Performance metrics appropriate for the problem\n",
    "- ‚úÖ Statistical significance testing\n",
    "- ‚úÖ Computational efficiency assessment\n",
    "- ‚úÖ Interpretability requirements\n",
    "- ‚úÖ Business constraints and deployment context\n",
    "- ‚úÖ Robustness to different data distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This notebook covered essential concepts in model evaluation and validation:\n",
    "\n",
    "### üéØ Key Learning Points:\n",
    "\n",
    "1. **Cross-Validation Strategy Selection**\n",
    "   - Match CV strategy to data characteristics\n",
    "   - Stratified K-Fold for imbalanced data\n",
    "   - Time Series Split for temporal data\n",
    "   - Simple splits for large datasets\n",
    "\n",
    "2. **Bias-Variance Tradeoff**\n",
    "   - Understand components of prediction error\n",
    "   - Optimize model complexity for best generalization\n",
    "   - Use validation curves to find optimal parameters\n",
    "\n",
    "3. **Performance Metrics Selection**\n",
    "   - Choose metrics based on problem type and business impact\n",
    "   - PR-AUC for extreme imbalance\n",
    "   - F1-Score for moderate imbalance\n",
    "   - MAE for robust regression\n",
    "\n",
    "4. **Model Selection Framework**\n",
    "   - Statistical testing for significance\n",
    "   - Multi-criteria decision making\n",
    "   - Consider practical constraints\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- Practice with different datasets and problem types\n",
    "- Implement custom evaluation metrics for specific business needs\n",
    "- Explore ensemble methods for improved performance\n",
    "- Study advanced topics like model calibration and uncertainty quantification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}