{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions - Part 10: Advanced Topics and Specialized Methods\n",
    "\n",
    "This notebook covers advanced machine learning topics including generative models, reinforcement learning, and specialized techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_blobs, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Generative Models - Gaussian Mixture Models and Variational Autoencoders\n",
    "\n",
    "**Question**: Implement Gaussian Mixture Models from scratch and explain the EM algorithm. Compare with a simple Variational Autoencoder implementation and analyze the different approaches to generative modeling.\n",
    "\n",
    "### Theory: Generative Models\n",
    "\n",
    "**1. Gaussian Mixture Model (GMM):**\n",
    "$$p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_k$ = mixing coefficient for component $k$\n",
    "- $\\mathcal{N}(x | \\mu_k, \\Sigma_k)$ = Gaussian distribution with mean $\\mu_k$ and covariance $\\Sigma_k$\n",
    "\n",
    "**2. EM Algorithm:**\n",
    "\n",
    "*E-step (Expectation):*\n",
    "$$\\gamma_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "*M-step (Maximization):*\n",
    "$$\\pi_k = \\frac{N_k}{N}, \\quad \\mu_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} x_n}{N_k}, \\quad \\Sigma_k = \\frac{\\sum_{n=1}^N \\gamma_{nk} (x_n - \\mu_k)(x_n - \\mu_k)^T}{N_k}$$\n",
    "\n",
    "Where $N_k = \\sum_{n=1}^N \\gamma_{nk}$\n",
    "\n",
    "**3. Variational Autoencoder (VAE):**\n",
    "$$\\log p(x) \\geq \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - KL(q(z|x) || p(z))$$\n",
    "\n",
    "Where:\n",
    "- $q(z|x)$ = encoder (approximate posterior)\n",
    "- $p(x|z)$ = decoder (likelihood)\n",
    "- $p(z)$ = prior distribution (usually $\\mathcal{N}(0, I)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModelCustom:\n",
    "    def __init__(self, n_components=2, max_iter=100, tol=1e-6, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Model parameters\n",
    "        self.weights_ = None\n",
    "        self.means_ = None\n",
    "        self.covariances_ = None\n",
    "        \n",
    "        # Training history\n",
    "        self.log_likelihood_history_ = []\n",
    "        self.responsibilities_ = None\n",
    "    \n",
    "    def _initialize_parameters(self, X):\n",
    "        \"\"\"Initialize GMM parameters\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights uniformly\n",
    "        self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "        \n",
    "        # Initialize means randomly\n",
    "        self.means_ = X[np.random.choice(n_samples, self.n_components, replace=False)]\n",
    "        \n",
    "        # Initialize covariances as identity matrices\n",
    "        self.covariances_ = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
    "    \n",
    "    def _multivariate_gaussian_pdf(self, X, mean, cov):\n",
    "        \"\"\"Compute multivariate Gaussian PDF\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Add small value to diagonal for numerical stability\n",
    "        cov_reg = cov + np.eye(n_features) * 1e-6\n",
    "        \n",
    "        # Compute PDF\n",
    "        diff = X - mean\n",
    "        cov_inv = np.linalg.inv(cov_reg)\n",
    "        cov_det = np.linalg.det(cov_reg)\n",
    "        \n",
    "        # Avoid numerical issues\n",
    "        if cov_det <= 0:\n",
    "            cov_det = 1e-6\n",
    "        \n",
    "        normalization = 1.0 / np.sqrt((2 * np.pi) ** n_features * cov_det)\n",
    "        exponent = -0.5 * np.sum(diff @ cov_inv * diff, axis=1)\n",
    "        \n",
    "        return normalization * np.exp(exponent)\n",
    "    \n",
    "    def _e_step(self, X):\n",
    "        \"\"\"Expectation step: compute responsibilities\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        # Compute responsibilities for each component\n",
    "        for k in range(self.n_components):\n",
    "            responsibilities[:, k] = (self.weights_[k] * \n",
    "                                    self._multivariate_gaussian_pdf(X, self.means_[k], self.covariances_[k]))\n",
    "        \n",
    "        # Normalize responsibilities\n",
    "        total_responsibility = np.sum(responsibilities, axis=1, keepdims=True)\n",
    "        total_responsibility[total_responsibility == 0] = 1e-8  # Avoid division by zero\n",
    "        responsibilities /= total_responsibility\n",
    "        \n",
    "        self.responsibilities_ = responsibilities\n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        \"\"\"Maximization step: update parameters\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Effective number of points assigned to each component\n",
    "        N_k = np.sum(responsibilities, axis=0)\n",
    "        \n",
    "        # Update weights\n",
    "        self.weights_ = N_k / n_samples\n",
    "        \n",
    "        # Update means\n",
    "        for k in range(self.n_components):\n",
    "            if N_k[k] > 0:\n",
    "                self.means_[k] = np.sum(responsibilities[:, k:k+1] * X, axis=0) / N_k[k]\n",
    "        \n",
    "        # Update covariances\n",
    "        for k in range(self.n_components):\n",
    "            if N_k[k] > 0:\n",
    "                diff = X - self.means_[k]\n",
    "                self.covariances_[k] = np.sum(\n",
    "                    responsibilities[:, k:k+1] * diff[:, :, np.newaxis] * diff[:, np.newaxis, :], \n",
    "                    axis=0\n",
    "                ) / N_k[k]\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        \"\"\"Compute log-likelihood of the data\"\"\"\n",
    "        likelihood = np.zeros(X.shape[0])\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            likelihood += (self.weights_[k] * \n",
    "                         self._multivariate_gaussian_pdf(X, self.means_[k], self.covariances_[k]))\n",
    "        \n",
    "        # Avoid log(0)\n",
    "        likelihood[likelihood <= 0] = 1e-8\n",
    "        return np.sum(np.log(likelihood))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit GMM using EM algorithm\"\"\"\n",
    "        self._initialize_parameters(X)\n",
    "        self.log_likelihood_history_ = []\n",
    "        \n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step\n",
    "            responsibilities = self._e_step(X)\n",
    "            \n",
    "            # M-step\n",
    "            self._m_step(X, responsibilities)\n",
    "            \n",
    "            # Compute log-likelihood\n",
    "            log_likelihood = self._compute_log_likelihood(X)\n",
    "            self.log_likelihood_history_.append(log_likelihood)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "            \n",
    "            prev_log_likelihood = log_likelihood\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict cluster assignments\"\"\"\n",
    "        responsibilities = self._e_step(X)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def sample(self, n_samples=1):\n",
    "        \"\"\"Generate samples from the fitted GMM\"\"\"\n",
    "        if self.weights_ is None:\n",
    "            raise ValueError(\"Model must be fitted before sampling\")\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Choose component based on weights\n",
    "            component = np.random.choice(self.n_components, p=self.weights_)\n",
    "            \n",
    "            # Sample from chosen component\n",
    "            sample = np.random.multivariate_normal(self.means_[component], self.covariances_[component])\n",
    "            samples.append(sample)\n",
    "        \n",
    "        return np.array(samples)\n",
    "\n",
    "# Test GMM implementation\n",
    "print(\"Testing Gaussian Mixture Model Implementation...\")\n",
    "\n",
    "# Generate synthetic data with known clusters\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "centers = [(-2, -2), (2, 2), (-2, 2)]\n",
    "X_gmm, y_true = make_blobs(n_samples=n_samples, centers=centers, \n",
    "                          cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Fit GMM\n",
    "gmm = GaussianMixtureModelCustom(n_components=3, random_state=42)\n",
    "gmm.fit(X_gmm)\n",
    "\n",
    "# Predict clusters\n",
    "y_pred_gmm = gmm.predict(X_gmm)\n",
    "\n",
    "print(f\"\\nGMM Results:\")\n",
    "print(f\"Final log-likelihood: {gmm.log_likelihood_history_[-1]:.2f}\")\n",
    "print(f\"Component weights: {gmm.weights_}\")\n",
    "print(f\"Component means:\")\n",
    "for i, mean in enumerate(gmm.means_):\n",
    "    print(f\"  Component {i}: [{mean[0]:.2f}, {mean[1]:.2f}]\")\n",
    "\n",
    "# Generate samples from fitted model\n",
    "generated_samples = gmm.sample(100)\n",
    "print(f\"\\nGenerated {generated_samples.shape[0]} samples from fitted GMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Variational Autoencoder implementation\n",
    "class SimpleVAE:\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim=64):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder parameters (mean and log variance)\n",
    "        self.encoder_W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        self.encoder_b1 = np.zeros(hidden_dim)\n",
    "        self.encoder_W_mu = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
    "        self.encoder_b_mu = np.zeros(latent_dim)\n",
    "        self.encoder_W_logvar = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
    "        self.encoder_b_logvar = np.zeros(latent_dim)\n",
    "        \n",
    "        # Decoder parameters\n",
    "        self.decoder_W1 = np.random.randn(latent_dim, hidden_dim) * 0.1\n",
    "        self.decoder_b1 = np.zeros(hidden_dim)\n",
    "        self.decoder_W2 = np.random.randn(hidden_dim, input_dim) * 0.1\n",
    "        self.decoder_b2 = np.zeros(input_dim)\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent space parameters\"\"\"\n",
    "        h = self.relu(np.dot(x, self.encoder_W1) + self.encoder_b1)\n",
    "        mu = np.dot(h, self.encoder_W_mu) + self.encoder_b_mu\n",
    "        logvar = np.dot(h, self.encoder_W_logvar) + self.encoder_b_logvar\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick for sampling\"\"\"\n",
    "        std = np.exp(0.5 * logvar)\n",
    "        eps = np.random.randn(*mu.shape)\n",
    "        return mu + std * eps\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent variables to reconstruction\"\"\"\n",
    "        h = self.relu(np.dot(z, self.decoder_W1) + self.decoder_b1)\n",
    "        reconstruction = self.sigmoid(np.dot(h, self.decoder_W2) + self.decoder_b2)\n",
    "        return reconstruction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through VAE\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.decode(z)\n",
    "        return reconstruction, mu, logvar, z\n",
    "    \n",
    "    def compute_loss(self, x, reconstruction, mu, logvar):\n",
    "        \"\"\"Compute VAE loss (reconstruction + KL divergence)\"\"\"\n",
    "        # Reconstruction loss (binary cross-entropy)\n",
    "        reconstruction_loss = -np.sum(\n",
    "            x * np.log(reconstruction + 1e-8) + \n",
    "            (1 - x) * np.log(1 - reconstruction + 1e-8)\n",
    "        ) / x.shape[0]\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * np.sum(1 + logvar - mu**2 - np.exp(logvar)) / x.shape[0]\n",
    "        \n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "    \n",
    "    def generate(self, n_samples=1):\n",
    "        \"\"\"Generate new samples from the learned distribution\"\"\"\n",
    "        # Sample from prior\n",
    "        z = np.random.randn(n_samples, self.latent_dim)\n",
    "        # Decode to data space\n",
    "        generated = self.decode(z)\n",
    "        return generated\n",
    "\n",
    "# Test VAE with simple 2D data\n",
    "print(\"\\nTesting Variational Autoencoder Implementation...\")\n",
    "\n",
    "# Normalize data for VAE\n",
    "X_vae = (X_gmm - X_gmm.min()) / (X_gmm.max() - X_gmm.min())\n",
    "\n",
    "# Initialize VAE\n",
    "vae = SimpleVAE(input_dim=2, latent_dim=2, hidden_dim=32)\n",
    "\n",
    "# Simple training loop (simplified)\n",
    "n_epochs = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    reconstruction, mu, logvar, z = vae.forward(X_vae)\n",
    "    \n",
    "    # Compute loss\n",
    "    total_loss, recon_loss, kl_loss = vae.compute_loss(X_vae, reconstruction, mu, logvar)\n",
    "    vae.loss_history.append(total_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Total Loss = {total_loss:.4f}, \"\n",
    "              f\"Recon Loss = {recon_loss:.4f}, KL Loss = {kl_loss:.4f}\")\n",
    "\n",
    "# Generate samples\n",
    "vae_samples = vae.generate(100)\n",
    "print(f\"\\nGenerated {vae_samples.shape[0]} samples from VAE\")\n",
    "\n",
    "# Compare models\n",
    "print(f\"\\nModel Comparison:\")\n",
    "print(f\"GMM - Explicit probabilistic model with interpretable components\")\n",
    "print(f\"VAE - Neural network-based with learned representations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_true, alpha=0.7)\n",
    "plt.title('Original Data (True Clusters)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# GMM predictions\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_pred_gmm, alpha=0.7)\n",
    "plt.title('GMM Predictions')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# GMM components\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_pred_gmm, alpha=0.3)\n",
    "# Plot component means\n",
    "for i, (mean, cov) in enumerate(zip(gmm.means_, gmm.covariances_)):\n",
    "    plt.scatter(mean[0], mean[1], marker='x', s=200, linewidths=3, label=f'Component {i}')\n",
    "    # Plot covariance ellipse\n",
    "    eigenvals, eigenvecs = np.linalg.eigh(cov)\n",
    "    angle = np.degrees(np.arctan2(eigenvecs[1, 0], eigenvecs[0, 0]))\n",
    "    width, height = 2 * np.sqrt(eigenvals)\n",
    "    ellipse = plt.matplotlib.patches.Ellipse(mean, width, height, angle=angle, \n",
    "                                           alpha=0.3, linewidth=2, fill=False)\n",
    "    plt.gca().add_patch(ellipse)\n",
    "plt.title('GMM Components')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "# GMM generated samples\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.scatter(generated_samples[:, 0], generated_samples[:, 1], alpha=0.7, color='red')\n",
    "plt.title('GMM Generated Samples')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# VAE latent space\n",
    "plt.subplot(2, 4, 5)\n",
    "reconstruction, mu, logvar, z = vae.forward(X_vae)\n",
    "plt.scatter(z[:, 0], z[:, 1], c=y_true, alpha=0.7)\n",
    "plt.title('VAE Latent Space')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "\n",
    "# VAE reconstruction\n",
    "plt.subplot(2, 4, 6)\n",
    "# Scale back reconstruction\n",
    "recon_scaled = reconstruction * (X_gmm.max() - X_gmm.min()) + X_gmm.min()\n",
    "plt.scatter(recon_scaled[:, 0], recon_scaled[:, 1], c=y_true, alpha=0.7)\n",
    "plt.title('VAE Reconstructions')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# VAE generated samples\n",
    "plt.subplot(2, 4, 7)\n",
    "vae_scaled = vae_samples * (X_gmm.max() - X_gmm.min()) + X_gmm.min()\n",
    "plt.scatter(vae_scaled[:, 0], vae_scaled[:, 1], alpha=0.7, color='purple')\n",
    "plt.title('VAE Generated Samples')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Training curves\n",
    "plt.subplot(2, 4, 8)\n",
    "plt.plot(gmm.log_likelihood_history_, 'b-', label='GMM Log-Likelihood', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('GMM Training Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# VAE loss curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(vae.loss_history, 'purple', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('VAE Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Reinforcement Learning - Q-Learning and Policy Gradients\n",
    "\n",
    "**Question**: Implement Q-learning for a simple grid world environment and compare with a basic policy gradient method. Explain the difference between value-based and policy-based methods.\n",
    "\n",
    "### Theory: Reinforcement Learning\n",
    "\n",
    "**1. Q-Learning (Value-based):**\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]$$\n",
    "\n",
    "Where:\n",
    "- $Q(s,a)$ = action-value function\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\gamma$ = discount factor\n",
    "- $r_{t+1}$ = immediate reward\n",
    "\n",
    "**2. Policy Gradient (REINFORCE):**\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot G_t]$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta(a|s)$ = policy parameterized by $\\theta$\n",
    "- $G_t$ = return (cumulative reward)\n",
    "- $J(\\theta)$ = expected return\n",
    "\n",
    "**3. Bellman Equation:**\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[r(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "**4. Value vs Policy Methods:**\n",
    "- Value-based: Learn optimal action-value function, derive policy\n",
    "- Policy-based: Directly optimize policy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size=5, goal_reward=10, step_penalty=-0.1):\n",
    "        self.size = size\n",
    "        self.goal_reward = goal_reward\n",
    "        self.step_penalty = step_penalty\n",
    "        \n",
    "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.n_actions = len(self.actions)\n",
    "        \n",
    "        # State space\n",
    "        self.n_states = size * size\n",
    "        \n",
    "        # Goal state (bottom-right corner)\n",
    "        self.goal_state = (size - 1, size - 1)\n",
    "        \n",
    "        # Current state\n",
    "        self.current_state = (0, 0)\n",
    "        \n",
    "        # Action effects\n",
    "        self.action_effects = {\n",
    "            0: (-1, 0),  # up\n",
    "            1: (0, 1),   # right\n",
    "            2: (1, 0),   # down\n",
    "            3: (0, -1)   # left\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.current_state = (0, 0)\n",
    "        return self.state_to_index(self.current_state)\n",
    "    \n",
    "    def state_to_index(self, state):\n",
    "        \"\"\"Convert (row, col) state to index\"\"\"\n",
    "        return state[0] * self.size + state[1]\n",
    "    \n",
    "    def index_to_state(self, index):\n",
    "        \"\"\"Convert index to (row, col) state\"\"\"\n",
    "        return (index // self.size, index % self.size)\n",
    "    \n",
    "    def is_valid_state(self, state):\n",
    "        \"\"\"Check if state is within grid bounds\"\"\"\n",
    "        return 0 <= state[0] < self.size and 0 <= state[1] < self.size\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action and return (next_state, reward, done)\"\"\"\n",
    "        # Calculate next state\n",
    "        delta = self.action_effects[action]\n",
    "        next_state = (self.current_state[0] + delta[0], self.current_state[1] + delta[1])\n",
    "        \n",
    "        # Check bounds\n",
    "        if not self.is_valid_state(next_state):\n",
    "            next_state = self.current_state  # Stay in place if hitting wall\n",
    "        \n",
    "        # Calculate reward\n",
    "        if next_state == self.goal_state:\n",
    "            reward = self.goal_reward\n",
    "            done = True\n",
    "        else:\n",
    "            reward = self.step_penalty\n",
    "            done = False\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        return self.state_to_index(next_state), reward, done\n",
    "    \n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\"Get valid actions from a state\"\"\"\n",
    "        return self.actions\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.95, \n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "        \n",
    "        # Training history\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "    \n",
    "    def choose_action(self, state, training=True):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            # Exploit: best known action\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-learning update rule\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.discount_factor * np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Q-learning update\n",
    "        self.q_table[state, action] += self.learning_rate * (target - self.q_table[state, action])\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def train(self, env, n_episodes=1000, max_steps_per_episode=100):\n",
    "        \"\"\"Train the Q-learning agent\"\"\"\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            for step in range(max_steps_per_episode):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                \n",
    "                self.update_q_value(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(steps)\n",
    "            self.decay_epsilon()\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-100:])\n",
    "                print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}, Epsilon: {self.epsilon:.3f}\")\n",
    "\n",
    "class PolicyGradientAgent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.01, discount_factor=0.95):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        # Policy parameters (simple linear policy)\n",
    "        self.policy_weights = np.random.randn(n_states, n_actions) * 0.1\n",
    "        \n",
    "        # Training history\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax function for policy probabilities\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    \n",
    "    def get_action_probabilities(self, state):\n",
    "        \"\"\"Get action probabilities for a state\"\"\"\n",
    "        logits = self.policy_weights[state]\n",
    "        return self.softmax(logits)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        probabilities = self.get_action_probabilities(state)\n",
    "        return np.random.choice(self.n_actions, p=probabilities)\n",
    "    \n",
    "    def compute_returns(self, rewards):\n",
    "        \"\"\"Compute discounted returns\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + self.discount_factor * G\n",
    "            returns.insert(0, G)\n",
    "        return np.array(returns)\n",
    "    \n",
    "    def update_policy(self, states, actions, returns):\n",
    "        \"\"\"Update policy using REINFORCE\"\"\"\n",
    "        for state, action, G in zip(states, actions, returns):\n",
    "            # Get action probabilities\n",
    "            probs = self.get_action_probabilities(state)\n",
    "            \n",
    "            # Compute gradient\n",
    "            gradient = np.zeros(self.n_actions)\n",
    "            gradient[action] = G * (1 - probs[action])\n",
    "            for a in range(self.n_actions):\n",
    "                if a != action:\n",
    "                    gradient[a] = -G * probs[a]\n",
    "            \n",
    "            # Update weights\n",
    "            self.policy_weights[state] += self.learning_rate * gradient\n",
    "    \n",
    "    def train(self, env, n_episodes=1000, max_steps_per_episode=100):\n",
    "        \"\"\"Train the policy gradient agent\"\"\"\n",
    "        for episode in range(n_episodes):\n",
    "            states, actions, rewards = [], [], []\n",
    "            \n",
    "            state = env.reset()\n",
    "            \n",
    "            for step in range(max_steps_per_episode):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                \n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Compute returns and update policy\n",
    "            returns = self.compute_returns(rewards)\n",
    "            self.update_policy(states, actions, returns)\n",
    "            \n",
    "            self.episode_rewards.append(sum(rewards))\n",
    "            self.episode_lengths.append(len(rewards))\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-100:])\n",
    "                print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n",
    "\n",
    "# Test RL implementations\n",
    "print(\"Testing Reinforcement Learning Implementations...\")\n",
    "\n",
    "# Create environment\n",
    "env = GridWorld(size=5)\n",
    "print(f\"Grid World Environment: {env.size}x{env.size}\")\n",
    "print(f\"Goal position: {env.goal_state}\")\n",
    "print(f\"Number of states: {env.n_states}\")\n",
    "print(f\"Number of actions: {env.n_actions}\")\n",
    "\n",
    "# Train Q-learning agent\n",
    "print(\"\\nTraining Q-Learning Agent...\")\n",
    "q_agent = QLearningAgent(env.n_states, env.n_actions)\n",
    "q_agent.train(env, n_episodes=500)\n",
    "\n",
    "# Train policy gradient agent\n",
    "print(\"\\nTraining Policy Gradient Agent...\")\n",
    "pg_agent = PolicyGradientAgent(env.n_states, env.n_actions)\n",
    "pg_agent.train(env, n_episodes=500)\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize results\n",
    "def evaluate_agent(agent, env, n_episodes=100, max_steps=50):\n",
    "    \"\"\"Evaluate trained agent\"\"\"\n",
    "    total_rewards = []\n",
    "    success_rate = 0\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            if hasattr(agent, 'q_table'):\n",
    "                action = agent.choose_action(state, training=False)\n",
    "            else:\n",
    "                action = agent.choose_action(state)\n",
    "            \n",
    "            state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                success_rate += 1\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    return np.mean(total_rewards), success_rate / n_episodes\n",
    "\n",
    "# Evaluate both agents\n",
    "q_avg_reward, q_success = evaluate_agent(q_agent, env)\n",
    "pg_avg_reward, pg_success = evaluate_agent(pg_agent, env)\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Q-Learning - Average Reward: {q_avg_reward:.2f}, Success Rate: {q_success:.2%}\")\n",
    "print(f\"Policy Gradient - Average Reward: {pg_avg_reward:.2f}, Success Rate: {pg_success:.2%}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Training curves\n",
    "plt.subplot(2, 3, 1)\n",
    "window = 50\n",
    "q_rewards_smooth = np.convolve(q_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "pg_rewards_smooth = np.convolve(pg_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(q_rewards_smooth, 'b-', label='Q-Learning', linewidth=2)\n",
    "plt.plot(pg_rewards_smooth, 'r-', label='Policy Gradient', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Q-table heatmap\n",
    "plt.subplot(2, 3, 2)\n",
    "q_values_grid = np.zeros((env.size, env.size))\n",
    "for i in range(env.size):\n",
    "    for j in range(env.size):\n",
    "        state_idx = env.state_to_index((i, j))\n",
    "        q_values_grid[i, j] = np.max(q_agent.q_table[state_idx])\n",
    "\n",
    "im = plt.imshow(q_values_grid, cmap='viridis')\n",
    "plt.colorbar(im)\n",
    "plt.title('Q-Learning Value Function')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "\n",
    "# Q-learning policy\n",
    "plt.subplot(2, 3, 3)\n",
    "policy_grid = np.zeros((env.size, env.size))\n",
    "action_symbols = ['↑', '→', '↓', '←']\n",
    "for i in range(env.size):\n",
    "    for j in range(env.size):\n",
    "        state_idx = env.state_to_index((i, j))\n",
    "        best_action = np.argmax(q_agent.q_table[state_idx])\n",
    "        policy_grid[i, j] = best_action\n",
    "        plt.text(j, i, action_symbols[best_action], ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.imshow(policy_grid, cmap='Set3', alpha=0.3)\n",
    "plt.title('Q-Learning Policy')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "plt.xticks(range(env.size))\n",
    "plt.yticks(range(env.size))\n",
    "\n",
    "# Policy gradient policy probabilities\n",
    "plt.subplot(2, 3, 4)\n",
    "pg_policy_entropy = np.zeros((env.size, env.size))\n",
    "for i in range(env.size):\n",
    "    for j in range(env.size):\n",
    "        state_idx = env.state_to_index((i, j))\n",
    "        probs = pg_agent.get_action_probabilities(state_idx)\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-8))\n",
    "        pg_policy_entropy[i, j] = entropy\n",
    "\n",
    "im = plt.imshow(pg_policy_entropy, cmap='plasma')\n",
    "plt.colorbar(im)\n",
    "plt.title('Policy Gradient Entropy')\n",
    "plt.xlabel('Column')\n",
    "plt.ylabel('Row')\n",
    "\n",
    "# Episode lengths\n",
    "plt.subplot(2, 3, 5)\n",
    "q_lengths_smooth = np.convolve(q_agent.episode_lengths, np.ones(window)/window, mode='valid')\n",
    "pg_lengths_smooth = np.convolve(pg_agent.episode_lengths, np.ones(window)/window, mode='valid')\n",
    "plt.plot(q_lengths_smooth, 'b-', label='Q-Learning', linewidth=2)\n",
    "plt.plot(pg_lengths_smooth, 'r-', label='Policy Gradient', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Length')\n",
    "plt.title('Episode Lengths')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Comparison metrics\n",
    "plt.subplot(2, 3, 6)\n",
    "methods = ['Q-Learning', 'Policy Gradient']\n",
    "avg_rewards = [q_avg_reward, pg_avg_reward]\n",
    "success_rates = [q_success * 100, pg_success * 100]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, avg_rewards, width, label='Avg Reward', alpha=0.8)\n",
    "plt.bar(x + width/2, success_rates, width, label='Success Rate (%)', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Performance')\n",
    "plt.title('Performance Comparison')\n",
    "plt.xticks(x, methods)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Differences:\")\n",
    "print(f\"Q-Learning (Value-based):\")\n",
    "print(f\"  - Learns action-value function Q(s,a)\")\n",
    "print(f\"  - Uses ε-greedy exploration\")\n",
    "print(f\"  - Off-policy learning\")\n",
    "print(f\"  - Deterministic optimal policy\")\n",
    "print(f\"\\nPolicy Gradient (Policy-based):\")\n",
    "print(f\"  - Directly learns policy parameters\")\n",
    "print(f\"  - Natural exploration through stochastic policy\")\n",
    "print(f\"  - On-policy learning\")\n",
    "print(f\"  - Can handle continuous action spaces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Meta-Learning and Few-Shot Learning\n",
    "\n",
    "**Question**: Implement a simple meta-learning algorithm (MAML-style) and demonstrate few-shot learning. Compare with traditional transfer learning approaches and analyze adaptation mechanisms.\n",
    "\n",
    "### Theory: Meta-Learning\n",
    "\n",
    "**1. Model-Agnostic Meta-Learning (MAML):**\n",
    "$$\\theta' = \\theta - \\alpha \\nabla_\\theta L_{task}(\\theta)$$\n",
    "$$\\theta_{new} = \\theta - \\beta \\nabla_\\theta \\sum_{tasks} L_{task}(\\theta')$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ = meta-parameters\n",
    "- $\\alpha$ = inner learning rate\n",
    "- $\\beta$ = outer learning rate\n",
    "- $L_{task}$ = task-specific loss\n",
    "\n",
    "**2. Few-Shot Learning Setup:**\n",
    "- Support set: $S = \\{(x_i, y_i)\\}_{i=1}^k$ (k examples per class)\n",
    "- Query set: $Q = \\{(x_j, y_j)\\}_{j=1}^m$ (test examples)\n",
    "- Goal: Learn from $S$ to classify $Q$\n",
    "\n",
    "**3. Meta-Learning Objective:**\n",
    "$$\\min_\\theta \\sum_{T_i \\sim p(T)} L_{T_i}(f_{\\theta_i'})$$\n",
    "\n",
    "Where $\\theta_i' = \\theta - \\alpha \\nabla_\\theta L_{T_i}(f_\\theta)$\n",
    "\n",
    "**4. Adaptation vs Transfer:**\n",
    "- Transfer Learning: Pre-train → Fine-tune\n",
    "- Meta-Learning: Learn to adapt quickly to new tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMAML:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, inner_lr=0.01, outer_lr=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        \n",
    "        # Initialize meta-parameters\n",
    "        self.meta_params = self._initialize_parameters()\n",
    "        \n",
    "        # Training history\n",
    "        self.meta_losses = []\n",
    "        self.adaptation_accuracies = []\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"Initialize neural network parameters\"\"\"\n",
    "        params = {\n",
    "            'W1': np.random.randn(self.input_dim, self.hidden_dim) * 0.1,\n",
    "            'b1': np.zeros(self.hidden_dim),\n",
    "            'W2': np.random.randn(self.hidden_dim, self.output_dim) * 0.1,\n",
    "            'b2': np.zeros(self.output_dim)\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X, params):\n",
    "        \"\"\"Forward pass through network\"\"\"\n",
    "        h = self.relu(np.dot(X, params['W1']) + params['b1'])\n",
    "        logits = np.dot(h, params['W2']) + params['b2']\n",
    "        probs = self.softmax(logits)\n",
    "        return probs, h\n",
    "    \n",
    "    def compute_loss(self, X, y, params):\n",
    "        \"\"\"Compute cross-entropy loss\"\"\"\n",
    "        probs, _ = self.forward(X, params)\n",
    "        # Convert labels to one-hot if needed\n",
    "        if len(y.shape) == 1:\n",
    "            y_onehot = np.zeros((y.shape[0], self.output_dim))\n",
    "            y_onehot[np.arange(y.shape[0]), y] = 1\n",
    "            y = y_onehot\n",
    "        \n",
    "        loss = -np.sum(y * np.log(probs + 1e-8)) / X.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def compute_gradients(self, X, y, params):\n",
    "        \"\"\"Compute gradients using backpropagation\"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        probs, h = self.forward(X, params)\n",
    "        \n",
    "        # Convert labels to one-hot if needed\n",
    "        if len(y.shape) == 1:\n",
    "            y_onehot = np.zeros((y.shape[0], self.output_dim))\n",
    "            y_onehot[np.arange(y.shape[0]), y] = 1\n",
    "            y = y_onehot\n",
    "        \n",
    "        # Backward pass\n",
    "        # Output layer gradients\n",
    "        dlogits = (probs - y) / batch_size\n",
    "        dW2 = np.dot(h.T, dlogits)\n",
    "        db2 = np.sum(dlogits, axis=0)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dh = np.dot(dlogits, params['W2'].T)\n",
    "        dh_relu = dh * (h > 0)  # ReLU derivative\n",
    "        \n",
    "        dW1 = np.dot(X.T, dh_relu)\n",
    "        db1 = np.sum(dh_relu, axis=0)\n",
    "        \n",
    "        gradients = {\n",
    "            'W1': dW1,\n",
    "            'b1': db1,\n",
    "            'W2': dW2,\n",
    "            'b2': db2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def inner_update(self, X_support, y_support, params, num_steps=1):\n",
    "        \"\"\"Perform inner loop adaptation\"\"\"\n",
    "        adapted_params = {k: v.copy() for k, v in params.items()}\n",
    "        \n",
    "        for _ in range(num_steps):\n",
    "            gradients = self.compute_gradients(X_support, y_support, adapted_params)\n",
    "            \n",
    "            # Update parameters\n",
    "            for key in adapted_params:\n",
    "                adapted_params[key] -= self.inner_lr * gradients[key]\n",
    "        \n",
    "        return adapted_params\n",
    "    \n",
    "    def meta_update(self, tasks):\n",
    "        \"\"\"Perform meta-update using multiple tasks\"\"\"\n",
    "        meta_gradients = {k: np.zeros_like(v) for k, v in self.meta_params.items()}\n",
    "        total_loss = 0\n",
    "        \n",
    "        for X_support, y_support, X_query, y_query in tasks:\n",
    "            # Inner loop: adapt to support set\n",
    "            adapted_params = self.inner_update(X_support, y_support, self.meta_params)\n",
    "            \n",
    "            # Compute loss on query set\n",
    "            query_loss = self.compute_loss(X_query, y_query, adapted_params)\n",
    "            total_loss += query_loss\n",
    "            \n",
    "            # Compute meta-gradients\n",
    "            query_gradients = self.compute_gradients(X_query, y_query, adapted_params)\n",
    "            \n",
    "            # Add to meta-gradients\n",
    "            for key in meta_gradients:\n",
    "                meta_gradients[key] += query_gradients[key]\n",
    "        \n",
    "        # Average gradients across tasks\n",
    "        num_tasks = len(tasks)\n",
    "        for key in meta_gradients:\n",
    "            meta_gradients[key] /= num_tasks\n",
    "        \n",
    "        # Update meta-parameters\n",
    "        for key in self.meta_params:\n",
    "            self.meta_params[key] -= self.outer_lr * meta_gradients[key]\n",
    "        \n",
    "        avg_loss = total_loss / num_tasks\n",
    "        return avg_loss\n",
    "    \n",
    "    def evaluate_adaptation(self, X_support, y_support, X_query, y_query, num_steps=1):\n",
    "        \"\"\"Evaluate adaptation on a single task\"\"\"\n",
    "        # Before adaptation\n",
    "        probs_before, _ = self.forward(X_query, self.meta_params)\n",
    "        pred_before = np.argmax(probs_before, axis=1)\n",
    "        acc_before = np.mean(pred_before == y_query)\n",
    "        \n",
    "        # After adaptation\n",
    "        adapted_params = self.inner_update(X_support, y_support, self.meta_params, num_steps)\n",
    "        probs_after, _ = self.forward(X_query, adapted_params)\n",
    "        pred_after = np.argmax(probs_after, axis=1)\n",
    "        acc_after = np.mean(pred_after == y_query)\n",
    "        \n",
    "        return acc_before, acc_after\n",
    "\n",
    "class FewShotDataGenerator:\n",
    "    def __init__(self, n_classes=5, n_features=20, noise_level=0.1):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.noise_level = noise_level\n",
    "        \n",
    "        # Generate class prototypes\n",
    "        self.class_prototypes = np.random.randn(n_classes, n_features)\n",
    "    \n",
    "    def generate_task(self, n_way=3, k_shot=5, n_query=10):\n",
    "        \"\"\"Generate a few-shot learning task\"\"\"\n",
    "        # Sample classes for this task\n",
    "        task_classes = np.random.choice(self.n_classes, n_way, replace=False)\n",
    "        \n",
    "        X_support, y_support = [], []\n",
    "        X_query, y_query = [], []\n",
    "        \n",
    "        for i, class_idx in enumerate(task_classes):\n",
    "            prototype = self.class_prototypes[class_idx]\n",
    "            \n",
    "            # Generate support examples\n",
    "            support_examples = prototype + np.random.randn(k_shot, self.n_features) * self.noise_level\n",
    "            X_support.extend(support_examples)\n",
    "            y_support.extend([i] * k_shot)\n",
    "            \n",
    "            # Generate query examples\n",
    "            query_examples = prototype + np.random.randn(n_query, self.n_features) * self.noise_level\n",
    "            X_query.extend(query_examples)\n",
    "            y_query.extend([i] * n_query)\n",
    "        \n",
    "        # Convert to numpy arrays and shuffle\n",
    "        X_support, y_support = np.array(X_support), np.array(y_support)\n",
    "        X_query, y_query = np.array(X_query), np.array(y_query)\n",
    "        \n",
    "        # Shuffle\n",
    "        support_idx = np.random.permutation(len(X_support))\n",
    "        query_idx = np.random.permutation(len(X_query))\n",
    "        \n",
    "        X_support, y_support = X_support[support_idx], y_support[support_idx]\n",
    "        X_query, y_query = X_query[query_idx], y_query[query_idx]\n",
    "        \n",
    "        return X_support, y_support, X_query, y_query\n",
    "\n",
    "# Test meta-learning implementation\n",
    "print(\"Testing Meta-Learning Implementation...\")\n",
    "\n",
    "# Initialize\n",
    "np.random.seed(42)\n",
    "n_features = 20\n",
    "n_way = 3  # 3-way classification\n",
    "k_shot = 5  # 5 examples per class\n",
    "n_query = 10  # 10 query examples per class\n",
    "\n",
    "# Create data generator and MAML model\n",
    "data_generator = FewShotDataGenerator(n_classes=10, n_features=n_features)\n",
    "maml = SimpleMAML(input_dim=n_features, hidden_dim=40, output_dim=n_way)\n",
    "\n",
    "print(f\"Few-shot setup: {n_way}-way, {k_shot}-shot\")\n",
    "print(f\"Features: {n_features}, Hidden units: 40\")\n",
    "\n",
    "# Meta-training\n",
    "n_meta_iterations = 200\n",
    "tasks_per_iteration = 4\n",
    "\n",
    "print(f\"\\nMeta-training for {n_meta_iterations} iterations...\")\n",
    "\n",
    "for iteration in range(n_meta_iterations):\n",
    "    # Generate batch of tasks\n",
    "    tasks = []\n",
    "    for _ in range(tasks_per_iteration):\n",
    "        task = data_generator.generate_task(n_way=n_way, k_shot=k_shot, n_query=n_query)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # Meta-update\n",
    "    meta_loss = maml.meta_update(tasks)\n",
    "    maml.meta_losses.append(meta_loss)\n",
    "    \n",
    "    # Evaluate adaptation periodically\n",
    "    if iteration % 50 == 0:\n",
    "        # Test on a new task\n",
    "        test_task = data_generator.generate_task(n_way=n_way, k_shot=k_shot, n_query=n_query)\n",
    "        acc_before, acc_after = maml.evaluate_adaptation(*test_task)\n",
    "        \n",
    "        print(f\"Iteration {iteration}: Meta Loss = {meta_loss:.4f}, \"\n",
    "              f\"Accuracy Before = {acc_before:.3f}, After = {acc_after:.3f}\")\n",
    "\n",
    "print(\"\\nMeta-training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with traditional transfer learning\n",
    "class TraditionalTransferLearning:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Pre-trained parameters (simulated)\n",
    "        self.pretrained_params = {\n",
    "            'W1': np.random.randn(input_dim, hidden_dim) * 0.1,\n",
    "            'b1': np.zeros(hidden_dim),\n",
    "            'W2': np.random.randn(hidden_dim, output_dim) * 0.1,\n",
    "            'b2': np.zeros(output_dim)\n",
    "        }\n",
    "    \n",
    "    def forward(self, X, params):\n",
    "        h = np.maximum(0, np.dot(X, params['W1']) + params['b1'])\n",
    "        logits = np.dot(h, params['W2']) + params['b2']\n",
    "        probs = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs = probs / np.sum(probs, axis=1, keepdims=True)\n",
    "        return probs, h\n",
    "    \n",
    "    def compute_gradients(self, X, y, params):\n",
    "        batch_size = X.shape[0]\n",
    "        probs, h = self.forward(X, params)\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y_onehot = np.zeros((y.shape[0], self.output_dim))\n",
    "            y_onehot[np.arange(y.shape[0]), y] = 1\n",
    "            y = y_onehot\n",
    "        \n",
    "        dlogits = (probs - y) / batch_size\n",
    "        dW2 = np.dot(h.T, dlogits)\n",
    "        db2 = np.sum(dlogits, axis=0)\n",
    "        \n",
    "        dh = np.dot(dlogits, params['W2'].T)\n",
    "        dh_relu = dh * (h > 0)\n",
    "        \n",
    "        dW1 = np.dot(X.T, dh_relu)\n",
    "        db1 = np.sum(dh_relu, axis=0)\n",
    "        \n",
    "        return {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "    \n",
    "    def fine_tune(self, X_support, y_support, num_steps=10):\n",
    "        params = {k: v.copy() for k, v in self.pretrained_params.items()}\n",
    "        \n",
    "        for _ in range(num_steps):\n",
    "            gradients = self.compute_gradients(X_support, y_support, params)\n",
    "            for key in params:\n",
    "                params[key] -= self.learning_rate * gradients[key]\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def evaluate(self, X_support, y_support, X_query, y_query, num_steps=10):\n",
    "        # Before fine-tuning\n",
    "        probs_before, _ = self.forward(X_query, self.pretrained_params)\n",
    "        pred_before = np.argmax(probs_before, axis=1)\n",
    "        acc_before = np.mean(pred_before == y_query)\n",
    "        \n",
    "        # After fine-tuning\n",
    "        finetuned_params = self.fine_tune(X_support, y_support, num_steps)\n",
    "        probs_after, _ = self.forward(X_query, finetuned_params)\n",
    "        pred_after = np.argmax(probs_after, axis=1)\n",
    "        acc_after = np.mean(pred_after == y_query)\n",
    "        \n",
    "        return acc_before, acc_after\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(\"\\nComprehensive Evaluation...\")\n",
    "\n",
    "transfer_model = TraditionalTransferLearning(n_features, 40, n_way)\n",
    "\n",
    "# Generate test tasks\n",
    "n_test_tasks = 50\n",
    "maml_results = {'before': [], 'after': []}\n",
    "transfer_results = {'before': [], 'after': []}\n",
    "\n",
    "for _ in range(n_test_tasks):\n",
    "    test_task = data_generator.generate_task(n_way=n_way, k_shot=k_shot, n_query=n_query)\n",
    "    \n",
    "    # MAML evaluation\n",
    "    acc_before, acc_after = maml.evaluate_adaptation(*test_task, num_steps=5)\n",
    "    maml_results['before'].append(acc_before)\n",
    "    maml_results['after'].append(acc_after)\n",
    "    \n",
    "    # Transfer learning evaluation\n",
    "    acc_before, acc_after = transfer_model.evaluate(*test_task, num_steps=20)\n",
    "    transfer_results['before'].append(acc_before)\n",
    "    transfer_results['after'].append(acc_after)\n",
    "\n",
    "# Compute statistics\n",
    "maml_before_mean = np.mean(maml_results['before'])\n",
    "maml_after_mean = np.mean(maml_results['after'])\n",
    "transfer_before_mean = np.mean(transfer_results['before'])\n",
    "transfer_after_mean = np.mean(transfer_results['after'])\n",
    "\n",
    "print(f\"\\nEvaluation Results on {n_test_tasks} test tasks:\")\n",
    "print(f\"MAML:\")\n",
    "print(f\"  Before adaptation: {maml_before_mean:.3f} ± {np.std(maml_results['before']):.3f}\")\n",
    "print(f\"  After adaptation:  {maml_after_mean:.3f} ± {np.std(maml_results['after']):.3f}\")\n",
    "print(f\"  Improvement: {maml_after_mean - maml_before_mean:.3f}\")\n",
    "\n",
    "print(f\"\\nTransfer Learning:\")\n",
    "print(f\"  Before fine-tuning: {transfer_before_mean:.3f} ± {np.std(transfer_results['before']):.3f}\")\n",
    "print(f\"  After fine-tuning:  {transfer_after_mean:.3f} ± {np.std(transfer_results['after']):.3f}\")\n",
    "print(f\"  Improvement: {transfer_after_mean - transfer_before_mean:.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Meta-learning curve\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(maml.meta_losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Meta-iteration')\n",
    "plt.ylabel('Meta Loss')\n",
    "plt.title('MAML Meta-training Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "# Adaptation comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "methods = ['MAML', 'Transfer Learning']\n",
    "before_scores = [maml_before_mean, transfer_before_mean]\n",
    "after_scores = [maml_after_mean, transfer_after_mean]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, before_scores, width, label='Before Adaptation', alpha=0.8)\n",
    "plt.bar(x + width/2, after_scores, width, label='After Adaptation', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Few-Shot Learning Performance')\n",
    "plt.xticks(x, methods)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Improvement distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "maml_improvements = np.array(maml_results['after']) - np.array(maml_results['before'])\n",
    "transfer_improvements = np.array(transfer_results['after']) - np.array(transfer_results['before'])\n",
    "\n",
    "plt.hist(maml_improvements, alpha=0.7, label='MAML', bins=15, density=True)\n",
    "plt.hist(transfer_improvements, alpha=0.7, label='Transfer Learning', bins=15, density=True)\n",
    "plt.xlabel('Accuracy Improvement')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Improvements')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample adaptation trajectory\n",
    "plt.subplot(2, 3, 4)\n",
    "sample_task = data_generator.generate_task(n_way=n_way, k_shot=k_shot, n_query=n_query)\n",
    "X_support, y_support, X_query, y_query = sample_task\n",
    "\n",
    "# Track adaptation over multiple steps\n",
    "adaptation_steps = range(1, 11)\n",
    "maml_trajectory = []\n",
    "transfer_trajectory = []\n",
    "\n",
    "for steps in adaptation_steps:\n",
    "    _, acc_maml = maml.evaluate_adaptation(X_support, y_support, X_query, y_query, steps)\n",
    "    _, acc_transfer = transfer_model.evaluate(X_support, y_support, X_query, y_query, steps)\n",
    "    maml_trajectory.append(acc_maml)\n",
    "    transfer_trajectory.append(acc_transfer)\n",
    "\n",
    "plt.plot(adaptation_steps, maml_trajectory, 'b-o', label='MAML', linewidth=2)\n",
    "plt.plot(adaptation_steps, transfer_trajectory, 'r-s', label='Transfer Learning', linewidth=2)\n",
    "plt.xlabel('Adaptation Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Adaptation Trajectory')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Error analysis\n",
    "plt.subplot(2, 3, 5)\n",
    "methods = ['MAML\\nBefore', 'MAML\\nAfter', 'Transfer\\nBefore', 'Transfer\\nAfter']\n",
    "means = [maml_before_mean, maml_after_mean, transfer_before_mean, transfer_after_mean]\n",
    "stds = [np.std(maml_results['before']), np.std(maml_results['after']), \n",
    "        np.std(transfer_results['before']), np.std(transfer_results['after'])]\n",
    "\n",
    "plt.bar(range(len(methods)), means, yerr=stds, capsize=5, alpha=0.8)\n",
    "plt.xlabel('Method and Stage')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Performance with Error Bars')\n",
    "plt.xticks(range(len(methods)), methods)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Statistical significance test (simplified)\n",
    "plt.subplot(2, 3, 6)\n",
    "from scipy import stats\n",
    "\n",
    "# Compare final performance\n",
    "t_stat, p_value = stats.ttest_ind(maml_results['after'], transfer_results['after'])\n",
    "\n",
    "comparison_text = f\"MAML vs Transfer Learning\\n\\n\"\n",
    "comparison_text += f\"MAML Mean: {maml_after_mean:.3f}\\n\"\n",
    "comparison_text += f\"Transfer Mean: {transfer_after_mean:.3f}\\n\\n\"\n",
    "comparison_text += f\"t-statistic: {t_stat:.3f}\\n\"\n",
    "comparison_text += f\"p-value: {p_value:.6f}\\n\\n\"\n",
    "comparison_text += f\"Significant difference: {p_value < 0.05}\"\n",
    "\n",
    "plt.text(0.1, 0.5, comparison_text, transform=plt.gca().transAxes, \n",
    "         fontsize=10, verticalalignment='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "plt.axis('off')\n",
    "plt.title('Statistical Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"1. MAML learns initialization that enables fast adaptation\")\n",
    "print(f\"2. Transfer learning relies on feature extraction + fine-tuning\")\n",
    "print(f\"3. MAML shows {'better' if maml_after_mean > transfer_after_mean else 'similar'} few-shot performance\")\n",
    "print(f\"4. Both methods demonstrate the importance of prior knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Advanced Topics and Specialized Methods\n",
    "\n",
    "This notebook covered three advanced machine learning topics:\n",
    "\n",
    "### 1. Generative Models\n",
    "- **Gaussian Mixture Models (GMM)**: Classical probabilistic approach using EM algorithm\n",
    "- **Variational Autoencoders (VAE)**: Neural network-based generative modeling\n",
    "- **Key Differences**: GMM provides interpretable components, VAE learns complex representations\n",
    "- **Applications**: Density estimation, data generation, unsupervised learning\n",
    "\n",
    "### 2. Reinforcement Learning\n",
    "- **Q-Learning**: Value-based method learning optimal action-value function\n",
    "- **Policy Gradient**: Policy-based method directly optimizing policy parameters\n",
    "- **Key Trade-offs**: Q-learning is sample efficient but limited to discrete actions; Policy gradients handle continuous actions but higher variance\n",
    "- **Applications**: Game playing, robotics, autonomous systems\n",
    "\n",
    "### 3. Meta-Learning and Few-Shot Learning\n",
    "- **MAML**: Model-Agnostic Meta-Learning for fast adaptation\n",
    "- **Transfer Learning**: Traditional pre-training + fine-tuning approach\n",
    "- **Key Innovation**: Learning to learn - optimizing for fast adaptation rather than just task performance\n",
    "- **Applications**: Few-shot classification, rapid adaptation, learning across domains\n",
    "\n",
    "### Common Themes\n",
    "1. **Adaptation**: All methods involve adapting to new scenarios (new data distributions, environments, tasks)\n",
    "2. **Prior Knowledge**: Leveraging previous experience to improve performance on new problems\n",
    "3. **Optimization**: Advanced optimization techniques beyond standard supervised learning\n",
    "4. **Generalization**: Going beyond memorization to learn transferable patterns\n",
    "\n",
    "These advanced topics represent the frontier of machine learning research and have practical applications in scenarios with limited data, dynamic environments, and complex data distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}