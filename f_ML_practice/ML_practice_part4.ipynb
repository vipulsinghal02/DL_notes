{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions Part 4: Linear Models and Regularization\n",
    "\n",
    "This notebook covers fundamental linear models, regularization techniques, and their theoretical foundations. Each question includes detailed explanations, mathematical derivations, and practical implementations.\n",
    "\n",
    "**Topics Covered:**\n",
    "- Linear regression variants and assumptions\n",
    "- Ridge, Lasso, and Elastic Net regularization\n",
    "- Logistic regression and generalized linear models\n",
    "- Feature selection and regularization paths\n",
    "- Coordinate descent and optimization techniques\n",
    "\n",
    "**Format:** Each question includes theory, implementation, and analysis sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, log_loss\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Linear Regression Assumptions and Diagnostics\n",
    "\n",
    "**Question:** What are the key assumptions of linear regression, and how would you test for violations? Implement diagnostic tests and demonstrate their use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "Linear regression assumes:\n",
    "1. **Linearity**: Relationship between X and y is linear\n",
    "2. **Independence**: Observations are independent\n",
    "3. **Homoscedasticity**: Constant variance of errors\n",
    "4. **Normality**: Errors are normally distributed\n",
    "5. **No multicollinearity**: Features are not perfectly correlated\n",
    "\n",
    "**Mathematical Model:**\n",
    "$$y = X\\beta + \\epsilon, \\quad \\epsilon \\sim N(0, \\sigma^2 I)$$\n",
    "\n",
    "**Diagnostic Tests:**\n",
    "- Residual plots for linearity and homoscedasticity\n",
    "- Q-Q plots for normality\n",
    "- VIF for multicollinearity\n",
    "- Durbin-Watson for independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_diagnostics(X, y, model=None):\n",
    "    \"\"\"\n",
    "    Comprehensive linear regression diagnostic tests.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        model: Fitted sklearn model (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of diagnostic results\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = LinearRegression().fit(X, y)\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    diagnostics = {}\n",
    "    \n",
    "    # 1. Linearity (correlation of residuals with fitted values)\n",
    "    linearity_corr = np.corrcoef(y_pred, residuals)[0, 1]\n",
    "    diagnostics['linearity_violation'] = abs(linearity_corr) > 0.1\n",
    "    \n",
    "    # 2. Homoscedasticity (Breusch-Pagan test)\n",
    "    from scipy.stats import chi2\n",
    "    aux_reg = LinearRegression().fit(X, residuals**2)\n",
    "    bp_statistic = len(X) * aux_reg.score(X, residuals**2)\n",
    "    bp_pvalue = 1 - chi2.cdf(bp_statistic, X.shape[1])\n",
    "    diagnostics['homoscedasticity_violation'] = bp_pvalue < 0.05\n",
    "    \n",
    "    # 3. Normality (Shapiro-Wilk test)\n",
    "    if len(residuals) <= 5000:  # Shapiro-Wilk limit\n",
    "        _, normality_pvalue = stats.shapiro(residuals)\n",
    "        diagnostics['normality_violation'] = normality_pvalue < 0.05\n",
    "    \n",
    "    # 4. Multicollinearity (VIF)\n",
    "    vif_scores = []\n",
    "    for i in range(X.shape[1]):\n",
    "        X_temp = np.delete(X, i, axis=1)\n",
    "        r_squared = LinearRegression().fit(X_temp, X[:, i]).score(X_temp, X[:, i])\n",
    "        vif = 1 / (1 - r_squared) if r_squared < 0.99 else float('inf')\n",
    "        vif_scores.append(vif)\n",
    "    \n",
    "    diagnostics['multicollinearity_violation'] = any(vif > 10 for vif in vif_scores)\n",
    "    diagnostics['max_vif'] = max(vif_scores)\n",
    "    \n",
    "    return diagnostics, residuals, y_pred\n",
    "\n",
    "# Generate test data with known violations\n",
    "n_samples, n_features = 500, 5\n",
    "X, y = make_regression(n_samples=n_samples, n_features=n_features, noise=10, random_state=42)\n",
    "\n",
    "# Add heteroscedasticity\n",
    "y = y + np.random.normal(0, abs(y) * 0.1)\n",
    "\n",
    "# Add multicollinearity\n",
    "X[:, -1] = X[:, 0] + np.random.normal(0, 0.1, n_samples)\n",
    "\n",
    "# Run diagnostics\n",
    "diagnostics, residuals, y_pred = linear_regression_diagnostics(X, y)\n",
    "\n",
    "print(\"Linear Regression Diagnostic Results:\")\n",
    "print(f\"Linearity violation: {diagnostics['linearity_violation']}\")\n",
    "print(f\"Homoscedasticity violation: {diagnostics['homoscedasticity_violation']}\")\n",
    "print(f\"Normality violation: {diagnostics['normality_violation']}\")\n",
    "print(f\"Multicollinearity violation: {diagnostics['multicollinearity_violation']}\")\n",
    "print(f\"Maximum VIF: {diagnostics['max_vif']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of diagnostic plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Residuals vs Fitted\n",
    "axes[0, 0].scatter(y_pred, residuals, alpha=0.6)\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Fitted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residuals vs Fitted Values')\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "# Scale-Location plot\n",
    "standardized_residuals = residuals / np.std(residuals)\n",
    "axes[1, 0].scatter(y_pred, np.sqrt(np.abs(standardized_residuals)), alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Fitted Values')\n",
    "axes[1, 0].set_ylabel('√|Standardized Residuals|')\n",
    "axes[1, 0].set_title('Scale-Location Plot')\n",
    "\n",
    "# Residual histogram\n",
    "axes[1, 1].hist(residuals, bins=30, density=True, alpha=0.7)\n",
    "x_norm = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "axes[1, 1].plot(x_norm, stats.norm.pdf(x_norm, 0, np.std(residuals)), 'r-', label='Normal')\n",
    "axes[1, 1].set_xlabel('Residuals')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('Residual Distribution')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Ridge Regression Theory and Implementation\n",
    "\n",
    "**Question:** Derive the Ridge regression solution and explain how the regularization parameter affects bias-variance tradeoff. Implement from scratch and compare with sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Ridge Regression Objective:**\n",
    "$$\\min_{\\beta} \\|y - X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_2^2$$\n",
    "\n",
    "**Closed-form Solution:**\n",
    "$$\\hat{\\beta}_{ridge} = (X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "\n",
    "**Bias-Variance Analysis:**\n",
    "- **Bias**: Increases with λ (shrinks coefficients toward zero)\n",
    "- **Variance**: Decreases with λ (stabilizes estimates)\n",
    "- **Total Error**: U-shaped curve in λ\n",
    "\n",
    "**Key Properties:**\n",
    "- Always has unique solution (even when X^TX is singular)\n",
    "- Shrinks coefficients proportionally\n",
    "- Equivalent to adding Gaussian prior on coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegressionCustom:\n",
    "    \"\"\"\n",
    "    Ridge Regression implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, fit_intercept=True):\n",
    "        self.alpha = alpha\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Ridge regression model.\"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            # Center the data\n",
    "            self.X_mean_ = np.mean(X, axis=0)\n",
    "            self.y_mean_ = np.mean(y)\n",
    "            X_centered = X - self.X_mean_\n",
    "            y_centered = y - self.y_mean_\n",
    "        else:\n",
    "            X_centered = X\n",
    "            y_centered = y\n",
    "            self.y_mean_ = 0\n",
    "        \n",
    "        # Ridge solution: (X^T X + λI)^(-1) X^T y\n",
    "        n_features = X_centered.shape[1]\n",
    "        A = X_centered.T @ X_centered + self.alpha * np.eye(n_features)\n",
    "        b = X_centered.T @ y_centered\n",
    "        \n",
    "        self.coef_ = np.linalg.solve(A, b)\n",
    "        \n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = self.y_mean_ - np.dot(self.X_mean_, self.coef_)\n",
    "        else:\n",
    "            self.intercept_ = 0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "    \n",
    "    def get_regularization_path(self, X, y, alphas):\n",
    "        \"\"\"Compute coefficient path for different alpha values.\"\"\"\n",
    "        coef_path = []\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            self.alpha = alpha\n",
    "            self.fit(X, y)\n",
    "            coef_path.append(self.coef_.copy())\n",
    "        \n",
    "        return np.array(coef_path)\n",
    "\n",
    "# Generate data for demonstration\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare custom implementation with sklearn\n",
    "alphas = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Custom implementation\n",
    "    ridge_custom = RidgeRegressionCustom(alpha=alpha)\n",
    "    ridge_custom.fit(X_train_scaled, y_train)\n",
    "    y_pred_custom = ridge_custom.predict(X_test_scaled)\n",
    "    mse_custom = mean_squared_error(y_test, y_pred_custom)\n",
    "    \n",
    "    # Sklearn implementation\n",
    "    ridge_sklearn = Ridge(alpha=alpha)\n",
    "    ridge_sklearn.fit(X_train_scaled, y_train)\n",
    "    y_pred_sklearn = ridge_sklearn.predict(X_test_scaled)\n",
    "    mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "    \n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'mse_custom': mse_custom,\n",
    "        'mse_sklearn': mse_sklearn,\n",
    "        'coef_diff': np.mean(np.abs(ridge_custom.coef_ - ridge_sklearn.coef_))\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Comparison between custom and sklearn Ridge implementations:\")\n",
    "print(results_df.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization path\n",
    "alphas_path = np.logspace(-3, 2, 50)\n",
    "ridge_custom = RidgeRegressionCustom()\n",
    "coef_path = ridge_custom.get_regularization_path(X_train_scaled, y_train, alphas_path)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Coefficient paths\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(coef_path.shape[1]):\n",
    "    plt.plot(alphas_path, coef_path[:, i], label=f'Feature {i+1}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (λ)')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Ridge Regularization Path')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Bias-variance tradeoff simulation\n",
    "plt.subplot(1, 2, 2)\n",
    "test_errors = []\n",
    "train_errors = []\n",
    "\n",
    "for alpha in alphas_path:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_pred = ridge.predict(X_train_scaled)\n",
    "    test_pred = ridge.predict(X_test_scaled)\n",
    "    \n",
    "    train_errors.append(mean_squared_error(y_train, train_pred))\n",
    "    test_errors.append(mean_squared_error(y_test, test_pred))\n",
    "\n",
    "plt.plot(alphas_path, train_errors, label='Training Error', alpha=0.7)\n",
    "plt.plot(alphas_path, test_errors, label='Test Error', alpha=0.7)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (λ)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal alpha\n",
    "optimal_idx = np.argmin(test_errors)\n",
    "optimal_alpha = alphas_path[optimal_idx]\n",
    "print(f\"Optimal alpha: {optimal_alpha:.4f}\")\n",
    "print(f\"Minimum test error: {test_errors[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Lasso Regression and Feature Selection\n",
    "\n",
    "**Question:** Explain why Lasso performs automatic feature selection while Ridge doesn't. Implement coordinate descent for Lasso and demonstrate the sparsity-inducing effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Lasso Objective:**\n",
    "$$\\min_{\\beta} \\frac{1}{2n}\\|y - X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1$$\n",
    "\n",
    "**Why Lasso Induces Sparsity:**\n",
    "- L1 penalty creates \"corners\" at coordinate axes\n",
    "- Contours of quadratic loss function intersect penalty region at sparse points\n",
    "- Subgradient includes zero, allowing coefficients to be exactly zero\n",
    "\n",
    "**Coordinate Descent Algorithm:**\n",
    "For each coordinate j:\n",
    "$$\\beta_j^{(new)} = S\\left(\\frac{1}{n}X_j^T(y - X_{-j}\\beta_{-j}), \\lambda\\right)$$\n",
    "\n",
    "Where S(z,λ) is the soft-thresholding operator:\n",
    "$$S(z, \\lambda) = \\text{sign}(z)(|z| - \\lambda)_+$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegressionCustom:\n",
    "    \"\"\"\n",
    "    Lasso Regression using coordinate descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, max_iter=1000, tol=1e-4, fit_intercept=True):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.n_iter_ = None\n",
    "    \n",
    "    def _soft_threshold(self, z, lambda_val):\n",
    "        \"\"\"Soft thresholding operator.\"\"\"\n",
    "        return np.sign(z) * np.maximum(np.abs(z) - lambda_val, 0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Lasso regression using coordinate descent.\"\"\"\n",
    "        X = np.array(X, dtype=float)\n",
    "        y = np.array(y, dtype=float)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Center data if fitting intercept\n",
    "        if self.fit_intercept:\n",
    "            self.X_mean_ = np.mean(X, axis=0)\n",
    "            self.y_mean_ = np.mean(y)\n",
    "            X = X - self.X_mean_\n",
    "            y = y - self.y_mean_\n",
    "        \n",
    "        # Normalize features for coordinate descent\n",
    "        X_norms = np.sqrt(np.sum(X**2, axis=0))\n",
    "        X_normalized = X / X_norms\n",
    "        \n",
    "        # Initialize coefficients\n",
    "        beta = np.zeros(n_features)\n",
    "        \n",
    "        # Coordinate descent\n",
    "        for iteration in range(self.max_iter):\n",
    "            beta_old = beta.copy()\n",
    "            \n",
    "            for j in range(n_features):\n",
    "                # Compute partial residual\n",
    "                r_j = y - X_normalized @ beta + beta[j] * X_normalized[:, j]\n",
    "                \n",
    "                # Coordinate update with soft thresholding\n",
    "                z_j = np.dot(X_normalized[:, j], r_j) / n_samples\n",
    "                beta[j] = self._soft_threshold(z_j, self.alpha)\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(beta - beta_old)) < self.tol:\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "        else:\n",
    "            self.n_iter_ = self.max_iter\n",
    "        \n",
    "        # Rescale coefficients\n",
    "        self.coef_ = beta / X_norms\n",
    "        \n",
    "        # Compute intercept\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = self.y_mean_ - np.dot(self.X_mean_, self.coef_)\n",
    "        else:\n",
    "            self.intercept_ = 0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "    \n",
    "    def get_regularization_path(self, X, y, alphas):\n",
    "        \"\"\"Compute Lasso path for different alpha values.\"\"\"\n",
    "        coef_path = []\n",
    "        sparsity_path = []\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            self.alpha = alpha\n",
    "            self.fit(X, y)\n",
    "            coef_path.append(self.coef_.copy())\n",
    "            sparsity_path.append(np.sum(np.abs(self.coef_) > 1e-6))\n",
    "        \n",
    "        return np.array(coef_path), np.array(sparsity_path)\n",
    "\n",
    "# Generate data with some irrelevant features\n",
    "n_samples, n_features = 100, 20\n",
    "n_informative = 5\n",
    "\n",
    "X, y = make_regression(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare implementations\n",
    "alpha = 1.0\n",
    "\n",
    "# Custom Lasso\n",
    "lasso_custom = LassoRegressionCustom(alpha=alpha)\n",
    "lasso_custom.fit(X_train_scaled, y_train)\n",
    "y_pred_custom = lasso_custom.predict(X_test_scaled)\n",
    "\n",
    "# Sklearn Lasso\n",
    "lasso_sklearn = Lasso(alpha=alpha, max_iter=1000)\n",
    "lasso_sklearn.fit(X_train_scaled, y_train)\n",
    "y_pred_sklearn = lasso_sklearn.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Custom Lasso - Test MSE: {mean_squared_error(y_test, y_pred_custom):.4f}\")\n",
    "print(f\"Sklearn Lasso - Test MSE: {mean_squared_error(y_test, y_pred_sklearn):.4f}\")\n",
    "print(f\"Custom Lasso - Non-zero coefficients: {np.sum(np.abs(lasso_custom.coef_) > 1e-6)}\")\n",
    "print(f\"Sklearn Lasso - Non-zero coefficients: {np.sum(np.abs(lasso_sklearn.coef_) > 1e-6)}\")\n",
    "print(f\"Custom Lasso - Iterations: {lasso_custom.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Lasso path and feature selection\n",
    "alphas_path = np.logspace(-3, 1, 50)\n",
    "lasso_custom = LassoRegressionCustom()\n",
    "coef_path, sparsity_path = lasso_custom.get_regularization_path(X_train_scaled, y_train, alphas_path)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Lasso path\n",
    "axes[0, 0].plot(alphas_path, coef_path)\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].set_xlabel('Alpha (λ)')\n",
    "axes[0, 0].set_ylabel('Coefficient Value')\n",
    "axes[0, 0].set_title('Lasso Regularization Path')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sparsity vs Alpha\n",
    "axes[0, 1].plot(alphas_path, sparsity_path, 'o-')\n",
    "axes[0, 1].set_xscale('log')\n",
    "axes[0, 1].set_xlabel('Alpha (λ)')\n",
    "axes[0, 1].set_ylabel('Number of Non-zero Coefficients')\n",
    "axes[0, 1].set_title('Feature Selection Effect')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Coefficient comparison at specific alpha\n",
    "test_alpha = 0.1\n",
    "lasso_test = LassoRegressionCustom(alpha=test_alpha)\n",
    "lasso_test.fit(X_train_scaled, y_train)\n",
    "\n",
    "feature_indices = np.arange(len(lasso_test.coef_))\n",
    "axes[1, 0].bar(feature_indices, lasso_test.coef_, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Feature Index')\n",
    "axes[1, 0].set_ylabel('Coefficient Value')\n",
    "axes[1, 0].set_title(f'Lasso Coefficients (α={test_alpha})')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# L1 vs L2 penalty visualization (2D case)\n",
    "theta = np.linspace(0, 2*np.pi, 1000)\n",
    "l1_x = np.cos(theta)\n",
    "l1_y = np.sin(theta)\n",
    "l1_constraint = np.abs(l1_x) + np.abs(l1_y) <= 1\n",
    "\n",
    "l2_x = np.cos(theta)\n",
    "l2_y = np.sin(theta)\n",
    "\n",
    "axes[1, 1].fill(l1_x[l1_constraint], l1_y[l1_constraint], alpha=0.3, label='L1 (Lasso)', color='red')\n",
    "axes[1, 1].plot(l2_x, l2_y, label='L2 (Ridge)', color='blue', linewidth=2)\n",
    "axes[1, 1].set_xlim(-1.5, 1.5)\n",
    "axes[1, 1].set_ylim(-1.5, 1.5)\n",
    "axes[1, 1].set_xlabel('β₁')\n",
    "axes[1, 1].set_ylabel('β₂')\n",
    "axes[1, 1].set_title('L1 vs L2 Penalty Regions')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Elastic Net and Combined Regularization\n",
    "\n",
    "**Question:** When would you use Elastic Net over Ridge or Lasso? Derive the optimization conditions and implement the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Elastic Net Objective:**\n",
    "$$\\min_{\\beta} \\frac{1}{2n}\\|y - X\\beta\\|_2^2 + \\lambda\\left[\\alpha\\|\\beta\\|_1 + \\frac{1-\\alpha}{2}\\|\\beta\\|_2^2\\right]$$\n",
    "\n",
    "**When to Use Elastic Net:**\n",
    "1. **Grouped selection**: When features are correlated, Lasso arbitrarily selects one\n",
    "2. **n << p**: When samples < features, Lasso selects at most n features\n",
    "3. **Stability**: Ridge stabilizes Lasso's variable selection\n",
    "\n",
    "**Coordinate Descent Update:**\n",
    "$$\\beta_j^{(new)} = \\frac{S(z_j, \\lambda\\alpha)}{1 + \\lambda(1-\\alpha)}$$\n",
    "\n",
    "Where $z_j = \\frac{1}{n}X_j^T(y - X_{-j}\\beta_{-j})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNetCustom:\n",
    "    \"\"\"\n",
    "    Elastic Net regression using coordinate descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, l1_ratio=0.5, max_iter=1000, tol=1e-4, fit_intercept=True):\n",
    "        self.alpha = alpha  # Overall regularization strength\n",
    "        self.l1_ratio = l1_ratio  # Mix between L1 and L2 (0=Ridge, 1=Lasso)\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.n_iter_ = None\n",
    "    \n",
    "    def _soft_threshold(self, z, lambda_val):\n",
    "        \"\"\"Soft thresholding operator.\"\"\"\n",
    "        return np.sign(z) * np.maximum(np.abs(z) - lambda_val, 0)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Elastic Net using coordinate descent.\"\"\"\n",
    "        X = np.array(X, dtype=float)\n",
    "        y = np.array(y, dtype=float)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Center data\n",
    "        if self.fit_intercept:\n",
    "            self.X_mean_ = np.mean(X, axis=0)\n",
    "            self.y_mean_ = np.mean(y)\n",
    "            X = X - self.X_mean_\n",
    "            y = y - self.y_mean_\n",
    "        \n",
    "        # Normalize features\n",
    "        X_norms = np.sqrt(np.sum(X**2, axis=0))\n",
    "        X_normalized = X / X_norms\n",
    "        \n",
    "        # Regularization parameters\n",
    "        lambda_l1 = self.alpha * self.l1_ratio\n",
    "        lambda_l2 = self.alpha * (1 - self.l1_ratio)\n",
    "        \n",
    "        # Initialize coefficients\n",
    "        beta = np.zeros(n_features)\n",
    "        \n",
    "        # Coordinate descent\n",
    "        for iteration in range(self.max_iter):\n",
    "            beta_old = beta.copy()\n",
    "            \n",
    "            for j in range(n_features):\n",
    "                # Partial residual\n",
    "                r_j = y - X_normalized @ beta + beta[j] * X_normalized[:, j]\n",
    "                \n",
    "                # Coordinate update\n",
    "                z_j = np.dot(X_normalized[:, j], r_j) / n_samples\n",
    "                \n",
    "                # Elastic Net update: soft threshold then scale\n",
    "                beta[j] = self._soft_threshold(z_j, lambda_l1) / (1 + lambda_l2)\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(beta - beta_old)) < self.tol:\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "        else:\n",
    "            self.n_iter_ = self.max_iter\n",
    "        \n",
    "        # Rescale coefficients\n",
    "        self.coef_ = beta / X_norms\n",
    "        \n",
    "        # Compute intercept\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = self.y_mean_ - np.dot(self.X_mean_, self.coef_)\n",
    "        else:\n",
    "            self.intercept_ = 0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return X @ self.coef_ + self.intercept_\n",
    "\n",
    "# Create dataset with correlated features to demonstrate Elastic Net benefits\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 100, 50\n",
    "\n",
    "# Generate correlated features\n",
    "X_base = np.random.randn(n_samples, 10)\n",
    "X_corr = []\n",
    "\n",
    "for i in range(5):\n",
    "    # Create groups of correlated features\n",
    "    base_feature = X_base[:, i]\n",
    "    for j in range(3):\n",
    "        noise = np.random.randn(n_samples) * 0.1\n",
    "        corr_feature = base_feature + noise\n",
    "        X_corr.append(corr_feature)\n",
    "\n",
    "# Add some random features\n",
    "for i in range(35):\n",
    "    X_corr.append(np.random.randn(n_samples))\n",
    "\n",
    "X = np.column_stack(X_corr)\n",
    "\n",
    "# Generate target with only first 15 features being relevant\n",
    "true_coef = np.zeros(n_features)\n",
    "true_coef[:15] = np.random.randn(15) * 2\n",
    "y = X @ true_coef + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare Ridge, Lasso, and Elastic Net\n",
    "alpha = 0.1\n",
    "models = {\n",
    "    'Ridge': Ridge(alpha=alpha),\n",
    "    'Lasso': Lasso(alpha=alpha),\n",
    "    'Elastic Net (0.5)': ElasticNet(alpha=alpha, l1_ratio=0.5),\n",
    "    'Custom Elastic Net': ElasticNetCustom(alpha=alpha, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    n_nonzero = np.sum(np.abs(model.coef_) > 1e-6)\n",
    "    \n",
    "    results[name] = {\n",
    "        'MSE': mse,\n",
    "        'R²': r2,\n",
    "        'Non-zero coef': n_nonzero\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Model Comparison on Correlated Features:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of l1_ratio in Elastic Net\n",
    "l1_ratios = np.linspace(0, 1, 11)\n",
    "alpha = 0.1\n",
    "\n",
    "elastic_results = []\n",
    "for l1_ratio in l1_ratios:\n",
    "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    n_nonzero = np.sum(np.abs(model.coef_) > 1e-6)\n",
    "    \n",
    "    elastic_results.append({\n",
    "        'l1_ratio': l1_ratio,\n",
    "        'mse': mse,\n",
    "        'n_nonzero': n_nonzero,\n",
    "        'coefficients': model.coef_.copy()\n",
    "    })\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# MSE vs l1_ratio\n",
    "mse_values = [r['mse'] for r in elastic_results]\n",
    "axes[0, 0].plot(l1_ratios, mse_values, 'o-')\n",
    "axes[0, 0].set_xlabel('L1 Ratio')\n",
    "axes[0, 0].set_ylabel('Test MSE')\n",
    "axes[0, 0].set_title('MSE vs L1 Ratio')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sparsity vs l1_ratio\n",
    "sparsity_values = [r['n_nonzero'] for r in elastic_results]\n",
    "axes[0, 1].plot(l1_ratios, sparsity_values, 'o-', color='orange')\n",
    "axes[0, 1].set_xlabel('L1 Ratio')\n",
    "axes[0, 1].set_ylabel('Number of Non-zero Coefficients')\n",
    "axes[0, 1].set_title('Sparsity vs L1 Ratio')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Coefficient paths for different l1_ratios\n",
    "coef_matrix = np.array([r['coefficients'] for r in elastic_results])\n",
    "for i in range(min(10, coef_matrix.shape[1])):\n",
    "    axes[1, 0].plot(l1_ratios, coef_matrix[:, i], alpha=0.7, label=f'Feature {i+1}')\n",
    "axes[1, 0].set_xlabel('L1 Ratio')\n",
    "axes[1, 0].set_ylabel('Coefficient Value')\n",
    "axes[1, 0].set_title('Coefficient Paths vs L1 Ratio')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature correlation heatmap (first 15 features)\n",
    "corr_matrix = np.corrcoef(X_train_scaled[:, :15].T)\n",
    "im = axes[1, 1].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1, 1].set_title('Feature Correlation Matrix')\n",
    "axes[1, 1].set_xlabel('Feature Index')\n",
    "axes[1, 1].set_ylabel('Feature Index')\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal l1_ratio\n",
    "optimal_idx = np.argmin(mse_values)\n",
    "optimal_l1_ratio = l1_ratios[optimal_idx]\n",
    "print(f\"Optimal L1 ratio: {optimal_l1_ratio:.2f}\")\n",
    "print(f\"Minimum test MSE: {mse_values[optimal_idx]:.4f}\")\n",
    "print(f\"Non-zero coefficients at optimum: {sparsity_values[optimal_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Logistic Regression and Maximum Likelihood\n",
    "\n",
    "**Question:** Derive the logistic regression cost function from maximum likelihood estimation. Implement gradient descent and compare with Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "**Logistic Model:**\n",
    "$$P(y=1|x) = \\sigma(x^T\\beta) = \\frac{1}{1 + e^{-x^T\\beta}}$$\n",
    "\n",
    "**Likelihood Function:**\n",
    "$$L(\\beta) = \\prod_{i=1}^n \\sigma(x_i^T\\beta)^{y_i}(1-\\sigma(x_i^T\\beta))^{1-y_i}$$\n",
    "\n",
    "**Log-Likelihood:**\n",
    "$$\\ell(\\beta) = \\sum_{i=1}^n [y_i \\log \\sigma(x_i^T\\beta) + (1-y_i) \\log(1-\\sigma(x_i^T\\beta))]$$\n",
    "\n",
    "**Cost Function (Negative Log-Likelihood):**\n",
    "$$J(\\beta) = -\\frac{1}{n}\\sum_{i=1}^n [y_i \\log \\sigma(x_i^T\\beta) + (1-y_i) \\log(1-\\sigma(x_i^T\\beta))]$$\n",
    "\n",
    "**Gradient:**\n",
    "$$\\nabla J(\\beta) = \\frac{1}{n}X^T(\\sigma(X\\beta) - y)$$\n",
    "\n",
    "**Hessian:**\n",
    "$$H = \\frac{1}{n}X^T W X, \\quad W = \\text{diag}(\\sigma(X\\beta)(1-\\sigma(X\\beta)))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionCustom:\n",
    "    \"\"\"\n",
    "    Logistic Regression with multiple optimization methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='gradient_descent', learning_rate=0.01, max_iter=1000, tol=1e-6, fit_intercept=True):\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.cost_history_ = []\n",
    "        self.n_iter_ = None\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Stable sigmoid function.\"\"\"\n",
    "        # Clip to prevent overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def _cost_function(self, y_true, y_pred_proba):\n",
    "        \"\"\"Binary cross-entropy cost function.\"\"\"\n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-15\n",
    "        y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "        \n",
    "        cost = -np.mean(y_true * np.log(y_pred_proba) + (1 - y_true) * np.log(1 - y_pred_proba))\n",
    "        return cost\n",
    "    \n",
    "    def _gradient(self, X, y, beta):\n",
    "        \"\"\"Compute gradient of cost function.\"\"\"\n",
    "        z = X @ beta\n",
    "        predictions = self._sigmoid(z)\n",
    "        gradient = X.T @ (predictions - y) / len(y)\n",
    "        return gradient\n",
    "    \n",
    "    def _hessian(self, X, beta):\n",
    "        \"\"\"Compute Hessian matrix.\"\"\"\n",
    "        z = X @ beta\n",
    "        predictions = self._sigmoid(z)\n",
    "        W = predictions * (1 - predictions)\n",
    "        # Add small regularization to ensure positive definiteness\n",
    "        W = W + 1e-8\n",
    "        hessian = X.T @ (W[:, np.newaxis] * X) / len(X)\n",
    "        return hessian\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit logistic regression model.\"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Add intercept term\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack([np.ones(len(X)), X])\n",
    "        \n",
    "        # Initialize parameters\n",
    "        beta = np.zeros(X.shape[1])\n",
    "        self.cost_history_ = []\n",
    "        \n",
    "        # Optimization\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Compute predictions and cost\n",
    "            z = X @ beta\n",
    "            predictions = self._sigmoid(z)\n",
    "            cost = self._cost_function(y, predictions)\n",
    "            self.cost_history_.append(cost)\n",
    "            \n",
    "            # Compute gradient\n",
    "            gradient = self._gradient(X, y, beta)\n",
    "            \n",
    "            if self.method == 'gradient_descent':\n",
    "                # Gradient descent update\n",
    "                beta_new = beta - self.learning_rate * gradient\n",
    "                \n",
    "            elif self.method == 'newton':\n",
    "                # Newton's method update\n",
    "                hessian = self._hessian(X, beta)\n",
    "                try:\n",
    "                    beta_new = beta - np.linalg.solve(hessian, gradient)\n",
    "                except np.linalg.LinAlgError:\n",
    "                    # Fall back to gradient descent if Hessian is singular\n",
    "                    beta_new = beta - self.learning_rate * gradient\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(beta_new - beta) < self.tol:\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "            \n",
    "            beta = beta_new\n",
    "        else:\n",
    "            self.n_iter_ = self.max_iter\n",
    "        \n",
    "        # Store coefficients\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = beta[0]\n",
    "            self.coef_ = beta[1:]\n",
    "        else:\n",
    "            self.intercept_ = 0\n",
    "            self.coef_ = beta\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities.\"\"\"\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack([np.ones(len(X)), X])\n",
    "            beta = np.concatenate([[self.intercept_], self.coef_])\n",
    "        else:\n",
    "            beta = self.coef_\n",
    "        \n",
    "        z = X @ beta\n",
    "        return self._sigmoid(z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make binary predictions.\"\"\"\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "# Generate binary classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_redundant=0, \n",
    "                         n_informative=10, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Compare optimization methods\n",
    "methods = {\n",
    "    'Gradient Descent': LogisticRegressionCustom(method='gradient_descent', learning_rate=0.1),\n",
    "    'Newton Method': LogisticRegressionCustom(method='newton'),\n",
    "    'Sklearn': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in methods.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "        if len(y_pred_proba.shape) > 1:  # sklearn returns 2D array\n",
    "            y_pred_proba = y_pred_proba[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "    \n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    if hasattr(model, 'n_iter_'):\n",
    "        iterations = model.n_iter_\n",
    "    else:\n",
    "        iterations = model.n_iter_[0] if hasattr(model, 'n_iter_') else 'N/A'\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Log Loss': logloss,\n",
    "        'Iterations': iterations\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Optimization Method Comparison:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence and decision boundary\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Convergence comparison\n",
    "gd_model = LogisticRegressionCustom(method='gradient_descent', learning_rate=0.1, max_iter=1000)\n",
    "newton_model = LogisticRegressionCustom(method='newton', max_iter=100)\n",
    "\n",
    "gd_model.fit(X_train_scaled, y_train)\n",
    "newton_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "axes[0, 0].plot(gd_model.cost_history_, label='Gradient Descent', alpha=0.8)\n",
    "axes[0, 0].plot(newton_model.cost_history_, label='Newton Method', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Cost (Log Loss)')\n",
    "axes[0, 0].set_title('Convergence Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate sensitivity\n",
    "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
    "final_costs = []\n",
    "iterations_to_converge = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = LogisticRegressionCustom(method='gradient_descent', learning_rate=lr, max_iter=2000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    final_costs.append(model.cost_history_[-1])\n",
    "    iterations_to_converge.append(model.n_iter_)\n",
    "\n",
    "axes[0, 1].semilogx(learning_rates, final_costs, 'o-', label='Final Cost')\n",
    "axes[0, 1].set_xlabel('Learning Rate')\n",
    "axes[0, 1].set_ylabel('Final Cost')\n",
    "axes[0, 1].set_title('Learning Rate Sensitivity')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Decision boundary (2D visualization)\n",
    "# Use only first 2 features for visualization\n",
    "X_2d = X_train_scaled[:, :2]\n",
    "model_2d = LogisticRegressionCustom(method='newton')\n",
    "model_2d.fit(X_2d, y_train)\n",
    "\n",
    "# Create mesh\n",
    "h = 0.02\n",
    "x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
    "y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = model_2d.predict_proba(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1, 0].contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
    "scatter = axes[1, 0].scatter(X_2d[:, 0], X_2d[:, 1], c=y_train, cmap='RdYlBu', edgecolors='black')\n",
    "axes[1, 0].set_xlabel('Feature 1')\n",
    "axes[1, 0].set_ylabel('Feature 2')\n",
    "axes[1, 0].set_title('Decision Boundary (2D Projection)')\n",
    "\n",
    "# Sigmoid function visualization\n",
    "z = np.linspace(-10, 10, 100)\n",
    "sigmoid_values = 1 / (1 + np.exp(-z))\n",
    "\n",
    "axes[1, 1].plot(z, sigmoid_values, 'b-', linewidth=2, label='σ(z) = 1/(1+e⁻ᶻ)')\n",
    "axes[1, 1].axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('z = xᵀβ')\n",
    "axes[1, 1].set_ylabel('P(y=1|x)')\n",
    "axes[1, 1].set_title('Sigmoid Function')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGradient Descent - Iterations to convergence: {gd_model.n_iter_}\")\n",
    "print(f\"Newton Method - Iterations to convergence: {newton_model.n_iter_}\")\n",
    "print(f\"Final cost difference: {abs(gd_model.cost_history_[-1] - newton_model.cost_history_[-1]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Linear Models and Regularization:\n",
    "\n",
    "1. **Linear Regression Assumptions**: \n",
    "   - Critical for valid inference and predictions\n",
    "   - Diagnostic tests help identify violations\n",
    "   - VIF > 10 indicates multicollinearity\n",
    "\n",
    "2. **Ridge Regression**:\n",
    "   - L2 penalty shrinks coefficients proportionally\n",
    "   - Always has unique solution: β = (XᵀX + λI)⁻¹Xᵀy\n",
    "   - Reduces variance at cost of increased bias\n",
    "\n",
    "3. **Lasso Regression**:\n",
    "   - L1 penalty induces sparsity (feature selection)\n",
    "   - Coordinate descent with soft thresholding\n",
    "   - Can select at most n features when n < p\n",
    "\n",
    "4. **Elastic Net**:\n",
    "   - Combines L1 and L2 penalties\n",
    "   - Handles correlated features better than Lasso\n",
    "   - l1_ratio = 0 → Ridge, l1_ratio = 1 → Lasso\n",
    "\n",
    "5. **Logistic Regression**:\n",
    "   - Maximum likelihood estimation\n",
    "   - Newton's method converges faster than gradient descent\n",
    "   - Sigmoid function provides probabilistic interpretation\n",
    "\n",
    "### Practical Guidelines:\n",
    "- Use Ridge when you want to keep all features\n",
    "- Use Lasso for automatic feature selection\n",
    "- Use Elastic Net when features are correlated\n",
    "- Always check assumptions and use diagnostics\n",
    "- Standardize features for regularized methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}