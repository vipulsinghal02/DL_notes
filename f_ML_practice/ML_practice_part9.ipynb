{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions - Part 9: Deep Learning Architectures\n",
    "\n",
    "This notebook covers advanced deep learning architectures and specialized neural network designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_digits, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Question**: Implement a CNN from scratch and explain the mathematical foundations of convolution, pooling, and feature mapping. Compare different architectures like LeNet and analyze the role of receptive fields.\n",
    "\n",
    "### Theory: Convolutional Neural Networks\n",
    "\n",
    "CNNs are specialized neural networks for processing grid-like data such as images. The key operations are:\n",
    "\n",
    "**1. Convolution Operation:**\n",
    "$$S(i,j) = (I * K)(i,j) = \\sum_m \\sum_n I(m,n)K(i-m, j-n)$$\n",
    "\n",
    "Where:\n",
    "- $S(i,j)$ = output feature map at position $(i,j)$\n",
    "- $I$ = input image\n",
    "- $K$ = convolution kernel/filter\n",
    "\n",
    "**2. Pooling Operation:**\n",
    "- Max pooling: $P(i,j) = \\max_{(m,n) \\in \\mathcal{R}(i,j)} I(m,n)$\n",
    "- Average pooling: $P(i,j) = \\frac{1}{|\\mathcal{R}(i,j)|} \\sum_{(m,n) \\in \\mathcal{R}(i,j)} I(m,n)$\n",
    "\n",
    "**3. Receptive Field:**\n",
    "$$RF_l = RF_{l-1} + (K_l - 1) \\prod_{i=1}^{l-1} S_i$$\n",
    "\n",
    "Where $RF_l$ is receptive field at layer $l$, $K_l$ is kernel size, $S_i$ is stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DCustom:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        fan_in = in_channels * kernel_size * kernel_size\n",
    "        fan_out = out_channels * kernel_size * kernel_size\n",
    "        std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "        \n",
    "        self.weights = np.random.normal(0, std, \n",
    "                                      (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.bias = np.zeros(out_channels)\n",
    "        \n",
    "        # For backpropagation\n",
    "        self.last_input = None\n",
    "        \n",
    "    def add_padding(self, x):\n",
    "        if self.padding == 0:\n",
    "            return x\n",
    "        return np.pad(x, ((0, 0), (0, 0), (self.padding, self.padding), \n",
    "                         (self.padding, self.padding)), mode='constant')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through convolution layer\"\"\"\n",
    "        self.last_input = x.copy()\n",
    "        batch_size, in_channels, in_height, in_width = x.shape\n",
    "        \n",
    "        # Add padding\n",
    "        x_padded = self.add_padding(x)\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        out_height = (in_height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (in_width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        # Initialize output\n",
    "        output = np.zeros((batch_size, self.out_channels, out_height, out_width))\n",
    "        \n",
    "        # Convolution operation\n",
    "        for b in range(batch_size):\n",
    "            for c_out in range(self.out_channels):\n",
    "                for h in range(out_height):\n",
    "                    for w in range(out_width):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        \n",
    "                        # Extract receptive field\n",
    "                        receptive_field = x_padded[b, :, h_start:h_end, w_start:w_end]\n",
    "                        \n",
    "                        # Compute convolution\n",
    "                        output[b, c_out, h, w] = np.sum(receptive_field * self.weights[c_out]) + self.bias[c_out]\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MaxPool2DCustom:\n",
    "    def __init__(self, kernel_size, stride=None):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride if stride is not None else kernel_size\n",
    "        self.last_input = None\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through max pooling layer\"\"\"\n",
    "        self.last_input = x.copy()\n",
    "        batch_size, channels, in_height, in_width = x.shape\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        out_height = (in_height - self.kernel_size) // self.stride + 1\n",
    "        out_width = (in_width - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        # Initialize output and mask for backpropagation\n",
    "        output = np.zeros((batch_size, channels, out_height, out_width))\n",
    "        self.mask = np.zeros_like(x)\n",
    "        \n",
    "        # Max pooling operation\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h in range(out_height):\n",
    "                    for w in range(out_width):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "                        \n",
    "                        # Extract pooling region\n",
    "                        pool_region = x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        \n",
    "                        # Find max value and its position\n",
    "                        max_val = np.max(pool_region)\n",
    "                        output[b, c, h, w] = max_val\n",
    "                        \n",
    "                        # Create mask for backpropagation\n",
    "                        mask_region = (pool_region == max_val)\n",
    "                        self.mask[b, c, h_start:h_end, w_start:w_end] = mask_region\n",
    "        \n",
    "        return output\n",
    "\n",
    "class CNNCustom:\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape  # (channels, height, width)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # LeNet-like architecture\n",
    "        self.conv1 = Conv2DCustom(input_shape[0], 6, kernel_size=5, padding=0)\n",
    "        self.pool1 = MaxPool2DCustom(kernel_size=2)\n",
    "        self.conv2 = Conv2DCustom(6, 16, kernel_size=5, padding=0)\n",
    "        self.pool2 = MaxPool2DCustom(kernel_size=2)\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        self.flattened_size = self._calculate_flattened_size()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1_weights = np.random.randn(self.flattened_size, 120) * 0.1\n",
    "        self.fc1_bias = np.zeros(120)\n",
    "        self.fc2_weights = np.random.randn(120, 84) * 0.1\n",
    "        self.fc2_bias = np.zeros(84)\n",
    "        self.fc3_weights = np.random.randn(84, num_classes) * 0.1\n",
    "        self.fc3_bias = np.zeros(num_classes)\n",
    "    \n",
    "    def _calculate_flattened_size(self):\n",
    "        \"\"\"Calculate the size after conv and pooling layers\"\"\"\n",
    "        # Simulate forward pass to get dimensions\n",
    "        dummy_input = np.zeros((1, *self.input_shape))\n",
    "        x = self.conv1.forward(dummy_input)\n",
    "        x = self.pool1.forward(x)\n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.pool2.forward(x)\n",
    "        return np.prod(x.shape[1:])\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the CNN\"\"\"\n",
    "        # Convolutional layers\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        # Flatten\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = np.dot(x, self.fc1_weights) + self.fc1_bias\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = np.dot(x, self.fc2_weights) + self.fc2_bias\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = np.dot(x, self.fc3_weights) + self.fc3_bias\n",
    "        \n",
    "        return self.softmax(x)\n",
    "    \n",
    "    def calculate_receptive_field(self):\n",
    "        \"\"\"Calculate receptive field at each layer\"\"\"\n",
    "        receptive_fields = []\n",
    "        \n",
    "        # Initial receptive field\n",
    "        rf = 1\n",
    "        stride_product = 1\n",
    "        \n",
    "        # Conv1: kernel=5, stride=1\n",
    "        rf = rf + (5 - 1) * stride_product\n",
    "        receptive_fields.append(('Conv1', rf))\n",
    "        \n",
    "        # Pool1: kernel=2, stride=2\n",
    "        rf = rf + (2 - 1) * stride_product\n",
    "        stride_product *= 2\n",
    "        receptive_fields.append(('Pool1', rf))\n",
    "        \n",
    "        # Conv2: kernel=5, stride=1\n",
    "        rf = rf + (5 - 1) * stride_product\n",
    "        receptive_fields.append(('Conv2', rf))\n",
    "        \n",
    "        # Pool2: kernel=2, stride=2\n",
    "        rf = rf + (2 - 1) * stride_product\n",
    "        stride_product *= 2\n",
    "        receptive_fields.append(('Pool2', rf))\n",
    "        \n",
    "        return receptive_fields\n",
    "\n",
    "# Test with simple image data\n",
    "print(\"Testing CNN Implementation...\")\n",
    "\n",
    "# Create synthetic image data\n",
    "np.random.seed(42)\n",
    "batch_size = 10\n",
    "input_shape = (1, 28, 28)  # Single channel, 28x28 images\n",
    "num_classes = 10\n",
    "\n",
    "# Generate random data\n",
    "X = np.random.randn(batch_size, *input_shape)\n",
    "y = np.random.randint(0, num_classes, batch_size)\n",
    "\n",
    "# Initialize CNN\n",
    "cnn = CNNCustom(input_shape, num_classes)\n",
    "\n",
    "# Forward pass\n",
    "output = cnn.forward(X)\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output probabilities sum: {np.sum(output, axis=1)[:5]}\")\n",
    "\n",
    "# Calculate receptive fields\n",
    "receptive_fields = cnn.calculate_receptive_field()\n",
    "print(\"\\nReceptive Field Analysis:\")\n",
    "for layer, rf in receptive_fields:\n",
    "    print(f\"{layer}: {rf}x{rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with PyTorch implementation\n",
    "class LeNetPyTorch(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNetPyTorch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Test PyTorch version\n",
    "lenet_pytorch = LeNetPyTorch(num_classes=10)\n",
    "X_torch = torch.randn(batch_size, *input_shape)\n",
    "with torch.no_grad():\n",
    "    output_pytorch = lenet_pytorch(X_torch)\n",
    "\n",
    "print(f\"PyTorch output shape: {output_pytorch.shape}\")\n",
    "print(f\"PyTorch probabilities sum: {torch.sum(output_pytorch, dim=1)[:5]}\")\n",
    "\n",
    "# Visualize feature maps\n",
    "def visualize_feature_maps(model, input_data, layer_name):\n",
    "    \"\"\"Visualize feature maps from a specific layer\"\"\"\n",
    "    if layer_name == 'conv1':\n",
    "        features = model.conv1.forward(input_data)\n",
    "        features = np.maximum(0, features)  # ReLU activation\n",
    "    elif layer_name == 'conv2':\n",
    "        x = model.conv1.forward(input_data)\n",
    "        x = np.maximum(0, x)\n",
    "        x = model.pool1.forward(x)\n",
    "        features = model.conv2.forward(x)\n",
    "        features = np.maximum(0, features)\n",
    "    \n",
    "    # Plot first 6 feature maps\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    fig.suptitle(f'Feature Maps from {layer_name}')\n",
    "    \n",
    "    for i in range(6):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        if i < features.shape[1]:\n",
    "            axes[row, col].imshow(features[0, i], cmap='viridis')\n",
    "            axes[row, col].set_title(f'Filter {i+1}')\n",
    "            axes[row, col].axis('off')\n",
    "        else:\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature maps\n",
    "sample_input = X[:1]  # Take first sample\n",
    "visualize_feature_maps(cnn, sample_input, 'conv1')\n",
    "visualize_feature_maps(cnn, sample_input, 'conv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Recurrent Neural Networks (RNNs) and LSTM\n",
    "\n",
    "**Question**: Implement RNNs and LSTM from scratch, explaining the vanishing gradient problem and how LSTM gates solve it. Demonstrate sequence prediction and analyze gradient flow.\n",
    "\n",
    "### Theory: RNNs and LSTM\n",
    "\n",
    "**1. Vanilla RNN:**\n",
    "$$h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$$\n",
    "$$y_t = W_{hy}h_t + b_y$$\n",
    "\n",
    "**2. LSTM Gates:**\n",
    "- Forget gate: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "- Input gate: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "- Candidate values: $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "- Cell state: $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
    "- Output gate: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "- Hidden state: $h_t = o_t * \\tanh(C_t)$\n",
    "\n",
    "**3. Vanishing Gradient Problem:**\n",
    "$$\\frac{\\partial L}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial L_t}{\\partial W} = \\sum_{t=1}^T \\frac{\\partial L_t}{\\partial h_t} \\prod_{k=1}^t \\frac{\\partial h_k}{\\partial h_{k-1}} \\frac{\\partial h_1}{\\partial W}$$\n",
    "\n",
    "The product $\\prod_{k=1}^t \\frac{\\partial h_k}{\\partial h_{k-1}}$ can vanish exponentially as $t$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNCustom:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.Wxh = np.random.randn(input_size, hidden_size) * 0.1\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "        self.Why = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        \n",
    "        self.bh = np.zeros((1, hidden_size))\n",
    "        self.by = np.zeros((1, output_size))\n",
    "        \n",
    "        # For gradient analysis\n",
    "        self.hidden_states = []\n",
    "        self.gradients = []\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, inputs, h_prev=None):\n",
    "        \"\"\"Forward pass through RNN\"\"\"\n",
    "        seq_len, batch_size = inputs.shape[0], inputs.shape[1]\n",
    "        \n",
    "        if h_prev is None:\n",
    "            h_prev = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        self.hidden_states = [h_prev]\n",
    "        outputs = []\n",
    "        \n",
    "        h = h_prev\n",
    "        for t in range(seq_len):\n",
    "            # RNN step\n",
    "            h = self.tanh(np.dot(inputs[t], self.Wxh) + np.dot(h, self.Whh) + self.bh)\n",
    "            y = np.dot(h, self.Why) + self.by\n",
    "            \n",
    "            self.hidden_states.append(h)\n",
    "            outputs.append(y)\n",
    "        \n",
    "        return np.array(outputs), h\n",
    "    \n",
    "    def analyze_gradients(self, sequence_length=20):\n",
    "        \"\"\"Analyze gradient magnitudes to demonstrate vanishing gradient problem\"\"\"\n",
    "        # Simulate gradient flow backward through time\n",
    "        gradients = []\n",
    "        \n",
    "        # Start with unit gradient at final timestep\n",
    "        grad = 1.0\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            # Approximate gradient flow: grad *= |dh/dh_prev|\n",
    "            # For tanh activation: derivative is (1 - tanh²(x))\n",
    "            # Multiply by weight matrix norm\n",
    "            tanh_derivative = 1.0 - 0.5  # Approximate average derivative\n",
    "            weight_norm = np.linalg.norm(self.Whh)\n",
    "            \n",
    "            grad *= tanh_derivative * weight_norm\n",
    "            gradients.append(grad)\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "class LSTMCustom:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights for gates\n",
    "        concat_size = input_size + hidden_size\n",
    "        \n",
    "        # Forget gate\n",
    "        self.Wf = np.random.randn(concat_size, hidden_size) * 0.1\n",
    "        self.bf = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Input gate\n",
    "        self.Wi = np.random.randn(concat_size, hidden_size) * 0.1\n",
    "        self.bi = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Candidate gate\n",
    "        self.Wc = np.random.randn(concat_size, hidden_size) * 0.1\n",
    "        self.bc = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Output gate\n",
    "        self.Wo = np.random.randn(concat_size, hidden_size) * 0.1\n",
    "        self.bo = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Output projection\n",
    "        self.Why = np.random.randn(hidden_size, output_size) * 0.1\n",
    "        self.by = np.zeros((1, output_size))\n",
    "        \n",
    "        # For analysis\n",
    "        self.gate_activations = {'forget': [], 'input': [], 'output': []}\n",
    "        self.cell_states = []\n",
    "        self.hidden_states = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(np.clip(x, -500, 500))\n",
    "    \n",
    "    def forward(self, inputs, h_prev=None, c_prev=None):\n",
    "        \"\"\"Forward pass through LSTM\"\"\"\n",
    "        seq_len, batch_size = inputs.shape[0], inputs.shape[1]\n",
    "        \n",
    "        if h_prev is None:\n",
    "            h_prev = np.zeros((batch_size, self.hidden_size))\n",
    "        if c_prev is None:\n",
    "            c_prev = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        # Reset tracking\n",
    "        self.gate_activations = {'forget': [], 'input': [], 'output': []}\n",
    "        self.cell_states = [c_prev]\n",
    "        self.hidden_states = [h_prev]\n",
    "        \n",
    "        outputs = []\n",
    "        h, c = h_prev, c_prev\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Concatenate input and previous hidden state\n",
    "            concat = np.concatenate([inputs[t], h], axis=1)\n",
    "            \n",
    "            # Forget gate\n",
    "            f = self.sigmoid(np.dot(concat, self.Wf) + self.bf)\n",
    "            \n",
    "            # Input gate\n",
    "            i = self.sigmoid(np.dot(concat, self.Wi) + self.bi)\n",
    "            \n",
    "            # Candidate values\n",
    "            c_tilde = self.tanh(np.dot(concat, self.Wc) + self.bc)\n",
    "            \n",
    "            # Update cell state\n",
    "            c = f * c + i * c_tilde\n",
    "            \n",
    "            # Output gate\n",
    "            o = self.sigmoid(np.dot(concat, self.Wo) + self.bo)\n",
    "            \n",
    "            # Update hidden state\n",
    "            h = o * self.tanh(c)\n",
    "            \n",
    "            # Output projection\n",
    "            y = np.dot(h, self.Why) + self.by\n",
    "            \n",
    "            # Store for analysis\n",
    "            self.gate_activations['forget'].append(f)\n",
    "            self.gate_activations['input'].append(i)\n",
    "            self.gate_activations['output'].append(o)\n",
    "            self.cell_states.append(c)\n",
    "            self.hidden_states.append(h)\n",
    "            \n",
    "            outputs.append(y)\n",
    "        \n",
    "        return np.array(outputs), h, c\n",
    "    \n",
    "    def analyze_gradient_flow(self, sequence_length=20):\n",
    "        \"\"\"Analyze how LSTM maintains gradient flow\"\"\"\n",
    "        gradients = []\n",
    "        \n",
    "        # LSTM gradient flow is better preserved due to cell state\n",
    "        grad = 1.0\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            # Simulate gradient flow through forget gate\n",
    "            # Gradient flows through cell state with forget gate multiplication\n",
    "            forget_gate_avg = 0.7  # Typical forget gate activation\n",
    "            grad *= forget_gate_avg  # Much slower decay than vanilla RNN\n",
    "            gradients.append(grad)\n",
    "        \n",
    "        return gradients\n",
    "\n",
    "# Test RNN implementations\n",
    "print(\"Testing RNN and LSTM Implementations...\")\n",
    "\n",
    "# Create sequence data\n",
    "np.random.seed(42)\n",
    "seq_len = 10\n",
    "batch_size = 5\n",
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 2\n",
    "\n",
    "# Generate random sequence\n",
    "X_seq = np.random.randn(seq_len, batch_size, input_size)\n",
    "\n",
    "# Test Vanilla RNN\n",
    "rnn = VanillaRNNCustom(input_size, hidden_size, output_size)\n",
    "outputs_rnn, final_h_rnn = rnn.forward(X_seq)\n",
    "\n",
    "print(f\"RNN - Input shape: {X_seq.shape}\")\n",
    "print(f\"RNN - Output shape: {outputs_rnn.shape}\")\n",
    "print(f\"RNN - Final hidden state shape: {final_h_rnn.shape}\")\n",
    "\n",
    "# Test LSTM\n",
    "lstm = LSTMCustom(input_size, hidden_size, output_size)\n",
    "outputs_lstm, final_h_lstm, final_c_lstm = lstm.forward(X_seq)\n",
    "\n",
    "print(f\"\\nLSTM - Output shape: {outputs_lstm.shape}\")\n",
    "print(f\"LSTM - Final hidden state shape: {final_h_lstm.shape}\")\n",
    "print(f\"LSTM - Final cell state shape: {final_c_lstm.shape}\")\n",
    "\n",
    "# Analyze gradient flow\n",
    "rnn_gradients = rnn.analyze_gradients(sequence_length=20)\n",
    "lstm_gradients = lstm.analyze_gradient_flow(sequence_length=20)\n",
    "\n",
    "print(f\"\\nGradient Analysis:\")\n",
    "print(f\"RNN gradient after 20 steps: {rnn_gradients[-1]:.2e}\")\n",
    "print(f\"LSTM gradient after 20 steps: {lstm_gradients[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient flow comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot gradient magnitudes\n",
    "plt.subplot(2, 2, 1)\n",
    "timesteps = range(1, len(rnn_gradients) + 1)\n",
    "plt.semilogy(timesteps, rnn_gradients, 'r-', label='Vanilla RNN', linewidth=2)\n",
    "plt.semilogy(timesteps, lstm_gradients, 'b-', label='LSTM', linewidth=2)\n",
    "plt.xlabel('Timesteps Back')\n",
    "plt.ylabel('Gradient Magnitude (log scale)')\n",
    "plt.title('Gradient Flow Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot LSTM gate activations\n",
    "if lstm.gate_activations['forget']:\n",
    "    forget_gates = np.array(lstm.gate_activations['forget'])\n",
    "    input_gates = np.array(lstm.gate_activations['input'])\n",
    "    output_gates = np.array(lstm.gate_activations['output'])\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    timesteps = range(forget_gates.shape[0])\n",
    "    plt.plot(timesteps, np.mean(forget_gates, axis=(1, 2)), 'g-', label='Forget Gate', linewidth=2)\n",
    "    plt.plot(timesteps, np.mean(input_gates, axis=(1, 2)), 'r-', label='Input Gate', linewidth=2)\n",
    "    plt.plot(timesteps, np.mean(output_gates, axis=(1, 2)), 'b-', label='Output Gate', linewidth=2)\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Average Gate Activation')\n",
    "    plt.title('LSTM Gate Activations')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "# Compare with PyTorch LSTM\n",
    "lstm_pytorch = nn.LSTM(input_size, hidden_size, batch_first=False)\n",
    "X_torch = torch.randn(seq_len, batch_size, input_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_pytorch, (h_final, c_final) = lstm_pytorch(X_torch)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(outputs_lstm[:, 0, 0], 'b-', label='Custom LSTM', linewidth=2)\n",
    "plt.plot(outputs_pytorch[:, 0, 0].numpy(), 'r--', label='PyTorch LSTM', linewidth=2)\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Output Value')\n",
    "plt.title('Output Comparison (First Feature)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hidden state evolution\n",
    "if lstm.hidden_states:\n",
    "    hidden_norms = [np.linalg.norm(h, axis=1).mean() for h in lstm.hidden_states[1:]]\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(len(hidden_norms)), hidden_norms, 'purple', linewidth=2)\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Average Hidden State Norm')\n",
    "    plt.title('Hidden State Evolution')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nComparison Summary:\")\n",
    "print(f\"Custom LSTM output range: [{outputs_lstm.min():.3f}, {outputs_lstm.max():.3f}]\")\n",
    "print(f\"PyTorch LSTM output range: [{outputs_pytorch.min():.3f}, {outputs_pytorch.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Attention Mechanisms and Transformers\n",
    "\n",
    "**Question**: Implement self-attention and multi-head attention from scratch. Explain the mathematical foundations of the Transformer architecture and demonstrate its advantages over RNNs for sequence modeling.\n",
    "\n",
    "### Theory: Attention and Transformers\n",
    "\n",
    "**1. Scaled Dot-Product Attention:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ = queries matrix $(n \\times d_k)$\n",
    "- $K$ = keys matrix $(m \\times d_k)$\n",
    "- $V$ = values matrix $(m \\times d_v)$\n",
    "- $d_k$ = dimension of keys/queries\n",
    "\n",
    "**2. Multi-Head Attention:**\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "Where:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "**3. Positional Encoding:**\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "**4. Self-Attention Benefits:**\n",
    "- Parallelizable computation\n",
    "- Direct connections between all positions\n",
    "- No vanishing gradient problem across sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionCustom:\n",
    "    def __init__(self, d_model, d_k=None, d_v=None):\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k if d_k is not None else d_model\n",
    "        self.d_v = d_v if d_v is not None else d_model\n",
    "        self.scale = np.sqrt(self.d_k)\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def softmax(self, x, axis=-1):\n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        x_max = np.max(x, axis=axis, keepdims=True)\n",
    "        exp_x = np.exp(x - x_max)\n",
    "        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"Scaled dot-product attention\"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = np.where(mask == 0, -1e9, scores)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = self.softmax(scores, axis=-1)\n",
    "        self.attention_weights = attention_weights\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = np.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class MultiHeadAttentionCustom:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.d_v = d_model // num_heads\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Weight matrices for each head\n",
    "        self.W_q = np.random.randn(num_heads, d_model, self.d_k) * 0.1\n",
    "        self.W_k = np.random.randn(num_heads, d_model, self.d_k) * 0.1\n",
    "        self.W_v = np.random.randn(num_heads, d_model, self.d_v) * 0.1\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.1\n",
    "        \n",
    "        # Individual attention mechanisms\n",
    "        self.attention = AttentionCustom(d_model, self.d_k, self.d_v)\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.all_attention_weights = []\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"Multi-head attention forward pass\"\"\"\n",
    "        batch_size, seq_len = Q.shape[0], Q.shape[1]\n",
    "        \n",
    "        # Store attention weights for each head\n",
    "        self.all_attention_weights = []\n",
    "        head_outputs = []\n",
    "        \n",
    "        # Process each attention head\n",
    "        for i in range(self.num_heads):\n",
    "            # Linear projections for this head\n",
    "            Q_head = np.matmul(Q, self.W_q[i])  # (batch_size, seq_len, d_k)\n",
    "            K_head = np.matmul(K, self.W_k[i])  # (batch_size, seq_len, d_k)\n",
    "            V_head = np.matmul(V, self.W_v[i])  # (batch_size, seq_len, d_v)\n",
    "            \n",
    "            # Add batch dimension for attention computation\n",
    "            Q_head = Q_head[:, np.newaxis, :, :]  # (batch_size, 1, seq_len, d_k)\n",
    "            K_head = K_head[:, np.newaxis, :, :]  # (batch_size, 1, seq_len, d_k)\n",
    "            V_head = V_head[:, np.newaxis, :, :]  # (batch_size, 1, seq_len, d_v)\n",
    "            \n",
    "            # Compute attention for this head\n",
    "            head_output, attention_weights = self.attention.scaled_dot_product_attention(\n",
    "                Q_head, K_head, V_head, mask\n",
    "            )\n",
    "            \n",
    "            # Remove the extra dimension\n",
    "            head_output = head_output[:, 0, :, :]  # (batch_size, seq_len, d_v)\n",
    "            \n",
    "            head_outputs.append(head_output)\n",
    "            self.all_attention_weights.append(attention_weights[:, 0, :, :])  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # Concatenate all heads\n",
    "        multi_head_output = np.concatenate(head_outputs, axis=-1)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = np.matmul(multi_head_output, self.W_o)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncodingCustom:\n",
    "    def __init__(self, d_model, max_seq_len=5000):\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = np.zeros((max_seq_len, d_model))\n",
    "        position = np.arange(0, max_seq_len)[:, np.newaxis]\n",
    "        \n",
    "        # Compute the positional encodings\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        self.pe = pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input\"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        return x + self.pe[:seq_len, :]\n",
    "\n",
    "class TransformerBlockCustom:\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.self_attention = MultiHeadAttentionCustom(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff_W1 = np.random.randn(d_model, d_ff) * 0.1\n",
    "        self.ff_b1 = np.zeros(d_ff)\n",
    "        self.ff_W2 = np.random.randn(d_ff, d_model) * 0.1\n",
    "        self.ff_b2 = np.zeros(d_model)\n",
    "        \n",
    "        # Layer normalization parameters (simplified)\n",
    "        self.ln1_weight = np.ones(d_model)\n",
    "        self.ln1_bias = np.zeros(d_model)\n",
    "        self.ln2_weight = np.ones(d_model)\n",
    "        self.ln2_bias = np.zeros(d_model)\n",
    "    \n",
    "    def layer_norm(self, x, weight, bias, eps=1e-6):\n",
    "        \"\"\"Layer normalization\"\"\"\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        normalized = (x - mean) / np.sqrt(var + eps)\n",
    "        return normalized * weight + bias\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"Position-wise feed-forward network\"\"\"\n",
    "        # First linear layer + ReLU\n",
    "        hidden = self.relu(np.matmul(x, self.ff_W1) + self.ff_b1)\n",
    "        # Second linear layer\n",
    "        output = np.matmul(hidden, self.ff_W2) + self.ff_b2\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Forward pass through transformer block\"\"\"\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attention_output = self.self_attention.forward(x, x, x, mask)\n",
    "        x = self.layer_norm(x + attention_output, self.ln1_weight, self.ln1_bias)\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm(x + ff_output, self.ln2_weight, self.ln2_bias)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test implementations\n",
    "print(\"Testing Attention and Transformer Implementations...\")\n",
    "\n",
    "# Create test data\n",
    "np.random.seed(42)\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_ff = 256\n",
    "\n",
    "# Generate random input\n",
    "X = np.random.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Test Multi-Head Attention\n",
    "mha = MultiHeadAttentionCustom(d_model, num_heads)\n",
    "attention_output = mha.forward(X, X, X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Multi-head attention output shape: {attention_output.shape}\")\n",
    "print(f\"Number of attention heads: {len(mha.all_attention_weights)}\")\n",
    "print(f\"Attention weights shape per head: {mha.all_attention_weights[0].shape}\")\n",
    "\n",
    "# Test Positional Encoding\n",
    "pe = PositionalEncodingCustom(d_model)\n",
    "X_with_pe = pe.forward(X)\n",
    "\n",
    "print(f\"\\nInput with positional encoding shape: {X_with_pe.shape}\")\n",
    "print(f\"Positional encoding added successfully\")\n",
    "\n",
    "# Test Transformer Block\n",
    "transformer_block = TransformerBlockCustom(d_model, num_heads, d_ff)\n",
    "transformer_output = transformer_block.forward(X_with_pe)\n",
    "\n",
    "print(f\"\\nTransformer block output shape: {transformer_output.shape}\")\n",
    "print(f\"Transformer processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns and positional encodings\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Visualize positional encodings\n",
    "plt.subplot(2, 3, 1)\n",
    "pe_matrix = pe.pe[:50, :50]  # First 50 positions and dimensions\n",
    "plt.imshow(pe_matrix, aspect='auto', cmap='RdBu')\n",
    "plt.title('Positional Encoding Pattern')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "\n",
    "# Visualize attention weights for different heads\n",
    "for head_idx in range(min(4, num_heads)):\n",
    "    plt.subplot(2, 3, head_idx + 2)\n",
    "    attention_matrix = mha.all_attention_weights[head_idx][0]  # First batch\n",
    "    plt.imshow(attention_matrix, cmap='Blues')\n",
    "    plt.title(f'Attention Head {head_idx + 1}')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.colorbar()\n",
    "\n",
    "# Compare computational complexity\n",
    "plt.subplot(2, 3, 6)\n",
    "sequence_lengths = np.arange(10, 200, 10)\n",
    "rnn_complexity = sequence_lengths * d_model  # O(n * d)\n",
    "attention_complexity = sequence_lengths ** 2 * d_model  # O(n² * d)\n",
    "\n",
    "plt.loglog(sequence_lengths, rnn_complexity, 'r-', label='RNN O(n·d)', linewidth=2)\n",
    "plt.loglog(sequence_lengths, attention_complexity, 'b-', label='Attention O(n²·d)', linewidth=2)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Computational Cost')\n",
    "plt.title('Complexity Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with PyTorch implementation\n",
    "class TransformerPyTorch(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test PyTorch version\n",
    "transformer_pytorch = TransformerPyTorch(d_model, num_heads, d_ff)\n",
    "X_torch = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_pytorch = transformer_pytorch(X_torch)\n",
    "\n",
    "print(f\"\\nComparison with PyTorch:\")\n",
    "print(f\"Custom transformer output range: [{transformer_output.min():.3f}, {transformer_output.max():.3f}]\")\n",
    "print(f\"PyTorch transformer output range: [{output_pytorch.min():.3f}, {output_pytorch.max():.3f}]\")\n",
    "\n",
    "# Analyze attention patterns\n",
    "print(f\"\\nAttention Analysis:\")\n",
    "for i, attention_weights in enumerate(mha.all_attention_weights[:3]):\n",
    "    # Calculate attention entropy (measure of how spread out the attention is)\n",
    "    entropy = -np.sum(attention_weights * np.log(attention_weights + 1e-9), axis=-1)\n",
    "    print(f\"Head {i+1} - Average attention entropy: {entropy.mean():.3f}\")\n",
    "    print(f\"Head {i+1} - Max attention weight: {attention_weights.max():.3f}\")\n",
    "\n",
    "print(f\"\\nAdvantages of Transformer over RNN:\")\n",
    "print(f\"1. Parallelization: All positions processed simultaneously\")\n",
    "print(f\"2. Direct connections: No information bottleneck through hidden states\")\n",
    "print(f\"3. No vanishing gradients: Direct gradient flow to all positions\")\n",
    "print(f\"4. Flexible attention: Can focus on relevant positions regardless of distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Deep Learning Architectures\n",
    "\n",
    "This notebook covered three fundamental deep learning architectures:\n",
    "\n",
    "### 1. Convolutional Neural Networks (CNNs)\n",
    "- **Key Concepts**: Convolution, pooling, feature maps, receptive fields\n",
    "- **Mathematical Foundation**: Convolution operation preserves spatial relationships\n",
    "- **Applications**: Image processing, computer vision, spatial pattern recognition\n",
    "- **Architecture**: LeNet-style with conv → pool → conv → pool → FC layers\n",
    "\n",
    "### 2. Recurrent Neural Networks and LSTM\n",
    "- **Key Concepts**: Sequential processing, hidden states, vanishing gradients\n",
    "- **Mathematical Foundation**: RNN processes sequences step-by-step, LSTM uses gates to control information flow\n",
    "- **Problem Solved**: LSTM gates mitigate vanishing gradient problem in long sequences\n",
    "- **Applications**: Natural language processing, time series, sequential data\n",
    "\n",
    "### 3. Attention Mechanisms and Transformers\n",
    "- **Key Concepts**: Self-attention, multi-head attention, positional encoding\n",
    "- **Mathematical Foundation**: Scaled dot-product attention enables direct connections between all positions\n",
    "- **Advantages**: Parallelizable, no vanishing gradients, flexible attention patterns\n",
    "- **Applications**: Machine translation, language modeling, sequence-to-sequence tasks\n",
    "\n",
    "### Architecture Evolution\n",
    "1. **RNNs**: Sequential processing, information bottleneck\n",
    "2. **LSTMs**: Better long-term dependencies, still sequential\n",
    "3. **Transformers**: Parallel processing, direct global connections\n",
    "\n",
    "Each architecture has specific strengths and is suited for different types of data and tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}