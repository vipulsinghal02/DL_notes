{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Practice Questions - Part 2: Data Preprocessing and Feature Engineering\n",
    "\n",
    "This notebook covers essential data preprocessing and feature engineering techniques. These skills are critical for building robust ML models that work well in production.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing these questions, you will:\n",
    "- Master different strategies for handling missing data\n",
    "- Understand when and how to apply feature scaling techniques\n",
    "- Learn various methods for encoding categorical variables\n",
    "- Apply feature selection techniques to improve model performance\n",
    "- Recognize and prevent data leakage in preprocessing pipelines\n",
    "\n",
    "## Difficulty Levels\n",
    "- ★☆☆ **Beginner**: Basic preprocessing concepts\n",
    "- ★★☆ **Intermediate**: Advanced techniques and edge cases\n",
    "- ★★★ **Advanced**: Complex scenarios and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, Normalizer,\n",
    "    LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, chi2, f_classif, mutual_info_classif,\n",
    "    RFE, SelectFromModel\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Missing Data Strategies ★★☆\n",
    "\n",
    "**Question:** You have a dataset with different types of missing data patterns. For each scenario below, recommend the most appropriate handling strategy and implement it:\n",
    "\n",
    "1. **Customer age**: 5% missing, normally distributed\n",
    "2. **Income**: 15% missing, right-skewed distribution\n",
    "3. **Product category**: 8% missing, categorical with clear hierarchy\n",
    "4. **Website engagement score**: 25% missing, only missing for users who never logged in\n",
    "5. **Survey responses**: 40% missing, likely missing not at random (MNAR)\n",
    "\n",
    "Explain your reasoning and demonstrate the impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1: Missing Data Strategies\n",
    "\n",
    "#### **Missing Data Types (MCAR, MAR, MNAR)**\n",
    "\n",
    "**MCAR (Missing Completely At Random)**: Missingness is independent of observed and unobserved data  \n",
    "**MAR (Missing At Random)**: Missingness depends on observed data but not unobserved data  \n",
    "**MNAR (Missing Not At Random)**: Missingness depends on the unobserved value itself\n",
    "\n",
    "#### **1. Customer Age (5% missing, normal distribution)**\n",
    "**Strategy: Mean/Median Imputation**\n",
    "- **Reasoning**: Low missingness rate, normal distribution suggests mean imputation is reasonable\n",
    "- **Alternative**: KNN imputation if other demographic features are available\n",
    "- **Implementation**: SimpleImputer with mean strategy\n",
    "\n",
    "#### **2. Income (15% missing, right-skewed)**\n",
    "**Strategy: Median Imputation or Log-transform then Mean**\n",
    "- **Reasoning**: Right-skewed distributions have outliers that make mean inappropriate\n",
    "- **Alternative**: Predictive imputation using other features\n",
    "- **Implementation**: SimpleImputer with median or KNNImputer\n",
    "\n",
    "#### **3. Product Category (8% missing, categorical hierarchy)**\n",
    "**Strategy: Mode Imputation or Hierarchical Imputation**\n",
    "- **Reasoning**: Use category hierarchy to impute at appropriate level\n",
    "- **Alternative**: Create \"Unknown\" category to preserve missingness information\n",
    "- **Implementation**: Custom imputation based on parent categories\n",
    "\n",
    "#### **4. Website Engagement (25% missing, systematic pattern)**\n",
    "**Strategy: Missingness Indicator + Imputation**\n",
    "- **Reasoning**: Missingness is informative (never logged in), should be preserved\n",
    "- **Method**: Create binary \"never_logged_in\" feature + impute with 0\n",
    "- **Implementation**: Add indicator variable before imputation\n",
    "\n",
    "#### **5. Survey Responses (40% missing, MNAR)**\n",
    "**Strategy: Multiple Imputation or Domain-Specific Modeling**\n",
    "- **Reasoning**: High missingness rate with potential bias requires careful handling\n",
    "- **Alternatives**: \n",
    "  - Use only complete cases if still sufficient sample size\n",
    "  - Model the missingness mechanism explicitly\n",
    "- **Implementation**: Multiple imputation with sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset with different missing data patterns\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate base data\n",
    "age = np.random.normal(35, 10, n_samples)\n",
    "income = np.random.lognormal(10.5, 0.5, n_samples)  # Right-skewed\n",
    "categories = np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], n_samples)\n",
    "logged_in = np.random.binomial(1, 0.7, n_samples)  # 70% log in\n",
    "engagement = np.where(logged_in, np.random.normal(50, 15, n_samples), np.nan)\n",
    "survey_response = np.random.normal(3, 1, n_samples)  # 1-5 scale\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'income': income,\n",
    "    'category': categories,\n",
    "    'engagement': engagement,\n",
    "    'survey_response': survey_response\n",
    "})\n",
    "\n",
    "# Introduce missing data patterns\n",
    "\n",
    "# 1. Age: 5% MCAR\n",
    "missing_age_idx = np.random.choice(n_samples, int(0.05 * n_samples), replace=False)\n",
    "df.loc[missing_age_idx, 'age'] = np.nan\n",
    "\n",
    "# 2. Income: 15% MAR (higher chance of missing for younger people)\n",
    "income_missing_prob = 1 / (1 + np.exp(0.1 * (df['age'] - 25)))  # Sigmoid\n",
    "income_missing_prob = income_missing_prob.fillna(0.15)  # Handle NaN ages\n",
    "income_missing = np.random.binomial(1, income_missing_prob)\n",
    "df.loc[income_missing == 1, 'income'] = np.nan\n",
    "\n",
    "# 3. Category: 8% MCAR\n",
    "missing_cat_idx = np.random.choice(n_samples, int(0.08 * n_samples), replace=False)\n",
    "df.loc[missing_cat_idx, 'category'] = np.nan\n",
    "\n",
    "# 4. Engagement: Already has systematic missingness (never logged in)\n",
    "# This is MAR - missing depends on login status\n",
    "\n",
    "# 5. Survey: 40% MNAR (people with extreme opinions less likely to respond)\n",
    "extreme_opinions = (df['survey_response'] < 2) | (df['survey_response'] > 4)\n",
    "survey_missing_prob = np.where(extreme_opinions, 0.6, 0.3)  # Higher missingness for extreme views\n",
    "survey_missing = np.random.binomial(1, survey_missing_prob)\n",
    "df.loc[survey_missing == 1, 'survey_response'] = np.nan\n",
    "\n",
    "print(\"Dataset with Missing Data Patterns:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(\"\\nMissing data summary:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "for col in missing_summary.index:\n",
    "    pct = missing_summary[col] / len(df) * 100\n",
    "    print(f\"{col:<15}: {missing_summary[col]:>4} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Visualize missing data patterns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Missing Data Patterns and Distributions', fontsize=16)\n",
    "\n",
    "# Missing data heatmap\n",
    "missing_matrix = df.isnull()\n",
    "sns.heatmap(missing_matrix, yticklabels=False, cbar=True, cmap='viridis', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Missing Data Pattern')\n",
    "axes[0, 0].set_xlabel('Features')\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 1].hist(df['age'].dropna(), bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 1].set_title('Age Distribution (Normal)')\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].axvline(df['age'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 1].axvline(df['age'].median(), color='orange', linestyle='--', label='Median')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Income distribution (log scale)\n",
    "axes[0, 2].hist(df['income'].dropna(), bins=30, alpha=0.7, color='lightgreen')\n",
    "axes[0, 2].set_title('Income Distribution (Right-skewed)')\n",
    "axes[0, 2].set_xlabel('Income')\n",
    "axes[0, 2].axvline(df['income'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 2].axvline(df['income'].median(), color='orange', linestyle='--', label='Median')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Category distribution\n",
    "category_counts = df['category'].value_counts()\n",
    "axes[1, 0].bar(category_counts.index, category_counts.values, alpha=0.7, color='lightcoral')\n",
    "axes[1, 0].set_title('Category Distribution')\n",
    "axes[1, 0].set_xlabel('Category')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Engagement vs Login status\n",
    "df_temp = df.copy()\n",
    "df_temp['logged_in'] = ~df_temp['engagement'].isnull()\n",
    "login_counts = df_temp['logged_in'].value_counts()\n",
    "axes[1, 1].bar(['Never Logged In', 'Logged In'], \n",
    "               [login_counts[False], login_counts[True]], \n",
    "               alpha=0.7, color=['red', 'green'])\n",
    "axes[1, 1].set_title('Engagement Missingness Pattern')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "# Survey response distribution\n",
    "axes[1, 2].hist(df['survey_response'].dropna(), bins=20, alpha=0.7, color='purple')\n",
    "axes[1, 2].set_title('Survey Response Distribution')\n",
    "axes[1, 2].set_xlabel('Survey Score (1-5)')\n",
    "axes[1, 2].axvline(df['survey_response'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMissing Data Analysis:\")\n",
    "print(f\"• Age: Low missingness ({missing_summary['age']/len(df):.1%}), normal distribution → Mean imputation\")\n",
    "print(f\"• Income: Moderate missingness ({missing_summary['income']/len(df):.1%}), skewed → Median imputation\")\n",
    "print(f\"• Category: Low missingness ({missing_summary['category']/len(df):.1%}) → Mode imputation\")\n",
    "print(f\"• Engagement: High systematic missingness ({missing_summary['engagement']/len(df):.1%}) → Indicator + Imputation\")\n",
    "print(f\"• Survey: Very high missingness ({missing_summary['survey_response']/len(df):.1%}), MNAR → Multiple imputation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement different imputation strategies and compare performance\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Create target variable for evaluation\n",
    "# Simulate a target that depends on our features\n",
    "np.random.seed(42)\n",
    "target = (0.1 * df['age'].fillna(df['age'].mean()) + \n",
    "          0.00001 * df['income'].fillna(df['income'].median()) +\n",
    "          0.01 * df['engagement'].fillna(0) +\n",
    "          df['survey_response'].fillna(3) +\n",
    "          np.random.normal(0, 1, len(df)))\n",
    "target = (target > target.median()).astype(int)  # Convert to binary classification\n",
    "\n",
    "# Strategy 1: Simple Imputation\n",
    "def simple_imputation_strategy(df):\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Age: Mean imputation\n",
    "    df_imputed['age'].fillna(df_imputed['age'].mean(), inplace=True)\n",
    "    \n",
    "    # Income: Median imputation\n",
    "    df_imputed['income'].fillna(df_imputed['income'].median(), inplace=True)\n",
    "    \n",
    "    # Category: Mode imputation\n",
    "    df_imputed['category'].fillna(df_imputed['category'].mode()[0], inplace=True)\n",
    "    \n",
    "    # Engagement: Create indicator + zero imputation\n",
    "    df_imputed['never_logged_in'] = df_imputed['engagement'].isnull().astype(int)\n",
    "    df_imputed['engagement'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Survey: Mean imputation (simple approach)\n",
    "    df_imputed['survey_response'].fillna(df_imputed['survey_response'].mean(), inplace=True)\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Strategy 2: Advanced Imputation\n",
    "def advanced_imputation_strategy(df):\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Age: KNN imputation\n",
    "    numeric_cols = ['age', 'income', 'engagement', 'survey_response']\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    \n",
    "    # Create engagement indicator first\n",
    "    df_imputed['never_logged_in'] = df_imputed['engagement'].isnull().astype(int)\n",
    "    df_imputed['engagement'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Apply KNN to numeric columns (except engagement which we handled)\n",
    "    numeric_subset = df_imputed[['age', 'income', 'survey_response']]\n",
    "    imputed_numeric = knn_imputer.fit_transform(numeric_subset)\n",
    "    df_imputed[['age', 'income', 'survey_response']] = imputed_numeric\n",
    "    \n",
    "    # Category: Mode imputation\n",
    "    df_imputed['category'].fillna(df_imputed['category'].mode()[0], inplace=True)\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Strategy 3: Complete Case Analysis (for comparison)\n",
    "def complete_case_strategy(df):\n",
    "    return df.dropna()\n",
    "\n",
    "# Apply strategies\n",
    "df_simple = simple_imputation_strategy(df)\n",
    "df_advanced = advanced_imputation_strategy(df)\n",
    "df_complete = complete_case_strategy(df)\n",
    "\n",
    "print(\"Imputation Strategy Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original dataset: {len(df)} samples\")\n",
    "print(f\"Complete cases: {len(df_complete)} samples ({len(df_complete)/len(df):.1%} retained)\")\n",
    "print(f\"Simple imputation: {len(df_simple)} samples (no loss)\")\n",
    "print(f\"Advanced imputation: {len(df_advanced)} samples (no loss)\")\n",
    "\n",
    "# Prepare features for modeling\n",
    "def prepare_features(df_imputed):\n",
    "    # One-hot encode category\n",
    "    df_encoded = pd.get_dummies(df_imputed, columns=['category'], prefix='cat')\n",
    "    return df_encoded.select_dtypes(include=[np.number])\n",
    "\n",
    "# Evaluate different strategies\n",
    "def evaluate_imputation_strategy(df_imputed, target_subset, strategy_name):\n",
    "    X = prepare_features(df_imputed)\n",
    "    y = target_subset\n",
    "    \n",
    "    # Simple model for evaluation\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "# Evaluate strategies\n",
    "results = {}\n",
    "\n",
    "# Complete cases\n",
    "complete_indices = df_complete.index\n",
    "results['Complete Cases'] = evaluate_imputation_strategy(\n",
    "    df_complete, target[complete_indices], 'Complete Cases'\n",
    ")\n",
    "\n",
    "# Simple imputation\n",
    "results['Simple Imputation'] = evaluate_imputation_strategy(\n",
    "    df_simple, target, 'Simple Imputation'\n",
    ")\n",
    "\n",
    "# Advanced imputation\n",
    "results['Advanced Imputation'] = evaluate_imputation_strategy(\n",
    "    df_advanced, target, 'Advanced Imputation'\n",
    ")\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"=\"*50)\n",
    "for strategy, (mean_score, std_score) in results.items():\n",
    "    print(f\"{strategy:<20}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "# Visualize impact of missing data handling\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Sample sizes\n",
    "strategies = list(results.keys())\n",
    "sample_sizes = [len(df_complete), len(df_simple), len(df_advanced)]\n",
    "accuracies = [results[s][0] for s in strategies]\n",
    "errors = [results[s][1] for s in strategies]\n",
    "\n",
    "# Plot 1: Sample sizes\n",
    "bars1 = axes[0].bar(strategies, sample_sizes, alpha=0.7, color=['red', 'orange', 'green'])\n",
    "axes[0].set_title('Sample Sizes After Handling Missing Data')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "for bar, size in zip(bars1, sample_sizes):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "                 str(size), ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Model performance\n",
    "bars2 = axes[1].bar(strategies, accuracies, yerr=errors, alpha=0.7, \n",
    "                    color=['red', 'orange', 'green'], capsize=5)\n",
    "axes[1].set_title('Model Performance by Strategy')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].set_ylim(0.5, max(accuracies) + 0.05)\n",
    "for bar, acc in zip(bars2, accuracies):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                 f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Feature correlation after imputation\n",
    "corr_matrix = prepare_features(df_advanced).corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0, ax=axes[2])\n",
    "axes[2].set_title('Feature Correlations (Advanced Imputation)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"• Complete case analysis loses {(1-len(df_complete)/len(df)):.1%} of data\")\n",
    "print(f\"• Simple imputation preserves all samples but may introduce bias\")\n",
    "print(f\"• Advanced imputation (KNN) often provides better feature relationships\")\n",
    "print(f\"• Adding missingness indicators preserves important information\")\n",
    "print(f\"• Strategy choice depends on missingness mechanism and business context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Feature Scaling Decisions ★★☆\n",
    "\n",
    "**Question:** You're building different types of models with the following features. For each model type, decide whether scaling is needed and which scaling method to use:\n",
    "\n",
    "**Features:**\n",
    "- Age (18-80)\n",
    "- Income (20,000-500,000)\n",
    "- Credit Score (300-850)\n",
    "- Number of transactions (0-1000)\n",
    "- Account balance (-10,000 to 100,000)\n",
    "\n",
    "**Models to consider:**\n",
    "1. Random Forest\n",
    "2. Logistic Regression\n",
    "3. SVM with RBF kernel\n",
    "4. K-Means clustering\n",
    "5. Principal Component Analysis (PCA)\n",
    "\n",
    "Implement and demonstrate the impact of different scaling choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2: Feature Scaling Decisions\n",
    "\n",
    "#### **Scaling Requirements by Algorithm Type**\n",
    "\n",
    "**Distance-Based Algorithms**: Require scaling (KNN, SVM, K-Means, PCA)  \n",
    "**Tree-Based Algorithms**: Don't require scaling (Random Forest, Decision Trees)  \n",
    "**Linear Algorithms**: Often benefit from scaling (Linear/Logistic Regression)  \n",
    "**Neural Networks**: Almost always require scaling\n",
    "\n",
    "#### **Scaling Method Selection**\n",
    "\n",
    "**StandardScaler (Z-score normalization)**:\n",
    "- Best for: Normal distributions, when preserving shape matters\n",
    "- Formula: (x - μ) / σ\n",
    "- Use when: Features are roughly normal, no strict bounds needed\n",
    "\n",
    "**MinMaxScaler**:\n",
    "- Best for: Bounded features, when preserving zero matters\n",
    "- Formula: (x - min) / (max - min)\n",
    "- Use when: Need features in [0,1] range, distribution shape matters\n",
    "\n",
    "**RobustScaler**:\n",
    "- Best for: Data with outliers\n",
    "- Formula: (x - median) / IQR\n",
    "- Use when: Outliers present, want robust statistics\n",
    "\n",
    "**Normalizer**:\n",
    "- Best for: When direction matters more than magnitude\n",
    "- Formula: x / ||x||₂\n",
    "- Use when: Text features, sparse data\n",
    "\n",
    "#### **Recommendations by Model**\n",
    "\n",
    "1. **Random Forest**: No scaling needed (tree-based)\n",
    "2. **Logistic Regression**: StandardScaler (helps convergence)\n",
    "3. **SVM with RBF**: StandardScaler or MinMaxScaler (distance-based)\n",
    "4. **K-Means**: StandardScaler (distance-based clustering)\n",
    "5. **PCA**: StandardScaler (variance-based dimensionality reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic financial dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create features with different scales\n",
    "age = np.random.randint(18, 81, n_samples)\n",
    "income = np.random.lognormal(10.8, 0.6, n_samples)  # 20k-500k range\n",
    "income = np.clip(income, 20000, 500000)\n",
    "credit_score = np.random.normal(650, 100, n_samples)\n",
    "credit_score = np.clip(credit_score, 300, 850)\n",
    "transactions = np.random.poisson(50, n_samples)  # 0-1000 range\n",
    "transactions = np.clip(transactions, 0, 1000)\n",
    "balance = np.random.normal(20000, 25000, n_samples)  # Can be negative\n",
    "balance = np.clip(balance, -10000, 100000)\n",
    "\n",
    "# Create DataFrame\n",
    "financial_df = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'income': income,\n",
    "    'credit_score': credit_score,\n",
    "    'transactions': transactions,\n",
    "    'balance': balance\n",
    "})\n",
    "\n",
    "# Create target variable (loan approval)\n",
    "approval_prob = 1 / (1 + np.exp(-(\n",
    "    0.05 * age + \n",
    "    0.00001 * income + \n",
    "    0.01 * credit_score + \n",
    "    0.002 * transactions + \n",
    "    0.00001 * balance - 15\n",
    ")))\n",
    "loan_approved = np.random.binomial(1, approval_prob)\n",
    "\n",
    "print(\"Financial Dataset Overview:\")\n",
    "print(\"=\"*50)\n",
    "print(financial_df.describe())\n",
    "print(f\"\\nLoan Approval Rate: {loan_approved.mean():.1%}\")\n",
    "\n",
    "# Visualize feature distributions and scales\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions and Scales', fontsize=16)\n",
    "\n",
    "features = ['age', 'income', 'credit_score', 'transactions', 'balance']\n",
    "colors = ['skyblue', 'lightgreen', 'orange', 'pink', 'purple']\n",
    "\n",
    "for i, (feature, color) in enumerate(zip(features, colors)):\n",
    "    row, col = divmod(i, 3)\n",
    "    axes[row, col].hist(financial_df[feature], bins=30, alpha=0.7, color=color)\n",
    "    axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "    axes[row, col].set_xlabel(feature.replace(\"_\", \" \").title())\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = financial_df[feature].mean()\n",
    "    std_val = financial_df[feature].std()\n",
    "    axes[row, col].axvline(mean_val, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[row, col].text(0.05, 0.95, f'μ={mean_val:.0f}\\nσ={std_val:.0f}', \n",
    "                        transform=axes[row, col].transAxes, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Feature scale comparison\n",
    "axes[1, 2].bar(range(len(features)), \n",
    "               [financial_df[f].std() for f in features], \n",
    "               alpha=0.7, color=colors)\n",
    "axes[1, 2].set_title('Feature Standard Deviations')\n",
    "axes[1, 2].set_ylabel('Standard Deviation')\n",
    "axes[1, 2].set_xticks(range(len(features)))\n",
    "axes[1, 2].set_xticklabels([f.replace('_', '\\n') for f in features], rotation=0)\n",
    "axes[1, 2].set_yscale('log')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFeature Scale Analysis:\")\n",
    "print(f\"{'Feature':<15} {'Min':<10} {'Max':<10} {'Mean':<10} {'Std':<10} {'Range':<15}\")\n",
    "print(\"-\" * 75)\n",
    "for feature in features:\n",
    "    data = financial_df[feature]\n",
    "    print(f\"{feature:<15} {data.min():<10.0f} {data.max():<10.0f} {data.mean():<10.0f} {data.std():<10.0f} {data.max()-data.min():<15.0f}\")\n",
    "\n",
    "print(f\"\\nScale Differences:\")\n",
    "print(f\"• Income has ~100x larger scale than age\")\n",
    "print(f\"• Balance can be negative, others are positive\")\n",
    "print(f\"• Different distributions: normal, log-normal, Poisson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scaling methods and their impact on various algorithms\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Prepare data\n",
    "X = financial_df.values\n",
    "y = loan_approved\n",
    "\n",
    "# Initialize scalers\n",
    "scalers = {\n",
    "    'No Scaling': None,\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "# Function to apply scaling\n",
    "def apply_scaling(X, scaler):\n",
    "    if scaler is None:\n",
    "        return X\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "# Evaluate different models with different scaling\n",
    "results = {}\n",
    "\n",
    "print(\"Model Performance with Different Scaling Methods:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Scaler':<15} {'Random Forest':<12} {'Logistic Reg':<12} {'SVM':<12} {'K-Means':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for scaler_name, scaler in scalers.items():\n",
    "    X_scaled = apply_scaling(X, scaler)\n",
    "    \n",
    "    # Random Forest (should be unaffected by scaling)\n",
    "    rf_scores = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=42), \n",
    "                                X_scaled, y, cv=5, scoring='accuracy')\n",
    "    rf_mean = rf_scores.mean()\n",
    "    \n",
    "    # Logistic Regression (should benefit from scaling)\n",
    "    lr_scores = cross_val_score(LogisticRegression(max_iter=1000, random_state=42), \n",
    "                                X_scaled, y, cv=5, scoring='accuracy')\n",
    "    lr_mean = lr_scores.mean()\n",
    "    \n",
    "    # SVM (very sensitive to scaling)\n",
    "    try:\n",
    "        svm_scores = cross_val_score(SVC(random_state=42), \n",
    "                                     X_scaled, y, cv=3, scoring='accuracy')  # Reduced CV for speed\n",
    "        svm_mean = svm_scores.mean()\n",
    "    except:\n",
    "        svm_mean = 0.0  # Failed due to scaling issues\n",
    "    \n",
    "    # K-Means clustering (unsupervised)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "    \n",
    "    print(f\"{scaler_name:<15} {rf_mean:<12.3f} {lr_mean:<12.3f} {svm_mean:<12.3f} {silhouette:<12.3f}\")\n",
    "    \n",
    "    results[scaler_name] = {\n",
    "        'Random Forest': rf_mean,\n",
    "        'Logistic Regression': lr_mean,\n",
    "        'SVM': svm_mean,\n",
    "        'K-Means': silhouette\n",
    "    }\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"• Random Forest: Accuracy (tree-based, scale-invariant)\")\n",
    "print(\"• Logistic Regression: Accuracy (linear, benefits from scaling)\")\n",
    "print(\"• SVM: Accuracy (distance-based, very sensitive to scaling)\")\n",
    "print(\"• K-Means: Silhouette Score (distance-based clustering)\")\n",
    "\n",
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Impact of Scaling on Different Algorithms', fontsize=16)\n",
    "\n",
    "# Performance comparison\n",
    "scaler_names = list(results.keys())\n",
    "models = ['Random Forest', 'Logistic Regression', 'SVM', 'K-Means']\n",
    "colors = ['skyblue', 'lightgreen', 'orange', 'pink']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    row, col = divmod(i, 2)\n",
    "    scores = [results[scaler][model] for scaler in scaler_names]\n",
    "    \n",
    "    bars = axes[row, col].bar(scaler_names, scores, alpha=0.7, color=colors[i])\n",
    "    axes[row, col].set_title(f'{model} Performance')\n",
    "    axes[row, col].set_ylabel('Score' if model != 'K-Means' else 'Silhouette Score')\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, scores):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "                            f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of scaling on feature distributions and PCA\n",
    "\n",
    "# Compare original vs scaled features\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "X_standard = standard_scaler.fit_transform(X)\n",
    "X_minmax = minmax_scaler.fit_transform(X)\n",
    "X_robust = robust_scaler.fit_transform(X)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "scaling_comparison = pd.DataFrame({\n",
    "    'Original_Age': X[:, 0],\n",
    "    'Standard_Age': X_standard[:, 0],\n",
    "    'MinMax_Age': X_minmax[:, 0],\n",
    "    'Robust_Age': X_robust[:, 0],\n",
    "    'Original_Income': X[:, 1],\n",
    "    'Standard_Income': X_standard[:, 1],\n",
    "    'MinMax_Income': X_minmax[:, 1],\n",
    "    'Robust_Income': X_robust[:, 1]\n",
    "})\n",
    "\n",
    "# Visualize scaling effects\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "fig.suptitle('Effect of Different Scaling Methods', fontsize=16)\n",
    "\n",
    "# Age distributions\n",
    "scaling_methods = ['Original', 'Standard', 'MinMax', 'Robust']\n",
    "age_cols = ['Original_Age', 'Standard_Age', 'MinMax_Age', 'Robust_Age']\n",
    "income_cols = ['Original_Income', 'Standard_Income', 'MinMax_Income', 'Robust_Income']\n",
    "\n",
    "for i, (method, age_col, income_col) in enumerate(zip(scaling_methods, age_cols, income_cols)):\n",
    "    # Age\n",
    "    axes[0, i].hist(scaling_comparison[age_col], bins=30, alpha=0.7, color='skyblue')\n",
    "    axes[0, i].set_title(f'Age - {method}')\n",
    "    axes[0, i].set_xlabel('Scaled Value')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Income\n",
    "    axes[1, i].hist(scaling_comparison[income_col], bins=30, alpha=0.7, color='lightgreen')\n",
    "    axes[1, i].set_title(f'Income - {method}')\n",
    "    axes[1, i].set_xlabel('Scaled Value')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA comparison with and without scaling\n",
    "print(\"\\nPCA Analysis: Impact of Scaling\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PCA without scaling\n",
    "pca_no_scale = PCA()\n",
    "pca_no_scale.fit(X)\n",
    "\n",
    "# PCA with standard scaling\n",
    "pca_scaled = PCA()\n",
    "pca_scaled.fit(X_standard)\n",
    "\n",
    "print(\"Explained Variance Ratio (First 3 Components):\")\n",
    "print(f\"Without Scaling: {pca_no_scale.explained_variance_ratio_[:3]}\")\n",
    "print(f\"With Scaling:    {pca_scaled.explained_variance_ratio_[:3]}\")\n",
    "\n",
    "print(f\"\\nCumulative Explained Variance (First 3 Components):\")\n",
    "print(f\"Without Scaling: {np.cumsum(pca_no_scale.explained_variance_ratio_[:3])}\")\n",
    "print(f\"With Scaling:    {np.cumsum(pca_scaled.explained_variance_ratio_[:3])}\")\n",
    "\n",
    "# Feature contributions to first PC\n",
    "print(f\"\\nFirst Principal Component Loadings:\")\n",
    "print(f\"{'Feature':<15} {'No Scaling':<12} {'With Scaling':<12}\")\n",
    "print(\"-\" * 40)\n",
    "for i, feature in enumerate(features):\n",
    "    loading_no_scale = pca_no_scale.components_[0, i]\n",
    "    loading_scaled = pca_scaled.components_[0, i]\n",
    "    print(f\"{feature:<15} {loading_no_scale:<12.4f} {loading_scaled:<12.4f}\")\n",
    "\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"• Without scaling: Income dominates PCA due to large variance\")\n",
    "print(f\"• With scaling: All features contribute more equally\")\n",
    "print(f\"• Random Forest: Unaffected by scaling (tree-based)\")\n",
    "print(f\"• Logistic Regression: Improved convergence with scaling\")\n",
    "print(f\"• SVM: Dramatic improvement with proper scaling\")\n",
    "print(f\"• K-Means: Better cluster separation with scaling\")\n",
    "\n",
    "# Summary recommendations\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"SCALING RECOMMENDATIONS BY ALGORITHM\")\n",
    "print(f\"=\"*60)\n",
    "recommendations = {\n",
    "    'Random Forest': 'No scaling needed',\n",
    "    'Logistic Regression': 'StandardScaler (improves convergence)',\n",
    "    'SVM': 'StandardScaler or MinMaxScaler (essential)',\n",
    "    'K-Means': 'StandardScaler (distance-based)',\n",
    "    'PCA': 'StandardScaler (variance-based)',\n",
    "    'Neural Networks': 'StandardScaler or MinMaxScaler',\n",
    "    'KNN': 'StandardScaler or MinMaxScaler'\n",
    "}\n",
    "\n",
    "for algorithm, recommendation in recommendations.items():\n",
    "    print(f\"{algorithm:<20}: {recommendation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Categorical Encoding Strategies ★★★\n",
    "\n",
    "**Question:** You have different types of categorical variables that need encoding. For each variable type below, choose the most appropriate encoding method and explain potential pitfalls:\n",
    "\n",
    "1. **Product Category** (5 categories, no order): Electronics, Clothing, Books, Home, Sports\n",
    "2. **Education Level** (ordinal): High School < Bachelor's < Master's < PhD\n",
    "3. **City** (high cardinality): 500+ unique cities\n",
    "4. **Customer ID** (identifier): Unique per customer\n",
    "5. **Day of Week** (cyclical): Monday through Sunday\n",
    "\n",
    "Implement different encoding strategies and compare their impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3: Categorical Encoding Strategies\n",
    "\n",
    "#### **Encoding Method Selection Guide**\n",
    "\n",
    "**One-Hot Encoding**:\n",
    "- Best for: Low cardinality nominal variables\n",
    "- Pros: No ordinal assumptions, interpretable\n",
    "- Cons: High dimensionality, sparse features\n",
    "- Use when: <10-20 categories, no natural order\n",
    "\n",
    "**Ordinal Encoding**:\n",
    "- Best for: Variables with natural order\n",
    "- Pros: Single feature, preserves order\n",
    "- Cons: Assumes equal spacing between categories\n",
    "- Use when: Clear hierarchical relationship\n",
    "\n",
    "**Target Encoding (Mean Encoding)**:\n",
    "- Best for: High cardinality variables\n",
    "- Pros: Reduces dimensionality, captures target relationship\n",
    "- Cons: Overfitting risk, requires regularization\n",
    "- Use when: Many categories, limited data per category\n",
    "\n",
    "**Binary Encoding**:\n",
    "- Best for: Medium cardinality variables\n",
    "- Pros: Lower dimensionality than one-hot\n",
    "- Cons: Less interpretable\n",
    "- Use when: 10-100 categories\n",
    "\n",
    "**Cyclical Encoding**:\n",
    "- Best for: Cyclical variables (time, angles)\n",
    "- Pros: Captures cyclical nature\n",
    "- Cons: Requires domain knowledge\n",
    "- Use when: Natural cyclical patterns\n",
    "\n",
    "#### **Specific Recommendations**\n",
    "\n",
    "1. **Product Category**: One-Hot Encoding (low cardinality, nominal)\n",
    "2. **Education Level**: Ordinal Encoding (clear hierarchy)\n",
    "3. **City**: Target Encoding or Embedding (high cardinality)\n",
    "4. **Customer ID**: Drop or use for grouping (identifier, not predictive)\n",
    "5. **Day of Week**: Cyclical Encoding (sin/cos transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset with different categorical variable types\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "# 1. Product Category (nominal, low cardinality)\n",
    "product_categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports']\n",
    "product_probs = [0.3, 0.25, 0.15, 0.2, 0.1]  # Different popularity\n",
    "product_category = np.random.choice(product_categories, n_samples, p=product_probs)\n",
    "\n",
    "# 2. Education Level (ordinal)\n",
    "education_levels = ['High School', 'Bachelor\\'s', 'Master\\'s', 'PhD']\n",
    "education_probs = [0.4, 0.35, 0.2, 0.05]\n",
    "education = np.random.choice(education_levels, n_samples, p=education_probs)\n",
    "\n",
    "# 3. City (high cardinality)\n",
    "# Generate 200 cities with Zipf distribution (realistic city size distribution)\n",
    "city_names = [f'City_{i:03d}' for i in range(200)]\n",
    "zipf_weights = 1 / np.arange(1, 201)  # Zipf distribution\n",
    "zipf_weights = zipf_weights / zipf_weights.sum()\n",
    "city = np.random.choice(city_names, n_samples, p=zipf_weights)\n",
    "\n",
    "# 4. Customer ID (identifier - should not be used directly)\n",
    "customer_id = [f'CUST_{i:06d}' for i in range(n_samples)]\n",
    "\n",
    "# 5. Day of Week (cyclical)\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_of_week = np.random.choice(days, n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "categorical_df = pd.DataFrame({\n",
    "    'customer_id': customer_id,\n",
    "    'product_category': product_category,\n",
    "    'education': education,\n",
    "    'city': city,\n",
    "    'day_of_week': day_of_week\n",
    "})\n",
    "\n",
    "# Create target variable influenced by categorical variables\n",
    "# Education effect (ordinal)\n",
    "education_effect = {'High School': 0, 'Bachelor\\'s': 1, 'Master\\'s': 2, 'PhD': 3}\n",
    "edu_score = np.array([education_effect[ed] for ed in education])\n",
    "\n",
    "# Product category effect (nominal)\n",
    "category_effect = {'Electronics': 2, 'Clothing': 1, 'Books': 3, 'Home': 1, 'Sports': 2}\n",
    "category_score = np.array([category_effect[cat] for cat in product_category])\n",
    "\n",
    "# City effect (some cities are better markets)\n",
    "city_effect = {city: np.random.normal(0, 1) for city in city_names}\n",
    "city_score = np.array([city_effect[c] for c in city])\n",
    "\n",
    "# Day of week effect (cyclical - weekend effect)\n",
    "day_mapping = {day: i for i, day in enumerate(days)}\n",
    "day_numeric = np.array([day_mapping[d] for d in day_of_week])\n",
    "weekend_effect = np.cos(2 * np.pi * day_numeric / 7)  # Cyclical pattern\n",
    "\n",
    "# Combine effects to create target\n",
    "target_logit = (0.5 * edu_score + \n",
    "                0.3 * category_score + \n",
    "                0.2 * city_score + \n",
    "                0.4 * weekend_effect + \n",
    "                np.random.normal(0, 0.5, n_samples))\n",
    "\n",
    "target = (target_logit > np.median(target_logit)).astype(int)\n",
    "\n",
    "print(\"Categorical Dataset Overview:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"Target distribution: {target.mean():.1%} positive class\")\n",
    "print(\"\\nCategorical Variable Summary:\")\n",
    "for col in ['product_category', 'education', 'city', 'day_of_week']:\n",
    "    unique_count = categorical_df[col].nunique()\n",
    "    print(f\"{col:<20}: {unique_count:>3} unique values\")\n",
    "\n",
    "# Show value counts for each categorical variable\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Categorical Variable Distributions', fontsize=16)\n",
    "\n",
    "# Product Category\n",
    "category_counts = categorical_df['product_category'].value_counts()\n",
    "axes[0, 0].bar(category_counts.index, category_counts.values, alpha=0.7)\n",
    "axes[0, 0].set_title('Product Category Distribution')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Education Level\n",
    "education_counts = categorical_df['education'].value_counts().reindex(education_levels)\n",
    "axes[0, 1].bar(education_counts.index, education_counts.values, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_title('Education Level Distribution')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# City (top 10)\n",
    "city_counts = categorical_df['city'].value_counts().head(10)\n",
    "axes[1, 0].bar(range(len(city_counts)), city_counts.values, alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Top 10 Cities Distribution')\n",
    "axes[1, 0].set_xlabel('City Rank')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Day of Week\n",
    "day_counts = categorical_df['day_of_week'].value_counts().reindex(days)\n",
    "axes[1, 1].bar(day_counts.index, day_counts.values, alpha=0.7, color='purple')\n",
    "axes[1, 1].set_title('Day of Week Distribution')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCardinality Analysis:\")\n",
    "print(f\"• Product Category: {categorical_df['product_category'].nunique()} categories (Low)\")\n",
    "print(f\"• Education: {categorical_df['education'].nunique()} levels (Ordinal)\")\n",
    "print(f\"• City: {categorical_df['city'].nunique()} cities (High cardinality)\")\n",
    "print(f\"• Day of Week: {categorical_df['day_of_week'].nunique()} days (Cyclical)\")\n",
    "print(f\"• Customer ID: {categorical_df['customer_id'].nunique()} IDs (Identifier)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement different encoding strategies\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import category_encoders as ce  # You may need to install: pip install category_encoders\n",
    "\n",
    "# For this demo, we'll implement target encoding manually\n",
    "def target_encode(X_train, X_test, y_train, column, smoothing=1.0):\n",
    "    \"\"\"\n",
    "    Target encoding with smoothing to prevent overfitting\n",
    "    \"\"\"\n",
    "    # Calculate global mean\n",
    "    global_mean = y_train.mean()\n",
    "    \n",
    "    # Calculate category means and counts\n",
    "    category_stats = pd.DataFrame({\n",
    "        'category': X_train[column],\n",
    "        'target': y_train\n",
    "    }).groupby('category').agg({\n",
    "        'target': ['count', 'mean']\n",
    "    })\n",
    "    \n",
    "    category_stats.columns = ['count', 'mean']\n",
    "    \n",
    "    # Apply smoothing\n",
    "    category_stats['smoothed_mean'] = (\n",
    "        (category_stats['mean'] * category_stats['count'] + global_mean * smoothing) /\n",
    "        (category_stats['count'] + smoothing)\n",
    "    )\n",
    "    \n",
    "    # Create mapping\n",
    "    encoding_map = category_stats['smoothed_mean'].to_dict()\n",
    "    \n",
    "    # Apply encoding\n",
    "    X_train_encoded = X_train[column].map(encoding_map).fillna(global_mean)\n",
    "    X_test_encoded = X_test[column].map(encoding_map).fillna(global_mean)\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    categorical_df, target, test_size=0.3, random_state=42, stratify=target\n",
    ")\n",
    "\n",
    "print(\"Encoding Strategy Comparison:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Strategy 1: One-Hot Encoding for Product Category\n",
    "def encode_strategy_1(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Strategy 1: Basic encodings\n",
    "    - Product Category: One-Hot\n",
    "    - Education: Ordinal\n",
    "    - City: Target Encoding\n",
    "    - Day: One-Hot\n",
    "    - Customer ID: Drop\n",
    "    \"\"\"\n",
    "    result_train = pd.DataFrame()\n",
    "    result_test = pd.DataFrame()\n",
    "    \n",
    "    # Product Category: One-Hot\n",
    "    product_dummies_train = pd.get_dummies(X_train['product_category'], prefix='product')\n",
    "    product_dummies_test = pd.get_dummies(X_test['product_category'], prefix='product')\n",
    "    # Ensure same columns\n",
    "    for col in product_dummies_train.columns:\n",
    "        if col not in product_dummies_test.columns:\n",
    "            product_dummies_test[col] = 0\n",
    "    product_dummies_test = product_dummies_test[product_dummies_train.columns]\n",
    "    \n",
    "    result_train = pd.concat([result_train, product_dummies_train], axis=1)\n",
    "    result_test = pd.concat([result_test, product_dummies_test], axis=1)\n",
    "    \n",
    "    # Education: Ordinal\n",
    "    education_mapping = {'High School': 0, 'Bachelor\\'s': 1, 'Master\\'s': 2, 'PhD': 3}\n",
    "    result_train['education_ordinal'] = X_train['education'].map(education_mapping)\n",
    "    result_test['education_ordinal'] = X_test['education'].map(education_mapping)\n",
    "    \n",
    "    # City: Target Encoding\n",
    "    city_train, city_test = target_encode(X_train, X_test, y_train, 'city')\n",
    "    result_train['city_target'] = city_train\n",
    "    result_test['city_target'] = city_test\n",
    "    \n",
    "    # Day: One-Hot\n",
    "    day_dummies_train = pd.get_dummies(X_train['day_of_week'], prefix='day')\n",
    "    day_dummies_test = pd.get_dummies(X_test['day_of_week'], prefix='day')\n",
    "    # Ensure same columns\n",
    "    for col in day_dummies_train.columns:\n",
    "        if col not in day_dummies_test.columns:\n",
    "            day_dummies_test[col] = 0\n",
    "    day_dummies_test = day_dummies_test[day_dummies_train.columns]\n",
    "    \n",
    "    result_train = pd.concat([result_train, day_dummies_train], axis=1)\n",
    "    result_test = pd.concat([result_test, day_dummies_test], axis=1)\n",
    "    \n",
    "    # Customer ID: Drop (not predictive)\n",
    "    \n",
    "    return result_train.fillna(0), result_test.fillna(0)\n",
    "\n",
    "# Strategy 2: Cyclical Encoding for Day of Week\n",
    "def encode_strategy_2(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Strategy 2: Advanced encodings\n",
    "    - Product Category: One-Hot\n",
    "    - Education: Ordinal\n",
    "    - City: Target Encoding\n",
    "    - Day: Cyclical (sin/cos)\n",
    "    - Customer ID: Drop\n",
    "    \"\"\"\n",
    "    result_train = pd.DataFrame()\n",
    "    result_test = pd.DataFrame()\n",
    "    \n",
    "    # Product Category: One-Hot (same as strategy 1)\n",
    "    product_dummies_train = pd.get_dummies(X_train['product_category'], prefix='product')\n",
    "    product_dummies_test = pd.get_dummies(X_test['product_category'], prefix='product')\n",
    "    for col in product_dummies_train.columns:\n",
    "        if col not in product_dummies_test.columns:\n",
    "            product_dummies_test[col] = 0\n",
    "    product_dummies_test = product_dummies_test[product_dummies_train.columns]\n",
    "    \n",
    "    result_train = pd.concat([result_train, product_dummies_train], axis=1)\n",
    "    result_test = pd.concat([result_test, product_dummies_test], axis=1)\n",
    "    \n",
    "    # Education: Ordinal (same as strategy 1)\n",
    "    education_mapping = {'High School': 0, 'Bachelor\\'s': 1, 'Master\\'s': 2, 'PhD': 3}\n",
    "    result_train['education_ordinal'] = X_train['education'].map(education_mapping)\n",
    "    result_test['education_ordinal'] = X_test['education'].map(education_mapping)\n",
    "    \n",
    "    # City: Target Encoding (same as strategy 1)\n",
    "    city_train, city_test = target_encode(X_train, X_test, y_train, 'city')\n",
    "    result_train['city_target'] = city_train\n",
    "    result_test['city_target'] = city_test\n",
    "    \n",
    "    # Day: Cyclical Encoding\n",
    "    day_mapping = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, \n",
    "                   'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "    \n",
    "    day_numeric_train = X_train['day_of_week'].map(day_mapping)\n",
    "    day_numeric_test = X_test['day_of_week'].map(day_mapping)\n",
    "    \n",
    "    # Convert to sin/cos\n",
    "    result_train['day_sin'] = np.sin(2 * np.pi * day_numeric_train / 7)\n",
    "    result_train['day_cos'] = np.cos(2 * np.pi * day_numeric_train / 7)\n",
    "    result_test['day_sin'] = np.sin(2 * np.pi * day_numeric_test / 7)\n",
    "    result_test['day_cos'] = np.cos(2 * np.pi * day_numeric_test / 7)\n",
    "    \n",
    "    return result_train.fillna(0), result_test.fillna(0)\n",
    "\n",
    "# Strategy 3: Label Encoding Everything (poor strategy)\n",
    "def encode_strategy_3(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Strategy 3: Label encoding for everything (demonstrating poor choices)\n",
    "    \"\"\"\n",
    "    result_train = pd.DataFrame()\n",
    "    result_test = pd.DataFrame()\n",
    "    \n",
    "    # Label encode everything\n",
    "    for col in ['product_category', 'education', 'city', 'day_of_week']:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on train, transform both\n",
    "        le.fit(X_train[col])\n",
    "        result_train[f'{col}_label'] = le.transform(X_train[col])\n",
    "        \n",
    "        # Handle unseen categories in test\n",
    "        test_encoded = []\n",
    "        for val in X_test[col]:\n",
    "            if val in le.classes_:\n",
    "                test_encoded.append(le.transform([val])[0])\n",
    "            else:\n",
    "                test_encoded.append(-1)  # Unseen category\n",
    "        result_test[f'{col}_label'] = test_encoded\n",
    "    \n",
    "    return result_train.fillna(0), result_test.fillna(0)\n",
    "\n",
    "# Apply encoding strategies\n",
    "X_train_s1, X_test_s1 = encode_strategy_1(X_train, X_test, y_train)\n",
    "X_train_s2, X_test_s2 = encode_strategy_2(X_train, X_test, y_train)\n",
    "X_train_s3, X_test_s3 = encode_strategy_3(X_train, X_test, y_train)\n",
    "\n",
    "print(f\"Encoding Results:\")\n",
    "print(f\"Strategy 1 (Mixed): {X_train_s1.shape[1]} features\")\n",
    "print(f\"Strategy 2 (Cyclical): {X_train_s2.shape[1]} features\")\n",
    "print(f\"Strategy 3 (Label): {X_train_s3.shape[1]} features\")\n",
    "\n",
    "# Evaluate strategies\n",
    "strategies = {\n",
    "    'Strategy 1 (Mixed)': (X_train_s1, X_test_s1),\n",
    "    'Strategy 2 (Cyclical)': (X_train_s2, X_test_s2),\n",
    "    'Strategy 3 (Label)': (X_train_s3, X_test_s3)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, (X_tr, X_te) in strategies.items():\n",
    "    # Random Forest\n",
    "    rf_scores = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                                X_tr, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr_scores = cross_val_score(LogisticRegression(max_iter=1000, random_state=42),\n",
    "                                X_tr, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results[name] = {\n",
    "        'RF': rf_scores.mean(),\n",
    "        'LR': lr_scores.mean(),\n",
    "        'Features': X_tr.shape[1]\n",
    "    }\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"{'Strategy':<20} {'Features':<10} {'Random Forest':<15} {'Logistic Reg':<15}\")\n",
    "print(\"-\" * 65)\n",
    "for strategy, metrics in results.items():\n",
    "    print(f\"{strategy:<20} {metrics['Features']:<10} {metrics['RF']:<15.4f} {metrics['LR']:<15.4f}\")\n",
    "\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"• Strategy 1: Appropriate encoding for each variable type\")\n",
    "print(f\"• Strategy 2: Cyclical encoding captures day-of-week patterns better\")\n",
    "print(f\"• Strategy 3: Label encoding creates artificial ordinality\")\n",
    "print(f\"• Target encoding helps with high-cardinality variables\")\n",
    "print(f\"• Feature count varies significantly between strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Feature Selection Techniques ★★★\n",
    "\n",
    "**Question:** You have a dataset with 100 features and 1000 samples for binary classification. Apply and compare different feature selection methods:\n",
    "\n",
    "1. **Filter Methods**: Chi-square test, mutual information\n",
    "2. **Wrapper Methods**: Recursive Feature Elimination (RFE)\n",
    "3. **Embedded Methods**: L1 regularization (LASSO)\n",
    "4. **Hybrid Approach**: Combine multiple methods\n",
    "\n",
    "Analyze the computational cost, selected features, and impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4: Feature Selection Techniques\n",
    "\n",
    "#### **Feature Selection Categories**\n",
    "\n",
    "**Filter Methods**:\n",
    "- Independent of ML algorithm\n",
    "- Fast computation\n",
    "- Based on statistical tests\n",
    "- Examples: Chi-square, correlation, mutual information\n",
    "\n",
    "**Wrapper Methods**:\n",
    "- Use ML algorithm performance\n",
    "- Computationally expensive\n",
    "- Account for feature interactions\n",
    "- Examples: RFE, forward/backward selection\n",
    "\n",
    "**Embedded Methods**:\n",
    "- Feature selection during model training\n",
    "- Algorithm-specific\n",
    "- Balance between filter and wrapper\n",
    "- Examples: LASSO, tree-based importance\n",
    "\n",
    "#### **When to Use Each Method**\n",
    "\n",
    "- **High Dimensionality**: Start with filter methods\n",
    "- **Small Datasets**: Wrapper methods may overfit\n",
    "- **Linear Models**: L1 regularization works well\n",
    "- **Tree Models**: Use built-in feature importance\n",
    "- **Time Constraints**: Filter methods are fastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic high-dimensional dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "n_informative = 20  # Only 20% of features are actually useful\n",
    "n_redundant = 10\n",
    "n_clusters_per_class = 2\n",
    "\n",
    "# Generate classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    n_redundant=n_redundant,\n",
    "    n_clusters_per_class=n_clusters_per_class,\n",
    "    class_sep=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create feature names\n",
    "feature_names = [f'feature_{i:03d}' for i in range(n_features)]\n",
    "feature_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "print(\"High-Dimensional Dataset:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"Features: {n_features}\")\n",
    "print(f\"Informative features: {n_informative}\")\n",
    "print(f\"Redundant features: {n_redundant}\")\n",
    "print(f\"Noise features: {n_features - n_informative - n_redundant}\")\n",
    "print(f\"Class distribution: {np.bincount(y)} ({y.mean():.1%} positive)\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"Training: {X_train.shape}\")\n",
    "print(f\"Testing: {X_test.shape}\")\n",
    "\n",
    "# Baseline performance (all features)\n",
    "baseline_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "baseline_scores = cross_val_score(baseline_rf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "baseline_performance = baseline_scores.mean()\n",
    "\n",
    "print(f\"\\nBaseline Performance (all {n_features} features):\")\n",
    "print(f\"Random Forest Accuracy: {baseline_performance:.4f} ± {baseline_scores.std():.4f}\")\n",
    "\n",
    "# Visualize feature importance from baseline model\n",
    "baseline_rf.fit(X_train, y_train)\n",
    "baseline_importance = baseline_rf.feature_importances_\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Feature importance distribution\n",
    "axes[0].hist(baseline_importance, bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0].set_title('Feature Importance Distribution (Random Forest)')\n",
    "axes[0].set_xlabel('Importance Score')\n",
    "axes[0].set_ylabel('Number of Features')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Top 20 feature importances\n",
    "top_20_idx = np.argsort(baseline_importance)[-20:]\n",
    "axes[1].barh(range(20), baseline_importance[top_20_idx], alpha=0.7, color='lightgreen')\n",
    "axes[1].set_title('Top 20 Most Important Features')\n",
    "axes[1].set_xlabel('Importance Score')\n",
    "axes[1].set_ylabel('Feature Rank')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFeature Importance Analysis:\")\n",
    "print(f\"Mean importance: {baseline_importance.mean():.6f}\")\n",
    "print(f\"Std importance: {baseline_importance.std():.6f}\")\n",
    "print(f\"Max importance: {baseline_importance.max():.6f}\")\n",
    "print(f\"Features with >mean importance: {(baseline_importance > baseline_importance.mean()).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement and compare different feature selection methods\n",
    "import time\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Store results\n",
    "selection_results = {}\n",
    "\n",
    "print(\"Feature Selection Methods Comparison:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Filter Method: Chi-square\n",
    "print(\"\\n1. Chi-square Test (Filter Method)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "\n",
    "# Need non-negative features for chi-square\n",
    "X_train_pos = X_train - X_train.min() + 1e-6\n",
    "X_test_pos = X_test - X_test.min() + 1e-6\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=20)\n",
    "X_train_chi2 = chi2_selector.fit_transform(X_train_pos, y_train)\n",
    "X_test_chi2 = chi2_selector.transform(X_test_pos)\n",
    "\n",
    "chi2_time = time.time() - start_time\n",
    "chi2_features = chi2_selector.get_support(indices=True)\n",
    "chi2_scores = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                              X_train_chi2, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "selection_results['Chi-square'] = {\n",
    "    'selected_features': chi2_features,\n",
    "    'n_features': len(chi2_features),\n",
    "    'computation_time': chi2_time,\n",
    "    'cv_score': chi2_scores.mean(),\n",
    "    'cv_std': chi2_scores.std()\n",
    "}\n",
    "\n",
    "print(f\"Selected features: {len(chi2_features)}\")\n",
    "print(f\"Computation time: {chi2_time:.4f} seconds\")\n",
    "print(f\"CV Accuracy: {chi2_scores.mean():.4f} ± {chi2_scores.std():.4f}\")\n",
    "\n",
    "# 2. Filter Method: Mutual Information\n",
    "print(\"\\n2. Mutual Information (Filter Method)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "\n",
    "mi_selector = SelectKBest(mutual_info_classif, k=20)\n",
    "X_train_mi = mi_selector.fit_transform(X_train, y_train)\n",
    "X_test_mi = mi_selector.transform(X_test)\n",
    "\n",
    "mi_time = time.time() - start_time\n",
    "mi_features = mi_selector.get_support(indices=True)\n",
    "mi_scores = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                            X_train_mi, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "selection_results['Mutual Info'] = {\n",
    "    'selected_features': mi_features,\n",
    "    'n_features': len(mi_features),\n",
    "    'computation_time': mi_time,\n",
    "    'cv_score': mi_scores.mean(),\n",
    "    'cv_std': mi_scores.std()\n",
    "}\n",
    "\n",
    "print(f\"Selected features: {len(mi_features)}\")\n",
    "print(f\"Computation time: {mi_time:.4f} seconds\")\n",
    "print(f\"CV Accuracy: {mi_scores.mean():.4f} ± {mi_scores.std():.4f}\")\n",
    "\n",
    "# 3. Wrapper Method: Recursive Feature Elimination\n",
    "print(\"\\n3. Recursive Feature Elimination (Wrapper Method)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "\n",
    "rfe_estimator = RandomForestClassifier(n_estimators=20, random_state=42)  # Reduced for speed\n",
    "rfe_selector = RFE(rfe_estimator, n_features_to_select=20, step=5)\n",
    "X_train_rfe = rfe_selector.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe_selector.transform(X_test)\n",
    "\n",
    "rfe_time = time.time() - start_time\n",
    "rfe_features = rfe_selector.get_support(indices=True)\n",
    "rfe_scores = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                             X_train_rfe, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "selection_results['RFE'] = {\n",
    "    'selected_features': rfe_features,\n",
    "    'n_features': len(rfe_features),\n",
    "    'computation_time': rfe_time,\n",
    "    'cv_score': rfe_scores.mean(),\n",
    "    'cv_std': rfe_scores.std()\n",
    "}\n",
    "\n",
    "print(f\"Selected features: {len(rfe_features)}\")\n",
    "print(f\"Computation time: {rfe_time:.4f} seconds\")\n",
    "print(f\"CV Accuracy: {rfe_scores.mean():.4f} ± {rfe_scores.std():.4f}\")\n",
    "\n",
    "# 4. Embedded Method: LASSO (L1 Regularization)\n",
    "print(\"\\n4. LASSO Regularization (Embedded Method)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "\n",
    "# Use LASSO with cross-validation to select alpha\n",
    "lasso_cv = LassoCV(cv=5, random_state=42, max_iter=1000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Select features with non-zero coefficients\n",
    "lasso_features = np.where(np.abs(lasso_cv.coef_) > 1e-6)[0]\n",
    "X_train_lasso = X_train[:, lasso_features]\n",
    "X_test_lasso = X_test[:, lasso_features]\n",
    "\n",
    "lasso_time = time.time() - start_time\n",
    "lasso_scores = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                               X_train_lasso, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "selection_results['LASSO'] = {\n",
    "    'selected_features': lasso_features,\n",
    "    'n_features': len(lasso_features),\n",
    "    'computation_time': lasso_time,\n",
    "    'cv_score': lasso_scores.mean(),\n",
    "    'cv_std': lasso_scores.std()\n",
    "}\n",
    "\n",
    "print(f\"Selected features: {len(lasso_features)}\")\n",
    "print(f\"Computation time: {lasso_time:.4f} seconds\")\n",
    "print(f\"CV Accuracy: {lasso_scores.mean():.4f} ± {lasso_scores.std():.4f}\")\n",
    "print(f\"LASSO alpha: {lasso_cv.alpha_:.6f}\")\n",
    "\n",
    "# 5. Hybrid Approach: Combine multiple methods\n",
    "print(\"\\n5. Hybrid Approach (Intersection of Methods)\")\n",
    "print(\"-\" * 40)\n",
    "start_time = time.time()\n",
    "\n",
    "# Find intersection of top features from multiple methods\n",
    "all_selected_features = {\n",
    "    'chi2': set(chi2_features),\n",
    "    'mi': set(mi_features),\n",
    "    'rfe': set(rfe_features),\n",
    "    'lasso': set(lasso_features)\n",
    "}\n",
    "\n",
    "# Features selected by at least 2 methods\n",
    "feature_votes = {}\n",
    "for method, features in all_selected_features.items():\n",
    "    for feature in features:\n",
    "        feature_votes[feature] = feature_votes.get(feature, 0) + 1\n",
    "\n",
    "hybrid_features = np.array([f for f, votes in feature_votes.items() if votes >= 2])\n",
    "X_train_hybrid = X_train[:, hybrid_features]\n",
    "X_test_hybrid = X_test[:, hybrid_features]\n",
    "\n",
    "hybrid_time = time.time() - start_time\n",
    "hybrid_scores = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "                                X_train_hybrid, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "selection_results['Hybrid'] = {\n",
    "    'selected_features': hybrid_features,\n",
    "    'n_features': len(hybrid_features),\n",
    "    'computation_time': hybrid_time,\n",
    "    'cv_score': hybrid_scores.mean(),\n",
    "    'cv_std': hybrid_scores.std()\n",
    "}\n",
    "\n",
    "print(f\"Selected features: {len(hybrid_features)}\")\n",
    "print(f\"Computation time: {hybrid_time:.4f} seconds\")\n",
    "print(f\"CV Accuracy: {hybrid_scores.mean():.4f} ± {hybrid_scores.std():.4f}\")\n",
    "\n",
    "# Add baseline to results\n",
    "selection_results['Baseline (All)'] = {\n",
    "    'selected_features': np.arange(n_features),\n",
    "    'n_features': n_features,\n",
    "    'computation_time': 0.0,\n",
    "    'cv_score': baseline_performance,\n",
    "    'cv_std': baseline_scores.std()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and analyze feature selection results\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\nFeature Selection Comparison Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<15} {'Features':<10} {'Time (s)':<10} {'CV Score':<12} {'Std':<10} {'vs Baseline':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "baseline_score = selection_results['Baseline (All)']['cv_score']\n",
    "\n",
    "for method, results in selection_results.items():\n",
    "    score_diff = results['cv_score'] - baseline_score\n",
    "    print(f\"{method:<15} {results['n_features']:<10} {results['computation_time']:<10.4f} \"\n",
    "          f\"{results['cv_score']:<12.4f} {results['cv_std']:<10.4f} {score_diff:>+8.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Feature Selection Methods Comparison', fontsize=16)\n",
    "\n",
    "methods = list(selection_results.keys())\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(methods)))\n",
    "\n",
    "# 1. Number of features\n",
    "n_features_list = [selection_results[m]['n_features'] for m in methods]\n",
    "bars1 = axes[0, 0].bar(methods, n_features_list, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_title('Number of Selected Features')\n",
    "axes[0, 0].set_ylabel('Number of Features')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "for bar, n_feat in zip(bars1, n_features_list):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    str(n_feat), ha='center', va='bottom')\n",
    "\n",
    "# 2. Computation time\n",
    "times = [selection_results[m]['computation_time'] for m in methods]\n",
    "bars2 = axes[0, 1].bar(methods, times, color=colors, alpha=0.7)\n",
    "axes[0, 1].set_title('Computation Time')\n",
    "axes[0, 1].set_ylabel('Time (seconds)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "for bar, time_val in zip(bars2, times):\n",
    "    if time_val > 0:\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1,\n",
    "                        f'{time_val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. CV Scores with error bars\n",
    "cv_scores = [selection_results[m]['cv_score'] for m in methods]\n",
    "cv_stds = [selection_results[m]['cv_std'] for m in methods]\n",
    "bars3 = axes[0, 2].bar(methods, cv_scores, yerr=cv_stds, color=colors, alpha=0.7, capsize=5)\n",
    "axes[0, 2].set_title('Cross-Validation Accuracy')\n",
    "axes[0, 2].set_ylabel('Accuracy')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].set_ylim(min(cv_scores) - 0.02, max(cv_scores) + 0.02)\n",
    "\n",
    "# 4. Feature overlap analysis\n",
    "feature_sets = {\n",
    "    'Chi-square': set(selection_results['Chi-square']['selected_features']),\n",
    "    'Mutual Info': set(selection_results['Mutual Info']['selected_features']),\n",
    "    'RFE': set(selection_results['RFE']['selected_features']),\n",
    "    'LASSO': set(selection_results['LASSO']['selected_features'])\n",
    "}\n",
    "\n",
    "# Calculate pairwise overlaps\n",
    "overlap_matrix = np.zeros((4, 4))\n",
    "method_names = list(feature_sets.keys())\n",
    "for i, method1 in enumerate(method_names):\n",
    "    for j, method2 in enumerate(method_names):\n",
    "        if i == j:\n",
    "            overlap_matrix[i, j] = len(feature_sets[method1])\n",
    "        else:\n",
    "            overlap = len(feature_sets[method1].intersection(feature_sets[method2]))\n",
    "            overlap_matrix[i, j] = overlap\n",
    "\n",
    "im = axes[1, 0].imshow(overlap_matrix, cmap='Blues', aspect='auto')\n",
    "axes[1, 0].set_title('Feature Overlap Between Methods')\n",
    "axes[1, 0].set_xticks(range(4))\n",
    "axes[1, 0].set_yticks(range(4))\n",
    "axes[1, 0].set_xticklabels([m.replace(' ', '\\n') for m in method_names], rotation=45)\n",
    "axes[1, 0].set_yticklabels(method_names)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[1, 0].text(j, i, int(overlap_matrix[i, j]), ha='center', va='center')\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "# 5. Performance vs complexity trade-off\n",
    "axes[1, 1].scatter(n_features_list[:-1], cv_scores[:-1], \n",
    "                   c=times[:-1], s=100, alpha=0.7, cmap='viridis')\n",
    "axes[1, 1].scatter(n_features_list[-1], cv_scores[-1], \n",
    "                   c='red', s=150, marker='*', label='Baseline')\n",
    "axes[1, 1].set_xlabel('Number of Features')\n",
    "axes[1, 1].set_ylabel('CV Accuracy')\n",
    "axes[1, 1].set_title('Performance vs Complexity')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Add method labels\n",
    "for i, method in enumerate(methods[:-1]):\n",
    "    axes[1, 1].annotate(method.split()[0], \n",
    "                        (n_features_list[i], cv_scores[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# 6. Feature importance comparison for selected features\n",
    "# Show baseline importance for features selected by hybrid method\n",
    "if len(hybrid_features) > 0:\n",
    "    hybrid_importance = baseline_importance[hybrid_features]\n",
    "    axes[1, 2].bar(range(len(hybrid_features)), hybrid_importance, alpha=0.7, color='green')\n",
    "    axes[1, 2].set_title('Baseline Importance of Hybrid-Selected Features')\n",
    "    axes[1, 2].set_xlabel('Feature Index (Hybrid Selection)')\n",
    "    axes[1, 2].set_ylabel('Baseline Importance')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, 'No features selected\\nby hybrid method', \n",
    "                    ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "    axes[1, 2].set_title('Hybrid Method Results')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis of results\n",
    "print(f\"\\nDetailed Analysis:\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Find best performing method\n",
    "best_method = max(selection_results.keys(), \n",
    "                  key=lambda x: selection_results[x]['cv_score'])\n",
    "print(f\"Best performing method: {best_method}\")\n",
    "print(f\"Score: {selection_results[best_method]['cv_score']:.4f}\")\n",
    "\n",
    "# Find most efficient method (score/time ratio)\n",
    "efficiency_scores = {}\n",
    "for method, results in selection_results.items():\n",
    "    if results['computation_time'] > 0:\n",
    "        efficiency = results['cv_score'] / results['computation_time']\n",
    "        efficiency_scores[method] = efficiency\n",
    "\n",
    "if efficiency_scores:\n",
    "    most_efficient = max(efficiency_scores.keys(), key=lambda x: efficiency_scores[x])\n",
    "    print(f\"Most efficient method: {most_efficient}\")\n",
    "    print(f\"Efficiency (score/time): {efficiency_scores[most_efficient]:.2f}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"• Filter methods (Chi-square, MI) are fastest but may miss interactions\")\n",
    "print(f\"• Wrapper methods (RFE) are slowest but account for feature interactions\")\n",
    "print(f\"• Embedded methods (LASSO) balance speed and performance\")\n",
    "print(f\"• Hybrid approaches can improve robustness\")\n",
    "print(f\"• Dimensionality reduction doesn't always improve performance\")\n",
    "print(f\"• Computational cost varies dramatically between methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### **Core Preprocessing Concepts Mastered**\n",
    "\n",
    "1. **Missing Data Handling**: Understanding MCAR, MAR, MNAR and appropriate imputation strategies\n",
    "2. **Feature Scaling**: Knowing when and which scaling method to apply for different algorithms\n",
    "3. **Categorical Encoding**: Choosing appropriate encoding based on cardinality and variable type\n",
    "4. **Feature Selection**: Comparing filter, wrapper, and embedded methods for dimensionality reduction\n",
    "\n",
    "### **Critical Decision Framework**\n",
    "\n",
    "**Missing Data Strategy Selection:**\n",
    "- **Low missingness (<5%)**: Simple imputation (mean/median/mode)\n",
    "- **Moderate missingness (5-25%)**: Advanced imputation (KNN, iterative)\n",
    "- **High missingness (>25%)**: Consider dropping or specialized techniques\n",
    "- **Systematic patterns**: Add missingness indicators\n",
    "\n",
    "**Scaling Method Selection:**\n",
    "- **Tree-based algorithms**: No scaling needed\n",
    "- **Distance-based algorithms**: StandardScaler or MinMaxScaler\n",
    "- **Linear algorithms**: StandardScaler for better convergence\n",
    "- **Data with outliers**: RobustScaler\n",
    "\n",
    "**Categorical Encoding Strategy:**\n",
    "- **Low cardinality (<10)**: One-hot encoding\n",
    "- **Ordinal variables**: Ordinal encoding\n",
    "- **High cardinality (>50)**: Target encoding or embeddings\n",
    "- **Cyclical variables**: Sin/cos transformation\n",
    "- **Identifiers**: Drop or use for grouping\n",
    "\n",
    "**Feature Selection Approach:**\n",
    "- **High dimensionality**: Start with filter methods\n",
    "- **Small datasets**: Be cautious with wrapper methods\n",
    "- **Linear models**: L1 regularization\n",
    "- **Tree models**: Built-in importance\n",
    "- **Production systems**: Consider computational constraints\n",
    "\n",
    "### **Common Pitfalls to Avoid**\n",
    "\n",
    "- **Data Leakage**: Fitting preprocessors on entire dataset before splitting\n",
    "- **Target Leakage**: Using target-dependent features for imputation\n",
    "- **Inconsistent Preprocessing**: Different preprocessing for train/test\n",
    "- **Ignoring Missingness Patterns**: Not investigating why data is missing\n",
    "- **Over-Engineering**: Applying complex preprocessing when simple methods suffice\n",
    "\n",
    "### **Best Practices**\n",
    "\n",
    "1. **Always fit preprocessors on training data only**\n",
    "2. **Use pipelines to ensure consistent preprocessing**\n",
    "3. **Validate preprocessing choices with domain experts**\n",
    "4. **Monitor preprocessing impact on model performance**\n",
    "5. **Document preprocessing decisions and rationale**\n",
    "\n",
    "### **Next Steps**\n",
    "\n",
    "Continue to Part 3 to master model evaluation and validation techniques that build upon proper data preprocessing.\n",
    "\n",
    "### **Practice Recommendations**\n",
    "\n",
    "1. Build preprocessing pipelines for different data types\n",
    "2. Practice identifying appropriate encoding strategies\n",
    "3. Implement custom preprocessing functions for domain-specific needs\n",
    "4. Experiment with feature selection on high-dimensional datasets\n",
    "5. Create preprocessing checklists for different project types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}