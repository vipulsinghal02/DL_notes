{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Basics Part 2 - Advanced Data Analysis\n",
    "\n",
    "Advanced pandas techniques for scientific data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Generate sample experimental data\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Indexing and MultiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical index for experimental data\n",
    "experiments = ['Exp_A', 'Exp_B', 'Exp_C']\n",
    "conditions = ['Control', 'Treatment_1', 'Treatment_2']\n",
    "timepoints = ['T0', 'T1', 'T2', 'T4', 'T8']\n",
    "\n",
    "# Create MultiIndex\n",
    "index = pd.MultiIndex.from_product(\n",
    "    [experiments, conditions, timepoints],\n",
    "    names=['experiment', 'condition', 'timepoint']\n",
    ")\n",
    "\n",
    "# Generate sample data\n",
    "n_samples = len(index)\n",
    "data = {\n",
    "    'cell_count': np.random.normal(100000, 20000, n_samples),\n",
    "    'viability': np.random.uniform(0.8, 0.99, n_samples),\n",
    "    'protein_conc': np.random.normal(2.5, 0.5, n_samples),\n",
    "    'glucose': np.random.normal(15, 3, n_samples)\n",
    "}\n",
    "\n",
    "df_multi = pd.DataFrame(data, index=index)\n",
    "\n",
    "print(\"MultiIndex DataFrame:\")\n",
    "print(df_multi.head(10))\n",
    "print(f\"\\nShape: {df_multi.shape}\")\n",
    "\n",
    "# Advanced indexing operations\n",
    "print(\"\\n1. Select all data for Exp_A:\")\n",
    "exp_a_data = df_multi.loc['Exp_A']\n",
    "print(exp_a_data.head())\n",
    "\n",
    "print(\"\\n2. Select Treatment_1 across all experiments:\")\n",
    "treatment_1_data = df_multi.loc[pd.IndexSlice[:, 'Treatment_1', :], :]\n",
    "print(treatment_1_data.head())\n",
    "\n",
    "print(\"\\n3. Cross-sectional data at T4:\")\n",
    "t4_data = df_multi.loc[pd.IndexSlice[:, :, 'T4'], :]\n",
    "print(t4_data.head())\n",
    "\n",
    "# Pivot and unstack operations\n",
    "print(\"\\n4. Unstacking timepoints:\")\n",
    "unstacked = df_multi['cell_count'].unstack('timepoint')\n",
    "print(unstacked.head())\n",
    "\n",
    "print(\"\\n5. Pivot table with multiple aggregations:\")\n",
    "pivot_table = df_multi.pivot_table(\n",
    "    values=['cell_count', 'viability'],\n",
    "    index='experiment',\n",
    "    columns='condition',\n",
    "    aggfunc={'cell_count': 'mean', 'viability': ['mean', 'std']}\n",
    ")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced GroupBy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for groupby operations\n",
    "df_flat = df_multi.reset_index()\n",
    "\n",
    "# Multiple groupby operations\n",
    "print(\"1. Multiple aggregations per column:\")\n",
    "agg_result = df_flat.groupby(['experiment', 'condition']).agg({\n",
    "    'cell_count': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'viability': ['mean', 'std'],\n",
    "    'protein_conc': lambda x: x.quantile(0.75),  # Custom function\n",
    "    'glucose': 'mean'\n",
    "})\n",
    "print(agg_result.head())\n",
    "\n",
    "# Named aggregations (pandas 0.25+)\n",
    "print(\"\\n2. Named aggregations:\")\n",
    "named_agg = df_flat.groupby(['experiment', 'condition']).agg(\n",
    "    avg_cell_count=('cell_count', 'mean'),\n",
    "    cell_count_cv=('cell_count', lambda x: x.std() / x.mean()),  # Coefficient of variation\n",
    "    viability_min=('viability', 'min'),\n",
    "    protein_range=('protein_conc', lambda x: x.max() - x.min())\n",
    ")\n",
    "print(named_agg.head())\n",
    "\n",
    "# Transform operations\n",
    "print(\"\\n3. Transform operations (z-score normalization):\")\n",
    "df_flat['cell_count_zscore'] = df_flat.groupby('condition')['cell_count'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "df_flat['viability_pct_of_max'] = df_flat.groupby('experiment')['viability'].transform(\n",
    "    lambda x: x / x.max() * 100\n",
    ")\n",
    "print(df_flat[['experiment', 'condition', 'cell_count', 'cell_count_zscore', \n",
    "              'viability', 'viability_pct_of_max']].head())\n",
    "\n",
    "# Rolling and expanding operations within groups\n",
    "print(\"\\n4. Rolling operations within groups:\")\n",
    "df_sorted = df_flat.sort_values(['experiment', 'condition', 'timepoint'])\n",
    "df_sorted['cell_count_rolling_mean'] = df_sorted.groupby(['experiment', 'condition'])['cell_count'].rolling(\n",
    "    window=3, min_periods=1\n",
    ").mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "print(df_sorted[['experiment', 'condition', 'timepoint', \n",
    "                'cell_count', 'cell_count_rolling_mean']].head(10))\n",
    "\n",
    "# Custom aggregation functions\n",
    "def growth_rate(series):\n",
    "    \"\"\"Calculate growth rate as (final - initial) / initial\"\"\"\n",
    "    if len(series) < 2:\n",
    "        return np.nan\n",
    "    sorted_series = series.sort_index()\n",
    "    return (sorted_series.iloc[-1] - sorted_series.iloc[0]) / sorted_series.iloc[0]\n",
    "\n",
    "def coefficient_of_variation(series):\n",
    "    \"\"\"Calculate coefficient of variation\"\"\"\n",
    "    return series.std() / series.mean() if series.mean() != 0 else np.nan\n",
    "\n",
    "print(\"\\n5. Custom aggregation functions:\")\n",
    "custom_agg = df_flat.groupby(['experiment', 'condition']).agg({\n",
    "    'cell_count': [growth_rate, coefficient_of_variation],\n",
    "    'viability': [growth_rate, 'mean'],\n",
    "    'protein_conc': ['mean', coefficient_of_variation]\n",
    "})\n",
    "print(custom_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic time series data\n",
    "start_date = datetime(2023, 1, 1)\n",
    "dates = pd.date_range(start=start_date, periods=365*2, freq='D')  # 2 years of daily data\n",
    "\n",
    "# Generate synthetic sensor data with trends and seasonality\n",
    "np.random.seed(42)\n",
    "trend = np.linspace(20, 25, len(dates))  # Gradual temperature increase\n",
    "seasonal = 3 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)  # Annual cycle\n",
    "noise = np.random.normal(0, 0.5, len(dates))\n",
    "temperature = trend + seasonal + noise\n",
    "\n",
    "# Add some missing values\n",
    "missing_indices = np.random.choice(len(dates), size=20, replace=False)\n",
    "temperature[missing_indices] = np.nan\n",
    "\n",
    "# Create DataFrame\n",
    "ts_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'temperature': temperature,\n",
    "    'humidity': np.random.normal(65, 10, len(dates)),\n",
    "    'pressure': np.random.normal(1013, 5, len(dates))\n",
    "})\n",
    "\n",
    "# Set date as index\n",
    "ts_df.set_index('date', inplace=True)\n",
    "\n",
    "print(\"Time series data:\")\n",
    "print(ts_df.head())\n",
    "print(f\"\\nMissing values: {ts_df.isnull().sum()}\")\n",
    "\n",
    "# Advanced resampling operations\n",
    "print(\"\\n1. Multiple resampling frequencies:\")\n",
    "weekly_stats = ts_df.resample('W').agg({\n",
    "    'temperature': ['mean', 'std', 'min', 'max'],\n",
    "    'humidity': 'mean',\n",
    "    'pressure': 'mean'\n",
    "})\n",
    "print(weekly_stats.head())\n",
    "\n",
    "monthly_stats = ts_df.resample('M').agg({\n",
    "    'temperature': ['mean', 'std'],\n",
    "    'humidity': ['mean', 'std'],\n",
    "    'pressure': ['mean', 'std']\n",
    "})\n",
    "print(\"\\nMonthly statistics:\")\n",
    "print(monthly_stats.head())\n",
    "\n",
    "# Handle missing values with interpolation\n",
    "print(\"\\n2. Interpolation methods:\")\n",
    "ts_df['temp_linear'] = ts_df['temperature'].interpolate(method='linear')\n",
    "ts_df['temp_cubic'] = ts_df['temperature'].interpolate(method='cubic')\n",
    "ts_df['temp_time'] = ts_df['temperature'].interpolate(method='time')\n",
    "\n",
    "# Show interpolation comparison for a subset with missing values\n",
    "subset_with_missing = ts_df.loc[ts_df['temperature'].isnull().shift(1) | \n",
    "                               ts_df['temperature'].isnull() | \n",
    "                               ts_df['temperature'].isnull().shift(-1)].dropna()\n",
    "print(subset_with_missing[['temperature', 'temp_linear', 'temp_cubic', 'temp_time']].head())\n",
    "\n",
    "# Rolling window operations\n",
    "print(\"\\n3. Rolling window analysis:\")\n",
    "ts_df['temp_7day_mean'] = ts_df['temperature'].rolling(window=7, center=True).mean()\n",
    "ts_df['temp_30day_std'] = ts_df['temperature'].rolling(window=30, center=True).std()\n",
    "ts_df['temp_rolling_corr'] = ts_df['temperature'].rolling(window=30).corr(ts_df['pressure'])\n",
    "\n",
    "print(ts_df[['temperature', 'temp_7day_mean', 'temp_30day_std', 'temp_rolling_corr']].head(40).tail(10))\n",
    "\n",
    "# Lag and lead operations\n",
    "print(\"\\n4. Lag and lead analysis:\")\n",
    "ts_df['temp_lag1'] = ts_df['temperature'].shift(1)\n",
    "ts_df['temp_lag7'] = ts_df['temperature'].shift(7)\n",
    "ts_df['temp_lead1'] = ts_df['temperature'].shift(-1)\n",
    "ts_df['temp_diff'] = ts_df['temperature'].diff()\n",
    "ts_df['temp_pct_change'] = ts_df['temperature'].pct_change()\n",
    "\n",
    "print(ts_df[['temperature', 'temp_lag1', 'temp_diff', 'temp_pct_change']].head(10))\n",
    "\n",
    "# Seasonal decomposition simulation\n",
    "print(\"\\n5. Manual seasonal analysis:\")\n",
    "ts_df['month'] = ts_df.index.month\n",
    "ts_df['day_of_year'] = ts_df.index.dayofyear\n",
    "ts_df['quarter'] = ts_df.index.quarter\n",
    "\n",
    "# Calculate seasonal patterns\n",
    "monthly_pattern = ts_df.groupby('month')['temperature'].mean()\n",
    "print(\"Monthly temperature pattern:\")\n",
    "print(monthly_pattern)\n",
    "\n",
    "# Anomaly detection using rolling statistics\n",
    "ts_df['temp_rolling_mean'] = ts_df['temperature'].rolling(window=30, center=True).mean()\n",
    "ts_df['temp_rolling_std'] = ts_df['temperature'].rolling(window=30, center=True).std()\n",
    "ts_df['temp_zscore'] = (ts_df['temperature'] - ts_df['temp_rolling_mean']) / ts_df['temp_rolling_std']\n",
    "ts_df['is_anomaly'] = np.abs(ts_df['temp_zscore']) > 2  # 2 standard deviations\n",
    "\n",
    "anomalies = ts_df[ts_df['is_anomaly']]\n",
    "print(f\"\\nFound {len(anomalies)} temperature anomalies\")\n",
    "if len(anomalies) > 0:\n",
    "    print(anomalies[['temperature', 'temp_rolling_mean', 'temp_zscore']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Data Transformation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complex experimental dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "experimental_data = pd.DataFrame({\n",
    "    'sample_id': [f'S{i:04d}' for i in range(n_samples)],\n",
    "    'treatment': np.random.choice(['Control', 'Drug_A', 'Drug_B', 'Combination'], n_samples),\n",
    "    'dose': np.random.choice([0, 0.1, 1.0, 10.0, 100.0], n_samples),\n",
    "    'cell_line': np.random.choice(['HeLa', 'MCF7', 'A549', 'HEK293'], n_samples),\n",
    "    'passage_number': np.random.randint(5, 25, n_samples),\n",
    "    'initial_density': np.random.normal(50000, 10000, n_samples),\n",
    "    'final_density': np.random.normal(200000, 50000, n_samples),\n",
    "    'viability': np.random.beta(8, 2, n_samples),  # Skewed towards higher values\n",
    "    'metabolic_activity': np.random.gamma(2, 2, n_samples),\n",
    "    'protein_content': np.random.lognormal(1, 0.5, n_samples),\n",
    "    'incubation_time': np.random.choice([24, 48, 72], n_samples)\n",
    "})\n",
    "\n",
    "print(\"Original dataset:\")\n",
    "print(experimental_data.head())\n",
    "print(f\"\\nDataset shape: {experimental_data.shape}\")\n",
    "print(f\"\\nData types:\\n{experimental_data.dtypes}\")\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\n1. Creating derived features:\")\n",
    "\n",
    "# Growth rate calculation\n",
    "experimental_data['growth_rate'] = (\n",
    "    experimental_data['final_density'] - experimental_data['initial_density']\n",
    ") / experimental_data['initial_density']\n",
    "\n",
    "# Fold change\n",
    "experimental_data['fold_change'] = (\n",
    "    experimental_data['final_density'] / experimental_data['initial_density']\n",
    ")\n",
    "\n",
    "# Dose response (log transformation)\n",
    "experimental_data['log_dose'] = np.log10(\n",
    "    experimental_data['dose'].replace(0, 0.001)  # Handle zero dose\n",
    ")\n",
    "\n",
    "# Efficiency metrics\n",
    "experimental_data['growth_efficiency'] = (\n",
    "    experimental_data['growth_rate'] * experimental_data['viability']\n",
    ")\n",
    "\n",
    "# Categorical encoding\n",
    "print(\"\\n2. Categorical encoding:\")\n",
    "\n",
    "# One-hot encoding\n",
    "treatment_dummies = pd.get_dummies(experimental_data['treatment'], prefix='treatment')\n",
    "cell_line_dummies = pd.get_dummies(experimental_data['cell_line'], prefix='cell_line')\n",
    "\n",
    "# Ordinal encoding for dose\n",
    "dose_mapping = {0: 0, 0.1: 1, 1.0: 2, 10.0: 3, 100.0: 4}\n",
    "experimental_data['dose_ordinal'] = experimental_data['dose'].map(dose_mapping)\n",
    "\n",
    "# Combine original data with encoded features\n",
    "experimental_encoded = pd.concat([\n",
    "    experimental_data, \n",
    "    treatment_dummies, \n",
    "    cell_line_dummies\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Dataset with encoded features: {experimental_encoded.shape}\")\n",
    "print(experimental_encoded[['treatment', 'treatment_Control', 'treatment_Drug_A']].head())\n",
    "\n",
    "# Binning and discretization\n",
    "print(\"\\n3. Binning continuous variables:\")\n",
    "\n",
    "# Equal-width binning\n",
    "experimental_data['passage_bin'] = pd.cut(\n",
    "    experimental_data['passage_number'], \n",
    "    bins=3, \n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "# Quantile-based binning\n",
    "experimental_data['density_quartile'] = pd.qcut(\n",
    "    experimental_data['initial_density'], \n",
    "    q=4, \n",
    "    labels=['Q1', 'Q2', 'Q3', 'Q4']\n",
    ")\n",
    "\n",
    "# Custom binning based on domain knowledge\n",
    "def viability_category(viability):\n",
    "    if viability < 0.7:\n",
    "        return 'Poor'\n",
    "    elif viability < 0.85:\n",
    "        return 'Good'\n",
    "    else:\n",
    "        return 'Excellent'\n",
    "\n",
    "experimental_data['viability_category'] = experimental_data['viability'].apply(viability_category)\n",
    "\n",
    "print(experimental_data[['passage_number', 'passage_bin', \n",
    "                        'initial_density', 'density_quartile',\n",
    "                        'viability', 'viability_category']].head())\n",
    "\n",
    "# Advanced transformations\n",
    "print(\"\\n4. Advanced transformations:\")\n",
    "\n",
    "# Box-Cox transformation for normalization\n",
    "from scipy import stats\n",
    "\n",
    "# Log transformation for skewed data\n",
    "experimental_data['protein_content_log'] = np.log1p(experimental_data['protein_content'])\n",
    "\n",
    "# Square root transformation\n",
    "experimental_data['metabolic_activity_sqrt'] = np.sqrt(experimental_data['metabolic_activity'])\n",
    "\n",
    "# Z-score normalization within groups\n",
    "experimental_data['viability_zscore_by_treatment'] = experimental_data.groupby('treatment')['viability'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std()\n",
    ")\n",
    "\n",
    "# Min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "experimental_data['growth_rate_scaled'] = scaler.fit_transform(\n",
    "    experimental_data[['growth_rate']]\n",
    ").flatten()\n",
    "\n",
    "print(experimental_data[['protein_content', 'protein_content_log',\n",
    "                        'metabolic_activity', 'metabolic_activity_sqrt',\n",
    "                        'growth_rate', 'growth_rate_scaled']].head())\n",
    "\n",
    "# Interaction features\n",
    "print(\"\\n5. Interaction features:\")\n",
    "\n",
    "# Polynomial features\n",
    "experimental_data['dose_squared'] = experimental_data['dose'] ** 2\n",
    "experimental_data['dose_viability_interaction'] = (\n",
    "    experimental_data['dose'] * experimental_data['viability']\n",
    ")\n",
    "\n",
    "# Cross-feature ratios\n",
    "experimental_data['protein_per_cell'] = (\n",
    "    experimental_data['protein_content'] / experimental_data['final_density'] * 1e6\n",
    ")\n",
    "\n",
    "# Time-based features\n",
    "experimental_data['growth_rate_per_hour'] = (\n",
    "    experimental_data['growth_rate'] / experimental_data['incubation_time']\n",
    ")\n",
    "\n",
    "print(experimental_data[['dose', 'dose_squared', 'dose_viability_interaction',\n",
    "                        'protein_per_cell', 'growth_rate_per_hour']].head())\n",
    "\n",
    "# Feature selection based on correlation\n",
    "print(\"\\n6. Feature correlation analysis:\")\n",
    "numeric_cols = experimental_data.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = experimental_data[numeric_cols].corr()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(\"Highly correlated feature pairs (|r| > 0.8):\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"{pair[0]} <-> {pair[1]}: {pair[2]:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {experimental_data.shape}\")\n",
    "print(f\"Number of numeric features: {len(numeric_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Create large dataset for performance testing\n",
    "np.random.seed(42)\n",
    "n_large = 100000\n",
    "\n",
    "print(\"Creating large dataset for performance testing...\")\n",
    "start_memory = get_memory_usage()\n",
    "\n",
    "large_df = pd.DataFrame({\n",
    "    'id': range(n_large),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_large),\n",
    "    'value1': np.random.normal(0, 1, n_large),\n",
    "    'value2': np.random.normal(100, 15, n_large),\n",
    "    'value3': np.random.exponential(2, n_large),\n",
    "    'text_data': [f'sample_{i}_{np.random.choice([\"x\", \"y\", \"z\"])}' for i in range(n_large)]\n",
    "})\n",
    "\n",
    "after_creation_memory = get_memory_usage()\n",
    "print(f\"Memory usage after creation: {after_creation_memory - start_memory:.1f} MB\")\n",
    "\n",
    "# Data type optimization\n",
    "print(\"\\n1. Data type optimization:\")\n",
    "print(\"Original data types and memory usage:\")\n",
    "print(large_df.dtypes)\n",
    "print(f\"Memory usage: {large_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Optimize data types\n",
    "large_df_optimized = large_df.copy()\n",
    "\n",
    "# Convert to categorical for repeated strings\n",
    "large_df_optimized['category'] = large_df_optimized['category'].astype('category')\n",
    "large_df_optimized['text_data'] = large_df_optimized['text_data'].astype('category')\n",
    "\n",
    "# Downcast numeric types\n",
    "large_df_optimized['id'] = pd.to_numeric(large_df_optimized['id'], downcast='integer')\n",
    "large_df_optimized['value2'] = pd.to_numeric(large_df_optimized['value2'], downcast='float')\n",
    "\n",
    "print(\"\\nOptimized data types and memory usage:\")\n",
    "print(large_df_optimized.dtypes)\n",
    "print(f\"Memory usage: {large_df_optimized.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "memory_savings = (\n",
    "    large_df.memory_usage(deep=True).sum() - large_df_optimized.memory_usage(deep=True).sum()\n",
    ") / large_df.memory_usage(deep=True).sum() * 100\n",
    "print(f\"Memory savings: {memory_savings:.1f}%\")\n",
    "\n",
    "# Performance comparison: vectorized vs iterative operations\n",
    "print(\"\\n2. Performance comparison - vectorized vs iterative:\")\n",
    "\n",
    "# Create test data\n",
    "test_data = pd.DataFrame({\n",
    "    'x': np.random.random(50000),\n",
    "    'y': np.random.random(50000)\n",
    "})\n",
    "\n",
    "# Iterative approach (slow)\n",
    "start_time = time.time()\n",
    "result_iterative = []\n",
    "for _, row in test_data.head(1000).iterrows():  # Only test on subset for speed\n",
    "    result_iterative.append(np.sqrt(row['x']**2 + row['y']**2))\n",
    "iterative_time = time.time() - start_time\n",
    "\n",
    "# Vectorized approach (fast)\n",
    "start_time = time.time()\n",
    "result_vectorized = np.sqrt(test_data['x']**2 + test_data['y']**2)\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"Iterative approach (1000 rows): {iterative_time:.4f} seconds\")\n",
    "print(f\"Vectorized approach (50000 rows): {vectorized_time:.4f} seconds\")\n",
    "print(f\"Speedup factor: {iterative_time / vectorized_time * 50:.0f}x\")\n",
    "\n",
    "# Efficient querying\n",
    "print(\"\\n3. Efficient querying techniques:\")\n",
    "\n",
    "# Method 1: Boolean indexing\n",
    "start_time = time.time()\n",
    "result1 = large_df_optimized[large_df_optimized['value1'] > 0]\n",
    "boolean_time = time.time() - start_time\n",
    "\n",
    "# Method 2: Query method\n",
    "start_time = time.time()\n",
    "result2 = large_df_optimized.query('value1 > 0')\n",
    "query_time = time.time() - start_time\n",
    "\n",
    "# Method 3: Using loc\n",
    "start_time = time.time()\n",
    "result3 = large_df_optimized.loc[large_df_optimized['value1'] > 0]\n",
    "loc_time = time.time() - start_time\n",
    "\n",
    "print(f\"Boolean indexing: {boolean_time:.4f} seconds\")\n",
    "print(f\"Query method: {query_time:.4f} seconds\")\n",
    "print(f\"Loc method: {loc_time:.4f} seconds\")\n",
    "\n",
    "# Chunked processing for very large datasets\n",
    "print(\"\\n4. Chunked processing:\")\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Process a chunk of data\"\"\"\n",
    "    return chunk.groupby('category')['value1'].mean()\n",
    "\n",
    "# Simulate chunked processing\n",
    "chunk_size = 10000\n",
    "results = []\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(large_df_optimized), chunk_size):\n",
    "    chunk = large_df_optimized.iloc[i:i+chunk_size]\n",
    "    chunk_result = process_chunk(chunk)\n",
    "    results.append(chunk_result)\n",
    "\n",
    "# Combine results\n",
    "combined_result = pd.concat(results, axis=1).mean(axis=1)\n",
    "chunked_time = time.time() - start_time\n",
    "\n",
    "# Compare with processing all at once\n",
    "start_time = time.time()\n",
    "direct_result = large_df_optimized.groupby('category')['value1'].mean()\n",
    "direct_time = time.time() - start_time\n",
    "\n",
    "print(f\"Chunked processing: {chunked_time:.4f} seconds\")\n",
    "print(f\"Direct processing: {direct_time:.4f} seconds\")\n",
    "print(f\"Results are similar: {np.allclose(combined_result.values, direct_result.values)}\")\n",
    "\n",
    "# Efficient aggregations\n",
    "print(\"\\n5. Efficient aggregation techniques:\")\n",
    "\n",
    "# Method 1: Multiple separate aggregations\n",
    "start_time = time.time()\n",
    "mean_val = large_df_optimized.groupby('category')['value1'].mean()\n",
    "std_val = large_df_optimized.groupby('category')['value1'].std()\n",
    "count_val = large_df_optimized.groupby('category')['value1'].count()\n",
    "separate_time = time.time() - start_time\n",
    "\n",
    "# Method 2: Single aggregation call\n",
    "start_time = time.time()\n",
    "combined_agg = large_df_optimized.groupby('category')['value1'].agg(['mean', 'std', 'count'])\n",
    "combined_time = time.time() - start_time\n",
    "\n",
    "print(f\"Separate aggregations: {separate_time:.4f} seconds\")\n",
    "print(f\"Combined aggregation: {combined_time:.4f} seconds\")\n",
    "print(f\"Speedup: {separate_time / combined_time:.1f}x\")\n",
    "\n",
    "# Using eval for complex expressions\n",
    "print(\"\\n6. Using eval for complex expressions:\")\n",
    "\n",
    "# Standard method\n",
    "start_time = time.time()\n",
    "result_standard = large_df_optimized['value1'] * large_df_optimized['value2'] + large_df_optimized['value3']\n",
    "standard_time = time.time() - start_time\n",
    "\n",
    "# Eval method\n",
    "start_time = time.time()\n",
    "result_eval = large_df_optimized.eval('value1 * value2 + value3')\n",
    "eval_time = time.time() - start_time\n",
    "\n",
    "print(f\"Standard method: {standard_time:.4f} seconds\")\n",
    "print(f\"Eval method: {eval_time:.4f} seconds\")\n",
    "print(f\"Results are equal: {result_standard.equals(result_eval)}\")\n",
    "\n",
    "final_memory = get_memory_usage()\n",
    "print(f\"\\nFinal memory usage: {final_memory:.1f} MB\")\n",
    "print(f\"Total memory increase: {final_memory - start_memory:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Plotting and Visualization Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset for visualization\n",
    "np.random.seed(42)\n",
    "viz_data = pd.DataFrame({\n",
    "    'experiment': np.repeat(['Exp1', 'Exp2', 'Exp3'], 100),\n",
    "    'treatment': np.tile(['Control', 'Treatment'] * 50, 3),\n",
    "    'concentration': np.tile([0, 0.1, 1, 10, 100] * 20, 3),\n",
    "    'response': np.random.normal(50, 15, 300) + np.tile([0, 10, 25, 35, 40] * 20, 3),\n",
    "    'time_point': np.tile(range(1, 101), 3),\n",
    "    'replicate': np.tile(range(1, 4), 100)\n",
    "})\n",
    "\n",
    "# Add some noise and trends\n",
    "viz_data['response'] += viz_data['time_point'] * 0.1 + np.random.normal(0, 5, len(viz_data))\n",
    "viz_data['response'] = np.maximum(viz_data['response'], 0)  # Ensure non-negative\n",
    "\n",
    "print(\"Visualization dataset:\")\n",
    "print(viz_data.head())\n",
    "print(f\"Shape: {viz_data.shape}\")\n",
    "\n",
    "# 1. Advanced pandas plotting\n",
    "print(\"\\n1. Pandas built-in plotting:\")\n",
    "\n",
    "# Subplot layout with pandas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Time series by experiment\n",
    "for exp in viz_data['experiment'].unique():\n",
    "    exp_data = viz_data[viz_data['experiment'] == exp]\n",
    "    exp_summary = exp_data.groupby('time_point')['response'].mean()\n",
    "    exp_summary.plot(ax=axes[0, 0], label=exp, alpha=0.7)\n",
    "axes[0, 0].set_title('Time Series by Experiment')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by treatment\n",
    "viz_data.boxplot(column='response', by='treatment', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Response by Treatment')\n",
    "axes[0, 1].set_xlabel('Treatment')\n",
    "\n",
    "# Histogram with overlay\n",
    "viz_data[viz_data['treatment'] == 'Control']['response'].hist(\n",
    "    ax=axes[1, 0], alpha=0.5, label='Control', bins=20\n",
    ")\n",
    "viz_data[viz_data['treatment'] == 'Treatment']['response'].hist(\n",
    "    ax=axes[1, 0], alpha=0.5, label='Treatment', bins=20\n",
    ")\n",
    "axes[1, 0].set_title('Response Distribution')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Scatter plot with correlation\n",
    "viz_data.plot.scatter(x='concentration', y='response', ax=axes[1, 1], alpha=0.6)\n",
    "axes[1, 1].set_title('Concentration vs Response')\n",
    "axes[1, 1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Pivot table visualization\n",
    "print(\"\\n2. Pivot table heatmap:\")\n",
    "\n",
    "# Create pivot table for heatmap\n",
    "pivot_data = viz_data.pivot_table(\n",
    "    values='response',\n",
    "    index='concentration',\n",
    "    columns='experiment',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pivot_data.values, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Mean Response')\n",
    "plt.xticks(range(len(pivot_data.columns)), pivot_data.columns)\n",
    "plt.yticks(range(len(pivot_data.index)), pivot_data.index)\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('Concentration')\n",
    "plt.title('Response Heatmap: Concentration vs Experiment')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(pivot_data.index)):\n",
    "    for j in range(len(pivot_data.columns)):\n",
    "        plt.text(j, i, f'{pivot_data.iloc[i, j]:.1f}', \n",
    "                ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Rolling correlation visualization\n",
    "print(\"\\n3. Rolling correlation analysis:\")\n",
    "\n",
    "# Create time series data\n",
    "ts_viz = viz_data.pivot_table(\n",
    "    values='response',\n",
    "    index='time_point',\n",
    "    columns='experiment',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Calculate rolling correlation\n",
    "rolling_corr = ts_viz['Exp1'].rolling(window=20).corr(ts_viz['Exp2'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Plot time series\n",
    "ts_viz.plot(ax=ax1, alpha=0.7)\n",
    "ax1.set_title('Time Series Data')\n",
    "ax1.set_ylabel('Response')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot rolling correlation\n",
    "rolling_corr.plot(ax=ax2, color='red', linewidth=2)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_title('Rolling Correlation: Exp1 vs Exp2 (20-point window)')\n",
    "ax2.set_ylabel('Correlation')\n",
    "ax2.set_xlabel('Time Point')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Advanced groupby visualization\n",
    "print(\"\\n4. Grouped analysis visualization:\")\n",
    "\n",
    "# Multi-level grouping and visualization\n",
    "grouped_stats = viz_data.groupby(['experiment', 'treatment', 'concentration']).agg({\n",
    "    'response': ['mean', 'std', 'count']\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "grouped_stats.columns = ['_'.join(col).strip() for col in grouped_stats.columns]\n",
    "grouped_stats = grouped_stats.reset_index()\n",
    "\n",
    "# Create dose-response curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, exp in enumerate(['Exp1', 'Exp2', 'Exp3']):\n",
    "    exp_data = grouped_stats[grouped_stats['experiment'] == exp]\n",
    "    \n",
    "    for treatment in ['Control', 'Treatment']:\n",
    "        treat_data = exp_data[exp_data['treatment'] == treatment]\n",
    "        \n",
    "        axes[i].errorbar(\n",
    "            treat_data['concentration'],\n",
    "            treat_data['response_mean'],\n",
    "            yerr=treat_data['response_std'],\n",
    "            label=treatment,\n",
    "            marker='o',\n",
    "            capsize=5,\n",
    "            capthick=2\n",
    "        )\n",
    "    \n",
    "    axes[i].set_xscale('symlog', linthresh=0.1)  # Handle zero concentration\n",
    "    axes[i].set_xlabel('Concentration')\n",
    "    axes[i].set_ylabel('Response')\n",
    "    axes[i].set_title(f'{exp} Dose-Response')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Statistical visualization\n",
    "print(\"\\n5. Statistical comparison visualization:\")\n",
    "\n",
    "# Confidence intervals\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_ci(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean - h, mean + h\n",
    "\n",
    "# Calculate statistics by group\n",
    "treatment_stats = []\n",
    "for exp in viz_data['experiment'].unique():\n",
    "    for treatment in viz_data['treatment'].unique():\n",
    "        subset = viz_data[(viz_data['experiment'] == exp) & \n",
    "                         (viz_data['treatment'] == treatment)]['response']\n",
    "        \n",
    "        mean_val = subset.mean()\n",
    "        ci_low, ci_high = calculate_ci(subset)\n",
    "        \n",
    "        treatment_stats.append({\n",
    "            'experiment': exp,\n",
    "            'treatment': treatment,\n",
    "            'mean': mean_val,\n",
    "            'ci_low': ci_low,\n",
    "            'ci_high': ci_high,\n",
    "            'error': ci_high - mean_val\n",
    "        })\n",
    "\n",
    "stats_df = pd.DataFrame(treatment_stats)\n",
    "\n",
    "# Create grouped bar plot with confidence intervals\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "experiments = stats_df['experiment'].unique()\n",
    "treatments = stats_df['treatment'].unique()\n",
    "x = np.arange(len(experiments))\n",
    "width = 0.35\n",
    "\n",
    "for i, treatment in enumerate(treatments):\n",
    "    data = stats_df[stats_df['treatment'] == treatment]\n",
    "    ax.bar(x + i*width, data['mean'], width, \n",
    "           yerr=data['error'], capsize=5,\n",
    "           label=treatment, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Experiment')\n",
    "ax.set_ylabel('Response (95% CI)')\n",
    "ax.set_title('Treatment Effect Comparison with Confidence Intervals')\n",
    "ax.set_xticks(x + width / 2)\n",
    "ax.set_xticklabels(experiments)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(stats_df.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}