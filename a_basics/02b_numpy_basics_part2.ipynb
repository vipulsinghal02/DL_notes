{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy Basics Part 2 - Advanced Numerical Computing\n",
    "\n",
    "Advanced NumPy techniques for scientific computing and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import linalg, optimize, signal, interpolate\n",
    "import time\n",
    "\n",
    "# Set random seed and print options\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Array Operations and Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced indexing with multiple conditions\n",
    "data_2d = np.random.randn(10, 8)\n",
    "print(\"Original 2D array shape:\", data_2d.shape)\n",
    "print(\"First few rows:\")\n",
    "print(data_2d[:3])\n",
    "\n",
    "# Boolean indexing with multiple conditions\n",
    "condition = (data_2d > 0.5) & (data_2d < 1.5)\n",
    "values_in_range = data_2d[condition]\n",
    "print(f\"\\nValues between 0.5 and 1.5: {len(values_in_range)} values\")\n",
    "print(f\"Sample values: {values_in_range[:5]}\")\n",
    "\n",
    "# Advanced fancy indexing\n",
    "row_indices = [1, 3, 7]\n",
    "col_indices = [2, 5, 6]\n",
    "selected_elements = data_2d[np.ix_(row_indices, col_indices)]\n",
    "print(f\"\\nSelected submatrix shape: {selected_elements.shape}\")\n",
    "print(selected_elements)\n",
    "\n",
    "# Broadcasting examples\n",
    "print(\"\\n=== Broadcasting Examples ===\")\n",
    "\n",
    "# Example 1: Normalize each row by its maximum\n",
    "row_max = data_2d.max(axis=1, keepdims=True)\n",
    "normalized_rows = data_2d / row_max\n",
    "print(f\"Original max per row: {data_2d.max(axis=1)[:3]}\")\n",
    "print(f\"Normalized max per row: {normalized_rows.max(axis=1)[:3]}\")\n",
    "\n",
    "# Example 2: Center each column (subtract column mean)\n",
    "col_means = data_2d.mean(axis=0)\n",
    "centered_data = data_2d - col_means\n",
    "print(f\"\\nOriginal column means: {data_2d.mean(axis=0)[:4]}\")\n",
    "print(f\"Centered column means: {centered_data.mean(axis=0)[:4]}\")\n",
    "\n",
    "# Example 3: Distance matrix computation using broadcasting\n",
    "points = np.random.rand(5, 2)  # 5 points in 2D\n",
    "print(f\"\\nPoints shape: {points.shape}\")\n",
    "print(\"Points:\")\n",
    "print(points)\n",
    "\n",
    "# Compute pairwise distances using broadcasting\n",
    "diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "distances = np.sqrt(np.sum(diff**2, axis=2))\n",
    "print(f\"\\nDistance matrix shape: {distances.shape}\")\n",
    "print(\"Distance matrix:\")\n",
    "print(distances)\n",
    "\n",
    "# Verify diagonal is zero (distance from point to itself)\n",
    "print(f\"Diagonal elements (should be 0): {np.diag(distances)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Linear Algebra and Decompositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data matrix (e.g., gene expression data)\n",
    "n_genes, n_samples = 100, 20\n",
    "expression_data = np.random.exponential(2, (n_genes, n_samples)) + np.random.normal(0, 0.1, (n_genes, n_samples))\n",
    "\n",
    "print(f\"Expression data shape: {expression_data.shape}\")\n",
    "print(f\"Data range: {expression_data.min():.3f} to {expression_data.max():.3f}\")\n",
    "\n",
    "# Singular Value Decomposition (SVD)\n",
    "print(\"\\n=== Singular Value Decomposition ===\")\n",
    "U, s, Vt = np.linalg.svd(expression_data, full_matrices=False)\n",
    "\n",
    "print(f\"U shape (genes × components): {U.shape}\")\n",
    "print(f\"Singular values shape: {s.shape}\")\n",
    "print(f\"Vt shape (components × samples): {Vt.shape}\")\n",
    "print(f\"First 5 singular values: {s[:5]}\")\n",
    "\n",
    "# Reconstruct data using different numbers of components\n",
    "def reconstruct_svd(U, s, Vt, n_components):\n",
    "    return U[:, :n_components] @ np.diag(s[:n_components]) @ Vt[:n_components, :]\n",
    "\n",
    "# Calculate reconstruction error for different numbers of components\n",
    "components_range = [1, 5, 10, 15, 20]\n",
    "reconstruction_errors = []\n",
    "\n",
    "for n_comp in components_range:\n",
    "    reconstructed = reconstruct_svd(U, s, Vt, n_comp)\n",
    "    error = np.linalg.norm(expression_data - reconstructed, 'fro')\n",
    "    reconstruction_errors.append(error)\n",
    "    print(f\"Components: {n_comp:2d}, Reconstruction error: {error:.3f}\")\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = s**2 / np.sum(s**2)\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(f\"\\nFirst 5 components explain {cumulative_variance[4]:.1%} of variance\")\n",
    "print(f\"First 10 components explain {cumulative_variance[9]:.1%} of variance\")\n",
    "\n",
    "# Principal Component Analysis using SVD\n",
    "print(\"\\n=== Principal Component Analysis ===\")\n",
    "\n",
    "# Center the data (subtract mean)\n",
    "centered_data = expression_data - np.mean(expression_data, axis=1, keepdims=True)\n",
    "\n",
    "# PCA via SVD\n",
    "U_pca, s_pca, Vt_pca = np.linalg.svd(centered_data, full_matrices=False)\n",
    "\n",
    "# Principal component scores (projection onto PC space)\n",
    "pc_scores = Vt_pca.T  # Each column is a PC, each row is a sample\n",
    "print(f\"PC scores shape: {pc_scores.shape}\")\n",
    "print(f\"First 3 samples, first 3 PCs:\")\n",
    "print(pc_scores[:3, :3])\n",
    "\n",
    "# Eigenvalue decomposition of covariance matrix\n",
    "print(\"\\n=== Eigenvalue Decomposition ===\")\n",
    "\n",
    "# Create covariance matrix\n",
    "cov_matrix = np.cov(centered_data)\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Sort by eigenvalues (descending)\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(f\"Covariance matrix shape: {cov_matrix.shape}\")\n",
    "print(f\"Top 5 eigenvalues: {eigenvalues[:5]}\")\n",
    "print(f\"Eigenvalue/SVD relationship check: {np.allclose(eigenvalues[:10], s_pca[:10]**2/(n_samples-1))}\")\n",
    "\n",
    "# QR Decomposition\n",
    "print(\"\\n=== QR Decomposition ===\")\n",
    "sample_matrix = np.random.randn(8, 6)\n",
    "Q, R = np.linalg.qr(sample_matrix)\n",
    "\n",
    "print(f\"Original matrix shape: {sample_matrix.shape}\")\n",
    "print(f\"Q shape: {Q.shape}, R shape: {R.shape}\")\n",
    "print(f\"Q is orthogonal: {np.allclose(Q @ Q.T, np.eye(8))}\")\n",
    "print(f\"R is upper triangular: {np.allclose(R, np.triu(R))}\")\n",
    "print(f\"Reconstruction check: {np.allclose(sample_matrix, Q @ R)}\")\n",
    "\n",
    "# Cholesky Decomposition (for positive definite matrices)\n",
    "print(\"\\n=== Cholesky Decomposition ===\")\n",
    "# Create a positive definite matrix\n",
    "A = np.random.randn(5, 5)\n",
    "pos_def_matrix = A @ A.T + np.eye(5)  # Ensure positive definite\n",
    "\n",
    "try:\n",
    "    L = np.linalg.cholesky(pos_def_matrix)\n",
    "    print(f\"Cholesky factor shape: {L.shape}\")\n",
    "    print(f\"L is lower triangular: {np.allclose(L, np.tril(L))}\")\n",
    "    print(f\"Reconstruction check: {np.allclose(pos_def_matrix, L @ L.T)}\")\n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"Matrix is not positive definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Processing and Fourier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic signal with multiple components\n",
    "fs = 1000  # Sampling frequency\n",
    "t = np.linspace(0, 2, 2*fs, endpoint=False)  # 2 seconds\n",
    "\n",
    "# Create composite signal\n",
    "freq1, freq2, freq3 = 50, 120, 200  # Frequencies in Hz\n",
    "signal_clean = (np.sin(2*np.pi*freq1*t) + \n",
    "               0.5*np.sin(2*np.pi*freq2*t) + \n",
    "               0.3*np.sin(2*np.pi*freq3*t))\n",
    "\n",
    "# Add noise\n",
    "noise = 0.2 * np.random.randn(len(t))\n",
    "signal_noisy = signal_clean + noise\n",
    "\n",
    "print(f\"Signal length: {len(signal_noisy)} samples\")\n",
    "print(f\"Sampling frequency: {fs} Hz\")\n",
    "print(f\"Duration: {len(signal_noisy)/fs} seconds\")\n",
    "\n",
    "# Fast Fourier Transform (FFT)\n",
    "print(\"\\n=== Fourier Analysis ===\")\n",
    "\n",
    "# Compute FFT\n",
    "fft_result = np.fft.fft(signal_noisy)\n",
    "frequencies = np.fft.fftfreq(len(signal_noisy), 1/fs)\n",
    "\n",
    "# Take only positive frequencies\n",
    "positive_freq_mask = frequencies > 0\n",
    "frequencies_pos = frequencies[positive_freq_mask]\n",
    "magnitude_pos = np.abs(fft_result[positive_freq_mask])\n",
    "power_spectrum = magnitude_pos**2\n",
    "\n",
    "# Find peaks in frequency domain\n",
    "peak_indices = np.where(power_spectrum > 0.1 * np.max(power_spectrum))[0]\n",
    "peak_frequencies = frequencies_pos[peak_indices]\n",
    "\n",
    "print(f\"Detected frequencies: {peak_frequencies[peak_frequencies < 300]}\")\n",
    "print(f\"Expected frequencies: {[freq1, freq2, freq3]}\")\n",
    "\n",
    "# Power Spectral Density using Welch's method\n",
    "from scipy.signal import welch\n",
    "\n",
    "freqs_welch, psd_welch = welch(signal_noisy, fs, nperseg=512)\n",
    "print(f\"\\nWelch PSD - Frequency resolution: {freqs_welch[1] - freqs_welch[0]:.2f} Hz\")\n",
    "\n",
    "# Filtering\n",
    "print(\"\\n=== Digital Filtering ===\")\n",
    "\n",
    "# Low-pass filter to remove high-frequency noise\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Design Butterworth filter\n",
    "cutoff_freq = 150  # Hz\n",
    "nyquist_freq = fs / 2\n",
    "normalized_cutoff = cutoff_freq / nyquist_freq\n",
    "\n",
    "b, a = butter(4, normalized_cutoff, btype='low')\n",
    "signal_filtered = filtfilt(b, a, signal_noisy)\n",
    "\n",
    "print(f\"Filter cutoff frequency: {cutoff_freq} Hz\")\n",
    "print(f\"Original signal RMS: {np.sqrt(np.mean(signal_noisy**2)):.3f}\")\n",
    "print(f\"Filtered signal RMS: {np.sqrt(np.mean(signal_filtered**2)):.3f}\")\n",
    "\n",
    "# Windowing for spectral analysis\n",
    "print(\"\\n=== Windowing Functions ===\")\n",
    "\n",
    "# Different window functions\n",
    "window_length = 512\n",
    "windows = {\n",
    "    'Rectangular': np.ones(window_length),\n",
    "    'Hanning': np.hanning(window_length),\n",
    "    'Hamming': np.hamming(window_length),\n",
    "    'Blackman': np.blackman(window_length)\n",
    "}\n",
    "\n",
    "# Apply windows to signal segment\n",
    "signal_segment = signal_noisy[:window_length]\n",
    "windowed_ffts = {}\n",
    "\n",
    "for window_name, window in windows.items():\n",
    "    windowed_signal = signal_segment * window\n",
    "    windowed_fft = np.fft.fft(windowed_signal)\n",
    "    windowed_ffts[window_name] = windowed_fft\n",
    "    \n",
    "    # Calculate spectral leakage (energy outside main peaks)\n",
    "    magnitude = np.abs(windowed_fft)\n",
    "    total_energy = np.sum(magnitude**2)\n",
    "    print(f\"{window_name:12s} - Total spectral energy: {total_energy:.0f}\")\n",
    "\n",
    "# Cross-correlation analysis\n",
    "print(\"\\n=== Cross-correlation ===\")\n",
    "\n",
    "# Create two related signals\n",
    "signal1 = np.sin(2*np.pi*50*t[:1000])\n",
    "signal2 = np.sin(2*np.pi*50*t[:1000] + np.pi/4)  # Phase-shifted\n",
    "\n",
    "# Add different noise levels\n",
    "signal1_noisy = signal1 + 0.1*np.random.randn(len(signal1))\n",
    "signal2_noisy = signal2 + 0.1*np.random.randn(len(signal2))\n",
    "\n",
    "# Compute cross-correlation\n",
    "cross_corr = np.correlate(signal1_noisy, signal2_noisy, mode='full')\n",
    "lags = np.arange(-len(signal2_noisy)+1, len(signal1_noisy))\n",
    "\n",
    "# Find peak correlation and corresponding lag\n",
    "max_corr_idx = np.argmax(np.abs(cross_corr))\n",
    "max_corr_lag = lags[max_corr_idx]\n",
    "max_corr_value = cross_corr[max_corr_idx]\n",
    "\n",
    "print(f\"Maximum correlation: {max_corr_value:.3f} at lag: {max_corr_lag}\")\n",
    "print(f\"Expected phase shift: π/4 radians = {np.pi/4:.3f}\")\n",
    "print(f\"Calculated phase shift: {max_corr_lag * 2*np.pi*50/fs:.3f} radians\")\n",
    "\n",
    "# Spectrogram (time-frequency analysis)\n",
    "print(\"\\n=== Time-Frequency Analysis ===\")\n",
    "\n",
    "# Create chirp signal (frequency changes over time)\n",
    "t_chirp = np.linspace(0, 1, fs)\n",
    "f0, f1 = 50, 200  # Start and end frequencies\n",
    "chirp_signal = signal.chirp(t_chirp, f0, 1, f1)\n",
    "\n",
    "# Compute spectrogram\n",
    "frequencies_spec, times_spec, Sxx = signal.spectrogram(chirp_signal, fs, nperseg=256)\n",
    "\n",
    "print(f\"Spectrogram shape: {Sxx.shape}\")\n",
    "print(f\"Time resolution: {times_spec[1] - times_spec[0]:.4f} seconds\")\n",
    "print(f\"Frequency resolution: {frequencies_spec[1] - frequencies_spec[0]:.2f} Hz\")\n",
    "print(f\"Frequency range: {frequencies_spec[0]:.1f} to {frequencies_spec[-1]:.1f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Statistical Operations and Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced random number generation\n",
    "print(\"=== Advanced Random Sampling ===\")\n",
    "\n",
    "# Set up random number generator for reproducibility\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate samples from various distributions\n",
    "n_samples = 10000\n",
    "\n",
    "# Parametric distributions\n",
    "samples_normal = rng.normal(loc=100, scale=15, size=n_samples)\n",
    "samples_lognormal = rng.lognormal(mean=2, sigma=0.5, size=n_samples)\n",
    "samples_gamma = rng.gamma(shape=2, scale=2, size=n_samples)\n",
    "samples_beta = rng.beta(a=2, b=5, size=n_samples)\n",
    "samples_poisson = rng.poisson(lam=3, size=n_samples)\n",
    "\n",
    "distributions = {\n",
    "    'Normal(100, 15)': samples_normal,\n",
    "    'LogNormal(2, 0.5)': samples_lognormal,\n",
    "    'Gamma(2, 2)': samples_gamma,\n",
    "    'Beta(2, 5)': samples_beta,\n",
    "    'Poisson(3)': samples_poisson.astype(float)\n",
    "}\n",
    "\n",
    "# Calculate statistics for each distribution\n",
    "for dist_name, samples in distributions.items():\n",
    "    mean_val = np.mean(samples)\n",
    "    std_val = np.std(samples)\n",
    "    skewness = np.mean(((samples - mean_val) / std_val) ** 3)\n",
    "    kurtosis = np.mean(((samples - mean_val) / std_val) ** 4) - 3  # Excess kurtosis\n",
    "    \n",
    "    print(f\"{dist_name:15s}: μ={mean_val:6.2f}, σ={std_val:6.2f}, skew={skewness:6.2f}, kurt={kurtosis:6.2f}\")\n",
    "\n",
    "# Bootstrap sampling for confidence intervals\n",
    "print(\"\\n=== Bootstrap Confidence Intervals ===\")\n",
    "\n",
    "# Original sample\n",
    "original_sample = rng.normal(50, 10, 100)\n",
    "original_mean = np.mean(original_sample)\n",
    "\n",
    "# Bootstrap resampling\n",
    "n_bootstrap = 10000\n",
    "bootstrap_means = []\n",
    "\n",
    "for _ in range(n_bootstrap):\n",
    "    bootstrap_sample = rng.choice(original_sample, size=len(original_sample), replace=True)\n",
    "    bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "\n",
    "bootstrap_means = np.array(bootstrap_means)\n",
    "\n",
    "# Calculate confidence intervals\n",
    "confidence_levels = [90, 95, 99]\n",
    "for conf_level in confidence_levels:\n",
    "    alpha = 100 - conf_level\n",
    "    lower_percentile = alpha / 2\n",
    "    upper_percentile = 100 - alpha / 2\n",
    "    \n",
    "    ci_lower = np.percentile(bootstrap_means, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_means, upper_percentile)\n",
    "    \n",
    "    print(f\"{conf_level}% CI: [{ci_lower:.3f}, {ci_upper:.3f}] (original mean: {original_mean:.3f})\")\n",
    "\n",
    "# Permutation testing\n",
    "print(\"\\n=== Permutation Testing ===\")\n",
    "\n",
    "# Create two groups with different means\n",
    "group1 = rng.normal(100, 15, 50)\n",
    "group2 = rng.normal(105, 15, 50)  # Slightly higher mean\n",
    "\n",
    "observed_diff = np.mean(group1) - np.mean(group2)\n",
    "print(f\"Observed difference in means: {observed_diff:.3f}\")\n",
    "\n",
    "# Permutation test\n",
    "combined_data = np.concatenate([group1, group2])\n",
    "n_permutations = 10000\n",
    "permuted_diffs = []\n",
    "\n",
    "for _ in range(n_permutations):\n",
    "    # Randomly shuffle and split\n",
    "    shuffled = rng.permutation(combined_data)\n",
    "    perm_group1 = shuffled[:len(group1)]\n",
    "    perm_group2 = shuffled[len(group1):]\n",
    "    \n",
    "    perm_diff = np.mean(perm_group1) - np.mean(perm_group2)\n",
    "    permuted_diffs.append(perm_diff)\n",
    "\n",
    "permuted_diffs = np.array(permuted_diffs)\n",
    "\n",
    "# Calculate p-value (two-tailed test)\n",
    "p_value = np.mean(np.abs(permuted_diffs) >= np.abs(observed_diff))\n",
    "print(f\"Permutation test p-value: {p_value:.4f}\")\n",
    "\n",
    "# Monte Carlo integration\n",
    "print(\"\\n=== Monte Carlo Integration ===\")\n",
    "\n",
    "# Estimate π using Monte Carlo method\n",
    "def estimate_pi(n_points):\n",
    "    # Generate random points in [0,1] x [0,1]\n",
    "    x = rng.uniform(0, 1, n_points)\n",
    "    y = rng.uniform(0, 1, n_points)\n",
    "    \n",
    "    # Count points inside unit circle\n",
    "    inside_circle = (x**2 + y**2) <= 1\n",
    "    pi_estimate = 4 * np.mean(inside_circle)\n",
    "    \n",
    "    return pi_estimate\n",
    "\n",
    "# Test with different sample sizes\n",
    "sample_sizes = [1000, 10000, 100000, 1000000]\n",
    "for n in sample_sizes:\n",
    "    pi_est = estimate_pi(n)\n",
    "    error = abs(pi_est - np.pi)\n",
    "    print(f\"n={n:7d}: π estimate = {pi_est:.6f}, error = {error:.6f}\")\n",
    "\n",
    "# Markov Chain Monte Carlo (simple random walk)\n",
    "print(\"\\n=== Simple MCMC Sampling ===\")\n",
    "\n",
    "def log_posterior(x, mu=0, sigma=1):\n",
    "    \"\"\"Log posterior for normal distribution\"\"\"\n",
    "    return -0.5 * ((x - mu) / sigma)**2\n",
    "\n",
    "def mcmc_sample(n_samples, initial_value=0, step_size=1):\n",
    "    \"\"\"Simple Metropolis-Hastings sampler\"\"\"\n",
    "    samples = []\n",
    "    current_x = initial_value\n",
    "    current_log_prob = log_posterior(current_x)\n",
    "    n_accepted = 0\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Propose new state\n",
    "        proposed_x = current_x + rng.normal(0, step_size)\n",
    "        proposed_log_prob = log_posterior(proposed_x)\n",
    "        \n",
    "        # Accept or reject\n",
    "        log_ratio = proposed_log_prob - current_log_prob\n",
    "        if log_ratio > 0 or rng.random() < np.exp(log_ratio):\n",
    "            current_x = proposed_x\n",
    "            current_log_prob = proposed_log_prob\n",
    "            n_accepted += 1\n",
    "        \n",
    "        samples.append(current_x)\n",
    "    \n",
    "    return np.array(samples), n_accepted / n_samples\n",
    "\n",
    "# Run MCMC\n",
    "mcmc_samples, acceptance_rate = mcmc_sample(10000, step_size=1.5)\n",
    "\n",
    "print(f\"MCMC acceptance rate: {acceptance_rate:.3f}\")\n",
    "print(f\"Sample mean: {np.mean(mcmc_samples):.3f} (expected: 0.000)\")\n",
    "print(f\"Sample std: {np.std(mcmc_samples):.3f} (expected: 1.000)\")\n",
    "\n",
    "# Check convergence (autocorrelation)\n",
    "def autocorrelation(x, max_lag=100):\n",
    "    \"\"\"Calculate autocorrelation function\"\"\"\n",
    "    n = len(x)\n",
    "    x_centered = x - np.mean(x)\n",
    "    autocorr = np.correlate(x_centered, x_centered, mode='full')\n",
    "    autocorr = autocorr[n-1:n-1+max_lag+1]\n",
    "    autocorr = autocorr / autocorr[0]  # Normalize\n",
    "    return autocorr\n",
    "\n",
    "# Calculate effective sample size\n",
    "autocorr = autocorrelation(mcmc_samples)\n",
    "# Find first lag where autocorrelation drops below threshold\n",
    "threshold = 0.1\n",
    "effective_lags = np.where(autocorr < threshold)[0]\n",
    "if len(effective_lags) > 0:\n",
    "    autocorr_time = effective_lags[0]\n",
    "    effective_n = len(mcmc_samples) / (2 * autocorr_time + 1)\n",
    "    print(f\"Autocorrelation time: {autocorr_time}\")\n",
    "    print(f\"Effective sample size: {effective_n:.0f}\")\n",
    "else:\n",
    "    print(\"High autocorrelation - need longer chain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Root Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, root, curve_fit, differential_evolution\n",
    "\n",
    "# Function optimization\n",
    "print(\"=== Function Optimization ===\")\n",
    "\n",
    "# Define test functions\n",
    "def rosenbrock(x):\n",
    "    \"\"\"Rosenbrock function - classic optimization test case\"\"\"\n",
    "    return np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)\n",
    "\n",
    "def himmelblau(x):\n",
    "    \"\"\"Himmelblau's function - has four global minima\"\"\"\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "\n",
    "# Optimize Rosenbrock function\n",
    "print(\"Rosenbrock function optimization:\")\n",
    "x0 = np.array([0.0, 0.0])  # Starting point\n",
    "result_rosenbrock = minimize(rosenbrock, x0, method='BFGS')\n",
    "\n",
    "print(f\"Success: {result_rosenbrock.success}\")\n",
    "print(f\"Minimum found at: {result_rosenbrock.x}\")\n",
    "print(f\"Function value: {result_rosenbrock.fun:.8f}\")\n",
    "print(f\"Number of iterations: {result_rosenbrock.nit}\")\n",
    "\n",
    "# Compare different optimization methods\n",
    "print(\"\\nComparison of optimization methods for Himmelblau function:\")\n",
    "methods = ['BFGS', 'CG', 'Powell', 'Nelder-Mead']\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "for method in methods:\n",
    "    result = minimize(himmelblau, x0, method=method)\n",
    "    print(f\"{method:12s}: x={result.x}, f={result.fun:.6f}, nit={result.nit:3d}\")\n",
    "\n",
    "# Global optimization with differential evolution\n",
    "print(\"\\nGlobal optimization (Differential Evolution):\")\n",
    "bounds = [(-5, 5), (-5, 5)]  # Search bounds\n",
    "result_global = differential_evolution(himmelblau, bounds, seed=42)\n",
    "print(f\"Global minimum: x={result_global.x}, f={result_global.fun:.6f}\")\n",
    "\n",
    "# Constrained optimization\n",
    "print(\"\\n=== Constrained Optimization ===\")\n",
    "\n",
    "def objective(x):\n",
    "    \"\"\"Objective function to minimize\"\"\"\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def constraint1(x):\n",
    "    \"\"\"Equality constraint: x[0] + x[1] = 1\"\"\"\n",
    "    return x[0] + x[1] - 1\n",
    "\n",
    "def constraint2(x):\n",
    "    \"\"\"Inequality constraint: x[0] >= 0\"\"\"\n",
    "    return x[0]\n",
    "\n",
    "# Define constraints\n",
    "constraints = [\n",
    "    {'type': 'eq', 'fun': constraint1},\n",
    "    {'type': 'ineq', 'fun': constraint2}\n",
    "]\n",
    "\n",
    "x0 = np.array([0.5, 0.5])\n",
    "result_constrained = minimize(objective, x0, method='SLSQP', constraints=constraints)\n",
    "\n",
    "print(f\"Constrained minimum: x={result_constrained.x}\")\n",
    "print(f\"Objective value: {result_constrained.fun:.6f}\")\n",
    "print(f\"Constraint 1 (should be ~0): {constraint1(result_constrained.x):.6f}\")\n",
    "print(f\"Constraint 2 (should be ≥0): {constraint2(result_constrained.x):.6f}\")\n",
    "\n",
    "# Root finding\n",
    "print(\"\\n=== Root Finding ===\")\n",
    "\n",
    "def equations(vars):\n",
    "    \"\"\"System of nonlinear equations\"\"\"\n",
    "    x, y = vars\n",
    "    eq1 = x**2 + y**2 - 1  # Circle\n",
    "    eq2 = x - y**2         # Parabola\n",
    "    return [eq1, eq2]\n",
    "\n",
    "# Find roots\n",
    "initial_guess = [0.5, 0.5]\n",
    "solution = root(equations, initial_guess, method='hybr')\n",
    "\n",
    "print(f\"Root found: x={solution.x}\")\n",
    "print(f\"Function value at root: {equations(solution.x)}\")\n",
    "print(f\"Success: {solution.success}\")\n",
    "\n",
    "# Curve fitting\n",
    "print(\"\\n=== Curve Fitting ===\")\n",
    "\n",
    "# Generate synthetic data\n",
    "def model_function(x, a, b, c):\n",
    "    \"\"\"Exponential decay model\"\"\"\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "# True parameters\n",
    "true_params = [10.0, 0.5, 2.0]\n",
    "x_data = np.linspace(0, 5, 50)\n",
    "y_true = model_function(x_data, *true_params)\n",
    "y_data = y_true + 0.5 * np.random.randn(len(x_data))  # Add noise\n",
    "\n",
    "# Fit curve\n",
    "initial_guess = [8.0, 0.3, 1.5]  # Starting guess\n",
    "fitted_params, covariance = curve_fit(model_function, x_data, y_data, p0=initial_guess)\n",
    "\n",
    "# Calculate parameter uncertainties\n",
    "param_errors = np.sqrt(np.diag(covariance))\n",
    "\n",
    "print(\"Parameter fitting results:\")\n",
    "param_names = ['a', 'b', 'c']\n",
    "for i, (name, true_val, fitted_val, error) in enumerate(zip(param_names, true_params, fitted_params, param_errors)):\n",
    "    print(f\"{name}: true={true_val:.3f}, fitted={fitted_val:.3f}±{error:.3f}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "y_fitted = model_function(x_data, *fitted_params)\n",
    "ss_res = np.sum((y_data - y_fitted)**2)\n",
    "ss_tot = np.sum((y_data - np.mean(y_data))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n",
    "\n",
    "# Robust fitting (less sensitive to outliers)\n",
    "print(\"\\n=== Robust Curve Fitting ===\")\n",
    "\n",
    "# Add outliers to data\n",
    "y_data_outliers = y_data.copy()\n",
    "outlier_indices = [10, 25, 40]\n",
    "y_data_outliers[outlier_indices] += np.array([3, -4, 2])  # Add outliers\n",
    "\n",
    "def robust_objective(params, x, y):\n",
    "    \"\"\"Robust objective function using Huber loss\"\"\"\n",
    "    residuals = y - model_function(x, *params)\n",
    "    # Huber loss (less sensitive to outliers)\n",
    "    delta = 1.0\n",
    "    huber_loss = np.where(np.abs(residuals) <= delta,\n",
    "                         0.5 * residuals**2,\n",
    "                         delta * np.abs(residuals) - 0.5 * delta**2)\n",
    "    return np.sum(huber_loss)\n",
    "\n",
    "# Fit with and without robust method\n",
    "normal_fit, _ = curve_fit(model_function, x_data, y_data_outliers, p0=initial_guess)\n",
    "robust_result = minimize(robust_objective, initial_guess, args=(x_data, y_data_outliers))\n",
    "robust_fit = robust_result.x\n",
    "\n",
    "print(\"Comparison with outliers present:\")\n",
    "for i, (name, true_val, normal_val, robust_val) in enumerate(zip(param_names, true_params, normal_fit, robust_fit)):\n",
    "    print(f\"{name}: true={true_val:.3f}, normal={normal_val:.3f}, robust={robust_val:.3f}\")\n",
    "\n",
    "# Calculate RMSE for both methods\n",
    "y_normal = model_function(x_data, *normal_fit)\n",
    "y_robust = model_function(x_data, *robust_fit)\n",
    "rmse_normal = np.sqrt(np.mean((y_true - y_normal)**2))\n",
    "rmse_robust = np.sqrt(np.mean((y_true - y_robust)**2))\n",
    "\n",
    "print(f\"\\nRMSE vs true function:\")\n",
    "print(f\"Normal fit: {rmse_normal:.4f}\")\n",
    "print(f\"Robust fit: {rmse_robust:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Optimization and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "\n",
    "# Memory-efficient data types\n",
    "print(\"=== Memory Optimization ===\")\n",
    "\n",
    "# Compare memory usage of different data types\n",
    "n = 1000000\n",
    "data_types = {\n",
    "    'float64': np.float64,\n",
    "    'float32': np.float32,\n",
    "    'float16': np.float16,\n",
    "    'int64': np.int64,\n",
    "    'int32': np.int32,\n",
    "    'int16': np.int16,\n",
    "    'int8': np.int8\n",
    "}\n",
    "\n",
    "print(f\"Memory usage for {n:,} elements:\")\n",
    "for dtype_name, dtype in data_types.items():\n",
    "    array = np.ones(n, dtype=dtype)\n",
    "    memory_mb = array.nbytes / (1024**2)\n",
    "    print(f\"{dtype_name:8s}: {memory_mb:6.1f} MB\")\n",
    "\n",
    "# Sparse matrices for memory efficiency\n",
    "print(\"\\n=== Sparse Matrices ===\")\n",
    "\n",
    "# Create a large sparse matrix\n",
    "matrix_size = 10000\n",
    "density = 0.001  # 0.1% non-zero elements\n",
    "\n",
    "# Generate random sparse matrix\n",
    "n_nonzero = int(matrix_size**2 * density)\n",
    "rows = np.random.randint(0, matrix_size, n_nonzero)\n",
    "cols = np.random.randint(0, matrix_size, n_nonzero)\n",
    "data = np.random.randn(n_nonzero)\n",
    "\n",
    "# Create sparse matrix\n",
    "sparse_matrix = coo_matrix((data, (rows, cols)), shape=(matrix_size, matrix_size))\n",
    "sparse_csr = sparse_matrix.tocsr()  # Convert to CSR format for efficient operations\n",
    "\n",
    "# Compare with dense matrix\n",
    "dense_matrix = sparse_matrix.toarray()\n",
    "\n",
    "print(f\"Matrix size: {matrix_size} × {matrix_size}\")\n",
    "print(f\"Density: {density:.1%}\")\n",
    "print(f\"Non-zero elements: {n_nonzero:,}\")\n",
    "print(f\"Dense matrix memory: {dense_matrix.nbytes / (1024**2):.1f} MB\")\n",
    "print(f\"Sparse matrix memory: {(sparse_csr.data.nbytes + sparse_csr.indices.nbytes + sparse_csr.indptr.nbytes) / (1024**2):.1f} MB\")\n",
    "print(f\"Memory savings: {100 * (1 - (sparse_csr.data.nbytes + sparse_csr.indices.nbytes + sparse_csr.indptr.nbytes) / dense_matrix.nbytes):.1f}%\")\n",
    "\n",
    "# Memory views and advanced indexing\n",
    "print(\"\\n=== Memory Views and Copy vs View ===\")\n",
    "\n",
    "# Create large array\n",
    "large_array = np.random.randn(1000, 1000)\n",
    "print(f\"Original array memory: {large_array.nbytes / (1024**2):.1f} MB\")\n",
    "\n",
    "# Different ways to access subarrays\n",
    "subarray_view = large_array[100:200, 200:300]  # Creates a view (no copy)\n",
    "subarray_copy = large_array[100:200, 200:300].copy()  # Creates a copy\n",
    "subarray_fancy = large_array[[100, 150, 199], :][:, [200, 250, 299]]  # Fancy indexing (creates copy)\n",
    "\n",
    "print(f\"Subarray view shares memory: {np.shares_memory(large_array, subarray_view)}\")\n",
    "print(f\"Subarray copy shares memory: {np.shares_memory(large_array, subarray_copy)}\")\n",
    "print(f\"Fancy indexed shares memory: {np.shares_memory(large_array, subarray_fancy)}\")\n",
    "\n",
    "# Memory mapping for large files\n",
    "print(\"\\n=== Memory Mapping ===\")\n",
    "\n",
    "# Create a large array and save to disk\n",
    "large_data = np.random.randn(5000, 1000).astype(np.float32)\n",
    "filename = 'large_data.npy'\n",
    "np.save(filename, large_data)\n",
    "\n",
    "# Load using memory mapping\n",
    "mmap_array = np.load(filename, mmap_mode='r')  # Read-only memory map\n",
    "\n",
    "print(f\"Original array size: {large_data.nbytes / (1024**2):.1f} MB\")\n",
    "print(f\"Memory mapped array uses minimal RAM\")\n",
    "print(f\"Can access data: {mmap_array[0, 0]:.3f}\")\n",
    "\n",
    "# Performance comparison: in-place vs copy operations\n",
    "print(\"\\n=== Performance Optimization ===\")\n",
    "\n",
    "# Create test arrays\n",
    "size = 1000000\n",
    "a = np.random.randn(size)\n",
    "b = np.random.randn(size)\n",
    "c = np.zeros(size)\n",
    "\n",
    "# Method 1: Copy operation\n",
    "start_time = time.time()\n",
    "result1 = a + b * 2.0\n",
    "copy_time = time.time() - start_time\n",
    "\n",
    "# Method 2: In-place operation\n",
    "start_time = time.time()\n",
    "np.multiply(b, 2.0, out=c)  # b * 2.0 -> c\n",
    "np.add(a, c, out=c)         # a + c -> c\n",
    "inplace_time = time.time() - start_time\n",
    "\n",
    "print(f\"Copy operation time: {copy_time:.4f} seconds\")\n",
    "print(f\"In-place operation time: {inplace_time:.4f} seconds\")\n",
    "print(f\"Speedup: {copy_time / inplace_time:.1f}x\")\n",
    "print(f\"Results are equal: {np.allclose(result1, c)}\")\n",
    "\n",
    "# Vectorization vs loops\n",
    "print(\"\\n=== Vectorization Performance ===\")\n",
    "\n",
    "def compute_distances_loop(points):\n",
    "    \"\"\"Compute pairwise distances using loops\"\"\"\n",
    "    n = len(points)\n",
    "    distances = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distances[i, j] = np.sqrt(np.sum((points[i] - points[j])**2))\n",
    "    return distances\n",
    "\n",
    "def compute_distances_vectorized(points):\n",
    "    \"\"\"Compute pairwise distances using broadcasting\"\"\"\n",
    "    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n",
    "    return np.sqrt(np.sum(diff**2, axis=2))\n",
    "\n",
    "# Test with small dataset for timing\n",
    "test_points = np.random.randn(100, 3)\n",
    "\n",
    "# Loop version\n",
    "start_time = time.time()\n",
    "dist_loop = compute_distances_loop(test_points)\n",
    "loop_time = time.time() - start_time\n",
    "\n",
    "# Vectorized version\n",
    "start_time = time.time()\n",
    "dist_vectorized = compute_distances_vectorized(test_points)\n",
    "vectorized_time = time.time() - start_time\n",
    "\n",
    "print(f\"Loop-based computation: {loop_time:.4f} seconds\")\n",
    "print(f\"Vectorized computation: {vectorized_time:.4f} seconds\")\n",
    "print(f\"Speedup: {loop_time / vectorized_time:.1f}x\")\n",
    "print(f\"Results are equal: {np.allclose(dist_loop, dist_vectorized)}\")\n",
    "\n",
    "# NumPy vs pure Python performance\n",
    "print(\"\\n=== NumPy vs Pure Python ===\")\n",
    "\n",
    "def python_sum_squares(arr):\n",
    "    \"\"\"Pure Python implementation\"\"\"\n",
    "    return sum(x**2 for x in arr)\n",
    "\n",
    "def numpy_sum_squares(arr):\n",
    "    \"\"\"NumPy implementation\"\"\"\n",
    "    return np.sum(arr**2)\n",
    "\n",
    "# Test data\n",
    "test_data = np.random.randn(100000)\n",
    "python_list = test_data.tolist()\n",
    "\n",
    "# Python version\n",
    "start_time = time.time()\n",
    "result_python = python_sum_squares(python_list)\n",
    "python_time = time.time() - start_time\n",
    "\n",
    "# NumPy version\n",
    "start_time = time.time()\n",
    "result_numpy = numpy_sum_squares(test_data)\n",
    "numpy_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pure Python: {python_time:.4f} seconds\")\n",
    "print(f\"NumPy: {numpy_time:.4f} seconds\")\n",
    "print(f\"NumPy speedup: {python_time / numpy_time:.1f}x\")\n",
    "print(f\"Results match: {abs(result_python - result_numpy) < 1e-10}\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "\n",
    "print(\"\\n=== Memory and Performance Summary ===\")\n",
    "print(\"1. Use appropriate data types (float32 vs float64, int32 vs int64)\")\n",
    "print(\"2. Leverage sparse matrices for sparse data\")\n",
    "print(\"3. Understand views vs copies\")\n",
    "print(\"4. Use memory mapping for large datasets\")\n",
    "print(\"5. Prefer in-place operations when possible\")\n",
    "print(\"6. Vectorize operations instead of loops\")\n",
    "print(\"7. NumPy is typically 10-100x faster than pure Python\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}