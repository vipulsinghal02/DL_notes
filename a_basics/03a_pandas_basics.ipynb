{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Basics for Scientists\n",
    "\n",
    "Data manipulation and analysis with pandas - essential concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display more columns and rows\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames and Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Series:\n",
      "Mon    23.1\n",
      "Tue    24.5\n",
      "Wed    22.8\n",
      "Thu    25.2\n",
      "Fri    26.1\n",
      "dtype: float64\n",
      "\n",
      "Wednesday temp: 22.8\n",
      "\n",
      "Experiment DataFrame:\n",
      "  sample_id  concentration  temperature   ph\n",
      "0        A1           0.10         25.0  7.2\n",
      "1        A2           0.20         25.5  7.4\n",
      "2        A3           0.15         24.8  7.1\n",
      "3        B1           0.30         26.2  6.8\n",
      "4        B2           0.25         25.9  7.0\n"
     ]
    }
   ],
   "source": [
    "# Series (1D labeled array)\n",
    "temperatures = pd.Series([23.1, 24.5, 22.8, 25.2, 26.1], \n",
    "                        index=['Mon', 'Tue', 'Wed', 'Thu', 'Fri'])\n",
    "print(\"Temperature Series:\")\n",
    "print(temperatures)\n",
    "print(f\"\\nWednesday temp: {temperatures['Wed']}\")\n",
    "\n",
    "# DataFrame from dictionary\n",
    "experiment_data = {\n",
    "    'sample_id': ['A1', 'A2', 'A3', 'B1', 'B2'],\n",
    "    'concentration': [0.1, 0.2, 0.15, 0.3, 0.25],\n",
    "    'temperature': [25.0, 25.5, 24.8, 26.2, 25.9],\n",
    "    'ph': [7.2, 7.4, 7.1, 6.8, 7.0]\n",
    "}\n",
    "\n",
    "# for data frame, create a dict with values as lists of same length. keys become column headers. \n",
    "df = pd.DataFrame(experiment_data)\n",
    "print(\"\\nExperiment DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (5, 4)\n",
      "\n",
      "Column names: ['sample_id', 'concentration', 'temperature', 'ph']\n",
      "\n",
      "Column names: Index(['sample_id', 'concentration', 'temperature', 'ph'], dtype='object')\n",
      "\n",
      "Data types:\n",
      "sample_id         object\n",
      "concentration    float64\n",
      "temperature      float64\n",
      "ph               float64\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "  sample_id  concentration  temperature   ph\n",
      "0        A1           0.10         25.0  7.2\n",
      "1        A2           0.20         25.5  7.4\n",
      "2        A3           0.15         24.8  7.1\n",
      "  sample_id  concentration  temperature   ph\n",
      "2        A3           0.15         24.8  7.1\n",
      "3        B1           0.30         26.2  6.8\n",
      "4        B2           0.25         25.9  7.0\n",
      "\n",
      "Summary statistics:\n",
      "       concentration  temperature        ph\n",
      "count       5.000000     5.000000  5.000000\n",
      "mean        0.200000    25.480000  7.100000\n",
      "std         0.079057     0.589067  0.223607\n",
      "min         0.100000    24.800000  6.800000\n",
      "25%         0.150000    25.000000  7.000000\n",
      "50%         0.200000    25.500000  7.100000\n",
      "75%         0.250000    25.900000  7.200000\n",
      "max         0.300000    26.200000  7.400000\n"
     ]
    }
   ],
   "source": [
    "# DataFrame info\n",
    "print(\"DataFrame shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nColumn names:\", df.columns)\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# First/last rows\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "print(df.tail(3))\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe()) #!! this one is useful! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:\n",
      "\n",
      "  sample_id  concentration  temperature   ph\n",
      "0        A1           0.10         25.0  7.2\n",
      "1        A2           0.20         25.5  7.4\n",
      "2        A3           0.15         24.8  7.1\n",
      "3        B1           0.30         26.2  6.8\n",
      "4        B2           0.25         25.9  7.0\n",
      "Temperature column:\n",
      "0    25.0\n",
      "1    25.5\n",
      "2    24.8\n",
      "3    26.2\n",
      "4    25.9\n",
      "Name: temperature, dtype: float64\n",
      "Temperature column:\n",
      "0    25.0\n",
      "1    25.5\n",
      "2    24.8\n",
      "3    26.2\n",
      "4    25.9\n",
      "Name: temperature, dtype: float64\n",
      "\n",
      "Temperature and pH:\n",
      "   temperature   ph\n",
      "0         25.0  7.2\n",
      "1         25.5  7.4\n",
      "2         24.8  7.1\n",
      "3         26.2  6.8\n",
      "4         25.9  7.0\n",
      "\n",
      "Conc and pH:\n",
      "   concentration   ph\n",
      "0           0.10  7.2\n",
      "1           0.20  7.4\n",
      "2           0.15  7.1\n",
      "3           0.30  6.8\n",
      "4           0.25  7.0\n",
      "\n",
      "First row:\n",
      "sample_id          A1\n",
      "concentration     0.1\n",
      "temperature      25.0\n",
      "ph                7.2\n",
      "Name: 0, dtype: object\n",
      "\n",
      "High concentration samples:\n",
      "  sample_id  concentration  temperature   ph\n",
      "3        B1           0.30         26.2  6.8\n",
      "4        B2           0.25         25.9  7.0\n",
      "\n",
      "High temp AND high pH:\n",
      "  sample_id  concentration  temperature   ph\n",
      "1        A2            0.2         25.5  7.4\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe:\\n\")\n",
    "print(df)\n",
    "\n",
    "# Column selection\n",
    "print(\"Temperature column:\")\n",
    "print(df['temperature'])\n",
    "\n",
    "# Column selection by index\n",
    "print(\"Temperature column:\")\n",
    "print(df.iloc[:,2])\n",
    "\n",
    "# Multiple columns\n",
    "print(\"\\nTemperature and pH:\")\n",
    "print(df[['temperature', 'ph']])\n",
    "\n",
    "# Multiple columns\n",
    "print(\"\\nConc and pH:\")\n",
    "print(df.iloc[:,[1,3]])\n",
    "\n",
    "# Row selection by index\n",
    "print(\"\\nFirst row:\")\n",
    "print(df.iloc[0])  # by position #!! note how to use index to get row. \n",
    "\n",
    "# Row selection by condition\n",
    "print(\"\\nHigh concentration samples:\")\n",
    "high_conc = df[df['concentration'] > 0.2]\n",
    "print(high_conc)\n",
    "\n",
    "# Multiple conditions\n",
    "filtered = df[(df['temperature'] > 25) & (df['ph'] > 7)]\n",
    "print(\"\\nHigh temp AND high pH:\")\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with new columns:\n",
      "  sample_id  concentration  temperature   ph  temp_fahrenheit ph_category\n",
      "0        A1           0.10         25.0  7.2            77.00       basic\n",
      "1        A2           0.20         25.5  7.4            77.90       basic\n",
      "2        A3           0.15         24.8  7.1            76.64       basic\n",
      "3        B1           0.30         26.2  6.8            79.16      acidic\n",
      "4        B2           0.25         25.9  7.0            78.62     neutral\n",
      "\n",
      "Sorted by concentration:\n",
      "  sample_id  concentration  temperature   ph  temp_fahrenheit ph_category\n",
      "0        A1           0.10         25.0  7.2            77.00       basic\n",
      "2        A3           0.15         24.8  7.1            76.64       basic\n",
      "1        A2           0.20         25.5  7.4            77.90       basic\n",
      "4        B2           0.25         25.9  7.0            78.62     neutral\n",
      "3        B1           0.30         26.2  6.8            79.16      acidic\n",
      "\n",
      "Grouped by pH category:\n",
      "ph_category\n",
      "acidic     26.2\n",
      "basic      25.1\n",
      "neutral    25.9\n",
      "Name: temperature, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Adding new columns\n",
    "df['temp_fahrenheit'] = df['temperature'] * 9/5 + 32\n",
    "df['ph_category'] = df['ph'].apply(lambda x: 'acidic' if x < 7 else 'neutral' if x == 7 else 'basic')\n",
    "#!! this was cool. lambda functions on df via apply (and conditionals)\n",
    "print(\"DataFrame with new columns:\")\n",
    "print(df)\n",
    "\n",
    "# Sorting\n",
    "print(\"\\nSorted by concentration:\")\n",
    "print(df.sort_values('concentration'))\n",
    "\n",
    "# Grouping\n",
    "print(\"\\nGrouped by pH category:\")\n",
    "grouped = df.groupby('ph_category')['temperature'].mean()\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sort_values in module pandas.core.frame:\n",
      "\n",
      "sort_values(by: 'IndexLabel', *, axis: 'Axis' = 0, ascending: 'bool | list[bool] | tuple[bool, ...]' = True, inplace: 'bool' = False, kind: 'SortKind' = 'quicksort', na_position: 'str' = 'last', ignore_index: 'bool' = False, key: 'ValueKeyFunc | None' = None) -> 'DataFrame | None' method of pandas.core.frame.DataFrame instance\n",
      "    Sort by the values along either axis.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    by : str or list of str\n",
      "        Name or list of names to sort by.\n",
      "    \n",
      "        - if `axis` is 0 or `'index'` then `by` may contain index\n",
      "          levels and/or column labels.\n",
      "        - if `axis` is 1 or `'columns'` then `by` may contain column\n",
      "          levels and/or index labels.\n",
      "    axis : \"{0 or 'index', 1 or 'columns'}\", default 0\n",
      "         Axis to be sorted.\n",
      "    ascending : bool or list of bool, default True\n",
      "         Sort ascending vs. descending. Specify list for multiple sort\n",
      "         orders.  If this is a list of bools, must match the length of\n",
      "         the by.\n",
      "    inplace : bool, default False\n",
      "         If True, perform operation in-place.\n",
      "    kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'\n",
      "         Choice of sorting algorithm. See also :func:`numpy.sort` for more\n",
      "         information. `mergesort` and `stable` are the only stable algorithms. For\n",
      "         DataFrames, this option is only applied when sorting on a single\n",
      "         column or label.\n",
      "    na_position : {'first', 'last'}, default 'last'\n",
      "         Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n",
      "         end.\n",
      "    ignore_index : bool, default False\n",
      "         If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      "    key : callable, optional\n",
      "        Apply the key function to the values\n",
      "        before sorting. This is similar to the `key` argument in the\n",
      "        builtin :meth:`sorted` function, with the notable difference that\n",
      "        this `key` function should be *vectorized*. It should expect a\n",
      "        ``Series`` and return a Series with the same shape as the input.\n",
      "        It will be applied to each column in `by` independently.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or None\n",
      "        DataFrame with sorted values or None if ``inplace=True``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.sort_index : Sort a DataFrame by the index.\n",
      "    Series.sort_values : Similar method for a Series.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({\n",
      "    ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n",
      "    ...     'col2': [2, 1, 9, 8, 7, 4],\n",
      "    ...     'col3': [0, 1, 9, 4, 2, 3],\n",
      "    ...     'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n",
      "    ... })\n",
      "    >>> df\n",
      "      col1  col2  col3 col4\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    2    B     9     9    c\n",
      "    3  NaN     8     4    D\n",
      "    4    D     7     2    e\n",
      "    5    C     4     3    F\n",
      "    \n",
      "    Sort by col1\n",
      "    \n",
      "    >>> df.sort_values(by=['col1'])\n",
      "      col1  col2  col3 col4\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    2    B     9     9    c\n",
      "    5    C     4     3    F\n",
      "    4    D     7     2    e\n",
      "    3  NaN     8     4    D\n",
      "    \n",
      "    Sort by multiple columns\n",
      "    \n",
      "    >>> df.sort_values(by=['col1', 'col2'])\n",
      "      col1  col2  col3 col4\n",
      "    1    A     1     1    B\n",
      "    0    A     2     0    a\n",
      "    2    B     9     9    c\n",
      "    5    C     4     3    F\n",
      "    4    D     7     2    e\n",
      "    3  NaN     8     4    D\n",
      "    \n",
      "    Sort Descending\n",
      "    \n",
      "    >>> df.sort_values(by='col1', ascending=False)\n",
      "      col1  col2  col3 col4\n",
      "    4    D     7     2    e\n",
      "    5    C     4     3    F\n",
      "    2    B     9     9    c\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    3  NaN     8     4    D\n",
      "    \n",
      "    Putting NAs first\n",
      "    \n",
      "    >>> df.sort_values(by='col1', ascending=False, na_position='first')\n",
      "      col1  col2  col3 col4\n",
      "    3  NaN     8     4    D\n",
      "    4    D     7     2    e\n",
      "    5    C     4     3    F\n",
      "    2    B     9     9    c\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    \n",
      "    Sorting with a key function\n",
      "    \n",
      "    >>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n",
      "       col1  col2  col3 col4\n",
      "    0    A     2     0    a\n",
      "    1    A     1     1    B\n",
      "    2    B     9     9    c\n",
      "    3  NaN     8     4    D\n",
      "    4    D     7     2    e\n",
      "    5    C     4     3    F\n",
      "    \n",
      "    Natural sort with the key argument,\n",
      "    using the `natsort <https://github.com/SethMMorton/natsort>` package.\n",
      "    \n",
      "    >>> df = pd.DataFrame({\n",
      "    ...    \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n",
      "    ...    \"value\": [10, 20, 30, 40, 50]\n",
      "    ... })\n",
      "    >>> df\n",
      "        time  value\n",
      "    0    0hr     10\n",
      "    1  128hr     20\n",
      "    2   72hr     30\n",
      "    3   48hr     40\n",
      "    4   96hr     50\n",
      "    >>> from natsort import index_natsorted\n",
      "    >>> df.sort_values(\n",
      "    ...     by=\"time\",\n",
      "    ...     key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n",
      "    ... )\n",
      "        time  value\n",
      "    0    0hr     10\n",
      "    3   48hr     40\n",
      "    2   72hr     30\n",
      "    4   96hr     50\n",
      "    1  128hr     20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.sort_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method groupby in module pandas.core.frame:\n",
      "\n",
      "groupby(by=None, axis: 'Axis | lib.NoDefault' = <no_default>, level: 'IndexLabel | None' = None, as_index: 'bool' = True, sort: 'bool' = True, group_keys: 'bool' = True, observed: 'bool | lib.NoDefault' = <no_default>, dropna: 'bool' = True) -> 'DataFrameGroupBy' method of pandas.core.frame.DataFrame instance\n",
      "    Group DataFrame using a mapper or by a Series of columns.\n",
      "    \n",
      "    A groupby operation involves some combination of splitting the\n",
      "    object, applying a function, and combining the results. This can be\n",
      "    used to group large amounts of data and compute operations on these\n",
      "    groups.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    by : mapping, function, label, pd.Grouper or list of such\n",
      "        Used to determine the groups for the groupby.\n",
      "        If ``by`` is a function, it's called on each value of the object's\n",
      "        index. If a dict or Series is passed, the Series or dict VALUES\n",
      "        will be used to determine the groups (the Series' values are first\n",
      "        aligned; see ``.align()`` method). If a list or ndarray of length\n",
      "        equal to the selected axis is passed (see the `groupby user guide\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups>`_),\n",
      "        the values are used as-is to determine the groups. A label or list\n",
      "        of labels may be passed to group by the columns in ``self``.\n",
      "        Notice that a tuple is interpreted as a (single) key.\n",
      "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "        Split along rows (0) or columns (1). For `Series` this parameter\n",
      "        is unused and defaults to 0.\n",
      "    \n",
      "        .. deprecated:: 2.1.0\n",
      "    \n",
      "            Will be removed and behave like axis=0 in a future version.\n",
      "            For ``axis=1``, do ``frame.T.groupby(...)`` instead.\n",
      "    \n",
      "    level : int, level name, or sequence of such, default None\n",
      "        If the axis is a MultiIndex (hierarchical), group by a particular\n",
      "        level or levels. Do not specify both ``by`` and ``level``.\n",
      "    as_index : bool, default True\n",
      "        Return object with group labels as the\n",
      "        index. Only relevant for DataFrame input. as_index=False is\n",
      "        effectively \"SQL-style\" grouped output. This argument has no effect\n",
      "        on filtrations (see the `filtrations in the user guide\n",
      "        <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration>`_),\n",
      "        such as ``head()``, ``tail()``, ``nth()`` and in transformations\n",
      "        (see the `transformations in the user guide\n",
      "        <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation>`_).\n",
      "    sort : bool, default True\n",
      "        Sort group keys. Get better performance by turning this off.\n",
      "        Note this does not influence the order of observations within each\n",
      "        group. Groupby preserves the order of rows within each group. If False,\n",
      "        the groups will appear in the same order as they did in the original DataFrame.\n",
      "        This argument has no effect on filtrations (see the `filtrations in the user guide\n",
      "        <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#filtration>`_),\n",
      "        such as ``head()``, ``tail()``, ``nth()`` and in transformations\n",
      "        (see the `transformations in the user guide\n",
      "        <https://pandas.pydata.org/docs/dev/user_guide/groupby.html#transformation>`_).\n",
      "    \n",
      "        .. versionchanged:: 2.0.0\n",
      "    \n",
      "            Specifying ``sort=False`` with an ordered categorical grouper will no\n",
      "            longer sort the values.\n",
      "    \n",
      "    group_keys : bool, default True\n",
      "        When calling apply and the ``by`` argument produces a like-indexed\n",
      "        (i.e. :ref:`a transform <groupby.transform>`) result, add group keys to\n",
      "        index to identify pieces. By default group keys are not included\n",
      "        when the result's index (and column) labels match the inputs, and\n",
      "        are included otherwise.\n",
      "    \n",
      "        .. versionchanged:: 1.5.0\n",
      "    \n",
      "           Warns that ``group_keys`` will no longer be ignored when the\n",
      "           result from ``apply`` is a like-indexed Series or DataFrame.\n",
      "           Specify ``group_keys`` explicitly to include the group keys or\n",
      "           not.\n",
      "    \n",
      "        .. versionchanged:: 2.0.0\n",
      "    \n",
      "           ``group_keys`` now defaults to ``True``.\n",
      "    \n",
      "    observed : bool, default False\n",
      "        This only applies if any of the groupers are Categoricals.\n",
      "        If True: only show observed values for categorical groupers.\n",
      "        If False: show all values for categorical groupers.\n",
      "    \n",
      "        .. deprecated:: 2.1.0\n",
      "    \n",
      "            The default value will change to True in a future version of pandas.\n",
      "    \n",
      "    dropna : bool, default True\n",
      "        If True, and if group keys contain NA values, NA values together\n",
      "        with row/column will be dropped.\n",
      "        If False, NA values will also be treated as the key in groups.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    pandas.api.typing.DataFrameGroupBy\n",
      "        Returns a groupby object that contains information about the groups.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    resample : Convenience method for frequency conversion and resampling\n",
      "        of time series.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    See the `user guide\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/groupby.html>`__ for more\n",
      "    detailed usage and examples, including splitting an object into groups,\n",
      "    iterating through groups, selecting a group, aggregation, and more.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
      "    ...                               'Parrot', 'Parrot'],\n",
      "    ...                    'Max Speed': [380., 370., 24., 26.]})\n",
      "    >>> df\n",
      "       Animal  Max Speed\n",
      "    0  Falcon      380.0\n",
      "    1  Falcon      370.0\n",
      "    2  Parrot       24.0\n",
      "    3  Parrot       26.0\n",
      "    >>> df.groupby(['Animal']).mean()\n",
      "            Max Speed\n",
      "    Animal\n",
      "    Falcon      375.0\n",
      "    Parrot       25.0\n",
      "    \n",
      "    **Hierarchical Indexes**\n",
      "    \n",
      "    We can groupby different levels of a hierarchical index\n",
      "    using the `level` parameter:\n",
      "    \n",
      "    >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n",
      "    ...           ['Captive', 'Wild', 'Captive', 'Wild']]\n",
      "    >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n",
      "    >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n",
      "    ...                   index=index)\n",
      "    >>> df\n",
      "                    Max Speed\n",
      "    Animal Type\n",
      "    Falcon Captive      390.0\n",
      "           Wild         350.0\n",
      "    Parrot Captive       30.0\n",
      "           Wild          20.0\n",
      "    >>> df.groupby(level=0).mean()\n",
      "            Max Speed\n",
      "    Animal\n",
      "    Falcon      370.0\n",
      "    Parrot       25.0\n",
      "    >>> df.groupby(level=\"Type\").mean()\n",
      "             Max Speed\n",
      "    Type\n",
      "    Captive      210.0\n",
      "    Wild         185.0\n",
      "    \n",
      "    We can also choose to include NA in group keys or not by setting\n",
      "    `dropna` parameter, the default setting is `True`.\n",
      "    \n",
      "    >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\n",
      "    >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n",
      "    \n",
      "    >>> df.groupby(by=[\"b\"]).sum()\n",
      "        a   c\n",
      "    b\n",
      "    1.0 2   3\n",
      "    2.0 2   5\n",
      "    \n",
      "    >>> df.groupby(by=[\"b\"], dropna=False).sum()\n",
      "        a   c\n",
      "    b\n",
      "    1.0 2   3\n",
      "    2.0 2   5\n",
      "    NaN 1   4\n",
      "    \n",
      "    >>> l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]]\n",
      "    >>> df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"])\n",
      "    \n",
      "    >>> df.groupby(by=\"a\").sum()\n",
      "        b     c\n",
      "    a\n",
      "    a   13.0   13.0\n",
      "    b   12.3  123.0\n",
      "    \n",
      "    >>> df.groupby(by=\"a\", dropna=False).sum()\n",
      "        b     c\n",
      "    a\n",
      "    a   13.0   13.0\n",
      "    b   12.3  123.0\n",
      "    NaN 12.3   33.0\n",
      "    \n",
      "    When using ``.apply()``, use ``group_keys`` to include or exclude the\n",
      "    group keys. The ``group_keys`` argument defaults to ``True`` (include).\n",
      "    \n",
      "    >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
      "    ...                               'Parrot', 'Parrot'],\n",
      "    ...                    'Max Speed': [380., 370., 24., 26.]})\n",
      "    >>> df.groupby(\"Animal\", group_keys=True)[['Max Speed']].apply(lambda x: x)\n",
      "              Max Speed\n",
      "    Animal\n",
      "    Falcon 0      380.0\n",
      "           1      370.0\n",
      "    Parrot 2       24.0\n",
      "           3       26.0\n",
      "    \n",
      "    >>> df.groupby(\"Animal\", group_keys=False)[['Max Speed']].apply(lambda x: x)\n",
      "       Max Speed\n",
      "    0      380.0\n",
      "    1      370.0\n",
      "    2       24.0\n",
      "    3       26.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with missing values:\n",
      "  sample  value1  value2\n",
      "0     C1     1.2     5.6\n",
      "1     C2     NaN     7.8\n",
      "2     C3     3.4     NaN\n",
      "3     C4     2.1     9.1\n",
      "4     C5     NaN     4.3\n",
      "\n",
      "Missing values per column:\n",
      "sample    0\n",
      "value1    2\n",
      "value2    1\n",
      "dtype: int64\n",
      "\n",
      "Filled with column means:\n",
      "  sample    value1  value2\n",
      "0     C1  1.200000     5.6\n",
      "1     C2  2.233333     7.8\n",
      "2     C3  3.400000     6.7\n",
      "3     C4  2.100000     9.1\n",
      "4     C5  2.233333     4.3\n",
      "\n",
      "Rows without missing values:\n",
      "  sample  value1  value2\n",
      "0     C1     1.2     5.6\n",
      "3     C4     2.1     9.1\n"
     ]
    }
   ],
   "source": [
    "# Create data with missing values\n",
    "messy_data = {\n",
    "    'sample': ['C1', 'C2', 'C3', 'C4', 'C5'],\n",
    "    'value1': [1.2, np.nan, 3.4, 2.1, np.nan],\n",
    "    'value2': [5.6, 7.8, np.nan, 9.1, 4.3]\n",
    "}\n",
    "messy_df = pd.DataFrame(messy_data)\n",
    "\n",
    "print(\"Data with missing values:\")\n",
    "print(messy_df)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(messy_df.isnull().sum())\n",
    "\n",
    "# Fill missing values\n",
    "filled_mean = messy_df.fillna(messy_df.mean(numeric_only=True))\n",
    "print(\"\\nFilled with column means:\")\n",
    "print(filled_mean)\n",
    "\n",
    "# Drop rows with missing values\n",
    "clean_df = messy_df.dropna()\n",
    "print(\"\\nRows without missing values:\")\n",
    "print(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from CSV:\n",
      "  sample_id  concentration  temperature   ph  temp_fahrenheit ph_category\n",
      "0        A1           0.10         25.0  7.2            77.00       basic\n",
      "1        A2           0.20         25.5  7.4            77.90       basic\n",
      "2        A3           0.15         24.8  7.1            76.64       basic\n",
      "3        B1           0.30         26.2  6.8            79.16      acidic\n",
      "4        B2           0.25         25.9  7.0            78.62     neutral\n",
      "\n",
      "Excel export requires openpyxl package\n"
     ]
    }
   ],
   "source": [
    "# Save DataFrame to CSV\n",
    "df.to_csv('experiment_results.csv', index=False)\n",
    "\n",
    "# Read CSV back\n",
    "loaded_df = pd.read_csv('experiment_results.csv')\n",
    "print(\"Loaded from CSV:\")\n",
    "print(loaded_df.head())\n",
    "\n",
    "# Save to Excel (if openpyxl is available)\n",
    "try:\n",
    "    df.to_excel('experiment_results.xlsx', index=False)\n",
    "    print(\"\\nSaved to Excel successfully\")\n",
    "except ImportError:\n",
    "    print(\"\\nExcel export requires openpyxl package\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove('experiment_results.csv')\n",
    "if os.path.exists('experiment_results.xlsx'):\n",
    "    os.remove('experiment_results.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation and Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of large dataset:\n",
      "  treatment timepoint  measurement  replicate\n",
      "0         C        T1     9.143908          1\n",
      "1         A        T2     8.515186          2\n",
      "2         C        T3     8.593312          3\n",
      "3         C        T1     5.720759          4\n",
      "4         A        T2     8.741050          5\n",
      "\n",
      "Grouped summary:\n",
      "                    measurement                \n",
      "                           mean       std count\n",
      "treatment timepoint                            \n",
      "A         T1          10.719955  1.053743     4\n",
      "          T2           9.673253  1.813746     3\n",
      "          T3           8.882344  2.019497     3\n",
      "B         T1           9.541099       NaN     1\n",
      "          T2          11.514123  2.832725     4\n",
      "          T3          10.111040  2.217714     3\n",
      "C         T1           9.157237  3.632748     5\n",
      "          T2           9.691925  1.607277     3\n",
      "          T3           9.914087  1.500883     4\n",
      "\n",
      "Pivot table:\n",
      "timepoint         T1         T2         T3\n",
      "treatment                                 \n",
      "A          10.719955   9.673253   8.882344\n",
      "B           9.541099  11.514123  10.111040\n",
      "C           9.157237   9.691925   9.914087\n"
     ]
    }
   ],
   "source": [
    "# Create larger dataset for aggregation\n",
    "np.random.seed(42)\n",
    "large_data = {\n",
    "    'treatment': np.random.choice(['A', 'B', 'C'], 30),\n",
    "    'timepoint': np.tile(['T1', 'T2', 'T3'], 10),\n",
    "    'measurement': np.random.normal(10, 2, 30),\n",
    "    'replicate': np.tile(range(1, 11), 3)\n",
    "}\n",
    "large_df = pd.DataFrame(large_data)\n",
    "\n",
    "print(\"Sample of large dataset:\")\n",
    "print(large_df.head())\n",
    "\n",
    "# Group by multiple columns\n",
    "summary = large_df.groupby(['treatment', 'timepoint']).agg({\n",
    "    'measurement': ['mean', 'std', 'count']\n",
    "})\n",
    "print(\"\\nGrouped summary:\")\n",
    "print(summary)\n",
    "\n",
    "# Pivot table\n",
    "pivot = large_df.pivot_table(\n",
    "    values='measurement', \n",
    "    index='treatment', \n",
    "    columns='timepoint', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "print(\"\\nPivot table:\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:\n",
      "  sample_id  concentration\n",
      "0        A1           0.10\n",
      "1        A2           0.20\n",
      "2        B1           0.30\n",
      "3        B2           0.25\n",
      "\n",
      "Results:\n",
      "  sample_id  absorbance\n",
      "0        A1        0.45\n",
      "1        A2        0.67\n",
      "2        B1        0.89\n",
      "3        C1        0.32\n",
      "\n",
      "Inner join:\n",
      "  sample_id  concentration  absorbance\n",
      "0        A1            0.1        0.45\n",
      "1        A2            0.2        0.67\n",
      "2        B1            0.3        0.89\n",
      "\n",
      "Left join:\n",
      "  sample_id  concentration  absorbance\n",
      "0        A1           0.10        0.45\n",
      "1        A2           0.20        0.67\n",
      "2        B1           0.30        0.89\n",
      "3        B2           0.25         NaN\n",
      "\n",
      "Concatenated samples:\n",
      "  sample_id  concentration\n",
      "0        A1           0.10\n",
      "1        A2           0.20\n",
      "2        B1           0.30\n",
      "3        B2           0.25\n",
      "4        C1           0.40\n",
      "5        C2           0.35\n"
     ]
    }
   ],
   "source": [
    "# Create two related datasets\n",
    "samples = pd.DataFrame({\n",
    "    'sample_id': ['A1', 'A2', 'B1', 'B2'],\n",
    "    'concentration': [0.1, 0.2, 0.3, 0.25]\n",
    "})\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'sample_id': ['A1', 'A2', 'B1', 'C1'],\n",
    "    'absorbance': [0.45, 0.67, 0.89, 0.32]\n",
    "})\n",
    "\n",
    "print(\"Samples:\")\n",
    "print(samples)\n",
    "print(\"\\nResults:\")\n",
    "print(results)\n",
    "\n",
    "# Inner join (only matching records)\n",
    "inner_merged = pd.merge(samples, results, on='sample_id', how='inner')\n",
    "print(\"\\nInner join:\")\n",
    "print(inner_merged)\n",
    "\n",
    "# Left join (all samples, matching results)\n",
    "left_merged = pd.merge(samples, results, on='sample_id', how='left')\n",
    "print(\"\\nLeft join:\")\n",
    "print(left_merged)\n",
    "\n",
    "# Concatenate DataFrames\n",
    "more_samples = pd.DataFrame({\n",
    "    'sample_id': ['C1', 'C2'],\n",
    "    'concentration': [0.4, 0.35]\n",
    "})\n",
    "all_samples = pd.concat([samples, more_samples], ignore_index=True)\n",
    "print(\"\\nConcatenated samples:\")\n",
    "print(all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series data:\n",
      "            temperature   humidity\n",
      "date                              \n",
      "2023-01-01    20.959966  44.918467\n",
      "2023-01-02    22.358226  70.996470\n",
      "2023-01-03    21.608343  58.222679\n",
      "2023-01-04    25.403287  55.896167\n",
      "2023-01-05    26.746368  71.797163\n",
      "2023-01-06    27.663245  51.017921\n",
      "2023-01-07    27.682997  68.347954\n",
      "2023-01-08    27.264993  62.965614\n",
      "2023-01-09    24.378502  49.621701\n",
      "2023-01-10    23.129568  59.241963\n",
      "\n",
      "Weekly averages:\n",
      "            temperature   humidity\n",
      "date                              \n",
      "2023-01-01    20.959966  44.918467\n",
      "2023-01-08    25.532494  62.749138\n",
      "2023-01-15    23.754035  54.431832\n",
      "\n",
      "With 3-day rolling average:\n",
      "            temperature   humidity  temp_rolling_3day\n",
      "date                                                 \n",
      "2023-01-06    27.663245  51.017921          26.604300\n",
      "2023-01-07    27.682997  68.347954          27.364204\n",
      "2023-01-08    27.264993  62.965614          27.537079\n",
      "2023-01-09    24.378502  49.621701          26.442164\n",
      "2023-01-10    23.129568  59.241963          24.924355\n"
     ]
    }
   ],
   "source": [
    "# Create time series data\n",
    "dates = pd.date_range('2023-01-01', periods=10, freq='D')\n",
    "time_series = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'temperature': np.random.normal(25, 3, 10),\n",
    "    'humidity': np.random.normal(60, 10, 10)\n",
    "})\n",
    "\n",
    "# Set date as index\n",
    "time_series.set_index('date', inplace=True) #!! row names\n",
    "print(\"Time series data:\")\n",
    "print(time_series)\n",
    "\n",
    "# Resample to weekly averages\n",
    "weekly = time_series.resample('W').mean()\n",
    "print(\"\\nWeekly averages:\")\n",
    "print(weekly)\n",
    "\n",
    "# Rolling window calculations\n",
    "time_series['temp_rolling_3day'] = time_series['temperature'].rolling(window=3).mean()\n",
    "print(\"\\nWith 3-day rolling average:\")\n",
    "print(time_series.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarray_dataex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
