{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Scikit-learn Advanced Techniques: A Mathematical Deep Dive into Production-Ready Machine Learning\n\nThis notebook explores advanced machine learning techniques essential for building robust, production-ready systems. We'll dive deep into the mathematical foundations of preprocessing, feature engineering, hyperparameter optimization, ensemble methods, and model deployment strategies.\n\n## Advanced Machine Learning: Beyond Basic Algorithms\n\nWhile understanding individual algorithms is crucial, **real-world machine learning** requires mastery of the entire pipeline - from raw data to deployed models. This notebook bridges the gap between academic understanding and practical implementation.\n\n### The Production ML Challenge\n\n**Research vs. Production**: Key differences that matter\n\n**Research Setting**:\n- Clean, well-curated datasets\n- Focus on algorithmic novelty\n- Single metric optimization\n- Unlimited computational resources\n\n**Production Setting**:\n- Messy, real-world data\n- Multiple competing objectives\n- Strict latency and resource constraints\n- Need for reliability and interpretability\n\n### Mathematical Framework for Advanced ML\n\n**Multi-Objective Optimization**: Real-world ML involves multiple, often conflicting objectives:\n$$\\min_{\\theta} \\mathbf{f}(\\theta) = \\begin{bmatrix} \n\\text{Prediction Error}(\\theta) \\\\\n\\text{Complexity}(\\theta) \\\\\n\\text{Latency}(\\theta) \\\\\n\\text{Memory}(\\theta)\n\\end{bmatrix}$$\n\n**Pareto Optimality**: A solution $\\theta^*$ is Pareto optimal if no other solution exists that improves one objective without worsening another.\n\n**Robust Optimization**: Account for uncertainty in data distribution:\n$$\\min_{\\theta} \\max_{P \\in \\mathcal{U}} \\mathbb{E}_{(x,y) \\sim P}[\\mathcal{L}(y, f_{\\theta}(x))]$$\n\nwhere $\\mathcal{U}$ is an uncertainty set around the training distribution.\n\n### Advanced Concepts We'll Master\n\n**1. Sophisticated Preprocessing**:\n- Handling mixed data types mathematically\n- Advanced imputation techniques\n- Feature engineering automation\n- Pipeline optimization\n\n**2. Hyperparameter Optimization**:\n- Bayesian optimization theory\n- Multi-fidelity optimization\n- Evolutionary strategies\n- Early stopping mathematics\n\n**3. Advanced Ensemble Methods**:\n- Stacking theory and practice\n- Dynamic ensemble selection\n- Meta-learning approaches\n- Uncertainty quantification\n\n**4. Model Interpretation**:\n- SHAP value mathematics\n- Counterfactual explanations\n- Causal inference integration\n- Global vs. local explanations\n\n**5. Robust Evaluation**:\n- Cross-validation variants\n- Statistical significance testing\n- A/B testing for ML\n- Fairness metrics\n\n**6. Production Deployment**:\n- Model versioning\n- Drift detection algorithms\n- Online learning systems\n- MLOps pipeline design\n\n### Why Advanced Techniques Matter\n\n**Statistical Considerations**:\n- **Multiple Testing**: When trying many models, we need correction for multiple comparisons\n- **Selection Bias**: Hyperparameter tuning can introduce bias if not done carefully\n- **Distribution Shift**: Models must handle changing data distributions\n\n**Computational Considerations**:\n- **Scalability**: Algorithms must work with large datasets\n- **Resource Constraints**: Memory and time limitations in production\n- **Real-time Requirements**: Sub-second prediction latency\n\n**Business Considerations**:\n- **Interpretability**: Stakeholders need to understand model decisions\n- **Fairness**: Models must not discriminate against protected groups\n- **Reliability**: Systems must be robust to failures and edge cases\n\n### Mathematical Foundations for Robustness\n\n**Generalization Theory**: Understanding when models will work on new data\n\n**PAC Learning**: A model is $(1-\\epsilon, 1-\\delta)$-PAC learnable if:\n$$P(\\text{Error} \\leq \\epsilon) \\geq 1 - \\delta$$\n\n**VC Dimension**: Measure of model complexity affecting generalization\n\n**Rademacher Complexity**: Tighter generalization bounds:\n$$\\text{Generalization Gap} \\leq 2\\mathcal{R}_n(\\mathcal{F}) + \\sqrt{\\frac{\\log(1/\\delta)}{2n}}$$\n\n**Domain Adaptation**: When training and test distributions differ:\n$$\\epsilon_{\\text{target}} \\leq \\epsilon_{\\text{source}} + d_{\\mathcal{H}\\Delta\\mathcal{H}}(D_s, D_t) + \\lambda$$\n\nwhere $d_{\\mathcal{H}\\Delta\\mathcal{H}}$ is the $\\mathcal{H}\\Delta\\mathcal{H}$-distance between domains.\n\n### The Advanced ML Mindset\n\n**Think in Systems**: Models are components in larger systems\n**Embrace Uncertainty**: Quantify and communicate prediction uncertainty\n**Plan for Failure**: Build robust systems that degrade gracefully\n**Iterate Rapidly**: Use techniques that enable fast experimentation\n**Monitor Continuously**: Implement systems to detect when models fail\n\nThis notebook will transform your understanding from basic algorithm application to sophisticated machine learning system design, providing both theoretical foundations and practical tools for building production-ready ML systems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, metrics, preprocessing, model_selection, pipeline, compose\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.inspection import permutation_importance, plot_partial_dependence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style and random seed\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Advanced Preprocessing and Feature Engineering: The Mathematical Art of Data Transformation\n\nPreprocessing is often considered mundane, but it's where the most critical decisions are made. Poor preprocessing can doom even the best algorithms, while sophisticated preprocessing can make simple algorithms work remarkably well.\n\n### The Mathematics of Mixed Data Types\n\nReal-world datasets rarely contain only numerical features. We must handle **heterogeneous data** systematically, and this requires understanding the mathematical properties of different data types.\n\n**Data Type Taxonomy**:\n1. **Numerical**: Continuous (real-valued) or discrete (integer-valued)\n2. **Categorical**: Nominal (no order) or ordinal (ordered)\n3. **Text**: Unstructured language data\n4. **Temporal**: Time-series or date features\n5. **Spatial**: Geographic coordinates, images\n\n### Advanced Imputation Theory\n\nMissing data is inevitable, and naive approaches can introduce significant bias. We need sophisticated mathematical frameworks for handling missingness.\n\n**Missingness Mechanisms**:\n\n**1. Missing Completely at Random (MCAR)**:\n$$P(\\text{Missing} | \\text{Observed}, \\text{Unobserved}) = P(\\text{Missing})$$\n- Missingness is independent of all variables\n- Simple imputation methods are unbiased\n\n**2. Missing at Random (MAR)**:\n$$P(\\text{Missing} | \\text{Observed}, \\text{Unobserved}) = P(\\text{Missing} | \\text{Observed})$$\n- Missingness depends only on observed variables\n- Can be handled with proper imputation\n\n**3. Missing Not at Random (MNAR)**:\n$$P(\\text{Missing} | \\text{Observed}, \\text{Unobserved}) \\neq P(\\text{Missing} | \\text{Observed})$$\n- Missingness depends on unobserved values\n- Requires domain knowledge or specialized methods\n\n### K-Nearest Neighbors Imputation Mathematics\n\n**Algorithm**: For missing value in sample $i$, feature $j$:\n1. **Distance calculation**: Find $k$ most similar samples with observed $x_j$\n$$d(i, l) = \\sqrt{\\sum_{m \\neq j} (x_{im} - x_{lm})^2}$$\n\n2. **Weighted imputation**: \n$$\\hat{x}_{ij} = \\frac{\\sum_{l \\in \\mathcal{N}_k(i)} w_{il} x_{lj}}{\\sum_{l \\in \\mathcal{N}_k(i)} w_{il}}$$\n\nwhere weights can be:\n- **Uniform**: $w_{il} = 1$\n- **Distance-based**: $w_{il} = \\frac{1}{d(i,l) + \\epsilon}$\n- **Gaussian**: $w_{il} = \\exp(-d(i,l)^2/2\\sigma^2)$\n\n**Advantages**:\n- Preserves local data structure\n- Handles non-linear relationships\n- Works with mixed data types\n\n**Challenges**:\n- Computational complexity: $O(n^2 d)$\n- Sensitive to scaling and distance metric choice\n- Can propagate errors when many features are missing\n\n### Multiple Imputation Framework\n\n**Theoretical Foundation**: Generate $m$ plausible imputed datasets, analyze each, then combine results.\n\n**Rubin's Rules**: For parameter estimate $\\hat{\\theta}$:\n$$\\hat{\\theta}_{\\text{pooled}} = \\frac{1}{m}\\sum_{i=1}^{m} \\hat{\\theta}_i$$\n\n**Variance calculation**:\n$$\\text{Var}(\\hat{\\theta}_{\\text{pooled}}) = \\bar{W} + \\left(1 + \\frac{1}{m}\\right)B$$\n\nwhere:\n- $\\bar{W} = \\frac{1}{m}\\sum_{i=1}^{m} W_i$ (within-imputation variance)\n- $B = \\frac{1}{m-1}\\sum_{i=1}^{m}(\\hat{\\theta}_i - \\hat{\\theta}_{\\text{pooled}})^2$ (between-imputation variance)\n\n### Advanced Categorical Encoding\n\n**One-Hot Encoding Issues**:\n- **Curse of dimensionality**: Creates $k$ features for $k$ categories\n- **Sparsity**: Most values are zero\n- **Memory explosion**: For high-cardinality categorical features\n\n**Target Encoding (Mean Encoding)**:\n$$\\text{encoded}_c = \\frac{\\sum_{i: x_i = c} y_i}{\\sum_{i: x_i = c} 1}$$\n\n**Regularized Target Encoding**: Prevent overfitting with smoothing:\n$$\\text{encoded}_c = \\frac{n_c \\cdot \\bar{y}_c + \\alpha \\cdot \\bar{y}_{\\text{global}}}{n_c + \\alpha}$$\n\nwhere $\\alpha$ is smoothing parameter, $n_c$ is count of category $c$.\n\n**Leave-One-Out Encoding**: Prevent data leakage:\n$$\\text{encoded}_{c,i} = \\frac{\\sum_{j \\neq i, x_j = c} y_j}{\\sum_{j \\neq i, x_j = c} 1}$$\n\n**Binary Encoding**: Logarithmic space complexity\n- Convert categories to binary representation\n- Creates $\\lceil \\log_2(k) \\rceil$ features instead of $k$\n\n### Advanced Scaling Techniques\n\n**Quantile Uniform Transformation**:\nTransform to uniform distribution over $[0,1]$:\n$$F(x) = \\frac{\\text{rank}(x) - 0.5}{n}$$\n\n**Quantile Normal Transformation**:\nTransform to standard normal distribution:\n$$\\Phi^{-1}(F(x))$$\n\nwhere $\\Phi^{-1}$ is inverse CDF of standard normal.\n\n**Power Transformations**:\n**Box-Cox**: $y(\\lambda) = \\begin{cases} \\frac{x^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\ \\log(x) & \\text{if } \\lambda = 0 \\end{cases}$\n\n**Yeo-Johnson**: Extension that handles negative values:\n$$y(\\lambda) = \\begin{cases} \n\\frac{(x+1)^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, x \\geq 0 \\\\\n\\log(x+1) & \\text{if } \\lambda = 0, x \\geq 0 \\\\\n-\\frac{((-x)+1)^{2-\\lambda} - 1}{2-\\lambda} & \\text{if } \\lambda \\neq 2, x < 0 \\\\\n-\\log((-x)+1) & \\text{if } \\lambda = 2, x < 0\n\\end{cases}$$\n\n### Automated Feature Engineering\n\n**Polynomial Feature Generation**:\nFor features $\\mathbf{x} = [x_1, x_2, ..., x_d]$, generate all terms up to degree $p$:\n$$\\phi(\\mathbf{x}) = \\{x_1^{a_1} x_2^{a_2} \\cdots x_d^{a_d} : a_1 + a_2 + \\cdots + a_d \\leq p\\}$$\n\n**Total number of features**: $\\binom{d + p}{p}$\n\n**Interaction Detection**: Use statistical tests or model-based approaches:\n- **Pearson correlation**: For linear interactions\n- **Mutual information**: For non-linear interactions\n- **ANOVA F-test**: For categorical-numerical interactions\n\n### Advanced Pipeline Design\n\n**Column Transformer Mathematics**: Apply different transformations to different feature subsets:\n$$\\mathbf{X}_{\\text{transformed}} = [\\mathbf{T}_1(\\mathbf{X}_1), \\mathbf{T}_2(\\mathbf{X}_2), ..., \\mathbf{T}_k(\\mathbf{X}_k)]$$\n\nwhere $\\mathbf{X}_i$ are feature subsets and $\\mathbf{T}_i$ are transformations.\n\n**Feature Union**: Combine multiple feature extraction methods:\n$$\\phi(\\mathbf{x}) = [\\phi_1(\\mathbf{x}), \\phi_2(\\mathbf{x}), ..., \\phi_k(\\mathbf{x})]$$\n\n### Handling Imbalanced Data\n\n**Mathematical Framework**: When classes are highly imbalanced, standard metrics fail.\n\n**SMOTE (Synthetic Minority Oversampling Technique)**:\nFor minority sample $\\mathbf{x}_i$:\n1. Find $k$ nearest minority neighbors\n2. Generate synthetic sample: $\\mathbf{x}_{\\text{new}} = \\mathbf{x}_i + \\lambda(\\mathbf{x}_{\\text{neighbor}} - \\mathbf{x}_i)$\n3. Where $\\lambda \\sim \\text{Uniform}[0,1]$\n\n**Borderline-SMOTE**: Focus on samples near decision boundary\n**ADASYN**: Adaptive density-based generation\n\n**Cost-Sensitive Learning**: Modify loss function:\n$$\\mathcal{L}_{\\text{weighted}} = \\sum_{i=1}^{n} w_{y_i} \\mathcal{L}(y_i, \\hat{y}_i)$$\n\nwhere $w_c$ is cost for misclassifying class $c$.\n\n### Feature Selection Mathematics\n\n**Filter Methods**: Statistical relationships between features and target\n\n**Mutual Information**: \n$$I(X; Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$$\n\n**Chi-Square Test**: For categorical features\n$$\\chi^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$\n\n**Wrapper Methods**: Use model performance\n\n**Recursive Feature Elimination**: Iteratively remove least important features\n1. Train model on all features\n2. Rank features by importance\n3. Remove least important feature\n4. Repeat until desired number of features\n\n**Embedded Methods**: Feature selection during model training\n- **L1 Regularization**: Automatic feature selection through sparsity\n- **Tree-based importance**: Use feature importance from tree models\n\n### Advanced Text Feature Engineering\n\n**TF-IDF Mathematics**:\n$$\\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t)$$\n\nwhere:\n- $\\text{TF}(t,d) = \\frac{\\text{count}(t,d)}{\\sum_{t' \\in d} \\text{count}(t',d)}$\n- $\\text{IDF}(t) = \\log \\frac{|D|}{|\\{d \\in D : t \\in d\\}|}$\n\n**Word Embeddings**: Dense vector representations\n- **Word2Vec**: Skip-gram and CBOW\n- **GloVe**: Global vectors for word representation\n- **FastText**: Subword information\n\n### Time Series Feature Engineering\n\n**Lag Features**: $X_{t-1}, X_{t-2}, ..., X_{t-p}$\n\n**Rolling Statistics**:\n- **Moving average**: $\\bar{X}_t^{(w)} = \\frac{1}{w}\\sum_{i=0}^{w-1} X_{t-i}$\n- **Moving standard deviation**: $\\sigma_t^{(w)} = \\sqrt{\\frac{1}{w}\\sum_{i=0}^{w-1}(X_{t-i} - \\bar{X}_t^{(w)})^2}$\n\n**Fourier Features**: Extract frequency components\n$$X(f) = \\sum_{t=0}^{N-1} x_t e^{-2\\pi i ft/N}$$\n\n### Feature Engineering Best Practices\n\n**Domain Knowledge Integration**: \n- Understand business context\n- Create features that match domain expertise\n- Validate feature engineering with subject matter experts\n\n**Computational Considerations**:\n- **Memory complexity**: Track feature explosion\n- **Computation time**: Consider real-time constraints\n- **Storage requirements**: Balance richness vs. efficiency\n\n**Validation Strategy**:\n- **Time-based splits**: For temporal data\n- **Group-based splits**: When samples are not independent\n- **Nested cross-validation**: When doing feature selection\n\nThis foundation sets the stage for sophisticated preprocessing that can dramatically improve model performance while maintaining computational efficiency."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complex synthetic dataset with various data types\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate base classification data\n",
    "X_base, y_base = make_classification(n_samples=1000, n_features=10, n_informative=5, \n",
    "                                   n_redundant=2, n_clusters_per_class=1, \n",
    "                                   random_state=42)\n",
    "\n",
    "# Create a more complex dataset with mixed data types\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "    # Numerical features\n",
    "    'age': np.random.normal(35, 10, n_samples),\n",
    "    'income': np.random.lognormal(10, 1, n_samples),\n",
    "    'score1': X_base[:, 0],\n",
    "    'score2': X_base[:, 1],\n",
    "    'score3': X_base[:, 2],\n",
    "    \n",
    "    # Categorical features\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n_samples, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    \n",
    "    # Ordinal feature\n",
    "    'rating': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.1, 0.2, 0.4, 0.2, 0.1]),\n",
    "    \n",
    "    # Text feature (simplified)\n",
    "    'text_length': np.random.poisson(50, n_samples),\n",
    "    \n",
    "    # Target variable\n",
    "    'target': y_base\n",
    "})\n",
    "\n",
    "# Introduce missing values\n",
    "missing_mask = np.random.random((n_samples, len(dataset.columns))) < 0.05\n",
    "for col in ['age', 'income', 'score1', 'category', 'education']:\n",
    "    mask = missing_mask[:, dataset.columns.get_loc(col)]\n",
    "    dataset.loc[mask, col] = np.nan\n",
    "\n",
    "print(\"Dataset overview:\")\n",
    "print(dataset.head())\n",
    "print(f\"\\nDataset shape: {dataset.shape}\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "# Advanced preprocessing pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separate features and target\n",
    "X = dataset.drop('target', axis=1)\n",
    "y = dataset['target']\n",
    "\n",
    "# Define column types\n",
    "numeric_features = ['age', 'income', 'score1', 'score2', 'score3', 'text_length']\n",
    "categorical_features = ['category', 'region', 'education']\n",
    "ordinal_features = ['rating']\n",
    "\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"Numeric: {numeric_features}\")\n",
    "print(f\"Categorical: {categorical_features}\")\n",
    "print(f\"Ordinal: {ordinal_features}\")\n",
    "\n",
    "# Create preprocessing pipelines for different feature types\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', RobustScaler())  # More robust to outliers than StandardScaler\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('ordinal', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# Combine all transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('ord', ordinal_transformer, ordinal_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any remaining columns\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "numeric_feature_names = numeric_features\n",
    "categorical_feature_names = list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features))\n",
    "ordinal_feature_names = ordinal_features\n",
    "\n",
    "feature_names = numeric_feature_names + categorical_feature_names + ordinal_feature_names\n",
    "\n",
    "print(f\"\\nPreprocessed data shape: {X_preprocessed.shape}\")\n",
    "print(f\"Number of features after preprocessing: {len(feature_names)}\")\n",
    "print(f\"\\nFirst few feature names: {feature_names[:10]}\")\n",
    "\n",
    "# Feature engineering: polynomial features for numeric data\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features for a subset of numeric features\n",
    "poly_features = ['score1', 'score2', 'score3']\n",
    "poly_indices = [numeric_features.index(feat) for feat in poly_features]\n",
    "\n",
    "poly_transformer = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "X_poly = poly_transformer.fit_transform(X_preprocessed[:, poly_indices])\n",
    "\n",
    "# Combine original features with polynomial features\n",
    "X_final = np.hstack([X_preprocessed, X_poly])\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {X_final.shape}\")\n",
    "print(f\"Added {X_poly.shape[1]} polynomial features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Advanced Hyperparameter Optimization: The Mathematics of Intelligent Search\n\nHyperparameter optimization is one of the most crucial yet under-appreciated aspects of machine learning. The difference between random search and sophisticated optimization can mean the difference between a mediocre model and state-of-the-art performance.\n\n### The Hyperparameter Optimization Problem\n\n**Mathematical Formulation**: \n$$\\lambda^* = \\arg\\min_{\\lambda \\in \\Lambda} \\mathcal{L}_{\\text{val}}(\\mathcal{A}(\\mathcal{D}_{\\text{train}}, \\lambda))$$\n\nWhere:\n- $\\lambda$ represents hyperparameters\n- $\\Lambda$ is the hyperparameter space\n- $\\mathcal{A}$ is the learning algorithm\n- $\\mathcal{L}_{\\text{val}}$ is validation loss\n\n**Key Challenges**:\n1. **Expensive objective function**: Each evaluation requires training a model\n2. **No gradients**: Can't use gradient-based optimization\n3. **Noisy evaluations**: Cross-validation introduces variance\n4. **Mixed variable types**: Continuous, discrete, categorical hyperparameters\n5. **High dimensionality**: Many hyperparameters to optimize simultaneously\n\n### Traditional Approaches and Their Limitations\n\n**Grid Search**: Exhaustive search over predefined grid\n$$\\mathcal{G} = \\{\\lambda_1^{(1)}, \\lambda_1^{(2)}, ...\\} \\times \\{\\lambda_2^{(1)}, \\lambda_2^{(2)}, ...\\} \\times ...$$\n\n**Computational Complexity**: $O(|V_1| \\times |V_2| \\times ... \\times |V_d|)$ where $|V_i|$ is number of values for parameter $i$.\n\n**Curse of Dimensionality**: With $d$ parameters and $n$ values each, we need $n^d$ evaluations.\n\n**Random Search**: Sample hyperparameters randomly\n$$\\lambda^{(i)} \\sim p(\\lambda)$$\n\n**Bergstra & Bengio (2012) Result**: Random search is more efficient than grid search when only a few hyperparameters matter.\n\n**Mathematical Intuition**: If only $k$ out of $d$ hyperparameters are important, random search effectively searches over the important subspace, while grid search wastes evaluations on irrelevant dimensions.\n\n### Bayesian Optimization: The Principled Approach\n\n**Key Insight**: Use all previous evaluations to inform where to search next.\n\n**Mathematical Framework**:\n1. **Surrogate Model**: $p(f(\\lambda) | \\mathcal{D}_{1:t})$ - probability model of objective function\n2. **Acquisition Function**: $\\alpha(\\lambda | \\mathcal{D}_{1:t})$ - utility of evaluating point $\\lambda$\n3. **Optimization**: $\\lambda_{t+1} = \\arg\\max_{\\lambda} \\alpha(\\lambda | \\mathcal{D}_{1:t})$\n\n### Gaussian Process Surrogate Models\n\n**Gaussian Process**: Collection of random variables, any finite subset has joint Gaussian distribution.\n\n**GP Regression**: Given observations $\\mathcal{D} = \\{(\\lambda_i, y_i)\\}_{i=1}^t$:\n\n**Prior**: $f(\\lambda) \\sim \\mathcal{GP}(m(\\lambda), k(\\lambda, \\lambda'))$\n\n**Posterior Mean**:\n$$\\mu_t(\\lambda) = m(\\lambda) + \\mathbf{k}_t(\\lambda)^T (\\mathbf{K}_t + \\sigma^2\\mathbf{I})^{-1} (\\mathbf{y}_t - \\mathbf{m}_t)$$\n\n**Posterior Variance**:\n$$\\sigma_t^2(\\lambda) = k(\\lambda, \\lambda) - \\mathbf{k}_t(\\lambda)^T (\\mathbf{K}_t + \\sigma^2\\mathbf{I})^{-1} \\mathbf{k}_t(\\lambda)$$\n\nWhere:\n- $\\mathbf{K}_t$ is kernel matrix: $[\\mathbf{K}_t]_{ij} = k(\\lambda_i, \\lambda_j)$\n- $\\mathbf{k}_t(\\lambda) = [k(\\lambda, \\lambda_1), ..., k(\\lambda, \\lambda_t)]^T$\n\n**Kernel Functions**:\n\n**RBF Kernel**: $k(\\lambda, \\lambda') = \\sigma_f^2 \\exp\\left(-\\frac{\\|\\lambda - \\lambda'\\|^2}{2\\ell^2}\\right)$\n- Smooth functions\n- Infinite differentiability\n\n**Matérn Kernel**: $k(\\lambda, \\lambda') = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\frac{\\sqrt{2\\nu}\\|\\lambda - \\lambda'\\|}{\\ell}\\right)^\\nu K_\\nu\\left(\\frac{\\sqrt{2\\nu}\\|\\lambda - \\lambda'\\|}{\\ell}\\right)$\n- Controls smoothness via $\\nu$\n- More flexible than RBF\n\n### Acquisition Functions\n\n**Expected Improvement (EI)**:\n$$\\alpha_{\\text{EI}}(\\lambda) = \\mathbb{E}[\\max(f(\\lambda) - f(\\lambda^+), 0)]$$\n\nwhere $f(\\lambda^+)$ is current best value.\n\n**Closed Form**: \n$$\\alpha_{\\text{EI}}(\\lambda) = (\\mu(\\lambda) - f(\\lambda^+))\\Phi(Z) + \\sigma(\\lambda)\\phi(Z)$$\n\nwhere $Z = \\frac{\\mu(\\lambda) - f(\\lambda^+)}{\\sigma(\\lambda)}$, $\\Phi$ is CDF, $\\phi$ is PDF of standard normal.\n\n**Upper Confidence Bound (UCB)**:\n$$\\alpha_{\\text{UCB}}(\\lambda) = \\mu(\\lambda) + \\beta_t \\sigma(\\lambda)$$\n\n**Theoretical Guarantees**: With appropriate $\\beta_t$, UCB has sublinear regret bounds.\n\n**Probability of Improvement (PI)**:\n$$\\alpha_{\\text{PI}}(\\lambda) = P(f(\\lambda) > f(\\lambda^+)) = \\Phi\\left(\\frac{\\mu(\\lambda) - f(\\lambda^+)}{\\sigma(\\lambda)}\\right)$$\n\n**Trade-offs**:\n- **EI**: Balances exploitation and exploration well\n- **UCB**: Strong theoretical guarantees\n- **PI**: Simple but can be too conservative\n\n### Multi-Fidelity Optimization\n\n**Problem**: Full model evaluation is expensive. Can we use cheaper approximations?\n\n**Hyperband Algorithm**: Successive halving with multiple brackets\n\n**Mathematical Framework**:\n- Start with many configurations, small budgets\n- Eliminate poor performers\n- Increase budget for survivors\n\n**Successive Halving**: Given budget $B$ and $n$ configurations:\n```\nfor i = 0 to log_η(n)-1:\n    n_i = floor(n / η^i)\n    B_i = B * η^i / n\n    Train each configuration for B_i budget\n    Keep top n_{i+1} = floor(n_i / η) configurations\n```\n\n**BOHB (Bayesian Optimization + Hyperband)**:\n- Use BO to select configurations for Hyperband\n- Combine theoretical guarantees of Hyperband with practical efficiency of BO\n\n### Advanced Acquisition Functions\n\n**Knowledge Gradient**: Value of information from next evaluation\n$$\\alpha_{\\text{KG}}(\\lambda) = \\mathbb{E}[\\max_{\\lambda'} \\mu_{t+1}(\\lambda') - \\max_{\\lambda'} \\mu_t(\\lambda') | \\lambda_{t+1} = \\lambda]$$\n\n**Entropy Search**: Reduce uncertainty about optimum location\n$$\\alpha_{\\text{ES}}(\\lambda) = H[p^*] - \\mathbb{E}[H[p^* | y]]$$\n\nwhere $p^*$ is distribution over optimum location.\n\n**Parallel Acquisition**: For batch evaluation\n$$\\alpha_{\\text{batch}}(\\{\\lambda_1, ..., \\lambda_q\\}) = \\mathbb{E}[\\max_i f(\\lambda_i) - f(\\lambda^+)]$$\n\n### Handling Categorical and Conditional Variables\n\n**Mixed Variable Spaces**: Combine continuous, discrete, and categorical hyperparameters.\n\n**Kernel Design**: \n$$k(\\lambda, \\lambda') = k_{\\text{cont}}(\\lambda_{\\text{cont}}, \\lambda'_{\\text{cont}}) \\times k_{\\text{cat}}(\\lambda_{\\text{cat}}, \\lambda'_{\\text{cat}})$$\n\n**Categorical Kernel**: \n$$k_{\\text{cat}}(\\lambda, \\lambda') = \\begin{cases} 1 & \\text{if } \\lambda = \\lambda' \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\n**Conditional Variables**: Some hyperparameters only matter for certain algorithms\n- Use indicator variables\n- Specialized kernels for hierarchical spaces\n\n### Population-Based Methods\n\n**Genetic Algorithms**: Evolutionary optimization\n\n**Selection**: Choose parents based on fitness\n$$p(\\text{select } i) \\propto \\text{fitness}(i)$$\n\n**Crossover**: Combine parent hyperparameters\n$$\\lambda_{\\text{child}} = \\alpha \\lambda_{\\text{parent1}} + (1-\\alpha) \\lambda_{\\text{parent2}}$$\n\n**Mutation**: Random perturbations\n$$\\lambda' = \\lambda + \\mathcal{N}(0, \\sigma^2)$$\n\n**Differential Evolution**: \n$$\\lambda_{\\text{trial}} = \\lambda_{\\text{target}} + F(\\lambda_{\\text{best}} - \\lambda_{\\text{target}}) + F(\\lambda_{\\text{r1}} - \\lambda_{\\text{r2}})$$\n\n**Population-Based Training (PBT)**:\n- Train population of models simultaneously\n- Periodically copy weights from better performers\n- Mutate hyperparameters of copied models\n\n### Multi-Objective Optimization\n\n**Real-world scenarios**: Often optimize multiple conflicting objectives\n\n**Pareto Dominance**: $\\lambda_1$ dominates $\\lambda_2$ if:\n$$f_i(\\lambda_1) \\leq f_i(\\lambda_2) \\text{ for all } i \\text{ and } f_j(\\lambda_1) < f_j(\\lambda_2) \\text{ for some } j$$\n\n**Hypervolume**: Measure of Pareto front quality\n$$HV = \\text{Volume}\\left(\\bigcup_{\\lambda \\in \\text{Pareto}} [\\mathbf{f}(\\lambda), \\mathbf{r}]\\right)$$\n\nwhere $\\mathbf{r}$ is reference point.\n\n**Multi-Objective Expected Improvement**:\n$$\\alpha_{\\text{MOEI}}(\\lambda) = \\mathbb{E}[HV(\\text{Pareto} \\cup \\{\\mathbf{f}(\\lambda)\\}) - HV(\\text{Pareto})]$$\n\n### Early Stopping and Learning Curves\n\n**Mathematical Formulation**: Predict final performance from partial learning curves.\n\n**Exponential Model**: \n$$\\text{performance}(t) = \\alpha - \\beta e^{-\\gamma t}$$\n\n**Power Law Model**:\n$$\\text{performance}(t) = \\alpha - \\beta t^{-\\gamma}$$\n\n**Bayesian Learning Curve Extrapolation**:\n- Fit probabilistic model to partial curves\n- Predict probability that run will achieve good final performance\n- Stop runs with low probability early\n\n### Practical Implementation Strategies\n\n**Warm Starting**: Use previous experiments to initialize BO\n- Transfer learning between related problems\n- Meta-learning for hyperparameter initialization\n\n**Multi-Task Optimization**: Share information across related tasks\n$$f_t(\\lambda) \\sim \\mathcal{GP}(0, k_t(\\lambda, \\lambda') + k_{\\text{task}}(t, t') k_{\\text{shared}}(\\lambda, \\lambda'))$$\n\n**Asynchronous Optimization**: Handle variable evaluation times\n- Use acquisition functions that account for pending evaluations\n- Thompson sampling for parallelization\n\n### Cost-Aware Optimization\n\n**Variable Cost Hyperparameters**: Some configurations are more expensive to evaluate\n\n**Cost Model**: $c(\\lambda)$ - predicted cost of evaluating $\\lambda$\n\n**Cost-Aware EI**:\n$$\\alpha_{\\text{EI/cost}}(\\lambda) = \\frac{\\alpha_{\\text{EI}}(\\lambda)}{c(\\lambda)^\\beta}$$\n\n**Multi-Armed Bandit Formulation**: Trade-off between reward and cost\n$$\\text{Utility}(\\lambda) = \\frac{\\text{Expected Reward}(\\lambda)}{\\text{Expected Cost}(\\lambda)}$$\n\n### Hyperparameter Optimization Best Practices\n\n**Search Space Design**:\n- Use log-scale for learning rates: $\\log(\\text{lr}) \\sim \\text{Uniform}(-5, -1)$\n- Consider parameter interactions\n- Include sensible bounds\n\n**Evaluation Strategy**:\n- Use consistent random seeds\n- Proper cross-validation setup\n- Account for computational budget\n\n**Multi-Level Optimization**:\n1. **Coarse search**: Broad exploration\n2. **Fine search**: Local optimization around promising regions\n3. **Final validation**: Careful evaluation of best configurations\n\nThis mathematical foundation enables intelligent, efficient hyperparameter optimization that can dramatically improve model performance while minimizing computational cost."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, cross_val_score, \n",
    "    StratifiedKFold, learning_curve, validation_curve\n",
    ")\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Quick model comparison with cross-validation\n",
    "cv_scores = {}\n",
    "cv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\n=== Initial Model Comparison (5-fold CV) ===\")\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring='roc_auc')\n",
    "    cv_scores[name] = scores\n",
    "    print(f\"{name:20s}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# Advanced hyperparameter tuning with RandomizedSearchCV\n",
    "print(\"\\n=== Hyperparameter Tuning (Random Forest) ===\")\n",
    "\n",
    "# Define parameter distributions for RandomizedSearchCV\n",
    "rf_param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=50,  # Number of parameter settings to sample\n",
    "    cv=3,  # Reduced for speed\n",
    "    scoring='roc_auc',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {rf_random.best_params_}\")\n",
    "print(f\"Best cross-validation score: {rf_random.best_score_:.4f}\")\n",
    "\n",
    "# Grid search for fine-tuning around best parameters\n",
    "print(\"\\n=== Fine-tuning with GridSearchCV ===\")\n",
    "\n",
    "# Define a smaller grid around the best parameters\n",
    "best_params = rf_random.best_params_\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [max(50, best_params['n_estimators']-20), \n",
    "                    best_params['n_estimators'], \n",
    "                    best_params['n_estimators']+20],\n",
    "    'max_depth': [max(3, best_params['max_depth']-2), \n",
    "                 best_params['max_depth'], \n",
    "                 best_params['max_depth']+2],\n",
    "    'min_samples_split': [best_params['min_samples_split']],\n",
    "    'min_samples_leaf': [best_params['min_samples_leaf']],\n",
    "    'max_features': [best_params['max_features']],\n",
    "    'bootstrap': [best_params['bootstrap']],\n",
    "    'class_weight': [best_params['class_weight']]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Fine-tuned best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Fine-tuned best score: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Learning curves\n",
    "print(\"\\n=== Learning Curves ===\")\n",
    "\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_rf, X_train, y_train, cv=3, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='Training score')\n",
    "plt.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.3)\n",
    "plt.plot(train_sizes, val_scores.mean(axis=1), 'o-', label='Validation score')\n",
    "plt.fill_between(train_sizes, val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.3)\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation curves (hyperparameter impact)\n",
    "param_range = range(10, 101, 10)\n",
    "train_scores_val, test_scores_val = validation_curve(\n",
    "    RandomForestClassifier(random_state=42), X_train, y_train,\n",
    "    param_name='n_estimators', param_range=param_range,\n",
    "    cv=3, scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(param_range, train_scores_val.mean(axis=1), 'o-', label='Training score')\n",
    "plt.fill_between(param_range, train_scores_val.mean(axis=1) - train_scores_val.std(axis=1),\n",
    "                train_scores_val.mean(axis=1) + train_scores_val.std(axis=1), alpha=0.3)\n",
    "plt.plot(param_range, test_scores_val.mean(axis=1), 'o-', label='Validation score')\n",
    "plt.fill_between(param_range, test_scores_val.mean(axis=1) - test_scores_val.std(axis=1),\n",
    "                test_scores_val.mean(axis=1) + test_scores_val.std(axis=1), alpha=0.3)\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.title('Validation Curve (n_estimators)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_classif, mutual_info_classif, RFE, \n",
    "    SelectFromModel, SequentialFeatureSelector\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Train the best model on full training data\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"=== Feature Importance Analysis ===\")\n",
    "\n",
    "# 1. Built-in feature importance (Random Forest)\n",
    "feature_importance_builtin = best_rf.feature_importances_\n",
    "\n",
    "# 2. Permutation importance (more reliable)\n",
    "perm_importance = permutation_importance(\n",
    "    best_rf, X_train, y_train, n_repeats=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': [f'Feature_{i}' for i in range(X_train.shape[1])],\n",
    "    'builtin_importance': feature_importance_builtin,\n",
    "    'permutation_importance': perm_importance.importances_mean,\n",
    "    'permutation_std': perm_importance.importances_std\n",
    "})\n",
    "\n",
    "# Sort by permutation importance\n",
    "importance_df = importance_df.sort_values('permutation_importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Plot feature importance comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot top 20 features\n",
    "top_features = importance_df.head(20)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.barh(range(len(top_features)), top_features['builtin_importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Built-in Importance')\n",
    "plt.title('Random Forest Built-in Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.barh(range(len(top_features)), top_features['permutation_importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Permutation Importance')\n",
    "plt.title('Permutation Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(top_features['builtin_importance'], top_features['permutation_importance'])\n",
    "plt.xlabel('Built-in Importance')\n",
    "plt.ylabel('Permutation Importance')\n",
    "plt.title('Importance Methods Comparison')\n",
    "plt.plot([0, max(top_features['builtin_importance'])], \n",
    "         [0, max(top_features['builtin_importance'])], 'r--', alpha=0.5)\n",
    "\n",
    "# Feature selection methods comparison\n",
    "print(\"\\n=== Feature Selection Methods Comparison ===\")\n",
    "\n",
    "# 1. Univariate feature selection\n",
    "selector_univariate = SelectKBest(score_func=f_classif, k=20)\n",
    "X_train_univariate = selector_univariate.fit_transform(X_train, y_train)\n",
    "X_test_univariate = selector_univariate.transform(X_test)\n",
    "\n",
    "# 2. Recursive Feature Elimination\n",
    "selector_rfe = RFE(RandomForestClassifier(n_estimators=50, random_state=42), n_features_to_select=20)\n",
    "X_train_rfe = selector_rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = selector_rfe.transform(X_test)\n",
    "\n",
    "# 3. Model-based selection\n",
    "selector_model = SelectFromModel(RandomForestClassifier(n_estimators=50, random_state=42), \n",
    "                                max_features=20)\n",
    "X_train_model = selector_model.fit_transform(X_train, y_train)\n",
    "X_test_model = selector_model.transform(X_test)\n",
    "\n",
    "# 4. Sequential Feature Selection (forward)\n",
    "selector_sequential = SequentialFeatureSelector(\n",
    "    RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    n_features_to_select=20, direction='forward', cv=3\n",
    ")\n",
    "X_train_sequential = selector_sequential.fit_transform(X_train, y_train)\n",
    "X_test_sequential = selector_sequential.transform(X_test)\n",
    "\n",
    "# Compare performance of different feature selection methods\n",
    "feature_selection_results = {}\n",
    "datasets = {\n",
    "    'All Features': (X_train, X_test),\n",
    "    'Univariate (F-test)': (X_train_univariate, X_test_univariate),\n",
    "    'RFE': (X_train_rfe, X_test_rfe),\n",
    "    'Model-based': (X_train_model, X_test_model),\n",
    "    'Sequential': (X_train_sequential, X_test_sequential)\n",
    "}\n",
    "\n",
    "for method_name, (X_tr, X_te) in datasets.items():\n",
    "    # Train and evaluate model\n",
    "    rf_temp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_temp.fit(X_tr, y_train)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_score = cross_val_score(rf_temp, X_tr, y_train, cv=3, scoring='roc_auc').mean()\n",
    "    \n",
    "    # Test score\n",
    "    test_pred = rf_temp.predict_proba(X_te)[:, 1]\n",
    "    test_score = roc_auc_score(y_test, test_pred)\n",
    "    \n",
    "    feature_selection_results[method_name] = {\n",
    "        'n_features': X_tr.shape[1],\n",
    "        'cv_score': cv_score,\n",
    "        'test_score': test_score\n",
    "    }\n",
    "    \n",
    "    print(f\"{method_name:20s}: {X_tr.shape[1]:3d} features, \"\n",
    "          f\"CV: {cv_score:.4f}, Test: {test_score:.4f}\")\n",
    "\n",
    "# Plot feature selection comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "methods = list(feature_selection_results.keys())\n",
    "test_scores = [feature_selection_results[method]['test_score'] for method in methods]\n",
    "n_features = [feature_selection_results[method]['n_features'] for method in methods]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(methods)))\n",
    "for i, (method, score, n_feat) in enumerate(zip(methods, test_scores, n_features)):\n",
    "    plt.scatter(n_feat, score, s=100, c=[colors[i]], label=method)\n",
    "\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Test ROC AUC')\n",
    "plt.title('Feature Selection Methods Comparison')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Evaluation and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    plot_confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Get predictions from the best model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_pred_proba = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== Comprehensive Model Evaluation ===\")\n",
    "\n",
    "# Basic metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Average Precision (better for imbalanced datasets)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
    "\n",
    "# Comprehensive evaluation plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(ax=axes[0, 0], cmap='Blues')\n",
    "axes[0, 0].set_title('Confusion Matrix')\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[0, 1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "axes[0, 2].plot(recall, precision, linewidth=2, label=f'PR Curve (AP = {avg_precision:.3f})')\n",
    "baseline = np.sum(y_test) / len(y_test)\n",
    "axes[0, 2].axhline(y=baseline, color='k', linestyle='--', linewidth=1, label=f'Baseline ({baseline:.3f})')\n",
    "axes[0, 2].set_xlabel('Recall')\n",
    "axes[0, 2].set_ylabel('Precision')\n",
    "axes[0, 2].set_title('Precision-Recall Curve')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Prediction Distribution\n",
    "axes[1, 0].hist(y_pred_proba[y_test == 0], bins=20, alpha=0.5, label='Class 0', density=True)\n",
    "axes[1, 0].hist(y_pred_proba[y_test == 1], bins=20, alpha=0.5, label='Class 1', density=True)\n",
    "axes[1, 0].axvline(x=0.5, color='red', linestyle='--', label='Threshold')\n",
    "axes[1, 0].set_xlabel('Predicted Probability')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_title('Prediction Distribution')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Calibration Plot\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "axes[1, 1].plot(mean_predicted_value, fraction_of_positives, \"s-\", linewidth=2, label='Random Forest')\n",
    "axes[1, 1].plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "axes[1, 1].set_xlabel('Mean Predicted Probability')\n",
    "axes[1, 1].set_ylabel('Fraction of Positives')\n",
    "axes[1, 1].set_title('Calibration Plot')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Threshold Analysis\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    if np.sum(y_pred_thresh) > 0:  # Avoid division by zero\n",
    "        prec = metrics.precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "        rec = metrics.recall_score(y_test, y_pred_thresh)\n",
    "        f1 = metrics.f1_score(y_test, y_pred_thresh)\n",
    "    else:\n",
    "        prec = rec = f1 = 0\n",
    "    \n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "axes[1, 2].plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "axes[1, 2].plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "axes[1, 2].plot(thresholds, f1_scores, label='F1-Score', linewidth=2)\n",
    "axes[1, 2].axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Default Threshold')\n",
    "axes[1, 2].set_xlabel('Threshold')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_title('Threshold Analysis')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model Interpretation: Partial Dependence Plots\n",
    "print(\"\\n=== Model Interpretation ===\")\n",
    "\n",
    "# Select top features for partial dependence plots\n",
    "top_feature_indices = importance_df.head(4).index[:4].tolist()\n",
    "\n",
    "# Note: plot_partial_dependence requires feature names or indices\n",
    "try:\n",
    "    from sklearn.inspection import PartialDependenceDisplay\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature_idx in enumerate(top_feature_indices):\n",
    "        PartialDependenceDisplay.from_estimator(\n",
    "            best_rf, X_train, features=[feature_idx], \n",
    "            ax=axes[i], feature_names=[f'Feature_{feature_idx}']\n",
    "        )\n",
    "        axes[i].set_title(f'Partial Dependence: Feature {feature_idx}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not create partial dependence plots: {e}\")\n",
    "\n",
    "# SHAP values for model explanation (if SHAP is available)\n",
    "try:\n",
    "    import shap\n",
    "    print(\"\\n=== SHAP Analysis ===\")\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.TreeExplainer(best_rf)\n",
    "    shap_values = explainer.shap_values(X_test[:100])  # Subset for speed\n",
    "    \n",
    "    # Summary plot\n",
    "    shap.summary_plot(shap_values[1], X_test[:100], \n",
    "                     feature_names=[f'Feature_{i}' for i in range(X_test.shape[1])],\n",
    "                     show=False)\n",
    "    plt.title('SHAP Summary Plot (Class 1)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "except Exception as e:\n",
    "    print(f\"SHAP analysis failed: {e}\")\n",
    "\n",
    "# Model reliability analysis\n",
    "print(\"\\n=== Model Reliability Analysis ===\")\n",
    "\n",
    "# Prediction confidence analysis\n",
    "confidence = np.abs(y_pred_proba - 0.5) * 2  # Distance from decision boundary\n",
    "high_confidence = confidence > 0.8\n",
    "medium_confidence = (confidence > 0.4) & (confidence <= 0.8)\n",
    "low_confidence = confidence <= 0.4\n",
    "\n",
    "print(f\"High confidence predictions: {np.sum(high_confidence)} ({np.mean(high_confidence):.1%})\")\n",
    "print(f\"Medium confidence predictions: {np.sum(medium_confidence)} ({np.mean(medium_confidence):.1%})\")\n",
    "print(f\"Low confidence predictions: {np.sum(low_confidence)} ({np.mean(low_confidence):.1%})\")\n",
    "\n",
    "# Accuracy by confidence level\n",
    "if np.sum(high_confidence) > 0:\n",
    "    high_conf_accuracy = metrics.accuracy_score(y_test[high_confidence], y_pred[high_confidence])\n",
    "    print(f\"\\nAccuracy on high confidence predictions: {high_conf_accuracy:.3f}\")\n",
    "\n",
    "if np.sum(low_confidence) > 0:\n",
    "    low_conf_accuracy = metrics.accuracy_score(y_test[low_confidence], y_pred[low_confidence])\n",
    "    print(f\"Accuracy on low confidence predictions: {low_conf_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Ensemble Methods and Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=== Advanced Ensemble Methods ===\")\n",
    "\n",
    "# Define base models for ensemble\n",
    "base_models = {\n",
    "    'rf': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'gb': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'svm': SVC(probability=True, random_state=42),\n",
    "    'lr': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train individual models and collect predictions\n",
    "individual_scores = {}\n",
    "for name, model in base_models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "    individual_scores[name] = scores.mean()\n",
    "    print(f\"{name:20s}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# 1. Voting Classifier (Hard and Soft Voting)\n",
    "print(\"\\n=== Voting Classifiers ===\")\n",
    "\n",
    "# Hard voting\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=list(base_models.items()),\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Soft voting\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=list(base_models.items()),\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Evaluate voting classifiers\n",
    "voting_scores = {}\n",
    "for name, classifier in [('Hard Voting', voting_hard), ('Soft Voting', voting_soft)]:\n",
    "    scores = cross_val_score(classifier, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "    voting_scores[name] = scores.mean()\n",
    "    print(f\"{name:20s}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# 2. Model Stacking\n",
    "print(\"\\n=== Model Stacking ===\")\n",
    "\n",
    "# Generate cross-validation predictions for stacking\n",
    "cv_folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "stacking_features = np.zeros((len(X_train), len(base_models)))\n",
    "\n",
    "for i, (name, model) in enumerate(base_models.items()):\n",
    "    cv_predictions = cross_val_predict(model, X_train, y_train, cv=cv_folds, method='predict_proba')\n",
    "    stacking_features[:, i] = cv_predictions[:, 1]  # Use probability for class 1\n",
    "\n",
    "# Train meta-learner\n",
    "meta_learner = LogisticRegression(random_state=42)\n",
    "meta_learner.fit(stacking_features, y_train)\n",
    "\n",
    "# Evaluate stacking on test set\n",
    "# First, get base model predictions on test set\n",
    "test_stacking_features = np.zeros((len(X_test), len(base_models)))\n",
    "for i, (name, model) in enumerate(base_models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    test_stacking_features[:, i] = test_pred_proba\n",
    "\n",
    "# Meta-learner prediction\n",
    "stacking_pred_proba = meta_learner.predict_proba(test_stacking_features)[:, 1]\n",
    "stacking_score = roc_auc_score(y_test, stacking_pred_proba)\n",
    "print(f\"Stacking Score: {stacking_score:.4f}\")\n",
    "\n",
    "# 3. Bagging with different base estimators\n",
    "print(\"\\n=== Bagging Variants ===\")\n",
    "\n",
    "bagging_variants = {\n",
    "    'Bagging + Decision Tree': BaggingClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "        n_estimators=100, random_state=42\n",
    "    ),\n",
    "    'Bagging + SVM': BaggingClassifier(\n",
    "        base_estimator=SVC(probability=True, random_state=42),\n",
    "        n_estimators=10, random_state=42  # Fewer estimators due to SVM cost\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, model in bagging_variants.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "    print(f\"{name:30s}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# 4. AdaBoost\n",
    "ada_boost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "    n_estimators=100, random_state=42\n",
    ")\n",
    "ada_scores = cross_val_score(ada_boost, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "print(f\"AdaBoost:                     {ada_scores.mean():.4f} ± {ada_scores.std():.4f}\")\n",
    "\n",
    "# Ensemble comparison visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Compile all results\n",
    "all_results = {**individual_scores, **voting_scores, 'Stacking': stacking_score}\n",
    "\n",
    "# Add bagging and AdaBoost results\n",
    "for name, model in bagging_variants.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "    all_results[name] = scores.mean()\n",
    "\n",
    "all_results['AdaBoost'] = ada_scores.mean()\n",
    "\n",
    "# Plot comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "methods = list(all_results.keys())\n",
    "scores = list(all_results.values())\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(methods)))\n",
    "\n",
    "bars = plt.bar(range(len(methods)), scores, color=colors)\n",
    "plt.xticks(range(len(methods)), methods, rotation=45, ha='right')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.title('Ensemble Methods Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Learning curves for best ensemble\n",
    "best_ensemble = voting_soft  # or choose the best performing one\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_ensemble, X_train, y_train, cv=3,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "    scoring='roc_auc', n_jobs=-1\n",
    ")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='Training score')\n",
    "plt.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.3)\n",
    "plt.plot(train_sizes, val_scores.mean(axis=1), 'o-', label='Validation score')\n",
    "plt.fill_between(train_sizes, val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.3)\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.title('Ensemble Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance from stacking meta-learner\n",
    "plt.subplot(2, 2, 3)\n",
    "meta_coefs = np.abs(meta_learner.coef_[0])\n",
    "model_names = list(base_models.keys())\n",
    "plt.bar(model_names, meta_coefs)\n",
    "plt.ylabel('Meta-learner Coefficient (abs)')\n",
    "plt.title('Model Importance in Stacking')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Prediction correlation between models\n",
    "plt.subplot(2, 2, 4)\n",
    "correlation_matrix = np.corrcoef(stacking_features.T)\n",
    "im = plt.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "plt.colorbar(im)\n",
    "plt.xticks(range(len(model_names)), model_names)\n",
    "plt.yticks(range(len(model_names)), model_names)\n",
    "plt.title('Model Prediction Correlation')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(model_names)):\n",
    "        plt.text(j, i, f'{correlation_matrix[i, j]:.2f}',\n",
    "                ha='center', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest individual model: {max(individual_scores, key=individual_scores.get)} \"\n",
    "      f\"({max(individual_scores.values()):.4f})\")\n",
    "print(f\"Best ensemble method: {max(all_results, key=all_results.get)} \"\n",
    "      f\"({max(all_results.values()):.4f})\")\n",
    "print(f\"Improvement: {max(all_results.values()) - max(individual_scores.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment Preparation and Production Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"=== Model Deployment Preparation ===\")\n",
    "\n",
    "# 1. Create a complete pipeline for production\n",
    "class ProductionPipeline(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Complete pipeline for production deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessor, feature_selector, model):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.feature_selector = feature_selector\n",
    "        self.model = model\n",
    "        self.feature_names_ = None\n",
    "        self.training_date_ = None\n",
    "        self.version_ = \"1.0\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the complete pipeline\"\"\"\n",
    "        # Store training metadata\n",
    "        self.training_date_ = datetime.now()\n",
    "        self.feature_names_ = list(X.columns)\n",
    "        \n",
    "        # Fit preprocessing\n",
    "        X_preprocessed = self.preprocessor.fit_transform(X)\n",
    "        \n",
    "        # Fit feature selection\n",
    "        X_selected = self.feature_selector.fit_transform(X_preprocessed, y)\n",
    "        \n",
    "        # Fit model\n",
    "        self.model.fit(X_selected, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        X_processed = self._transform(X)\n",
    "        return self.model.predict(X_processed)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        X_processed = self._transform(X)\n",
    "        return self.model.predict_proba(X_processed)\n",
    "    \n",
    "    def _transform(self, X):\n",
    "        \"\"\"Internal transformation method\"\"\"\n",
    "        # Validate input features\n",
    "        if hasattr(X, 'columns'):\n",
    "            missing_features = set(self.feature_names_) - set(X.columns)\n",
    "            if missing_features:\n",
    "                raise ValueError(f\"Missing features: {missing_features}\")\n",
    "            \n",
    "            # Reorder columns to match training\n",
    "            X = X[self.feature_names_]\n",
    "        \n",
    "        # Apply transformations\n",
    "        X_preprocessed = self.preprocessor.transform(X)\n",
    "        X_selected = self.feature_selector.transform(X_preprocessed)\n",
    "        \n",
    "        return X_selected\n",
    "    \n",
    "    def get_metadata(self):\n",
    "        \"\"\"Get model metadata\"\"\"\n",
    "        return {\n",
    "            'version': self.version_,\n",
    "            'training_date': self.training_date_.isoformat() if self.training_date_ else None,\n",
    "            'feature_names': self.feature_names_,\n",
    "            'n_features': len(self.feature_names_) if self.feature_names_ else None,\n",
    "            'model_type': type(self.model).__name__,\n",
    "            'preprocessor_steps': list(self.preprocessor.named_transformers_.keys())\n",
    "        }\n",
    "\n",
    "# Create production pipeline\n",
    "production_pipeline = ProductionPipeline(\n",
    "    preprocessor=preprocessor,\n",
    "    feature_selector=selector_model,  # Use the best feature selector\n",
    "    model=best_rf\n",
    ")\n",
    "\n",
    "# Train the production pipeline\n",
    "production_pipeline.fit(X.iloc[:800], y.iloc[:800])  # Use subset for training\n",
    "\n",
    "print(\"Production pipeline trained successfully\")\n",
    "print(f\"Metadata: {production_pipeline.get_metadata()}\")\n",
    "\n",
    "# 2. Model validation and testing\n",
    "print(\"\\n=== Model Validation ===\")\n",
    "\n",
    "# Test on holdout data\n",
    "X_holdout = X.iloc[800:]\n",
    "y_holdout = y.iloc[800:]\n",
    "\n",
    "# Performance on holdout\n",
    "holdout_pred = production_pipeline.predict(X_holdout)\n",
    "holdout_pred_proba = production_pipeline.predict_proba(X_holdout)[:, 1]\n",
    "holdout_score = roc_auc_score(y_holdout, holdout_pred_proba)\n",
    "\n",
    "print(f\"Holdout AUC Score: {holdout_score:.4f}\")\n",
    "print(f\"Holdout Accuracy: {metrics.accuracy_score(y_holdout, holdout_pred):.4f}\")\n",
    "\n",
    "# Prediction latency test\n",
    "print(\"\\n=== Performance Testing ===\")\n",
    "\n",
    "# Single prediction latency\n",
    "single_sample = X_holdout.iloc[:1]\n",
    "start_time = time.time()\n",
    "single_pred = production_pipeline.predict_proba(single_sample)\n",
    "single_latency = time.time() - start_time\n",
    "\n",
    "print(f\"Single prediction latency: {single_latency*1000:.2f} ms\")\n",
    "\n",
    "# Batch prediction latency\n",
    "batch_sample = X_holdout.iloc[:100]\n",
    "start_time = time.time()\n",
    "batch_pred = production_pipeline.predict_proba(batch_sample)\n",
    "batch_latency = time.time() - start_time\n",
    "\n",
    "print(f\"Batch prediction latency (100 samples): {batch_latency*1000:.2f} ms\")\n",
    "print(f\"Average per sample: {batch_latency/100*1000:.2f} ms\")\n",
    "\n",
    "# 3. Model serialization\n",
    "print(\"\\n=== Model Serialization ===\")\n",
    "\n",
    "# Save with joblib (recommended for scikit-learn)\n",
    "model_filename = 'production_model.joblib'\n",
    "joblib.dump(production_pipeline, model_filename)\n",
    "model_size = os.path.getsize(model_filename) / (1024 * 1024)  # MB\n",
    "print(f\"Model saved as {model_filename} ({model_size:.2f} MB)\")\n",
    "\n",
    "# Save metadata separately\n",
    "metadata = production_pipeline.get_metadata()\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved as model_metadata.json\")\n",
    "\n",
    "# Load and test\n",
    "loaded_pipeline = joblib.load(model_filename)\n",
    "test_pred = loaded_pipeline.predict_proba(single_sample)\n",
    "print(f\"Model loaded successfully. Test prediction: {test_pred[0]}\")\n",
    "\n",
    "# 4. Model monitoring setup\n",
    "print(\"\\n=== Model Monitoring Setup ===\")\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"\n",
    "    Basic model monitoring class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data, reference_predictions):\n",
    "        self.reference_data = reference_data\n",
    "        self.reference_predictions = reference_predictions\n",
    "        self.reference_stats = self._calculate_stats(reference_data)\n",
    "    \n",
    "    def _calculate_stats(self, data):\n",
    "        \"\"\"Calculate basic statistics\"\"\"\n",
    "        return {\n",
    "            'mean': np.mean(data, axis=0),\n",
    "            'std': np.std(data, axis=0),\n",
    "            'min': np.min(data, axis=0),\n",
    "            'max': np.max(data, axis=0)\n",
    "        }\n",
    "    \n",
    "    def detect_drift(self, new_data, threshold=2.0):\n",
    "        \"\"\"Simple drift detection based on statistical distance\"\"\"\n",
    "        new_stats = self._calculate_stats(new_data)\n",
    "        \n",
    "        # Calculate z-score for means\n",
    "        z_scores = np.abs((new_stats['mean'] - self.reference_stats['mean']) / \n",
    "                         (self.reference_stats['std'] + 1e-8))\n",
    "        \n",
    "        drift_detected = np.any(z_scores > threshold)\n",
    "        \n",
    "        return {\n",
    "            'drift_detected': drift_detected,\n",
    "            'max_z_score': np.max(z_scores),\n",
    "            'affected_features': np.where(z_scores > threshold)[0].tolist()\n",
    "        }\n",
    "    \n",
    "    def prediction_drift(self, new_predictions, threshold=0.1):\n",
    "        \"\"\"Detect drift in prediction distribution\"\"\"\n",
    "        ref_mean = np.mean(self.reference_predictions)\n",
    "        new_mean = np.mean(new_predictions)\n",
    "        \n",
    "        drift = abs(new_mean - ref_mean) > threshold\n",
    "        \n",
    "        return {\n",
    "            'prediction_drift': drift,\n",
    "            'reference_mean': ref_mean,\n",
    "            'new_mean': new_mean,\n",
    "            'difference': abs(new_mean - ref_mean)\n",
    "        }\n",
    "\n",
    "# Set up monitoring with training data\n",
    "monitor = ModelMonitor(\n",
    "    reference_data=X_final[:800],\n",
    "    reference_predictions=production_pipeline.predict_proba(X.iloc[:800])[:, 1]\n",
    ")\n",
    "\n",
    "# Test drift detection with holdout data\n",
    "holdout_processed = production_pipeline._transform(X_holdout)\n",
    "drift_result = monitor.detect_drift(holdout_processed)\n",
    "pred_drift_result = monitor.prediction_drift(holdout_pred_proba)\n",
    "\n",
    "print(f\"Data drift detected: {drift_result['drift_detected']}\")\n",
    "print(f\"Max Z-score: {drift_result['max_z_score']:.3f}\")\n",
    "print(f\"Prediction drift detected: {pred_drift_result['prediction_drift']}\")\n",
    "print(f\"Prediction difference: {pred_drift_result['difference']:.3f}\")\n",
    "\n",
    "# 5. Production checklist\n",
    "print(\"\\n=== Production Deployment Checklist ===\")\n",
    "checklist = [\n",
    "    \"✓ Model pipeline created and tested\",\n",
    "    \"✓ Performance validated on holdout data\",\n",
    "    \"✓ Latency benchmarked\",\n",
    "    \"✓ Model serialized and can be loaded\",\n",
    "    \"✓ Metadata saved for tracking\",\n",
    "    \"✓ Monitoring system implemented\",\n",
    "    \"TODO: Set up A/B testing framework\",\n",
    "    \"TODO: Implement logging for predictions\",\n",
    "    \"TODO: Set up automated retraining\",\n",
    "    \"TODO: Configure alerting for drift detection\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(item)\n",
    "\n",
    "# Cleanup\n",
    "import os\n",
    "for file in [model_filename, 'model_metadata.json']:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"Cleaned up {file}\")\n",
    "\n",
    "print(\"\\n=== Production Considerations Summary ===\")\n",
    "print(\"1. Always validate on truly unseen data\")\n",
    "print(\"2. Monitor for data and prediction drift\")\n",
    "print(\"3. Log predictions for continuous learning\")\n",
    "print(\"4. Implement proper error handling\")\n",
    "print(\"5. Set up A/B testing for model updates\")\n",
    "print(\"6. Plan for model retraining schedule\")\n",
    "print(\"7. Document model assumptions and limitations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}